{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The folder structure can be seen here - \\n    README.md                            \\n    [task-name-folder]/                                # natural_language_inference, paraphrase_generation, question_answering, relation_extraction, topic_models\\n        ├── [article-counter-folder]/                  # ranges between 0 to 100 since we annotated varying numbers of articles per task\\n        │   ├── [articlename].pdf                      # scholarly article pdf\\n        │   ├── [articlename]-Grobid-out.txt           # plaintext output from the [Grobid parser](https://github.com/kermitt2/grobid)\\n        │   ├── [articlename]-Stanza-out.txt           # plaintext preprocessed output from [Stanza](https://github.com/stanfordnlp/stanza)\\n        │   ├── sentences.txt                          # annotated Contribution sentences in the file\\n        │   ├── entities.txt                           # annotated entities in the Contribution sentences\\n        │   └── info-units/                            # the folder containing information units in JSON format\\n        │   │   └── research-problem.json              # `research problem` mandatory information unit in json format\\n        │   │   └── model.json                         # `model` information unit in json format; in some articles it is called `approach`\\n        │   │   └── ...                                # there are 12 information units in all and each article may be annotated by 3 or 6\\n        │   └── triples/                               # the folder containing information unit triples one per line\\n        │   │   └── research-problem.txt               # `research problem` triples (one research problem statement per line)\\n        │   │   └── model.txt                          # `model` triples (one statement per line)\\n        │   │   └── ...                                # there are 12 information units in all and each article may be annotated by 3 or 6\\n        │   └── ...                                    # there are between 1 to 100 articles annotated for each task, so this repeats for the remaining annotated articles\\n        └── ...                                        # there are 24 tasks selected overall, so this repeats 23 more times'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The folder structure can be seen here - \n",
    "    README.md                            \n",
    "    [task-name-folder]/                                # natural_language_inference, paraphrase_generation, question_answering, relation_extraction, topic_models\n",
    "        ├── [article-counter-folder]/                  # ranges between 0 to 100 since we annotated varying numbers of articles per task\n",
    "        │   ├── [articlename].pdf                      # scholarly article pdf\n",
    "        │   ├── [articlename]-Grobid-out.txt           # plaintext output from the [Grobid parser](https://github.com/kermitt2/grobid)\n",
    "        │   ├── [articlename]-Stanza-out.txt           # plaintext preprocessed output from [Stanza](https://github.com/stanfordnlp/stanza)\n",
    "        │   ├── sentences.txt                          # annotated Contribution sentences in the file\n",
    "        │   ├── entities.txt                           # annotated entities in the Contribution sentences\n",
    "        │   └── info-units/                            # the folder containing information units in JSON format\n",
    "        │   │   └── research-problem.json              # `research problem` mandatory information unit in json format\n",
    "        │   │   └── model.json                         # `model` information unit in json format; in some articles it is called `approach`\n",
    "        │   │   └── ...                                # there are 12 information units in all and each article may be annotated by 3 or 6\n",
    "        │   └── triples/                               # the folder containing information unit triples one per line\n",
    "        │   │   └── research-problem.txt               # `research problem` triples (one research problem statement per line)\n",
    "        │   │   └── model.txt                          # `model` triples (one statement per line)\n",
    "        │   │   └── ...                                # there are 12 information units in all and each article may be annotated by 3 or 6\n",
    "        │   └── ...                                    # there are between 1 to 100 articles annotated for each task, so this repeats for the remaining annotated articles\n",
    "        └── ...                                        # there are 24 tasks selected overall, so this repeats 23 more times\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 545.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 450.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 510.01it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 689.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 512.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 459.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 508.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 375.67it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 544.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 266.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 387.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 308.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 249.22it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 201.84it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 440.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 373.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 287.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 275.23it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 323.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 432.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 363.65it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 371.91it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 334.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 244.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# walk through all directories\n",
    "list_of_directories = []\n",
    "for (_, dirnames, _) in os.walk('.'):\n",
    "    list_of_directories.extend(dirnames)\n",
    "    break\n",
    "\n",
    "# append all results in the dataframe\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# loop through all sentences\n",
    "for k in (list_of_directories):\n",
    "    sub_folders=glob.glob(\"./\"+k+\"/*\")\n",
    "    for i in tqdm.tqdm(sub_folders):\n",
    "        try:\n",
    "            path_for_sentences = glob.glob(i+\"/*-Stanza-out.txt\")[0]\n",
    "            path_for_labels = glob.glob(i+\"/sentences.txt\")[0]\n",
    "            f = open(path_for_sentences, \"r\")\n",
    "            lines = [line.rstrip() for line in f]\n",
    "            h = open(path_for_labels, \"r\")\n",
    "            labels = [int(line.rstrip())-1 for line in h]\n",
    "            d = {'contents': lines, 'label': [0]*len(lines)}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.loc[labels,'label'] = 1\n",
    "            results = pd.concat([results, df], axis=0).reset_index(drop=True)\n",
    "        except:\n",
    "            # for debugging\n",
    "            print(path_for_labels)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "909486705bbbcd2c5c0355f1bcb3a97e3439526a8a28230af4920aa299409bdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
