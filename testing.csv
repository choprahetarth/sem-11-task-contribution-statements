,contents,label
0,title,0
1,Incorporating Glosses into Neural Word Sense Disambiguation,1
2,abstract,0
3,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,1
4,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,1
5,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",0
6,"In this paper , we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge .",0
7,"Therefore , we propose GAS : a gloss - augmented WSD neural network which jointly encodes the context and glosses of the target word .",0
8,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",0
9,We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information .,0
10,The experimental results show that our model outperforms the state - of - theart systems on several English all - words WSD datasets .,0
11,Introduction,0
12,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,0
13,There are several lines of research on WSD .,0
14,Knowledge - based methods focus on exploiting lexical resources to infer the senses of word in the context .,0
15,Supervised methods usually train multiple classifiers with manual designed features .,0
16,"Although supervised methods can achieve the state - of - the - art performance , there are still two major challenges .",0
17,"Firstly , supervised methods usually train a dedicated classifier for each word individually ( often called word expert ) .",0
18,So it can not easily scale up to all - words WSD task which requires to disambiguate all the polysemous word in texts,0
19,1 .,0
20,"Recent neural - based methods solve this problem by building a unified model for all the polysemous words , but they still ca n't beat the best word expert system .",0
21,"Secondly , all the neural - based methods always only consider the local context of the target word , ignoring the lexical resources like which are widely used in the knowledge - based methods .",0
22,"The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .",0
23,"Recent studies have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of To this end , our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words .",0
24,We further consider extending the original gloss through its semantic relations in our framework .,0
25,"As shown in , the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation .",0
26,"Therefore , we integrate not only the original gloss but also the related glosses of hypernyms and hyponyms into the neural network .",0
27,"The hypernym ( green node ) and hyponyms ( blue nodes ) for the 2nd sense bed 2 of bed , which means a plot of ground in which plants are growing , rather than the bed for sleeping in .",0
28,"The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc .",0
29,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",1
30,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,1
31,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",1
32,The main contributions of this paper are listed as follows :,0
33,"To the best of our knowledge , our model is the first to incorporate the glosses into an end - to - end neural WSD model .",0
34,"In this way , our model can benefit from not only massive labeled data but also rich lexical knowledge .",0
35,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",0
36,We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context .,0
37,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,0
38,The experimental results on several English all - words WSD benchmark datasets show that our model outperforms the state - of - theart systems .,0
39,Related Work,0
40,"Knowledge - based , supervised and neural - based methods have already been applied to WSD task ) .",0
41,Knowledge - based WSD methods mainly exploit two kinds of knowledge to disambiguate polysemous words :,0
42,"1 ) The gloss , which defines a word sense meaning , is mainly used in Lesk algorithm and its variants .",0
43,"2 ) The structure of the semantic network , whose nodes are synsets 2 and edges are semantic relations , is mainly used in graph - based algorithms .",0
44,Supervised methods usually involve each target word as a separate classification problem ( often called word expert ) and train classifiers based on manual designed features .,0
45,"Although word expert supervised WSD methods perform best in terms of accuray , they are less flexible than knowledge - based methods in the allwords WSD task .",0
46,"To deal with this problem , recent neural - based methods aim to build a unified classifier which shares parameters among all the polysemous words .",0
47,leverages the bidirectional long short - term memory network which shares model parameters among all the polysemous words .,0
48,transfers the WSD problem into a neural sequence labeling task .,0
49,"However , none of the neural - based methods can totally beat the best word expert supervised methods on English all - words WSD datasets .",0
50,"What 's more , all of the previous supervised methods and neural - based methods rarely take the lexical resources like WordNet into consideration .",0
51,Recent studies on sense embeddings have proved that lexical resources are helpful .,0
52,trains word sense embeddings through learning sentence level embeddings from glosses using a convolutional neural networks .,0
53,extends word embeddings to sense embeddings by using the constraints and semantic relations in Word Net .,0
54,They achieve an improvement of more than 1 % in WSD performance when using sense embeddings as WSD features for SVM classifier .,0
55,This work shows that integrating structural information of lexical resources can help to word expert supervised methods .,0
56,"However , sense embeddings can only indirectly help to WSD ( as SVM classifier features ) .",0
57,shows that the coarse - grained semantic labels in WordNet can help to WSD in a multi - task learning framework .,0
58,"As far as we know , there is no study directly integrates glosses or semantic relations of the Word - Net into an end - to - end model .",0
59,"In this paper , we focus on how to integrate glosses into a unified neural WSD system .",0
60,Memory network is initially proposed to solve question answering problems .,0
61,"Recent researches show that memory network obtains the state - of - the - art results in many NLP tasks such as sentiment classification and analysis , poetry generation , spoken language understanding , etc .",0
62,"Inspired by the success of memory network used in many NLP tasks , we introduce it into WSD .",0
63,We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses .,0
64,Incorporating Glosses into Neural Word Sense Disambiguation,1
65,"In this section , we first give an overview of the proposed model GAS : a gloss - augmented WSD neural network which integrates the context and the glosses of the target word into a unified framework .",0
66,"After that , each individual module is described in detail .",0
67,Architecture of GAS,0
68,The overall architecture of the proposed model is shown in .,0
69,It consists of four modules :,0
70,Context Module :,0
71,The context module encodes the local context ( a sequence of surrounding words ) of the target word into a distributed vector representation .,0
72,Gloss Module :,0
73,"Like the context module , the gloss module encodes all the glosses of the target word into a separate vector representations of the same size .",0
74,"In other words , we can get |s t | word sense representations according to |s t | 3 senses of the target word , where |s t | is the sense number of the target word wt .",0
75,Memory Module :,0
76,The memory module is employed to model the semantic relationship between the context embedding and gloss embedding produced by context module and gloss module respectively .,0
77,Scoring Module :,0
78,"In order to benefit from both labeled contexts and gloss knowledge , the scoring module takes the context embedding from context module and the last step result from the memory module as input .",0
79,Finally it generates a probability distribution overall the possible senses of the target word .,0
80,Detailed architecture of the proposed model is shown in .,0
81,The next four sections will show detailed configurations in each module .,0
82,Context Module,0
83,"Context module encodes the context of the target word into a vector representation , which is also called context embedding in this paper .",0
84,We leverage the bidirectional long short - term memory network ( Bi - LSTM ) for taking both the preceding and following words of the target word into consideration .,0
85,"The input of this module [ x 1 , . . . , x t?1 , x t+1 , . . . , x Tx ] is a sequence of words surrounding the target word x t , where T x is the length of the context .",0
86,After applying a lookup operation over the pre-trained word embedding matrix M ?,0
87,"R DV , we transfer a one hot vector xi into a D-dimensional vector .",0
88,"Then , the forward LSTM reads the segment ( x 1 , . . . , x t?1 ) on the left of the target word x t and calculates a sequence of forward hidden states",0
89,"The backward LSTM reads the segment ( x Tx , . . . , x t+1 ) on the right of the target word x t and calculates a sequence of backward hidden states ( ? ? h Tx , . . . , ? ? h t+1 ) .",0
90,The context vector c is finally concatenated as,0
91,Scoring Module,0
92,"The scoring module calculates the scores for all the related senses {s 1 t , s 2 t , . . . , s pt } corresponding to the target word x t and finally outputs a sense probability distribution overall senses .",0
93,The overall score for each word sense is determined by gloss attention ?,0
94,"TM i from the last pass in the memory module , where TM is the number of passes in the memory module .",0
95,The e TM ( ? TM without Softmax ) is regarded as the gloss score .,0
96,"Meanwhile , a fully - connected layer is employed to calculate the context score .",0
97,where W xt ?,0
98,"R | st|2 n , b xt ? R | st | , |s t | is the number of senses for the target word x t and n is the number of hidden units in the LSTM .",0
99,"It 's noteworthy that in Equation 11 , each ambiguous word x t has its corresponding weight matrix W xt and bias b xt in the scoring module .",0
100,"In order to balance the importance of background knowledge and labeled data , we introduce a parameter ? ?",0
101,RN 7 in the scoring module which is jointly learned during the training process .,0
102,The probability distribution ?,0
103,overall the word senses of the target word is calculated as :,0
104,where ?,0
105,"xt is the parameter for word x t , and ? xt ?",0
106,"[ 0 , 1 ] .",0
107,"During training , all model parameters are jointly learned by minimizing a standard crossentropy loss between ?",0
108,and the true label y.,0
109,Gloss Module,0
110,"The gloss module encodes each gloss of the target word into a fixed size vector like the context vector c , which is also called gloss embedding .",0
111,We further enrich the gloss information by taking semantic relations and their associated glosses into consideration .,0
112,This module contains a gloss reader layer and a relation fusion layer .,0
113,Gloss reader layer generates a vector representations fora gloss .,0
114,Relation fusion layer aims at modeling the semantic relations of each gloss in the expanded glosses list which consists of related glosses of the original gloss .,0
115,Our model GAS with extended glosses is denoted as GAS ext .,0
116,"GAS only encodes the original gloss , while GAS ext encodes the expanded glosses from hypernymy and hyponymy relations ( details in ) .",0
117,Original Gloss,0
118,"Extended Glosses g i : Detailed architecture of our proposed model , which consists of a context module , a gloss module , a memory module and a scoring module .",0
119,The context module encodes the adjacent words surrounding the target word into a vector c.,0
120,The gloss module encodes the original gloss or extended glosses into a vector g i .,0
121,"In the memory module , we calculate the inner relationship ( as attention ) between context c and each gloss g i and then update the memory as mi at pass i .",0
122,"In the scoring module , we make final predictions based on the last pass attention of memory module and the context vector c. Note that GAS only uses the original gloss , while GAS ext uses the entended glosses through hypernymy and hyponymy relations .",0
123,"In other words , the relation fusion layer ( grey dotted box ) only belongs to GAS ext .",0
124,where : is the concatenation operator .,0
125,Gloss Reader Layer,0
126,Gloss reader layer contains two parts : gloss expansion and gloss encoder .,0
127,Gloss expansion is to enrich the original gloss information through its hypernymy and hyponymy relations in WordNet .,0
128,Gloss encoder is to encode each gloss into a vector representation .,0
129,Gloss Expansion :,0
130,We only expand the glosses of nouns and verbs via their corresponding hypernyms and hyponyms .,0
131,There are two reasons :,0
132,One is that most of polysemous words ( about 80 % ) are nouns and verbs ; the other is that the most frequent relations among word senses for nouns and verbs are the hypernymy and hyponymy relations 4 .,0
133,The original gloss is denoted as g 0 .,0
134,Breadthfirst search method with a limited depth K is employed to extract the related glosses .,0
135,"The glosses of hypernyms within K depth are denoted as [ g ?1 , g ?2 , . . . , g ?L 1 ].",0
136,"The glosses of hyponyms within K depth are denoted as [ g + 1 , g +2 , . . . , g +L",0
137,2 ] 5 . Note that g + 1 and g ? 1 are the glosses of the nearest word sense .,0
138,Gloss Encoder :,0
139,"We denote the j -th 6 gloss in 4 In WordNet , more than 95 % of relations for nouns and 80 % for verbs are hypernymy and hyponymy relations .",0
140,"Since one synset has one or more direct hypernyms and hyponyms , L1 >= K and L2 >= K. 6 Since GAS do n't have gloss expansion , j is always 0 and gi = g i 0 .",0
141,See more in .,0
142,the expanded glosses list for i th sense of the target word as a sequence of G words .,0
143,"Like the context encoder , the gloss encoder also leverages Bi - LSTM units to process the words sequence of the gloss .",0
144,The gloss representation g i j is computed as the concatenation of the last hidden states of the forward and backward LSTM .,0
145,"where j ? [?L 1 , . . . , ? 1 , 0 , + 1 , . . . , + L 2 ] and : is the concatenation operator .",0
146,Relation Fusion Layer,0
147,Relation fusion layer models the hypernymy and hyponymy relations of the target word sense .,0
148,A forward LSTM is employed to encode the hypernyms ' glosses of i th sense,0
149,.,0
150,"In order to highlight the original gloss g i 0 , the enhanced i th sense representation is concatenated as the final state of the forward and backward LSTM .",0
151,Memory Module,0
152,"The memory module has two inputs : the context vector c from the context module and the gloss vectors {g 1 , g 2 , . . . , g |st | } from the gloss module , where |s t | is the number of word senses .",0
153,We model the inner relationship between the context and glosses by attention calculation .,0
154,"Since onepass attention calculation may not fully reflect the relationship between the context and glosses ( details in Section 4.4.2 ) , the memory module adopts a repeated deliberation process .",0
155,"The process repeats reading gloss vectors in the following passes , in order to highlight the correct word sense for the following scoring module by a more accurate attention calculation .",0
156,"After each pass , we update the memory to refine the states of the current pass .",0
157,"Therefore , memory module contains two phases : attention calculation and memory update .",0
158,Attention Calculation :,0
159,"For each pass k , the attention e k i of gloss g i is generally computed as",0
160,where m k ?1 is the memory vector in the ( k ? 1 ) th pass while c is the context vector .,0
161,"The scoring function f calculates the semantic relationship of the gloss and context , taking the vector set ( g i , m k?1 , c ) as input .",0
162,"In the first pass , the attention reflects the similarity of context and each gloss .",0
163,"In the next pass , the attention reflects the similarity of adapted memory and each gloss .",0
164,A dot product is applied to calculate the similarity of each gloss vector and context ( or memory ) vector .,0
165,We treat c as m 0 .,0
166,"So , the attention ?",0
167,k i of gloss g i at pass k is computed as a dot product of g i and m k?1 :,0
168,Memory Update :,0
169,"After calculating the attention , we store the memory state in u k which is a weighted sum of gloss vectors and is computed as",0
170,where n is the hidden size of LSTM in the context module and gloss module .,0
171,"And then , we update the memory vector m k from last pass memory m k?1 , context vector c , and memory state u k .",0
172,We propose two memory update methods :,0
173,Linear : we update the memory vector m k by a linear transformation from m k?1,0
174,where H ?,0
175,R 2 n2 n .,0
176,Concatenation : we get anew memory for kth pass by taking both the gloss embedding and context embedding into consideration,0
177,"where : is the concatenation operator , W ?",0
178,R n6 n and b ?,0
179,R 2 n .,0
180,Experiments and Evaluation,0
181,Dataset,0
182,Evaluation Dataset : we evaluate our model on several English all - words WSD datasets .,0
183,"For fair comparison , we use the benchmark datasets proposed by which includes five standard all - words fine - grained WSD datasets from the Senseval and SemEval competitions .",0
184,"They are Senseval - 2 ( SE2 ) , Senseval - 3 task 1 ( SE3 ) , SemEval - 07 task 17 ( SE7 ) , SemEval - 13 task 12 ( SE13 ) , and SemEval - 15 task 13 ( SE15 ) .",0
185,"Following by , we choose SE7 , the smallest test set as the development ( validation ) set , which consists of 455 labeled instances .",0
186,"The last four test sets consist of 6798 labeled instances with four types of target words , namely nouns , verbs , adverbs and adjectives .",0
187,We extract word sense glosses from WordNet3.0 because maps all the sense annotations 8 from its original version to 3.0 .,0
188,Training Dataset :,0
189,"We choose SemCor 3.0 as the training set , which was also used by , , , , etc .",0
190,"It consists of 226,036 sense annotations from 352 documents , which is the largest manually annotated corpus for WSD .",0
191,Note that all the systems listed in are trained on SemCor 3.0 .,0
192,Implementation Details,0
193,"We use the validation set ( SE7 ) to find the optimal settings of our framework : the hidden state size n , the number of passes | T M | , the optimizer , etc .",0
194,"We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process .",1
195,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",1
196,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",1
197,We assign gloss expansion depth K the value of 4 .,1
198,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",1
199,We use Adam optimizer in the training process with 0.001 initial learning rate .,1
200,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",1
201,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,1
202,Systems to be Compared,0
203,"In this section , we describe several knowledgebased methods , supervised methods and neuralbased methods which perform well on the English all - words WSD datasets for comparison .",0
204,by using a word similarity function defined on a distributional semantic space to calculate the gloss - context overlap .,0
205,This work shows that glosses are important to WSD and enriching : F1-score ( % ) for fine - grained English all - words WSD on the test sets .,0
206,Bold font indicates best systems .,0
207,The * represents the neural network models using external knowledge .,0
208,"The fives blocks list the MFS baseline , two knowledge - based systems , two supervised systems ( feature - based ) , three neuralbased systems and our models , respectively .",0
209,Knowledge - based Systems,1
210,. gloss information via its semantic relations can help to WSD .,0
211,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,1
212,Supervised Systems,1
213,The supervised systems mentioned in this paper refers to traditional feature - based systems which train a dedicated classifier for every word individually ( word expert ) .,0
214,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .",1
215,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,1
216,Neural - based Systems,1
217,Neural - based systems aim to build an end - to - end unified neural network for all the polysemous words in texts .,0
218,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,1
219,Note that this model is equivalent to our model if we remove the gloss module and memory module of GAS .,0
220,"Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS : transfers WSD into a sequence learning task and propose a multi - task learning framework for WSD , POS tagging and coarse - grained semantic labels ( LEX ) .",1
221,"These two models have used the external knowledge , for the LEX is based on lexicographer files in WordNet .",0
222,"Moreover , we introduce MFS baseline , which simply selects the most frequent sense in the training data set .",0
223,Results and Discussion,0
224,English all - words results,1
225,"In this section , we show the performance of our proposed model in the English all - words task .",0
226,Table 1 shows the F1 - score results on the four test sets mentioned in Section 4.1 .,0
227,The systems in the first four blocks are implemented by except for the single Bi - LSTM model .,0
228,The last block lists the performance of our proposed model GAS and its variant GAS ext which extends the gloss module in GAS .,0
229,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,1
230,"Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",1
231,Compared with other three neural - based methods in the Context :,0
232,He plays a pianist in the film Glosses Pass 1 Pass 2 Pass 3 Pass 4 Pass 5 g 1 : participate in games or sport g 2 : perform music on a instrument g 3 : act a role or part : F1-score ( % ) of different passes from 1 to 5 on the test data sets .,0
233,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,1
234,". fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",0
235,"The IMS +emb , which trains a dedicated classifier for each word individually ( word expert ) with massive manual designed features including word embeddings , is hard to beat for neural networks models .",0
236,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .",0
237,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,0
238,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",0
239,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",0
240,This proves that incorporating extended glosses through its hypernyms and hyponyms into the neural network models can boost the performance for WSD .,0
241,Multiple Passes Analysis,1
242,"To better illustrate the influence of multiple passes , we give an example in .",0
243,"Consider the situation that we meet an unknown word x 11 , we look 11 x refers to wordplay in reality .",0
244,up from the dictionary and find three word senses and their glosses corresponding to x .,0
245,We try to figure out the correct meaning of x according to its context and glosses of different word senses by the proposed memory module .,0
246,"In the first pass , the first sense is excluded , for there are no relevance between the context and g 1 .",0
247,"But the g 2 and g 3 may need repeated deliberation , for word pianist is similar to the word music and role in the two glosses .",0
248,"By re-reading the context and gloss information of the target word in the following passes , the correct word sense g 3 attracts much more attention than the other two senses .",0
249,Such rereading process can be realized by multi-pass operation in the memory module .,0
250,"Furthermore , shows the effectiveness of multi-pass operation in the memory module .",0
251,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .",1
252,"The reason of this phenomenon is that for most target words , one main word sense accounts for the majority of their appearances .",0
253,"Therefore , inmost circumstances , one - pass inference can lead to the correct word senses .",0
254,Case studies in show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for wordplay .,0
255,"In Table 3 , with the increasing number of passes , the F1 - score increases .",1
256,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .",1
257,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,0
258,Conclusions and Future Work,0
259,"In this paper , we seek to address the problem of integrating the glosses knowledge of the ambiguous word into a neural network for WSD .",0
260,We further extend the gloss information through its semantic relations in WordNet to better infer the context .,0
261,"In this way , we not only make use of labeled context data but also exploit the background knowledge to disambiguate the word sense .",0
262,Results on four English all - words WSD data sets show that our best model outperforms the existing methods .,0
263,There is still one challenge left for the future .,0
264,"We just extract the gloss , missing the structural properties or graph information of lexical resources .",0
265,"In the next step , we will consider integrating the rich structural information into the neural network for Word Sense Disambiguation .",0
266,title,0
267,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,1
268,abstract,0
269,"Due to recent technical and scientific advances , we have a wealth of information hidden in unstructured text data such as offline / online narratives , research articles , and clinical reports .",0
270,"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .",1
271,"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .",1
272,This paper attempts to address the problem of oneclassifier - per - one - word WSD algorithms by proposing a single Bidirectional Long Short - Term Memory ( BLSTM ) network which by considering senses and context sequences works on all ambiguous words collectively .,0
273,"Evaluated on SensEval - 3 benchmark , we show the result of our model is comparable with top - performing WSD algorithms .",0
274,We also discuss how applying additional modifications alleviates the model fault and the need for more training data .,0
275,Introduction,0
276,"Word Sense Disambiguation ( WSD ) is an important problem in Natural Language Processing ( NLP ) , both in its own right and as a steppingstone to other advanced tasks in the NLP pipeline , applications such as machine translation and question answering .",0
277,"WSD specifically deals with identifying the correct sense of a word , among a set of given candidate senses for that word , when it is presented in a brief narrative ( surrounding text ) which is generally referred to as context .",0
278,Consider the ambiguous word ' cold '.,0
279,"In the sentence "" He started to give me a cold shoulder after that experiment "" , the possible senses for cold can be cold temperature ( S1 ) , a cold sensation ( S2 ) , common cold ( S3 ) , or a negative emotional reaction ( S4 ) .",0
280,"Therefore , the ambiguous word cold is specified along with the sense set { S1 , S2 , S3 , S4 } and our goal is to identify the correct sense S4 ( as the closest meaning ) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context .",0
281,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .",1
282,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .",1
283,"By evaluating our onemodel - fits - all WSD network over the public gold standard dataset of SensEval - 3 , we demonstrate that the accuracy of our model in terms of F- measure is comparable with the state - of - the - art WSD algorithms '.",0
284,We outline the organization of the rest of the paper as follows .,0
285,"In Section 2 , we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings .",0
286,Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3 .,0
287,We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable .,0
288,"Finally , in Section 5 , we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine .",0
289,Background and Related Work,0
290,"Generally , there are three categories of WSD algorithms : supervised , knowledgebased , and unsupervised .",0
291,Supervised algorithms consist of automatically inducing classification models or rules from labeled examples .,0
292,Knowledge - based WSD approaches are dependent on manually created lexical resources such as WordNet and the Unified Medical Language System 4 ( UMLS ) .,0
293,Unsupervised algorithms may employ topic modeling - based methods to disambiguate when the senses are known ahead of time .,0
294,For a thorough survey of WSD algorithms refer to Navigli .,0
295,Neural Embeddings for WSD,0
296,"In the past few years , there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks .",0
297,"Word embeddings are typically represented as a dense real - valued low dimensional matrix W ( i.e. a lookup table ) of size d v , where dis the predefined embedding dimension and v is the vocabulary size .",0
298,Each column of the matrix is an embedding vector associated with a word in the vocabulary and each row of the matrix represents a latent feature .,0
299,These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model .,0
300,GloVe is one of the existing unsupervised learning algorithms for obtaining these vector representations of the words in which training is performed on aggregated global word - word co-occurrence statistics from a corpus .,0
301,"Besides word embeddings , recently , computation of sense embeddings has gained the attention of numerous studies as well .",0
302,"For example ,",0
303,Chen et al .,0
304,adapted neural word embeddings to compute different sense embeddings ( of the same word ) and showed competitive performance on the SemEval - 2007 data .,0
305,Bidirectional LSTM,0
306,"Long Short - Term Memory ( LSTM ) , introduced by Hochreiter and Schmidhuber ( 1997 ) , is a gated recurrent neural network ( RNN ) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs .",0
307,"Unlike feedforward neural networks , RNNs have cyclic connections making them powerful for modeling sequences .",0
308,A Bidirectional LSTM is made up of two reversed unidirectional LSTMs .,0
309,"For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word , which is necessary to correctly classify its sense .",0
310,One Single BLSTM network for WSD,0
311,"Given a document and the position of a target word , our model computes a probability distribution over possible senses related to that word .",0
312,"The architecture of our model , depicted in , consist of 6 layers which area sigmoid layer ( at the top ) , a fully - connected layer , a concatenation layer , a BLSTM layer , a cosine layer , and a sense and word embeddings layer ( on the bottom ) .",0
313,"In contrast to other supervised neural WSD networks in which generally a softmax layer - with across entropy or hinge loss - is parameterized by the context words and selects the corresponding weight matrix and bias vector for each ambiguous word 's senses , our network shares parameters overall words ' senses .",0
314,"While remaining computationally efficient , this structure aims to encode statistical information across different words enabling the network to select the true sense ( or even a proper word ) in a blank space within a context .",0
315,"Due to the replacement of their softmax layers with a sigmoid layer in our network , we need to impose a modification in the input of the model .",0
316,"For this purpose , not only the contextual features are going to make the input of the network , but also , the sense for which we are interested to find out whether that given context makes sense or not ( no pun intended ) would be provided to the network .",0
317,"Next , the context words would be transferred to a sequence of word embeddings while the sense would be represented as a sense embedding ( the shaded embeddings in ) .",0
318,"For a set of candidate senses ( i.e. {s 1 , ... , s n } ) for an ambiguous term , after computing cosine similarities of each sense embedding with the word embeddings of the context words , we expect the sequence result of similarities between the true sense and the surrounding context communicate a pattern - like information that can be encoded through our BLSTM network ; for the incorrect senses this premise does not hold .",0
319,Several WSD studies already incorporated the idea of sense - context cosine similarities in their models [ 19 ] .,0
320,Model Definition,0
321,"For one instance ( or one document ) , the input of the network consists of a sense and a list of context words ( left and right ) which paired together form a list of context components .",0
322,"For the context D which encompasses the ambiguous term T , that takes the set of predefined candidate senses {s 1 , ... , s n } , the input for the sense s i for which we are interested in to find out whether the context is a proper match will be determined by Eq. ( 1 ) .",0
323,"Then , this input is copied ( next ) to | D | positions of the context to form the first pair of the context components .",0
324,"Here , v v v s ( s i ) is the one - hot representation of the sense corresponding to s i ? {s 1 , ... , s n }.",0
325,A one - hot representation is a vector with dimension V s consisting of | V s |?1 zeros and a single one which index indicates the sense .,0
326,The V s size is equal to the number of all senses in the language ( or the domain of interest ) .,0
327,Eq.,0
328,( 1 ) will have the effect of picking the column ( i.e. sense embeddings ) from W l s corresponding to that sense .,0
329,The W l s ( stored in the sense embeddings lookup table ) is initialized randomly since no sense embedding is computed a priori .,0
330,"Regarding the context words inputs that form the second pairs of context components , at position min the same context D the input is determined by :",0
331,"Here , v v v w ( w m ) is the one - hot representation of the word corresponding tow m ?",0
332,"D. Similar to a sense one - hot representation ( V s ) , this one - hot representation is a vector with dimension V w consisting of | V w |?1 zeros and a single one which index indicates the word in the context .",0
333,The V w size is equal to the number of words in the language ( or the domain of interest ) .,0
334,Eq. ( 2 ) will choose the column ( i.e. word embeddings ) from W x w corresponding to that word .,0
335,"The W x w ( stored in the word embeddings lookup table ) can be initialized using pre-trained word embeddings ; in this work , Glo Ve vectors are used .",0
336,"On the other hand , the output of the network that is examining sense s i i?",0
337,where W out ?,0
338,R 150 and bout ?,0
339,"R are the weights and the bias of the classification layer ( sigmoid ) , and h cl is the result of the merge layer ( concatenation ) .",0
340,"When we train the network , for an instance with the correct sense and the given context as inputs , ?",0
341,"si is set to be 1.0 , and for incorrect senses they are set to be 0.0 .",0
342,"During testing , however , among all the senses , the output of the network fora sense that gives the highest value of ?",0
343,"si will be considered as the true sense of the ambiguous term , in other words , the correct sense would be : arg max si {?",0
344,"s 1 , ..., ? sn } , s i ? {s 1 , ... , s n } .",0
345,"By applying softmax to the result of estimated classification values , {? s 1 , ..., ? sn } , we can show them as probabilities ; this facilitates interpretation of the results .",0
346,"Further , the hidden layer h cl is computed as",0
347,where ReLU means rectified linear unit ; [ h L C?1 ; h R C+1 ] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM when the last context components are met .,0
348,W hand b hare the weights and bias for the hidden layer .,0
349,Validation for Selection of Hyper- parameters,0
350,"Sens Eval - 3 data on which the network is evaluated , consist of separate training and test samples .",0
351,In order to find hyper - parameters of the network 5 % of the training samples were used for the validation in advance .,0
352,"Once the hyperparameters are selected , the whole network is trained on all training samples prior to testing .",0
353,"As to the loss function employed for the network , even though is it common to use ( binary ) cross entropy loss function when the last unit is a sigmoidal classification , we observed that mean square error led to better results for the final argmax classification ( Eq. ( 4 ) ) that we used .",0
354,"Regarding parameter optimization , RMSprop is employed .",0
355,"Also , all weights including embeddings are updated during training .",0
356,Dropout and Dropword,0
357,Dropout is a regularization technique for neural network models where randomly selected neurons are ignored during training .,0
358,"This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass , and any weight updates are not applied to the neuron on the backward pass .",0
359,"The effect is that the network becomes less sensitive to the specific weights of neurons , resulting in better generalization , and a network that is less likely to overfit the training data .",0
360,"In our network , dropout is applied to the embeddings as well as the outputs of the merge and fully - connected layers .",0
361,"Following the dropout logic , dropword is the word level generalizations of it , but in word dropout the word is set to zero while in dropword it is replaced with a specific tag .",0
362,The tag is subsequently treated just like one word in the vocabulary .,0
363,The motivation for doing dropword and word dropout is to decrease the dependency on individual words in the training context .,0
364,"Since by replacing word dropout with dropword we observed no change in the results , only word dropout was applied to the sequence of context words during training .",0
365,Experiments,0
366,"In SensEval - 3 data ( lexical sample task 5 ) , the sense inventory used for nouns and adjectives is WordNet 1.7.1 whereas verbs are annotated with senses from Wordsmyth 6 . presents the number of words under each part of speech , and the average number of senses for each class .",0
367,"As stated , training and test data are supplied as the instances of this task ; and the task consist of disambiguating one indicated word within a context .",0
368,Experimental Settings,0
369,The hyper-parameters that were determined during the validation is presented in .,0
370,The preprocessing of the data was conducted by lower - casing all the words in the documents and removing numbers .,0
371,This results in a vocabulary size of | V | = 29044 .,0
372,Words not present in the training set are considered unknown during testing .,0
373,"Also , in order to have fixed - size contexts around the ambiguous words , the padding and truncating are applied to them whenever needed .",0
374,Results,0
375,Between - all - models comparisons,1
376,- When SensEval - 3 task was launched 47 submissions ( supervised and unsupervised algorithms ) were received addressing this task .,0
377,"Afterward , some other papers tried to work on this data and reported their results in separate articles as well .",0
378,We compare the result of our model with the top - performing and low - performing algorithms ( supervised ) .,0
379,"We show our single model sits among the 5 top - performing algorithms , considering that in other algorithms for each ambiguous word one separate classifier is trained ( i.e. in the same number of ambiguous words in a language there have to be classifiers ; which means 57 classifiers for this specific task ) .",1
380,shows the results of the top - performing and low - performing supervised algorithms .,1
381,The first two algorithms represent the state - of - the - art models of supervised WSD when evaluated on SensEval - 3 .,0
382,Multi-classifier BLSTM consists of deep neural networks which make use of pre-trained word embeddings .,0
383,"While the lower layers of these networks are shared , upper layers of each network are responsible to individually classify the ambiguous that word the network is associated with .",0
384,IMS + adapted CW is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs .,0
385,"In contrast to Multi - classifier BLSTM , this model relies on features such as POS tags , collocations , and surrounding words to achieve their result .",0
386,"For these two models , softmax constitutes the output layers of all networks .",0
387,htsa 3 was the winner of the SensEval - 3 lexical sample .,0
388,"It is a Naive Bayes system applied mainly to raw words , lemmas , and POS tags with correction of the a-priori frequencies .",0
389,"IRST - Kernels utilizes kernel methods for pattern abstraction , paradigmatic and syntagmatic information and unsupervised term proximity on British National Corpus ( BNC ) , in SVM classifiers .",0
390,"Likewise , nusels makes use of SVM classifiers with a combination of knowledge sources ( part - of - speech of neighboring words , words in context , local collocations , syntactic relations .",0
391,The second part of the table lists the low - performing supervised algorithms .,0
392,Considering their ranking scores we see that there are unsupervised methods that outperform these supervised algorithms .,0
393,Within - our - model comparisons,1
394,"- Besides several internal experiments to examine the importance of some hyper - parameters to our network , we investigated if the sequential follow of cosine similarities computed between a true sense and its preceding and succeeding context words carries a pattern - like information that can be encoded with BLSTM .",0
395,presents the results of these experiments .,0
396,The first row shows the best result of the network that we described above ( and depicted in ) .,0
397,Each of the other rows shows one change that we applied to the network to seethe behavior of the network in terms of F- measure .,0
398,"In the middle part , we are specifically concerned about the importance of the presence of a BLSTM layer in our network .",0
399,"So , we introduced some fundamental changes in the input or in the structure of the network .",0
400,"Generally , it is expected that the cosine similarities of closer words ( in the context ) to the true sense be larger than the incorrect senses ' ; however , if a series of cosine similarities can be encoded through an LSTM ( or BLSTM ) network should be experimented .",0
401,"We observe if reverse the sequential follow of information into our Bidirectional LSTM , we shuffle the order of the context words , or even replace our Bidirectional LSTMs with two different fully - connected networks of the same size 50 ( the size of the LSTMs outputs ) , the achieved results were notably less than 72.5 % .",1
402,"In the third section of the table , we report our changes to the hyper - parameters .",0
403,"Specifically , we seethe importance of using GloVe as pre-trained word embeddings , how word dropout improves generalization , and how context size plays an important role in the final classification result ( showing one of our experiments ) .",0
404,Discussion,0
405,"From the results of , we notice our single WSD network , despite eliminating the problem of having a large number of WSD classifiers , still falls short when is compared with the state - of - the - art WSD algorithms .",0
406,"Based on our intuition and supported by some of our preliminary experiments , this deficiency stems from an important factor in our BLSTM network .",0
407,"Since no sense embedding is made publicly available for use , the sense embeddings are initialized randomly ; yet , word embeddings are initialized by pre-trained GloVe vectors in order to benefit from the semantic and syntactic properties of the context words conveyed by these embeddings .",0
408,"That is to say , the separate spaces that the sense embeddings and the ( context ) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data .",0
409,"Furthermore , this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful .",0
410,"Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the ( informative ) definition content words of senses , or taking the average of the GloVe embeddings of the ( informative ) context words in their training samples - did not give us a better result than our random initialization .",0
411,"Our preliminary experiments though in which we replaced all Glo Ve embeddings in the network with sense embeddings ( using a method proposed by Chen et al. ) , showed considerable improvements in the results of some ambiguous words .",0
412,That means both senses and context words ( while they can be ambiguous by themselves ) come from one vector space .,0
413,"In other words , the context would also be represented by the possible senses that it s words can take .",0
414,"This idea not only can help to improve the results of the current model , it can also avoid the need fora large amount of training data since senses can be seen in both places , center and context , to be trained .",0
415,Conclusion,0
416,"In contrast to common one - classifier - per- each - word supervised WSD algorithms , we developed our single network of BLSTM that is able to effectively exploit word orders and achieve comparable results with the best - performing supervised algorithms .",0
417,This single WSD BLSTM network is language and domain independent and can be applied to resource - poor languages ( or domains ) as well .,0
418,"As an ongoing project , we also provided a direction which can lead us to the improvement of the results of the current network using pre-trained sense embeddings .",0
419,"For future work , besides following the discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings , we plan to examine the network on technical domains such as biomedicine as well .",0
420,"In this case , our model will be evaluated on MSH WSD dataset 8 prepared by National Library of Medicine 9 ( NLM ) .",0
421,"Also , construction of sense embeddings using ( extended ) definitions of senses can be tested .",0
422,"Moreover , considering that for many senses we have at least one ( lexically ) unambiguous word representing that sense , we also aim to experiment with unsupervised ( pre - ) training of our network which benefits form quarry management by which more training data will be automatically collected from the web .",0
423,title,0
424,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,1
425,abstract,0
426,"In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .",0
427,"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .",1
428,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",1
429,Introduction,0
430,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",1
431,"Various approaches have been proposed to achieve WSD : Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as primary resources , and use algorithms such as lexical similarity measures or graph - based measures .",0
432,"Supervised methods , on the other hand , exploit sense annotated corpora as training instances fora classifier such as SVM , or more recently by a neural network .",0
433,"Finally , unsupervised methods automatically iden - tify the different senses of words from unannotated or parallel corpora ( e.g. ) .",0
434,Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns ( for instance ) .,0
435,"State of the art classifiers used to combine specific features such as the parts of speech and the lemmas of surrounding words , but they are now replaced by neural networks which learn their own representation of words .",0
436,One major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora :,0
437,"In the annotated corpus SemCor , the largest manually sense annotated corpus available , words are annotated with 33 760 different sense keys , which corresponds to only approximately 16 % of the sense inventory of WordNet , the lexical database of reference widely used in WSD .",0
438,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing .",0
439,"In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .",1
440,"Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .",1
441,"Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .",0
442,"For instance , let us consider the word "" mouse "" and two of its senses which are the computer mouse and the animal mouse .",0
443,"We only need to know the notions of "" animal "" and "" electronic de-vice "" to distinguish them , and all notions that are more specialized such as "" rodent "" or "" mammal "" are therefore superfluous .",0
444,"By grouping them , we can benefit from all other instances of electronic devices or animals in a training corpus , even if they do not mention the word "" mouse "" .",0
445,"Contributions : In this paper , we hypothesize that only a subset of WordNet senses could be considered to disambiguate all words of the lexical database .",0
446,"Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .",0
447,"By using these techniques , we are able to greatly improve the coverage of supervised WSD systems , nearly eliminating the need fora backoff strategy that is currently used inmost systems when dealing with a word which has never been observed in the training data .",0
448,"We evaluate our method on a state of the art WSD neural network , based on pretrained contextualized word vector representations , and we present results that significantly outperform the state of the art on every standard WSD evaluation task .",0
449,"Finally , we provide a documented tool for training and evaluating neural WSD models , as well as our best pretrained model in a dedicated GitHub repository 1 .",0
450,Related Work,0
451,"In WSD , several recent advances have been made in the creation of new neural architectures for supervised models and the integration of knowledge into these systems .",0
452,Multiple works also exploit the idea of grouping together related senses .,0
453,"In this section , we give an overview of these works .",0
454,WSD Based on a Language Model,0
455,"In this type of approach , that has been initiated by and reimplemented by , the central component is a neural language model able to predict a word with consideration for the words surrounding it , thanks to a recurrent neural network trained on a massive quantity of unannotated data .",0
456,"Once the language model is trained , it is used to produce sense vectors that result from averaging the word vectors predicted by the language model at all positions of words annotated with the given sense .",0
457,"At test time , the language model is used to predict a vector according to the surrounding context , 1 https://github.com/getalp/disambiguate and the sense closest to the predicted vector is assigned to each word .",0
458,These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner .,0
459,"However , sense annotated corpora are still indispensable to contruct the sense vectors .",0
460,WSD Based on a Softmax Classifier,0
461,"In these systems , the main neural network directly classifies and attributes a sense to each input word through a probability distribution computed by a softmax function .",0
462,"Sense annotations are simply seen as tags put on every word , like a POS - tagging task for instance .",0
463,We can distinguish two separate branches of these types of neural networks :,0
464,"1 . Those in which we have several distinct and token - specific neural networks ( or classifiers ) for every different word in the dictionary ) , each of them being able to manage a particular word and its particular senses .",0
465,"For instance , one of the classifiers is specialized in choosing between the four possible senses of the noun "" mouse "" .",0
466,"This type of approach is particularly fitted for the lexical sample tasks , where a small and finite set of very ambiguous words have to be sense annotated in several contexts , but it can also be used in all - words word sense disambiguation tasks .",0
467,2 .,0
468,Those in which we have a larger and general neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used .,0
469,"The advantage of the first branch of approaches is that in order to disambiguate a word , limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words .",0
470,"To put things in perspective , the average number of senses of polysemous words in WordNet is approximately 3 , whereas the total number of senses considering all words is 206 941 .",0
471,"The second approach , however , has an interesting property : all senses reside in the same vector space and hence share features in the hidden layers of the network .",0
472,"This allows the model to predict an identical sense for two different words ( i.e. synonyms ) , but it also offers the possibility to predict a sense fora word not present in the dictionary ( e.g. neologism , spelling mistake ... ) .",0
473,"Finally , in two recent articles , and have proposed an improvement of these type of architectures , by computing an attention between the context of a target word and the gloss of its different senses .",0
474,"Thus , their work is one of the first to incorporate knowledge from WordNet into a WSD neural network .",0
475,Sense Clustering,0
476,Methods,0
477,Several works exploit the idea of grouping together mutiple Word Net sense tags in order to create a coarser sense inventory which can potentially be more useful in some NLP tasks .,0
478,"In the works of , the authors propose a supervised system that learns and predicts "" Supersense "" tags , which belong to the set of the broad semantic categories of senses , organizing the sense inventory of WordNet .",0
479,"This tagset consists , in their work , of 26 categories for nouns ( such as "" food "" , "" person "" or "" object "" ) , and 15 categories for verbs ( such as "" emotion "" or "" weather "" ) .",0
480,"By predicting supersense tags instead of the usual fine - grained sense tags of WordNet , the output vocabulary of their system is shrinked to only 41 different classes , and this leads to a small and easy - to - train model able to perform partial WSD , which could be useful and sufficient for other NLP tasks where the fine - grained distinction is not necessary .",0
481,"In , the authors propose several methods for creating "" Basic Level Concepts "" ( BLC ) , groups of related senses with a generally smaller size than supersenses , and which can be controlled by a threshold variable .",0
482,"Their methods rely on the semantic relationships between senses of WordNet , and , in the same way as , they evaluated their clusters on a modified WSD task , where supersenses or BLC have to be predicted instead of the original sense tags from WordNet .",0
483,The main difference between our work and these works is that our end goal is to improve finegrained WSD systems .,0
484,"Even though our methods generate clusters of related senses , we guarantee that two different senses of a lemma reside in two different clusters , so at the end , even if our supervised system produces a cluster tag fora target word , we are still able to find back the true sense tag , by simply keeping track of which sense key of its lemma belongs to the predicted group .",0
485,Sense Vocabulary,0
486,Compression,0
487,"Current state of the art supervised WSD systems such as , , and are all confronted to the following issues :",0
488,"1 . Due to the small number of manually sense annotated corpora available , a target word may never be observed during the training , and therefore the system is notable to annotate it .",0
489,"2 . For the same reason , a word may have been observed , but not all of its senses .",0
490,"In this case the system is able to annotate the word , but if the expected sense has never been observed , the output will be wrong , regardless of the architecture of the supervised system .",0
491,3 . Training a neural network to predict a tag which belongs to the set of all WordNet senses can become extremely slow and requires a lot of parameters with a large output vocabulary .,0
492,And this vocabulary goes up to 206 941 if we consider all word - senses of WordNet .,0
493,"In order to overcome all these issues , we propose a method for grouping together multiple sense tags that refer in fact to the same concept .",0
494,"In consequence , the output vocabulary decreases , the ability of the trained system to generalize improves , as well as its coverage .",0
495,From Senses to Synsets : A Vocabulary Compression Based on Synonymy,0
496,"In the lexical database WordNet , senses are organized in sets of synonyms called synsets .",0
497,A synset is technically a group of one or more word - senses that have the same definition and consequently the same meaning .,0
498,"For instance , the first senses of "" eye "" , "" optic "" and "" oculus "" all refer to a common synset which definition is "" the organ of sight "" .",0
499,"Illustrated in , the word - sense to synset mapping is hence away of compressing the output vocabulary , and it is already applied in many works , while not being always explicitly stated .",0
500,This method clearly helps to improve the coverage of supervised systems however .,0
501,"Indeed , if the verb "" help "" is observed in the annotated data in its first sense , the context surrounding the target word can be used to later annotate the verb "" assist "" or "" aid "" with the same valid synset tag . :",0
502,"Word - sense to synset mapping ( compression through synonymy ) applied on the first two senses of the words "" help "" , "" aid "" and "" assist "" .",0
503,"Going further , other information from WordNet can help the system to generalize .",0
504,Our first new method takes advantage of the hypernymy and hyponymy relationships to achieve the same idea .,0
505,Compression through Hypernymy and Hyponymy Relationships,0
506,"According to , hypernymy and hyponymy are two semantic relationships which correspond to a particular case of sense inclusion : the hyponym of a term is a specialization of this term , whereas its hypernym is a generalization .",0
507,"For instance , a "" mouse "" is a type of "" rodent "" which is in turn a type of "" animal "" .",0
508,"In WordNet , these relationships bind nearly every noun together in a tree structure 2 that goes from the generic root , the node "" entity "" to the most specific leaves , for instance the node "" whitefooted mouse "" .",0
509,"These relationships are also present on several verbs : for instance "" add "" is away of "" compute "" which is away of "" reason "" .",0
510,"For the sake of WSD , just like grouping together the senses of the same synset helps to better generalize , we hypothesize that grouping together the synsets of the same hypernymy relationship also helps in the same way .",0
511,The general idea of our method is that the most specialized concepts in WordNet are often superfluous for WSD .,0
512,"Indeed , considering a small subset of WordNet that only consists of the word "" mouse "" , its first sense ( the small rodent ) , its fourth sense ( the elec - We computed that 41 607 on the 44 449 polysemous nouns of WordNet ( 94 % ) are part of this hierarchy .",0
513,"tronic device ) , and all of their hypernyms .",0
514,This is illustrated in .,0
515,"We can see that every concept that is more specialized than the concepts "" artifact "" and "" living _ thing "" could be removed .",0
516,"We could map every tag of "" mouse# 1 "" to the tag of "" living _ thing# 1 "" and we could still be able to disambiguate this word , but with a benefit : all other "" living things "" and animals in the sense annotated data could be tagged with the same sense .",0
517,They would give examples of what is an animal and then show how to differentiate the small rodent from the hand - operated electronic device .,0
518,"Therefore , the goal of our method is to map every sense of WordNet to its highest ancestor in the hypernymy hierarchy , but with the following constraints :",0
519,"First , this ancestor must discriminate all the different senses of the target word .",0
520,"Second , we need to preserve the hypernyms that are indispensable to discriminate the senses of the other words in the dictionary .",0
521,"For instance , we can not map "" mouse# 1 "" to "" living _ thing# 1 "" , because the more specific tag "" animal# 1 "" is essential to distinguish the two senses of the word "" prey "" ( one sense describes a person , the other describes an animal ) .",0
522,Our method thus works in two steps :,0
523,"1 . We mark as "" necessary "" the children of the first common ancestor of every pair of senses of every word of Word Net .",0
524,"2 . We map every sense to its first ancestor in the hypernymy hierarchy that has been previously marked as "" necessary "" .",0
525,"As a result , the most specific synsets of the tree that are not indispensable for discriminating any word of the lexical inventory are automatically removed from the vocabulary .",0
526,"In other words , the set of synsets that is left in the vocabulary is the smallest subset of all synsets that are necessary to distinguish every sense of every word of WordNet , following the hypernym and hyponym links .",0
527,Compression through all semantic relationships,0
528,"In addition to hypernymy and hyponymy , Word - Net contains several other relationships between synsets , such as the instance relationship ( e.g. "" Albert Einstein "" is an instance of "" physicist ' ) , the meronymy ( X is part of Y , or X is a member of Y ) and it s counterpart the holonymy , the antonymy ( X is the opposite of Y ) , etc .",0
529,"We hence propose a second method for sense vocabulary compression , that considers all the semantic relationships offered by WordNet , in order to form clusters of related synsets .",0
530,"For instance , using all semantic relationships , we could form a cluster containing "" physicist "" , "" physics "" ( domain category ) , "" Albert Einstein "" ( instance of ) , "" astronomer "" ( hyponym ) , but also further related senses such as "" photon "" , because it is a meronym of "" radiation "" , which is a hyponym of "" energy "" , which belongs to the same domain category of "" physics "" .",0
531,Our method works by constructing these clusters iteratively :,0
532,"First , we initialize the set of clusters C with one synset in each cluster .",0
533,"C ={c 0 , c 1 , ... , c n } S = {s 0 , s 1 , ... , s n } C ={{s 0 } , {s 1 } , ... , {s n }}",0
534,"Then at each step , we sort C by sizes of clusters , and we peek the smallest one c x and the smallest related cluster to c x , c y .",0
535,We define a cluster being related to another if they contain at least one synset that have a semantic link together .,0
536,"We merge c x and c y together , and we verify that the operation still allows to discriminate the different senses of all words in the lexical database .",0
537,"If it is not the case , we cancel the merge and we try another semantic link .",0
538,"If no link is possible , we try to create one with the next smallest cluster , and if no further link can be created , the algorithm stops .",0
539,"In , we show a possible set of clusters that could result from our method , focusing on two senses of the word "" Weber "" and only on a few relationships . :",0
540,"Example of clusters of sense that could result from our method , if we limit our view to two senses of the word "" Weber "" and only some relationship links .",0
541,This method produces clusters significantly larger than the method based on hypernyms .,0
542,"On average , a cluster has 5 senses with the hypernym method , whereas it has 17 senses with this method .",0
543,"This method , unlike the previous one , is also stochastic , because the formation of clusters depends on the underlying order of iteration when multiple clusters are the same size .",0
544,"However , because we always sort clusters by size before creating a link , we observed that the final vocabulary size ( i.e. number of clusters ) is always between 11 000 and 13 000 .",0
545,"In the following , we consider a resulting mapping where the algorithm stopped after 105 774 steps .",0
546,Method,0
547,Vocabulary size : Effects of the sense vocabulary compression on the vocabulary size and on the coverage of the SemCor .,0
548,"In , we show the effect of the common compression through synonyms , our first proposed compression through hypernyms , and our second method of compression through all semantic relationships , on the size of the vocabulary of Word - Net sense tags , and on the coverage of the SemCor corpus .",0
549,"As we can see , the sense vocabulary size is drastically decreased , and the coverage of the same corpus really improved .",0
550,"In order to evaluate our sense vocabulary compression methods , we applied them on a neural WSD system based on a softmax classifier capable of classifying a word in all possible synsets of Word - Net ( see subsection 2.2 ) .",0
551,We implemented a system similar to 's BiLSTM but with some key differences .,0
552,"In particular , we used BERT contextualized word vectors in input of our network , Transformer encoder layers instead of LSTM layers as hidden units , our output vocabulary only consists of sense tags seen during training ( mapped according to the compression method used ) , and we ignore the network 's predictions on words that are not annotated .",0
553,Implementation details,0
554,"For BERT , we used the model named "" bert - largecased "" of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .",1
555,"Due to the fact that BERT 's internal tokenizer sometimes split words in multiples tokens ( i.e. [ "" rodent "" ] becomes [ "" rode "" , "" # # nt "" ] ) , we trained our system to predict a sense tag on the first token only of a splitted annotated word .",0
556,"For the Transformer encoder layers , we used the same parameters as the "" base "" model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .",1
557,"Finally , because BERT already encodes the position of the words inside their vectors , we did not add any positional encoding .",0
558,Training,0
559,"We compared our sense vocabulary compression methods on two training sets : The SemCor , and the concatenation of the SemCor and the Princeton WordNet Gloss Corpus ( WNGC ) .",0
560,"The latter is a corpus distributed as part of WordNet since its version 3.0 , and it consists of the definitions ( glosses ) of every synset of WordNet , with words manually or semi-automatically sense annotated .",0
561,We used the version of these corpora given as part of the UFSAC 2.1 resource 4 .,0
562,We performed every training for 20 epochs .,0
563,"At the beginning of each epoch , we shuffled the training set .",0
564,"We evaluated our model at the end of every epoch on a development set , and we kept only the one which obtained the best F1 WSD score .",0
565,"The development set was composed of 4 000 random sentences taken from the Princeton WordNet Gloss Corpus for the models trained on the Sem - Cor , and 4 000 random sentences extracted from the whole training set for the other models .",0
566,"For each training set , we trained three systems :",0
567,"1 . A "" baseline "" system that predicts a tag belonging to all the synset tags seen during training , thus using the common vocabulary compression through synonyms method .",0
568,"2 . A "" hypernyms "" system which applies our vocabulary compression through hypernyms algorithm on the training corpus .",0
569,"3 . A "" all relations "" system which applies our second vocabulary compression through all relations on the training corpus .",0
570,"We trained with mini-batches of 100 sentences , truncated to 80 words , and we used Adam with a learning rate of 0.0001 as the optimization method .",0
571,All models have been trained on one Nvidia 's Titan X GPU .,0
572,The number of parameters of individual models are displayed in .,0
573,"As we can see , our compression methods drastically reduce the number of parameters , by a factor of 1.2 to 2 .",0
574,Evaluation,0
575,"We evaluated our models on all evaluation corpora commonly used in WSD , that is the English all - words WSD tasks of the evaluation campaigns SensEval / Sem Eval .",0
576,"We used the fine - grained evaluation corpora from the evaluation framework of , which consists of Sen - s Eval 2 , SensEval 3 , SemEval 2007 task 17 , SemEval 2013 task 12 and SemEval 2015 task 13 , as well as the "" ALL "" corpus consisting of the concatenation of all pre- : F1 scores ( % ) on the English WSD tasks of the evaluation campaigns SensEval / Sem Eval .",0
577,"The task "" ALL "" is the concatenation of SE2 , SE3 , SE07 17 , SE13 and SE15 .",0
578,The first sense is assigned on words for which none of its sense has been observed during the training .,0
579,Results in bold are to our knowledge the best results obtained on the task .,0
580,Scores prefixed by a dagger ( ) are not provided by the authors but are deduced from their other scores .,0
581,vious ones .,0
582,We also compared our result on the coarse - grained task 7 of SemEval 2007 which is not present in this framework .,0
583,"For each evaluation , we trained 8 independent models , and we give the score obtained by an ensemble system that averages their predictions through a geometric mean . :",0
584,"Coverage of our systems on the task "" ALL "" .",0
585,""" Backoff on Monosemics "" means that monosemic words are considered annotated .",0
586,"In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .",1
587,Our methods greatly improves their coverage on the evaluation tasks however .,0
588,"As we can see in , on the total of 7 253 words to annotate for the corpus "" ALL "" , the baseline system trained on the SemCor is notable to annotate 491 of them , while the vocabulary compression through hypernyms reduces this number to 91 and 24 for the compression through all relations .",0
589,"When adding the Princeton WordNet Gloss Corpus to the training set , only one word ( the monosemic adjective "" cytotoxic "" ) can not be annotated with the system that uses the compression through all relations because it s sense has not been observed during training .",0
590,"If we exclude the monosemic words , the system based on our compression method through all relations miss only one word ( the adverb "" eloquently "" ) when trained on the SemCor , and has a coverage to 100 % when the WNGC is addded .",0
591,"In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .",1
592,Ablation Study,0
593,"In order to give a better understanding of the origin of our scores , we provide a study of the impact of our main parameters on the results .",0
594,"In addition to the training corpus and the vocabulary compression method , we chose two parameters that differentiate us from the state of the art : the pretrained word embeddings model and the ensembling method , and we have made them vary .",0
595,"For the word embeddings model , we experimented with BERT as in our main results , with ELMo , and with GloVe , the same pre-trained word embeddings used by .",0
596,"For ELMo , we used the model trained on : Ablation study on the task "" ALL "" ( i.e. the concatenation of all SensEval / SemEval tasks ) .",0
597,"For systems that do not use ensemble , we display the mean score ( x ) of eight individually trained models along with its standard deviation ( ? ) .",0
598,"Wikipedia and the monolingual news crawl data from For GloVe , we used the model trained on Wikipedia 2014 and Gigaword 5 .",0
599,"6 Due to the fact that Glo Ve embeddings do not encode the position of the words ( a word has the same vector representation in any context ) , we used bidirectional LSTM cells of size 1 000 for each direction , instead of Transformer encoders for this set of experiments .",0
600,"In addition , because the vocabulary of GloVe is finite and all words are lowercased , we lowercased the inputs , and we assigned a vector filled with zeros to outof - vocabulary words .",0
601,"For the ensembling method , we either perform ensembling as in our main results , by averaging the prediction of 8 models trained separately or we give the mean and the standard deviation of the scores of the 8 models evaluated separately .",0
602,"As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .",1
603,"Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .",1
604,"Finally , using ensembles adds roughly another 1 point to the final F1 score .",1
605,5 https://allennlp.org/elmo,0
606,6 https://nlp.stanford.edu/projects/glove/,0
607,"Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .",0
608,"However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .",1
609,Conclusion,0
610,"In this paper , we presented two new methods that improve the coverage and the capacity of generalization of supervised WSD systems , by narrowing down the number of different sense in WordNet in order to keep only the senses that are essential for differentiating the meaning of all words of the lexical database .",0
611,"On the scale of the whole lexical database , we showed that these methods can shrink the total number of different sense tags in WordNet to only 6 % of the original size , and that the coverage of an identical training corpus has more than doubled .",0
612,"We implemented a state of the art WSD neural network and we showed that these methods compress the size of the underlying models by a factor of 1.2 to 2 , and greatly improve their coverage on the evaluation tasks .",0
613,"As a result , we reach a coverage of 99. 99 % of the evaluation tasks ( 1 word missing on 7 253 ) when training a system on the SemCor only , and 100 % when adding the WNGC to the training data , on the pol-ysemic words .",0
614,"Therefore , the need fora backoff strategy is nearly eliminated .",0
615,"Finally , our method combined with the recent advances in contextualized word embeddings and with a training corpus composed of sense annotated glosses , our system achieves scores that considerably outperform the state of the art on all WSD evaluation tasks .",0
616,title,0
617,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,1
618,abstract,0
619,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",1
620,"In this paper , we propose a novel embedding method specifically designed for NED .",1
621,The proposed method jointly maps words and entities into the same continuous vector space .,0
622,We extend the skip - gram model by using two models .,0
623,"The KB graph model learns the relatedness of entities using the link structure of the KB , whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words .",0
624,"By combining contexts based on the proposed embedding with standard NED features , we achieved state - of - theart accuracy of 93.1 % on the standard CoNLL dataset and 85.2 % on the TAC 2010 dataset .",0
625,Introduction,0
626,"Named Entity Disambiguation ( NED ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",0
627,"NED has lately been extensively studied ) and used as a fundamental component in numerous tasks , such as information extraction , knowledge base population , and semantic search .",0
628,We use Wikipedia as KB in this paper .,0
629,The main difficulty in NED is ambiguity in the meaning of entity mentions .,0
630,"For example , the mention "" Washington "" in a document can refer to various entities , such as the state , or the capital of the US , the actor Denzel Washington , the first US president George Washington , and soon .",0
631,"In order to resolve these ambiguous mentions into references to the correct entities , early approaches focused on modeling textual context , such as the similarity between contextual words and encyclopedic descriptions of a candidate entity .",0
632,"Most state - of - theart methods use more sophisticated global approaches , where all mentions in a document are simultaneously disambiguated based on global coherence among disambiguation decisions .",0
633,Word embedding methods are also becoming increasingly popular .,0
634,"These involve learning continuous vector representations of words from large , unstructured text corpora .",0
635,The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,0
636,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .",1
637,"In this model , similar words and entities are placed close to one another in a vector space .",1
638,"Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .",1
639,"This enables us to easily measure the contextual information for NED , such as the similarity between a context word and a candidate entity , and the relatedness of entities required to model coherence .",0
640,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .",1
641,Our model consists of the following three models based on the skip - gram model :,1
642,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",1
643,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .",0
644,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .",1
645,Textual context similarity is measured according to vector similarity between an entity and words in a document .,0
646,Coherence is measured based on the relatedness between the target entity and other entities in a document .,0
647,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .",1
648,We tested the proposed method using two standard NED datasets : the CoNLL dataset and the TAC 2010 dataset .,0
649,Experimental results revealed that our method outperforms state - of - the - art methods on both datasets by significant margins .,0
650,"Moreover , we conducted experiments to separately assess the quality of the vector representation of entities using an entity relatedness dataset , and discovered that our method successfully learns the quality representations of entities .",0
651,Joint Embedding of Words and Entities,0
652,"In this section , we first describe the conventional skip - gram model for learning word embedding .",0
653,We then explain our method to construct an embedding that jointly maps words and entities into the same continuous d-dimensional vector space .,0
654,We extend the skip - gram model by adding the KB graph model and the anchor context model .,0
655,Skip- gram Model for Word Similarity,0
656,The training objective of the skip - gram model is to find word representations that are useful to predict context words given the target word .,0
657,"Formally , given a sequence of T words w 1 , w 2 , ... , w T , the model aims to maximize the following objective function :",0
658,"where c is the size of the context window , wt denotes the target word , and w t +j is its context word .",0
659,The conditional probability P ( w t+j |w t ) is computed using the following softmax function :,0
660,where,0
661,"Wis a set containing all words in the vocabulary , and V w ?",0
662,Rd and U w ?,0
663,"Rd denote the vectors of word win matrices V and U , respectively .",0
664,"The skip - gram model is trained to optimize the above function L w , and V are used as the resulting vector representations of words .",0
665,Extending the Skip - gram Model,0
666,We extend the skip - gram model to learn the vector representations of entities .,0
667,We expand matrices V and U to include the vectors of entities V e ?,0
668,Rd and U e ?,0
669,Rd in addition to the vectors for words .,0
670,KB Graph Model,0
671,We use an internal link structure in KB to enable the model to learn the relatedness between pairs of entities .,0
672,Wikipedia Link - based Measure ( WLM ) ) is a method to measure entity relatedness based on its link structure .,0
673,It has been used as a standard method to compute the relatedness of entities for modeling coherence in past NED studies .,0
674,The relatedness between two entities is computed using the following function :,0
675,where E is the set of all entities in KB and Ce is the set of entities with a link to an entity e.,0
676,"Intuitively , WLM assumes that entities with similar incoming links are related .",0
677,"Despite its simplicity , WLM yields state - of - the - art performance .",0
678,"Inspired by WLM , the KB graph model simply learns to place entities with similar incoming links near one another in the vector space .",0
679,We formalize this as the following objective function :,0
680,We compute the conditional probability P ( e o |e i ) using the following softmax function :,0
681,We train the model to predict the incoming links Ce given an entity e.,0
682,"Therefore ,",0
683,Ce plays a similar role to context words in the skip - gram model .,0
684,Anchor Context Model,0
685,"If we add only the KB graph model to the skipgram model , the vectors of words and entities do not interact , and can be placed in different subspaces of the vector space .",0
686,"To address this issue , we introduce the anchor context model to place similar words and entities near one another in the vector space .",0
687,The idea underlying this model is to leverage KB anchors and their context words to train the model .,0
688,"As mentioned in Section 1 , we use Wikipedia as a KB .",0
689,It contains many internal anchors that can be safely treated as unambiguous occurrences of referent KB entities .,0
690,"By using these anchors , we can easily obtain many occurrences of entities and their corresponding context words directly from the KB .",0
691,"As in the skip - gram model , we simply train the model to predict the context words of an entity pointed to by the target anchor .",0
692,The objective function is as follows :,0
693,"where A denotes a set of anchors in the KB , each of which contains a pair of a referent entity e i and a set of its context words Q .",0
694,"Here , Q contains the previous c words and the next c words .",0
695,Note that | A | equals the number of internal anchors in the KB .,0
696,"As in past models , the conditional probability P ( w o |e i ) is computed using the softmax function :",0
697,"Using the proposed model , we align the vector representations of words and entities by placing words and entities with similar context words close to one another in the vector space .",0
698,Training,0
699,"Considering the three model components mentioned above , we propose the following objective function by linearly combining the above objective functions :",0
700,"The training of the model is intended to maximize the above function , and the resulting matrix V is used to embed words and entities .",0
701,"One of the problems in training our model is that the normalizers contained in the softmax functions P ( w t+j |w t ) , P ( e o |e i ) , and P ( w o |e i ) are computationally very expensive because they involve summation overall words W or entities E .",0
702,"To address this problem , we use negative sampling ( NEG ) to convert original objective functions into computationally feasible ones .",0
703,NEG is defined by the following objective function :,0
704,where ?( x ) = 1 / ( 1 + exp (?x ) ) and g is the number of negative samples .,0
705,We replace the log P ( w t+j |w t ) term in Eq. ( 1 ) with the above objective function .,0
706,"Consequently , the objective function is transformed from that in Eq.",0
707,( 1 ) to a simple objective function of the binary classification to distinguish the observed word wt from words drawn from noise distribution P neg ( w ) .,0
708,We also replace log P ( e o |e i ) in Eq. ( 4 ) and log P ( w o |e i ) in Eq. ( 6 ) in the same manner .,0
709,Note that NEG takes a negative distribution P neg ( w ) as a free parameter .,0
710,"Following , we use the unigram distribution of words ( U ( w ) ) raised to the 3 / 4 th power ( i.e. , U ( w ) 3 / 4 / Z , where Z is a normalization constant ) in the skip - gram model and the anchor context model .",0
711,"In the KB graph model , we use a uniform distribution over KB entities E as the negative distribution .",0
712,We use Wikipedia to train all the above models .,0
713,Optimization is carried out simultaneously to maximize the transformed objective function by iterating over Wikipedia pages several times .,0
714,We use stochastic gradient descent ( SGD ) for the optimization .,0
715,"The optimization is performed using a multiprocess - based implementation of our model using Python , Cython , and NumPy configured with OpenBLAS with storing matrices V and U in the shared memory .",0
716,"To improve speed , we decide not to introduce locks to the shared matrices .",0
717,Embedding,0
718,"In this section , we explain our NED method using our proposed embedding .",0
719,Let us formally define the task .,0
720,"Given a set of entity mentions M = {m 1 , m 2 , ... , m N } in a document d with an entity set E = {e 1 , e 2 , ... , e K } in the KB , the task is defined as resolving mentions ( e.g. , "" Washington "" ) into their referent entities ( e.g. , Washington D.C. ) .",0
721,We introduce two measures that have been frequently observed in past NED studies : entity prior P ( e ) and prior probability P ( e |m ) .,0
722,"We define entity prior P ( e ) = | A e , * |/|A * , * | where A * , * denotes all anchors in the KB and A e , * is the set of anchors that point to entity e.",0
723,"Prior probability is defined as P ( e |m ) = | A e , m |/| A * , m | where A * , m represents all anchors with the same surface as mention min KB and A e , m is a subset of A * , m that points to entity e.",0
724,We separate the NED task into two sub - tasks : candidate generation and mention disambiguation .,0
725,"In candidate generation , candidates of referent entities are generated for each mention .",0
726,Details of candidate generation are provided in Section 4.3.1 .,0
727,Mention Disambiguation,0
728,"Given a document d and mention m with its candidate referent entities {e 1 , e 2 , ... , e k } generated in the candidate generation step , the task is to disambiguate mention m by selecting the most relevant entity from the candidate entities .",0
729,The key to improving the performance of this task is to effectively model the context .,0
730,We propose two novel methods to model the context using the proposed embedding .,0
731,"Further , we combine these two models with several standard NED features using supervised machine learning described in 3.1.3 .",0
732,Modeling Textual Context,0
733,Textual context is designed based on the assumption that an entity is more likely to appear if the context of a given mention is similar to that of the entity .,0
734,We propose a method to measure the similarity between textual context and entity using the proposed embedding by first deriving the vector representation of the context and then computing the similarity between the context and the entity using cosine similarity .,0
735,"To derive the vector of context , we average the vectors of context words :",0
736,where W cm is a set of the context words of mention m and v w ?,0
737,V denotes the vector representation of word w .,0
738,We use all noun words in document d as context words .,0
739,"1 Moreover , we ignore a context word if the surface of mention m contains it .",0
740,We then measure the similarity between candidate entity and the derived textual context by using cosine similarity between v cw and the vector of entity v e .,0
741,Modeling Coherence,0
742,It has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED .,0
743,"However , this is a chickenand - egg problem because the assignment of entities to mentions , which is required to measure coherence , is not possible prior to performing NED .",0
744,"Similar to past work , we address this problem by employing a simple twostep approach : we first train the machine learning model using the coherence score among unambiguous mentions 2 , in addition to other features , and then retrain the model using the coherence score among the predicted entity assignments instead .",0
745,"To estimate coherence , we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity e.",0
746,"Note that context entities are unambiguous entities in the first step , and predicted entities are used instead in the second step .",0
747,"To derive the vector representation of context entities , we average their vector representations :",0
748,where E cm denotes the set of context entities described above .,0
749,"To estimate the coherence score , we again use cosine similarity between the vector of entity v e and that of context entities v ce .",0
750,Learning to Rank,0
751,"To combine the proposed contextual information described above with standard NED features , we employ a method of supervised machine learning to rank the candidate entities given mention m and document d.",0
752,"In particular , we use Gradient Boosted Regression Trees ( GBRT ) , a stateof - the - art point - wise learning - to - rank algorithm widely used for various tasks , which has been recently adopted for the sort of tasks for which we employ it here .",0
753,"GBRT consists of an ensemble of regression trees , and predicts a relevance score given an instance .",0
754,We use the GBRT implementation in scikit - learn 3 and the logistic loss is used as the loss function .,0
755,"The main parameters of GBRT are the number of iterations ? , the learning rate ? , and the maximum depth of the decision trees ?.",0
756,"With regard to the features of machine learning , we first use prior probability ( P ( e |m ) ) and entity prior ( P ( e ) ) .",0
757,"Further , we include a feature representing the maximum prior probability of the candidate entity e of all mentions in the document .",0
758,We also add the number of entity candidates for mention m as a feature .,0
759,The above set of four features is called base features in the rest of the paper .,0
760,We also use several string similarity features used in past work on NED .,0
761,"These features aim to capture the similarity between the title of entity e and the surface of mention m , and consist of the edit distance , whether the title of entity e exactly equals or contains the surface of mention m , and whether the title of entity e starts or ends with the surface of mention m .",0
762,"Finally , we include contextual features measured using the proposed embedding .",0
763,"We use cosine similarity between the candidate entity and the textual context ( see Section 3.1.1 ) , and similarity between an entity and contextual entities ( see Section 3.1.2 ) .",0
764,"Furthermore , we include the rank of entity e among candidate entities of mention m , sorted according to these two similarity scores in descending order .",0
765,Experiments,0
766,"In this section , we describe the setup and results of our experiments .",0
767,"In addition to experiments on the NED task , we separately assessed the quality of pairwise entity relatedness in order to test the 3 http://scikit-learn.org / effectiveness of our method in capturing pairwise similarity between pairs of entities .",0
768,We first describe the details of the training of the embedding and then present the experimental results .,0
769,Training for the Proposed Embedding,0
770,"To train the proposed embedding , we used the December 2014 version of the Wikipedia dump 4 .",0
771,"We first removed the pages for navigation , maintenance , and discussion , and used the remaining 4.9 million pages .",0
772,We parsed the Wikipedia pages and extracted text and anchors from each page .,0
773,We further tokenized the text using the Apache OpenNLP tokenizer .,0
774,We also filtered out rare words that appeared fewer than five times in the corpus .,0
775,We thus obtained approximately 2 billion tokens and 73 million anchors .,0
776,"The total number of words and entities in the embedding were approximately 2.1 million and 5 million , respectively .",0
777,"Consequently , the number of rows of matrices V and U were 7.1 million .",0
778,The number of dimensions d of the embedding was set to 500 .,0
779,"Following , we also used learning rate ? = 0.025 which linearly decreased with the iterations of the Wikipedia dump .",0
780,"Regarding the other parameters , we set the size of the context window c = 10 and the negative samples g = 30 .",0
781,The model was trained online by iterating over pages in the Wikipedia dump 10 times .,0
782,The training lasted approximately five days using a server with a 40 - core CPU on Amazon EC2 .,0
783,Entity Relatedness,0
784,"To test the quality of the vector representation of entities , we conducted an experiment using a dataset for entity relatedness created by Ceccarelli et al ..",0
785,"The dataset consists of training , test , and validation sets , and we only use the test set .",0
786,"The test set contains 3,314 entities , where each entity has 91 candidate entities with gold - standard labels indicating whether the two entities are related .",0
787,"Following , we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities , and computed the two standard measures : normalized discounted cumulative gain ( NDCG ) and mean average precision ( MAP ) .",0
788,We adopted WLM as baseline .,0
789,shows the results .,0
790,The score for WLM was obtained from Huang et al . .,0
791,Our method clearly outperformed WLM .,0
792,The results show that our method accurately captures pairwise entity relatedness .,0
793,Named Entity Disambiguation,0
794,Setup,0
795,We now explain our experimental setup for the NED task .,0
796,We tested the performance of our proposed method on two standard NED datasets : the CoNLL dataset and the TAC 2010 dataset .,0
797,The details of these datasets are provided below .,0
798,"Moreover , as with the corpus used in the embedding , we used the December 2014 version of the Wikipedia dump as the referent KB , and to derive the prior probability as well as the entity prior .",0
799,"To find the best parameters for our machine learning model , we ran a parameter search on the CoNLL development set .",0
800,"We used ? = 10 , 000 trees , and tested all combinations of the learning rate ? = { 0.01 , 0.02 , 0.03 , 0.05 } and the maximum depth of the decision trees ? = { 3 , 4 , 5 }.",0
801,"We computed their accuracy on the dataset , and found that the parameters did not significantly affect performance ( 1.0 % at most ) .",0
802,We used ? = 0.02 and ? = 4 which yielded the best performance .,0
803,CoNLL,0
804,The CoNLL dataset is a popular NED dataset constructed by Hoffart et al ..,0
805,"The dataset is based on NER data from the CoNLL 2003 shared task , and consists of training , development , and test sets , containing 946 , 216 , and 231 documents , respectively .",0
806,We trained our machine learning model using the training set and reported its performance using the test set .,0
807,We also used the development set for the parameter tuning described above .,0
808,"Following , we only used 27,816 mentions with valid entries in the KB and reported the standard micro - ( aggregates overall mentions ) and macro - ( aggregates overall documents ) accuracies of the top - ranked candidate entities to assess disambiguation perfor - mance .",0
809,"For candidate generation , we use the following two resources :",0
810,"1 ) a public dataset recently built by ( denoted by PPRforNED ) for the sake of compatibility with their state - of - the - art results , and 2 ) a dictionary built using a standard YAGO means relation dataset .",0
811,"Moreover , we used PPRforNED for the parameter tuning of the machine learning model and for error analysis .",0
812,TAC 2010,0
813,The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference ( TAC ),0
814,"5 . The dataset is based on news articles from various agencies and Web log data , and consists of a training and a test set containing 1,043 and 1,013 documents , respectively .",0
815,"Following past work , we used mentions only with a valid entry in the KB , and reported the micro -accuracy score of the top - ranked candidate entities .",0
816,We trained our model using the training set and assessed its performance using the test set .,0
817,"Consequently , we evaluated our model on 1,020 mentions contained in the test set .",0
818,"For candidate generation , we used a dictionary that was directly built from the Wikipedia dump mentioned previously .",0
819,"Similar to past work , we retrieved possible mention surfaces of an entity from ( 1 ) the title of the entity , ( 2 ) the title of another entity redirecting to the entity , and ( 3 ) the names of anchors that point to the entity .",0
820,We retained the top 50 candidates through their entity priors for computational efficiency .,0
821,Comparison with State - of - the - art Methods,0
822,We compared our method with the following recently proposed state - of - the - art methods :,0
823,Hoffart et al. ) is a graphbased approach that finds a dense subgraph of entities in a document to address NED .,0
824,He et al.,0
825,uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED .,0
826,Chisholm and Hachey uses a Wikilinks dataset to improve the performance of NED . 82.5 81.7 - 85.6 84.0 81.0 88.7 - 80.7 91.8,0
827,89.9 -: Accuracy scores of the proposed method and the state - of - the - art methods .,0
828,"Pershina et al. improved NED by modeling coherence using the personalized page rank algorithm , and achieved the best - known accuracy on the CoNLL dataset .",0
829,shows the experimental results of our proposed method .,0
830,Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets .,1
831,"Moreover , we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset .",1
832,"Further , shows the experimental results of our proposed method as well as those of stateof - the - art methods .",0
833,Our method outperformed all the state - of - the - art methods on both datasets .,1
834,Results,0
835,Feature Study,0
836,We conducted a feature study on our method .,0
837,"We began with base features , added various features to our system incrementally , and reported their impact on performance .",0
838,We then introduced our twostep approach to achieve the final results .,0
839,shows the results .,0
840,"Surprisingly , we attained results comparable with those of some state - of - the - art methods on the both datasets by only using base features .",0
841,Adding string similarity features slightly further improved performance .,0
842,We observed significant improvement when adding textual context features based on our proposed embedding .,0
843,Our method outperformed some state - of - the - art methods without using coherence .,0
844,"Further , coherence based on unambiguous entity mentions and our two - step approach significantly improved performance on the CoNLL dataset .",0
845,"However , it did not contribute to performance on the TAC 2010 dataset .",0
846,This was because of the significant difference in the density of entity mentions between the datasets .,0
847,"The CoNLL dataset contains approximately 20 entity mentions per document , but the TAC 2010 only contains approximately one mention per document which is unarguably insufficient to model coherence .",0
848,Error Analysis,0
849,We also conducted an error analysis on the CoNLL test set with candidate generation using PPRforNED dataset .,0
850,"We observed that approximately 48.6 % errors were caused by metonymy mentions ( i.e. , mentions with more than one plausible annotation ) .",0
851,"In particular , our NED method often erred when an incorrect entity was highly popular and exactly matched the mention surface ( e.g. , "" South Africa "" referring to the entity South Africa national rugby union team rather than the entity South Africa ) .",0
852,"This makes sense because our machine learning model uses the popularity statistics of the KB ( i.e. , prior probability and entity prior ) , and the string similarity between the title of the entity and the mention surface .",0
853,This problem is discussed further in .,0
854,"Furthermore , because our method depends on the presence of KB anchors in order to learn entity representation , it arguably fails to learn satisfactory representations of tail entities ( i.e. , entities rarely referred to by anchors ) , thus resulting in disambiguation errors .",0
855,"We discovered that nearly 9.6 % errors were due to referent entities with less than 10 inbound KB anchors , and 4.5 % involved entities with no inbound KB anchor .",0
856,"These errors might be addressed using KB data other than KB anchors , such as the description of the entities and the KB categories in order to avoid dependence on the KB anchors .",0
857,This remains part of our future work .,0
858,Related Work,0
859,Early NED methods addressed the problem as a well - studied word sense disambiguation problem .,0
860,These methods primarily focused on modeling the similarity of textual ( local ) context .,0
861,Most recent stateof - the - art methods focus on modeling coherence among disambiguated entities in the same document .,0
862,These approaches have also been called collective or global approaches in the literature .,0
863,Learning the representations of entities for NED has been addressed in past literature .,0
864,used random walks on KB graphs to construct vector representations of entities and documents to address NED .,0
865,Blanco et al .,0
866,"proposed a method to map entities into the word embedding ( i.e. , Word2 vec ) space using entity descriptions in the KB and applied it for NED .",0
867,He et al. used deep neural networks to compute representations of entities and contexts of mentions directly from the KB .,0
868,"Similarly ,",0
869,Sun et al .,0
870,"proposed a method based on deep neural networks to model representations of mentions , contexts of mentions , and entities .",0
871,"Huang et al . also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method ( i.e. , WLM ) for NED .",0
872,"Further , Hu et al . used hierarchical information in the KB to build entity embedding and applied it to model coherence .",0
873,"Unlike these methods , our proposed approach involves jointly learning vector representations of entities as well as words , hence enabling the accurate computation of the semantic similarity among its items to model both the textual context and coherence .",0
874,"Moreover , Yaghoobzadeh and Schutze ( Yaghoobzadeh and Schtze , 2015 ) addressed an entity typing task by building an embedding of words and entities on a corpus with annotated entities ( i.e. , FACC1 ) using the skip - gram model .",0
875,"Compared to our method , in addition to the significant difference between their task and NED , their embedding does not incorporate the link graph data of KB , which is known to be highly important for NED .",0
876,"Furthermore , in the context of knowledge graph embedding , another tenor of recent works has been published .",0
877,These methods focus on learning vector representations of entities to primarily address the link prediction task that aims to predict anew fact based on existing facts in KB .,0
878,"Particularly ,",0
879,Wang et al .,0
880,"have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task , which is somewhat analogous to our experimental results .",0
881,Conclusions,0
882,"In this paper , we proposed an embedding method to jointly map words and entities into the same continuous vector space .",0
883,Our method enables us to effectively model both textual and global contexts .,0
884,"Further , armed with these context models , our NED method outperforms state - of - the - art NED methods .",0
885,"In future work , we intend to improve our model by leveraging relevant knowledge , such as relations in a knowledge graph ( e.g. , Freebase ) .",0
886,We would also like to seek applications of our proposed embedding other than NED .,0
887,title,0
888,Deep Joint Entity Disambiguation with Local Neural Attention,1
889,abstract,0
890,"We propose a novel deep learning model for joint document - level entity disambiguation , which leverages learned neural representations .",0
891,"Key components are entity embeddings , a neural attention mechanism over local context windows , and a differentiable joint inference stage for disambiguation .",0
892,Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention - entity maps .,0
893,Extensive experiments show that we are able to obtain competitive or stateof - the - art accuracy at moderate computational costs .,0
894,Introduction,0
895,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,1
896,This task is challenging due to the inherent ambiguity between surface form mentions such as names and the entities they refer to .,0
897,This many - to - many ambiguity can often be captured partially by name- entity co-occurrence counts extracted from entity - linked corpora .,0
898,"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .",1
899,"Many stateof - the - art methods aim to combine the benefits of both , which is also the philosophy we follow in this paper .",0
900,What is specific to our approach is that we use embeddings of entities as a common representation to assess local as well as global evidence .,0
901,"In recent years , many text and language understanding tasks have been advanced by neural network architectures .",0
902,"However , despite recent work , competitive ED systems still largely employ manually designed features .",0
903,Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions .,0
904,The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .,1
905,"To the best of our knowledge , our approach is the first to carryout this program with full rigor .",0
906,Contributions and Related Work,0
907,"There is avast prior research on entity disambiguation , highlighted by .",0
908,We will focus hereon a discussion of our main contributions in relation to prior work .,0
909,Entity Embeddings .,0
910,"We have developed a simple , yet effective method to embed entities and words in a common vector space .",0
911,"This follows the popular line of work on word embeddings , e.g. , which was recently extended to entities and ED by .",0
912,"In contrast to the above methods that require data about entity - entity co-occurrences which often suffers from sparsity , we rather bootstrap entity embeddings from their canonical entity pages and local context of their hyperlink annotations .",0
913,This allows for more efficient training and alleviates the need to compile co-linking statistics .,0
914,"These vector representations area key component to avoid hand - engineered features , multiple disambiguation steps , or the need for additional ad hoc heuristics when solving the ED task .",0
915,Context Attention .,0
916,We present a novel attention mechanism for local ED .,0
917,"Inspired by mem-ory networks of and insights of , our model deploys attention to select words that are informative for the disambiguation decision .",0
918,A learned combination of the resulting context - based entity scores and a mention - entity prior yields the final local scores .,0
919,"Our local model achieves better accuracy than the local probabilistic model of , as well as the feature - engineered local model of .",0
920,"As an added benefit , our model has a smaller memory footprint and it 's very fast for both training and testing .",0
921,There have been other deep learning approaches to define local context models for ED .,0
922,"For instance use convolutional neural networks ( CNNs ) and stacked denoising auto - encoders , respectively , to learn representations of textual documents and canonical entity pages .",0
923,Entities for each mention are locally scored based on cosine similarity with the respective document embedding .,0
924,"Ina similar local setting , embed mentions , their immediate contexts and their candidate entities using word embeddings and CNNs .",0
925,"However , their entity representations are restrictively built from entity titles and entity categories only .",0
926,"Unfortunately , the above models are rather ' blackbox ' ( as opposed to ours which reveals the attention focus ) and were never extended to perform joint document disambiguation .",0
927,Collective Disambiguation .,0
928,"Last , a novel deep learning architecture for global ED is proposed .",0
929,"Mentions in a document are resolved jointly , using a conditional random field with parametrized potentials .",0
930,We suggest to learn the latter by casting loopy belief propagation ( LBP ) as a rolled - out deep network .,0
931,"This is inspired by similar approaches in computer vision , e.g. , and allows us to backpropagate through the ( truncated ) message passing , thereby optimizing the CRF potentials to work well in conjunction with the inference scheme .",0
932,Our model is thus trained end - to - end with the exception of the pre-trained word and entity embeddings .,0
933,"Previous work has investigated different approximation techniques , including : random graph walks , personalized PageRank , intermention voting , graph pruning , integer linear programming , or ranking SVMs .",0
934,Mostly connected to our approach is where LBP is used for inference ( but not learning ) in a probabilistic graphical model and where a single round of message passing with attention is performed .,0
935,"To our knowledge , we are one of the first to investigate differentiable message passing for NLP problems .",0
936,Learning Entity Embeddings,0
937,"Ina first step , we propose to train entity vectors that can be used for the ED task ( and potentially for other tasks ) .",0
938,These embeddings compress the semantic meaning of entities and drastically reduce the need for manually designed features or co-occurrence statistics .,0
939,Entity embeddings are bootstrapped from word embeddings and are trained independently for each entity .,0
940,"A few arguments motivate this decision : ( i ) there is no need for entity co-occurrence statistics that suffer from sparsity issues and / or large memory footprints ; ( ii ) vectors of entities in a subset domain of interest can be trained separately , obtaining potentially significant speed - ups and memory savings that would otherwise be prohibitive for large entity KBs ; 1 ( iii ) entities can be easily added in an incremental manner , which is important in practice ; ( iv ) the approach extends well into the tail of rare entities with few linked occurrences ; ( v ) empirically , we achieve better quality compared to methods that use entity cooccurrence statistics .",0
941,Our model embeds words and entities in the same low - dimensional vector space in order to exploit geometric similarity between them .,0
942,We start with a pre-trained word embedding map x : W ?,0
943,Rd that is known to encode semantic meaning of words w ?,0
944,W ; specifically we use word2 vec pretrained vectors .,0
945,"We extend this map to entities E , i.e. x : E ?",0
946,"Rd , as described below .",0
947,We assume a generative model in which words that co-occur with an entity e are sampled from a conditional distribution p ( w |e ) when they are generated .,0
948,"Empirically , we collect word - entity cooccurrence counts # ( w , e ) from two sources : ( i ) the canonical KB description page of the entity ( e.g. entity 's Wikipedia page in our case ) , and ( ii ) the windows of fixed size surrounding mentions of the entity in an annotated corpus ( e.g. Wikipedia hyperlinks in our case ) .",0
949,"These counts define a practical approximation of the above word - entity conditional distribution , i.e.p ( w |e ) ? # ( w , e ) .",0
950,"We call this the "" positive "" distribution of words related to the entity .",0
951,"Next , let q ( w ) be a generic word probability distribution which we use for sampling "" negative "" words unrelated to a specific entity .",0
952,"As in , we choose a smoothed unigram distribution q ( w ) = p ( w ) ?",0
953,for some ? ?,0
954,"( 0 , 1 ) .",0
955,The desired outcome is that vectors of positive words are closer ( in terms of dot product ) to the embedding of entity e compared to vectors of random words .,0
956,Let w + ? p ( w|e ) and w ? ? q ( w ) .,0
957,"Then , we use a max-margin objective to infer the optimal embedding for entity e:",0
958,where ? >,0
959,0 is a margin parameter and [] + is the ReLU function .,0
960,"The above loss is optimized using stochastic gradient descent with projection over sampled pairs ( w + , w ? ) .",0
961,Note that the entity vector is directly optimized on the unit sphere which is important in order to obtain qualitative embeddings .,0
962,We empirically assess the quality of our entity embeddings on entity similarity and ED tasks as detailed in Section 7 and Appendix A .,0
963,"The technique described in this section can also be applied , in principle , for computing embeddings of general text documents , but a comparison with such methods is left as future work .",0
964,Local Model with Neural Attention,0
965,We now explain our local ED approach that uses word and entity embeddings to steer a neural attention mechanism .,0
966,"We build on the insight that only a few context words are informative for resolving an ambiguous mention , something that has been exploited before in .",0
967,Focusing only on those words helps reducing noise and improves disambiguation .,0
968,observe the same problem and adopt the restrictive strategy of removing all non-nouns .,0
969,"Here , we assume that a context word maybe relevant , if it is strongly related to at least one of the entity candidates of a given mention .",0
970,Context Scores .,0
971,Let us assume that we have computed a mention - entity priorp ( e |m ) ( procedure detailed in Section 6 ) .,0
972,"In addition , for each mention m , a pruned candidate set ? ( m ) of at most S entities has been identified .",0
973,"Our model , depicted in , computes a score for each e ? ? ( m ) based on the K-word local context c = {w 1 , . . . , w K } surrounding m , as well as on the prior .",0
974,"It is a composition of differentiable functions , thus it is smooth from input to output , allowing us to easily compute gradients and backpropagate through it .",0
975,Each word w ?,0
976,c and entity e ? ? ( m ) is mapped to its embedding via the pre-trained map x ( cf. Section 3 ) .,0
977,We then compute an unnormalized support score for each word in the context as follows :,0
978,where A is a parameterized diagonal matrix .,0
979,The weight is high if the word is strongly related to at least one candidate entity .,0
980,We often observe that uninformative words ( e.g. similar to stop words ) receive non-negligible scores which add undesired noise to our local context model .,0
981,"As a consequence , we ( hard ) prune to the top R ?",0
982,K words with the highest scores 2 and apply a softmax function on these weights .,0
983,Define the reduced context : c,0
984,"Then , the final attention weights are explicitly",0
985,"Finally , we define a ? - weighted context - based entity - mention score via",0
986,where B is another trainable diagonal matrix .,0
987,We will later use the same architecture for the unary scores of our global ED model .,0
988,Local Score Combination .,0
989,We integrate these context scores with the context - independent scores encoded inp ( e |m ) .,0
990,We find a flexible choice for f to be important and superior to a nave weighted average combination model .,0
991,"We therefore use a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities , which we regularize as suggested in by constraining the sum of squares of all weights in the linear layer .",0
992,We use standard projected SGD for training .,0
993,The same network is also used in Section 5 .,0
994,"Prediction is done independently for each mention mi and context c i by maximizing the ?( e , mi , c i ) score .",0
995,Learning the Local Model .,0
996,Entity and word embeddings are pre-trained as discussed in Section 3 .,0
997,"Thus , the only learnable parameters are the diagonal matrices A and B , plus the parameters off .",0
998,Having few parameters helps to avoid overfitting and to be able to train with little annotated data .,0
999,"We assume that a set of known mention - entity pairs { ( m , e * ) } with their respective context windows have been extracted from a corpus .",0
1000,"For model fitting , we then utilize a max - margin loss that ranks ground truth entities higher than other candidate entities .",0
1001,This leads us to the objective :,0
1002,"g ( e , m ) :",0
1003,where ? >,0
1004,0 is a margin parameter and Dis a training set of entity annotated documents .,0
1005,We aim to find a ? ( i.e. parameterized by ? ) such that the score of the correct entity e * referenced by m is at least a margin ?,0
1006,higher than that of any other candidate entity e.,0
1007,"Whenever this is not the case , the margin violation becomes the experienced loss .",0
1008,Document - Level Deep Model,0
1009,"Next , we address global ED assuming document coherence among entities .",0
1010,"We therefore introduce the notion of a document as consisting of a set of mentions m = m 1 , . . . , m n , along with their context windows c = c 1 , . . . c n .",0
1011,Our goal is to define a joint probability distribution over,0
1012,Each such e selects one candidate entity for each mention in the document .,0
1013,"Obviously , the state space of e grows exponentially in the number of mentions n.",0
1014,where C is a diagonal matrix .,0
1015,"Similar to , the above normalization helps balancing the unary and pairwise terms across documents with different numbers of mentions .",0
1016,"The function value g(e , m , c ) is supposedly high for semantically related sets of entities that also have local support .",0
1017,"The goal of a global ED prediction method is to perform maximum - aposteriori on this CRF to find the set of entities e that maximize g( e , m , c ) .",0
1018,Differentiable Inference .,0
1019,Training and prediction in binary CRF models as the one above is NP - hard .,0
1020,"Therefore , in learning one usually maximizes a likelihood approximation and during operations ( i.e. in prediction ) one may use an approximate inference procedure , often based on message - passing .",0
1021,"Among many challenges of these approaches , it is worth pointing out that weaknesses of the approximate inference procedure are generally not captured during learning .",0
1022,"Inspired by , we use truncated fitting of loopy belief propagation ( LBP ) to a fixed number of message passing iterations .",0
1023,"Our model directly optimizes the marginal likelihoods , using the same networks for learning and prediction .",0
1024,"As noted by , this method is robust to model mis-specification , avoids inherent difficulties of partition functions and is faster compared to double - loop likelihood training ( where , for each stochastic update , inference is run until convergence is achieved ) .",0
1025,Our architecture is shown in .,0
1026,"A neural network with T layers encodes T message passing iterations of synchronous max - product LBP 3 which is designed to find the most likely ( MAP ) entity assignments that maximize g ( e , m , c ) .",0
1027,"We also use message damping , which is known to speed - up and stabilize convergence of message passing .",0
1028,"Formally , in iteration t , mention mi votes for entity candidate e ? ? ( m j ) of mention m j using the normalized log - message mt i?j ( e ) computed as :",0
1029,"Herein the first part just reflects the CRF potentials , whereas the second part is defined as",0
1030,where ? ?,0
1031,"( 0 , 1 ] is a damping factor .",0
1032,"Note that , without loss of generality , we simplify the LBP procedure by dropping the factor nodes .",0
1033,The messages at first iteration ( layer ) are set to zero .,0
1034,"After T iterations ( network layers ) , the beliefs ( marginals ) are computed as :",0
1035,"Similar to the local case , we obtain accuracy improvement when combining the mention - entity priorp ( e |m ) with marginal i ( e ) using the same non-linear combination function f from Equation 6 as follows :",0
1036,"The learned function f for global ED is nontrivial ( see , showing that the influence of the prior tends to weaken for larger ( e ) , whereas it has a dominating influence whenever the document - level evidence is weak .",0
1037,We also experimented with the prior integrated directly inside the unary factors ?,0
1038,"i ( e i ) , but results were worse because , in some cases , the global entity interaction is notable to recover from strong incorrect priors ( e.g. country names have a strong prior towards the respective countries as opposed to national sports teams ) .",0
1039,"Parameters of our global model are the diagonal matrices A , B , C and the weights of the f network .",0
1040,"As before , we find a margin based objective to be the most effective and we suggest to fit parameters by minimizing a ranking loss 4 defined as :",0
1041,"Computing this objective is trivial by running T times the steps described by Eqs. ( 10 ) , ( 11 ) , followed in the end by the step in Eq. ( 13 ) .",0
1042,Each step is differentiable and the gradient of the model parameters can be computed on the resulting marginals and back - propagated over messages using chain rule .,0
1043,"At test time , marginals ? i ( e ) are computed jointly per document using this network , but prediction is done independently for each mention mi by maximizing its respective marginal score .",0
1044,Candidate Selection,0
1045,We use a mention - entity priorp ( e |m ) both as a feature and for entity candidate selection .,0
1046,It is 4 Optimizing a marginal log - likelihood loss function performed worse ..,0
1047,WLM is a well - known similarity method of computed by averaging probabilities from two indexes build from mention entity hyperlink count statistics from Wikipedia and a large Web corpus .,0
1048,"Moreover , we add the YAGO dictionary of , where each candidate receives a uniform prior .",0
1049,"Candidate selection , i.e. construction of ? ( e ) , is done for each input mention as follows : first , the top 30 candidates are selected based on the prior p ( e |m ) .",0
1050,"Then , in order to optimize for memory and run time ( LBP has complexity quadratic in S ) , we keep only 7 of these entities based on the following heuristic : ( i ) the top 4 entities based on p ( e |m ) are selected , ( ii ) the top 3 entities based on the local context - entity similarity measured using the function from Eq. 5 are selected .",0
1051,"5 . We refrain from annotating mentions without any candidate entity , implying that precision and recall can be different in our case .",0
1052,"Ina few cases , generic mentions of persons ( e.g. "" Peter "" ) are coreferences of more specific mentions ( e.g. "" Peter Such "" ) from the same document .",0
1053,"We employ a simple heuristic to address this issue : for each mention m , if there exist mentions of persons that contain m as a continuous subse - Methods AIDA - B Local models priorp ( e|m ) 71.9 86.4 87.9 87.2 our ( local , K=100 , R = 50 ) 88.8 Global models 86.6 ( 87.6 88.7 89.0 91.0 91.5 our 92.22 0.14 quence of words , then we consider the merged set of the candidate sets of these specific mentions as the candidate set for the mention m .",0
1054,We decide that a mention refers to a person if it s most probable candidate byp ( e |m ) is a person .,0
1055,Experiments,0
1056,ED Datasets,0
1057,We validate our ED models on some of the most popular available datasets used by our predecessors 6 .,0
1058,We provide statistics in .,0
1059,AIDA - CoNLL dataset is one of the biggest manually annotated ED datasets .,0
1060,"It contains training ( AIDA - train ) , validation ( AIDA - A ) and test ( AIDA - B ) sets .",0
1061,"MSNBC ( MSB ) , AQUAINT ( AQ ) and ACE2004 ( ACE ) datasets cleaned and updated by WNED - WIKI ( WW ) and WNED - CWEB ( CWEB ) : are larger , but automatically extracted , thus less reliable .",0
1062,Are built from the ClueWeb and Wikipedia corpora by .,0
1063,Training Details and ( Hyper ) Parameters,0
1064,We explain training details of our approach .,0
1065,All models are implemented in the Torch framework .,1
1066,Entity Vectors Training & Relatedness Evaluation .,0
1067,"For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .",1
1068,Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .,1
1069,We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .,1
1070,"Subsequently , Wikipedia hyperlinks of the respective entities are used for learning until validation score ( described below ) stops improving .",0
1071,"In each iteration , 20 positive words , each with 5 negative words , are sampled and used for optimization as explained in Section 3 .",0
1072,We use Adagrad with a learning rate of 0.3 .,1
1073,"We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .",1
1074,We remove stop words before training .,0
1075,"Since our method allows to train the embedding of each entity independently of other entities , we decide for efficiency reasons ( and without loss of generality ) to learn only the vectors of all entities appearing as mention candidates in all the test datasets described in Sec. 7.1 , a total of 270000 entities .",0
1076,Training of those takes 20 hours on a single TitanX GPU with 12 GB of memory .,1
1077,We test and validate our entity embeddings on the entity relatedness dataset of .,0
1078,It contains 3319 and 3673 queries for the test and validation sets .,0
1079,Each query consist of one target entity and up to 100 candidate entities with gold standard binary labels indicating if the two entities are related .,0
1080,The associated task requires ranking of related candidate entities higher than the others .,0
1081,"Following previous work , we use different evaluation metrics : normalized discounted cumulative gain ( NDCG ) and mean average precision ( MAP ) .",0
1082,The validation score used during learning is then the sum of the four metrics showed in .,0
1083,We perform candidate ranking based on cosine similarity of entity pairs .,0
1084,Local and Global Model Training .,0
1085,"Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .",1
1086,"We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .",1
1087,Variable size mini-batches consisting of all mentions in a document are used during training .,0
1088,We remove stop words .,0
1089,"Hyper- parameters of the best validated global model are : ? = 0.01 , K = 100 , R = 25 , S = 7 , ? = 0.5 , T = 10 .",0
1090,"For the local model , R = 50 was best .",0
1091,Validation accuracy is computed after each 5 epochs .,0
1092,"To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .",1
1093,"Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .",1
1094,"By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .",0
1095,This is necessary to avoid overfitting when learning from a very small training set .,0
1096,"We also experimented with diagonal plus low - rank matrices , but encountered quality degradation .",0
1097,Entity Similarity Results,0
1098,Results for the entity similarity task are shown in .,0
1099,Our method outperforms the well established Wikipedia link measure and the method of using less information ( only word - entity statistics ) .,0
1100,We note that the best result on this dataset was reported in the unpublished work of .,0
1101,"Their entity embeddings are trained on many more sources of information ( e.g. KG links , relations , entity types ) .",0
1102,"However , our focus was to prove that lightweight trained embeddings useful for the ED task can also perform decently for the entity sim - : ED accuracy on AIDA - B for our best system splitted by Wikipedia hyperlink frequency and mention prior of the ground truth entity , in cases where the gold entity appears in the candidate set .",0
1103,ilarity task .,0
1104,"We emphasize that our global ED model outperforms Huang 's ED model , likely due to the power of our local and joint neural network architectures .",0
1105,"For example , our attention mechanism clearly benefits from explicitly embedding words and entities in the same space .",0
1106,ED Baselines & Results,0
1107,We compare with systems that report state - of - theart results on the datasets .,0
1108,Some baseline scores from are taken from .,0
1109,The best results for the AIDA datasets are reported by and .,0
1110,"We do not compare against since , as noted also by , their mention index artificially includes the gold entity ( guaranteed gold recall ) , which is not a realistic setting .",0
1111,"For a fair comparison with prior work , we use in - KB accuracy and micro F1 ( averaged per mention ) metrics to evaluate our approach .",0
1112,Results are shown in .,0
1113,"We run our system 5 times , each time we pick the best model on the validation set , and report results on the test set for these models .",0
1114,We obtain state of the art accuracy on AIDA which is the largest and hardest ( by the accuracy of thep ( e |m ) baseline ) manually created ED dataset .,1
1115,We are also competitive on the other datasets .,0
1116,"It should be noted that all the other methods use , at least partially , engineered features .",0
1117,"The merit of our proposed method is to show that , with the exception of thep ( e |m ) feature , a neural network is able to learn the best features for ED without requiring expert input .",0
1118,"To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .",1
1119,shows that our method performs well in these harder cases . :,1
1120,Examples of context words selected by our local attention mechanism .,0
1121,Distinct words are sorted decreasingly by attention weights and only words with non-zero weights are shown .,0
1122,Hyperparameter Studies,0
1123,"In , we analyze the effect of two hyperparameters .",0
1124,"First , we see that hard attention ( i.e. R < K ) helps reducing the noise from uninformative context words ( as opposed to keeping all words when R = K ) .",0
1125,"Second , we see that a small number of LBP iterations ( hard - coded in our network ) is enough to obtain good accuracy .",0
1126,This speeds up training and testing compared to traditional methods that run LBP until convergence .,0
1127,An explanation is that a truncated version of LBP can perform well enough if used at both training and test time .,0
1128,Qualitative Analysis of Local Model,0
1129,In we show some examples of context words attended by our local model for correctly solved hard cases ( where the mention prior of the correct entity is low ) .,0
1130,One can notice that words relevant for at least one entity candidate are chosen by our model inmost of the cases .,0
1131,Error Analysis,0
1132,We analyse some of the errors made by our model on the AIDA - B dataset .,0
1133,"We mostly observe three situations : i ) annotation errors , i i ) gold entities that do not appear in mentions ' candidate sets , or iii ) gold entities with very low p ( e |m ) prior whose mentions have an incorrect entity candidate with high prior .",0
1134,"For example , the mention "" Italians "" refers in some specific context to the entity "" Italy national football team "" rather than the entity representing the country .",0
1135,The contextual information is not strong enough in this case to avoid an incorrect prediction .,0
1136,"On the other hand , there are situations where the context can be misleading , e.g. a document heavily discussing about cricket will favor resolving the mention "" Australia "" to the entity "" Australia national cricket team "" instead of the gold entity "" Australia "" ( naming a location of cricket games in the given context ) .",0
1137,Conclusion,0
1138,"We have proposed a novel deep learning architecture for entity disambiguation that combines entity embeddings , a contextual attention mechanism , an adaptive local score combination , as well as unrolled differentiable message passing for global inference .",0
1139,"Compared to many other methods , we do not rely on hand - engineered features , nor on an extensive corpus for entity co-occurrences or relatedness .",0
1140,"Our system is fully differentiable , although we chose to pre-train word and entity embeddings .",0
1141,Extensive experiments show the competitiveness of our approach across a wide range of corpora .,0
1142,"In the future , we would like to extend this system to perform nil detection , coreference resolution and mention detection .",0
1143,Our code and data are publicly available : http://github.com/dalab/deep-ed,1
1144,title,0
1145,LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT,0
1146,Work done as a Google AI Resident,0
1147,abstract,0
1148,"Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .",1
1149,"Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity - centric , we investigate the use of a fill - in - the - blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned .",0
1150,"We show that large scale training of neural models allows us to learn high quality entity representations , and we demonstrate successful results on four domains : ( 1 ) existing entity - level typing benchmarks , including a 64 % error reduction over previous work on TypeNet ; ( 2 ) a novel few - shot category reconstruction task ; ( 3 ) existing entity linking benchmarks , where we match the state - of - the - art on CoNLL - Aida without linking - specific features and obtain a score of 89.8 % on TAC - KBP 2010 without using any alias table , external knowledge base or in domain training data and ( 4 ) answering trivia questions , which uniquely identify entities .",0
1151,"Our global entity representations encode fine - grained type categories , such as Scottish footballers , and can answer trivia questions such as Who was the last inmate of Spandau jail in Berlin ?",0
1152,INTRODUCTION,0
1153,A long term goal of artificial intelligence has been the development and population of an entitycentric representation of human knowledge .,0
1154,Efforts have been made to create the knowledge representation with knowledge engineers or crowdsourcers .,0
1155,"However , these methods have relied heavily on human definitions of their ontologies , which are both limited in scope and brittle in nature .",0
1156,"Conversely , due to recent advances in deep learning , we can now learn robust general purpose representations of words and contextualized phrases directly from large textual corpora .",0
1157,"In particular , we observe that existing methods of building contextualized phrase representations capture a significant amount of local semantic context .",0
1158,"We hypothesize that by learning an entity encoder which aggregates all of the textual contexts in which an entity is seen , we should be able to extract and condense general purpose knowledge about that entity .",0
1159,Consider the following contexts in which an entity mention has been replaced a [ MASK ] :,0
1160,". . . the second woman in space , 19 years after .",0
1161,". . . , a Russian factory worker , was the first woman in space . . . . . . , the first woman in space , entered politics . . . .",0
1162,"As readers , we understand that first woman in space is a unique identifier , and we are able to fill in the blank unambiguously .",0
1163,"The central hypothesis of this paper is that , by matching entities to the contexts in which they are mentioned , we should be able to build a representation for Valentina Tereshkova that encodes the fact that she was the first woman in space , that she was a politician , etc. and that we should be able to use these representations across a wide variety of downstream entity - centric tasks .",0
1164,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .",1
1165,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .",1
1166,"Through these experiments , we show that :",0
1167,RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .,0
1168,"We demonstrate significant improvements over previous work on established benchmarks , including a 64 % error reduction in the TypeNet low data setting .",0
1169,We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .,0
1170,Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .,0
1171,"On the CoNLL - Aida benchmark , RELIC achieves a 94.9 % accuracy , matching the state - of - the - art of , despite not using any entity linking - specific features .",0
1172,"On the TAC - KBP 2010 benchmark RELIC achieves 89.8 % accuracy , just behind the top ranked system , which makes use of external knowledge bases , alias tables , and taskspecific hand - engineered features .",0
1173,"RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .",0
1174,"For entity linking , the opposite is true .",0
1175,"We can treat the RELIC embedding matrix as a store of knowledge , and retrieve answers to questions through nearest neighbor search .",0
1176,We show that this approach correctly answers 51 % of the questions in the TriviaQA reading comprehension task despite not using the task 's evidence text at inference time .,0
1177,"The questions answered correctly by RELIC are surprisingly complex , such as Who was the last inmate of Spandau jail in Berlin ?",0
1178,RELATED WORK,0
1179,Entity linking,0
1180,The most widely studied entity - level task is entity linking - mapping each entity mention onto a unique entity identifier .,0
1181,"The Wikification task , in particular , is similar to the work presented in this paper , as it requires systems to map mentions to the Wikipedia pages describing the entities mentioned .",0
1182,"There is significant previous work that makes use of neural context and entity encoders in downstream entity linking systems , but that previous work focuses solely on discriminating between entities that match a given mention according to an external alias table .",0
1183,Here we go further in investigating the degree to which RELIC can capture world knowledge about entities .,0
1184,Mention - level entity typing,0
1185,Another well studied task is mention - level entity typing ( e.g. .,0
1186,"In this task , entities are labeled with types that are supported by the immediate textual context .",0
1187,"For example , given the sentence ' Michelle Obama attended her book signing ' , Michelle Obama should be assigned the type author but not lawyer .",0
1188,"Subsequently , mention - level entity typing systems make use of contextualized representations of the entity mention , rather than the global entity representations that we focus on here .",0
1189,Entity - level typing,0
1190,"An alternative notion of entity typing is entity - level typing , where each entity should be associated with all of the types supported by a corpus .",0
1191,"and introduce entity - level typing tasks , which we describe more in Section 5.2 .",0
1192,"Entity - level typing is an important task in information extraction , since most common ontologies make use of entity type systems .",0
1193,Such tasks provide a strong method of evaluating learned global representations of entities .,0
1194,Using knowledge bases,0
1195,"There has been a strong line of work in learning representations of entities by building knowledge base embeddings , and by jointly embedding knowledge bases and information from textual mentions .",0
1196,"extended this work to the SPADES fill - in - the - blank task , which is a close counterpart to RELIC 's training setup .",0
1197,"However , we note that all examples in SPADES correspond to a fully connected sub - graph in Freebase .",0
1198,"Subsequently , the contents are very limited in domain and show that it is essential to use the contents of Freebase to do well on this task .",0
1199,"We consider the unconstrained Trivia QA task , introduced in Section 5.5 , to be a better evaluation for open domain knowledge representations .",0
1200,Fill - in - the - blank tasks,0
1201,"There has been significant previous work in using fill - in - the - blank losses to learn context independent word representations , and context - dependent word and phrase representations .",0
1202,"Cloze - style tasks , in which a system must choose which of a few entities best fill a blanked out span , have also been proposed as a method of evaluating reading comprehension .",0
1203,"For entities , Long et al .",0
1204,"consider a similar fill - in - the - blank task as ours , which they frame as rare entity prediction .",0
1205,and train entity representations using a fill - in - the - blank style loss and a bag - of - words representation of mention contexts .,0
1206,"in particular take an approach that is very similar in motivation to RELIC , but which focuses on learning entity representations for use as features in downstream classifiers that model non-linear interactions between a small number of candidate entities .",0
1207,"In Section 5.4 , we show that Yamada et al .",0
1208,"Our training data is a corpus of ( context , entity ) pairs",0
1209,identifies an entity that corresponds to the single entity mention in x i .,0
1210,We train RELIC to correctly match the entities in D to their mentions .,0
1211,"We will experiment with settings where the mentions are unchanged from the original corpus , as well as settings wherewith some probability m ( the mask rate ) all of the words in the mention have been replaced with the uninformative [ MASK ] symbol .",0
1212,We hypothesize that this parameter will play a role in the effectiveness of learned representations in downstream tasks .,0
1213,"For clean training data , we extract our corpus from English Wikipedia 1 .",0
1214,See Section 4 for details .,0
1215,CONTEXT ENCODER,0
1216,"We embed each context in D into a fixed length vector using a Transformer text encoder , initialized with parameters from the BERT - base model released by .",0
1217,All parameters are then trained further using the objective presented below in Section 3.4 .,0
1218,"We take the output of the Transformer corresponding to the initial [ CLS ] token in BERT 's sequence representation as our context encoding , and we linearly project this into Rd using a learned weight matrix W ?",0
1219,R d768 to get a context embedding in the same space as our entity embeddings .,0
1220,ENTITY EMBEDDINGS,0
1221,Each entity e ?,0
1222,E has a unique and abstract Wikidata QID 2 .,0
1223,RELIC maps these unique IDs directly onto a dedicated vector in Rd via a | E | d dimensional embedding matrix .,0
1224,"In our experiments , we have a distinct embedding for every concept that has an English Wikipedia page , resulting in 5 m entity embeddings overall .",0
1225,RELIC TRAINING LOSS,0
1226,"RELIC optimizes the parameters of the context encoder and entity embedding table to maximize the compatibility between observed ( context , entity ) pairs .",0
1227,Let g ( x ) ?,0
1228,"Rd be a context encoder , and let f ( e ) ?",0
1229,Rd bean embedding function that maps each entity to its d dimensional representation via a lookup operation .,0
1230,We define a compatibility score between the entity e and the context x as the scaled cosine similarity 3,0
1231,"where the scaling factor a is a learned parameter , following .",0
1232,"Now , given a context x , the conditional probability that e was the entity seen with x is defined as",0
1233,and we train RELIC by maximizing the average log probability 1 | D |,0
1234,"In practice , the definition of probability in Equation 2 is prohibitively expensive for large | E| ( we use | E | ? 5 M ) .",0
1235,"Therefore , we use a noise contrastive loss .",0
1236,"We sample K negative entities from a noise distribution p noise ( e ) : e 1 , e 2 , . . . , e K ? p noise ( e ) ( 4 ) Denoting e 0 := e , we then compute our per-example loss using cross entropy :",0
1237,"In practice , we train our model with minibatch gradient descent and use all other entries in the batch as negatives .",0
1238,"That is , in a batch of size 4 , entities for rows 1 , 2 , 3 will be used as negatives for row 0 .",0
1239,This is roughly equivalent top noise ( e ) being proportional to entity frequency .,0
1240,EXPERIMENTAL SETUP,0
1241,"To train RELIC , we obtain data from the 2018 - 10 - 22 dump of English Wikipedia .",1
1242,We take E to be the set of all entities in Wikipedia ( of which there are over 5 million ) .,0
1243,"For each occurrence of a hyperlink , we take the context as the surrounding sentence , replace all tokens in the anchor text with a single [ MASK ] symbol with probability m ( see Section 5.3 fora discussion of different masking rates ) and set the ground truth to be the linked entity .",0
1244,We limit each context sentence to 128 tokens .,1
1245,"In this way , we collect a high - quality corpus of over 112M ( context , entity ) pairs .",0
1246,"Note in particular that an entity never co-occurs with text on its own Wikipedia page , since a page will not hyperlink to itself .",0
1247,We set the entity embedding size to d = 300 .,1
1248,We train the model using Tensor Flow,1
1249,EVALUATION,0
1250,We evaluate RELIC 's ability to : ( 1 ) solve the entity linking task without access to any task specific alias tables or features ;,0
1251,( 2 ) accurately capture entity properties that have been hand - coded into TypeNet and Wikipedia categories ;,0
1252,( 3 ) capture trivia knowledge specific to individual entities .,0
1253,"First we present results on established entity linking and entity typing tasks , to compare RELIC 's performance to established baselines and we show that the choice of masking strategy ( Section 3 ) has a significant and opposite impact on performance on these tasks .",0
1254,"We hypothesize that RELIC is approaching an upper bound on established entity - level typing tasks , and we introduce a much harder category completion task that uses RELIC to populate complex Wikipedia categories .",0
1255,"We also apply RELIC 's context encoder and entity embeddings to the task of end - to - end trivia question answering , and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems .",0
1256,ENTITY LINKING,1
1257,RELIC can be used to directly solve the entity linking problem .,0
1258,We just need to find the single entity that maximizes the cosine similarity in Equation 1 fora given context .,0
1259,"For the entity linking task , we create a context from the document 's first 64 tokens as well as the 64 tokens around the mention to be linked .",0
1260,"This choice of context is well suited to the documents in the CoNLL - Aida and TAC - KBP 2010 datasets , since those documents tend to be news articles in which the introduction is particularly information dense .",0
1261,In we show performance for RELIC in two settings .,0
1262,"First , we report the accuracy for the pure RELIC model with no in - domain tuning .",0
1263,"Then , we report the accuracy fora RELIC model that has been tuned on the CoNLL - Aida training set .",0
1264,"On the CoNLL - Aida benchmark , we also adopt a standard alias table for this tuned model , as is commonly done in previous entity linking work .",0
1265,It is clear that for the CoNLL - Aida benchmark in - domain tuning is essential .,0
1266,We hypothesize that this is because of the dataset 's bias towards certain types of news content that is very unlike our Wikipedia pre-training data - specifically sports reports .,0
1267,"However , when we do adopt the standard CoNLL - Aida training set and alias table , RELIC matches the state of the art on this benchmark , despite using far fewer hand engineered resources use the large Wikidata knowledge base to create entity representations ) .",1
1268,"We do not make use of the TAC - KBP 2010 training set or alias table , and we observe that RELIC is already competitive without these enhancements 5",0
1269,"It is significant that RELIC matches the performance of , which uses the large hand engineered Wikidata knowledge base to represent entities .",0
1270,This supports our central hypothesis that it is possible to capture the knowledge that has previously been manually encoded 4 Our finetuned CoNLL result uses the alias table of at inference time .,0
1271,We do reduce the candidate set from the 5 m entities covered by RELIC to the 818 k entities in the TAC - KBP 2010 knowledge base to avoid ontological misalignment .,0
1272,System,0
1273,Type Net Type Net - Low Data ( 5 % ) 78.6 58.8 RELIC 90.1 85.3 : Mean Average Precision on Type Net tasks .,0
1274,RELIC 's gains are particularly striking in the low data setting from .,0
1275,"in knowledge bases , using entity embeddings learned from textual contexts alone .",0
1276,"In the rest of this section , we will show further support for our hypothesis by recreating parts of the Freebase and Wikipedia ontologies , and by using RELIC to answer trivia questions .",0
1277,"Finally , we believe that RELIC 's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features .",1
1278,"As shown in , use a small set of well chosen discrete features to increase the performance of their embedding based approach by 10 points .",0
1279,"These features could be simply integrated into RELIC 's model , but we consider them to be orthogonal to this paper 's investigation of purely learned representations .",0
1280,F1,0
1281,P@1 Acc 82.3 91.0 56.5 RELIC 87.9 94.8 68.3 RELIC with 5 % of FIGMENT training data 83.3 90.9,0
1282,59.3 : Performance on FIGMENT .,0
1283,"We report P@1 ( proportion of entities whose top ranked types are correct ) , Micro F1 aggregated overall ( entity , type ) compatibility decisions , and overall accuracy of entity labeling decisions .",0
1284,"RELIC outperforms prior work , even with only 5 % of the training data .",1
1285,ENTITY - LEVEL FINE TYPING,1
1286,"We evaluate RELIC 's ability to capture entity properties on the FIGMENT and TypeNet entity - level fine typing tasks which contain 102 and 1,077 types drawn from the Freebase ontology .",0
1287,The task in both datasets is to predict the set of fine - grained types that apply to a given entity .,0
1288,We train a simple 2 - layer feedforward network that takes as input RELIC 's embedding f ( e ) of the entity e and outputs a binary vector indicating which types apply to that entity .,0
1289,show that RELIC significantly outperforms prior results on both datasets .,1
1290,"For FIGMENT , is an ensemble of several standard representation learning techniques : word2 vec skip - gram contexts , structured skip - gram contexts , and FastText representations of the entity names .",0
1291,"For TypeNet , aggregate mention - level types and train with a structured loss based on the TypeNet hierarchy , but is still outperformed by our flat classifier of binary labels .",1
1292,We expect that including a hierarchical loss is orthogonal to our approach and could improve our results further .,0
1293,The most striking results in,0
1294,EFFECT OF MASKING,1
1295,"In Section 3 we introduced the concept of masking entity mentions , and predicting on the basis of the context in which they are discussed , not the manner in which they are named .",0
1296,show the effect of training RELIC with different mask rates .,0
1297,"It is clear that masking mentions during training is beneficial for entity typing tasks , but detrimental for entity linking .",1
1298,This is in accordance with our intuitions .,0
1299,"Modeling mention surface forms is essential for linking , since these mentions are given attest time and names are extremely discriminative .",0
1300,"However , once the mention is known the model only needs to distinguish between different entities with the same name ( e.g. President Washington , University of Washington , Washington State ) and this distinction rarely requires deep knowledge of each entity 's properties .",0
1301,"Subsequently , our best typing models are those that are forced : TypeNet entity - level typing m AP on the development set for RELIC models trained with different masking rates .",0
1302,"A higher mask rate leads to better performance , both in low and high - data situations .",1
1303,"to capture more of the context in which each entity is mentioned , because they are not allowed to rely on the mention itself .",0
1304,The divergence between the trends in suggests that there may not be one set of entity embeddings that are optimum for all tasks .,0
1305,"However , we would like to point out that that a mask rate of 10 % , RELIC nears optimum performance on most tasks .",1
1306,"The optimum mask rate is an open research question , that will likely depend on entity frequency as well as other data statistics .",0
1307,FEW - SHOT CATEGORY COMPLETION,1
1308,The entity - level typing tasks discussed above involve an in - domain training step .,0
1309,"Furthermore , due to the incompleteness of the the FIGMENT and TypeNet type systems , we also believe that RELIC 's performance is approaching the upper bound on both of these supervised tasks .",1
1310,"Therefore , to properly measure RELIC 's ability to capture complex types from fill - in - the - blank training alone , we propose :",0
1311,"1 . anew category completion task that does not involve any task specific optimization , 2 . anew Wikipedia category based evaluation set that contains much more complex compound types , such as Scottish footballers , We use this new task to compare RELIC to the embeddings learned by .",0
1312,"In the new category completion task , we represent each category by randomly sampling three exemplar entities , and calculating the centroid of their RELIC embeddings .",0
1313,"We then rank all other entities according to their dot -product with this centroid , and report the mean average precision ( MAP ) of the resultant ranking .",0
1314,"First , we apply this evaluation to the TypeNet type system introduced in .",0
1315,"These types are well - curated , but tend to represent high - level categories .",0
1316,"To measure the degree to which our entity embeddings capture finer grained type information , we construct an aditional dataset based on Wikipedia categories 6 .",0
1317,"These tend to be compound types , such as Actresses from London , which capture many aspects of an entity - in this case gender , profession , and place of birth .",0
1318,From 45.0 -: Answer exact match on Trivia QA .,0
1319,"RELIC 's fast nearest neighbor search over entities achieves 80 % of the performance of ORQA , which runs a BERT - based reading comprehesion model over multiple retrieved evidence passages .",0
1320,"Unlike ORQA and RELIC , the classifier baseline and SLQA have access to a single evidence document that is known to contain the answer .",0
1321,As a result they are solving a much easier task .,0
1322,beddings capture entities which are much closer to the exemplars .,0
1323,"In fact , we identify several false negatives in these examples .",0
1324,TRIVIA QUESTION ANSWERING,1
1325,Our final experiment tests RELIC 's ability to answer trivia questions - which can be considered high precision categories that only apply to a single entity - using retrieval of encoded entities .,0
1326,"Trivia QA ) is a question - answering dataset containing questions sourced from trivia websites , and the answers are usually entities with Wikipedia pages .",0
1327,"The standard Trivia QA setup is a reading comprehension task , where answers are extracted from evidence documents .",0
1328,"Here , we answer questions in Trivia QA without access to any evidence documents attest time .",0
1329,"Model and training Given a question , we apply the context encoder g from Section 3.4 , and retrieve 1 out of 5 M entities using cosine similarity .",0
1330,"For training , we initialize both g and f from RELIC training .",0
1331,"We tune only g's parameters by optimizing the loss in Equation 5 applied to ( question , answer entity ) pairs , rather than the ( context , entity ) pairs seen during RELIC 's training .",0
1332,Results,0
1333,Trivia,0
1334,"QA results are shown in , and randomly sampled RELIC predictions are illustrated in .",0
1335,All systems other than RELIC in have access to evidence text at inference time .,0
1336,"In the open domain unfiltered setting , ORQA retrieves this text from a cache of Wikipedia .",0
1337,"In the more standard verified - web reading comprehension task , the classifier baseline and SLQA are provided with a single document that is known to contain the answer .",0
1338,We consider ORQA to be the most relevant point of comparison for RELIC .,0
1339,We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .,1
1340,"However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .",1
1341,"It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .",1
1342,"We then introduce a novel few - shot category reconstruction task and when comparing to , we found that RELIC is better able to capture complex compound types .",0
1343,"Our method also proves successful for entity linking , where we match the state of the art on CoNLL - Aida despite not using linkingspecific features and fare similarly to the best system on TAC - KBP 2010 despite not using an alias table , any external knowledge bases , linking - specific features or even in - domain training data .",0
1344,"Finally , we show that our RELIC embeddings can be used to answer trivia questions directly , without access to any evidence documents .",0
1345,"We encourage researchers to further explore the properties of our entity representations and BERT context encoder , which we will release publicly .",0
1346,title,0
1347,Deep contextualized word representations,1
1348,abstract,0
1349,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",1
1350,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",0
1351,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",0
1352,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",0
1353,Introduction,0
1354,Pre-trained word representations are a key component in many neural language understanding models .,1
1355,"However , learning high quality representations can be challenging .",0
1356,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",0
1357,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",1
1358,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,1
1359,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,1
1360,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",1
1361,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",1
1362,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",1
1363,Combining the internal states in this manner allows for very rich word representations .,0
1364,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",1
1365,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .",0
1366,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,0
1367,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",0
1368,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",0
1369,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",0
1370,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM .",0
1371,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",0
1372,1,0
1373,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",0
1374,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",0
1375,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",0
1376,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",0
1377,Other recent work has also focused on learning context - dependent representations .,0
1378,context2vec uses a bidirectional Long Short Term Memory ( LSTM ; to encode the context around a pivot word .,0
1379,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,0
1380,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",0
1381,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",0
1382,"We also generalize these approaches to deep contextual representations , which we show work well across abroad range of diverse NLP tasks .",0
1383,1 http://allennlp.org/elmo,0
1384,Previous work has also shown that different layers of deep biRNNs encode different types of information .,0
1385,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing or CCG super tagging .",0
1386,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",0
1387,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",0
1388,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",0
1389,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,0
1390,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",0
1391,Dai and Le,0
1392,Bidirectional language models,0
1393,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k 1 ) :",0
1394,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,0
1395,"At each position k , each LSTM layer outputs a context - dependent representation ! h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ! h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",0
1396,"A backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",0
1397,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations h LM k , j oft k given ( t k+1 , . . . , t N ) .",0
1398,A bi LM combines both a forward and backward LM .,0
1399,Our formulation jointly maximizes the log likelihood of the forward and backward directions :,0
1400,We tie the parameters for both the token representation (? x ) and Softmax layer (? s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction .,0
1401,"Overall , this formulation is similar to the approach of , with the exception that we share some weights between directions instead of using completely independent parameters .",0
1402,"In the next section , we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers .",0
1403,ELMo,0
1404,ELMo is a task specific combination of the intermediate layer representations in the biLM .,0
1405,"For each token t k , a L-layer biLM computes a set of 2L + 1 representations",0
1406,"where h LM k ,0 is the token layer and h LM",0
1407,", for each biLSTM layer .",0
1408,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",0
1409,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",0
1410,"More generally , we compute a task specific weighting of all biLM layers :",0
1411,"( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter task allows the task model to scale the entire ELMo vector .",0
1412,is of practical importance to aid the optimization process ( see supplemental material for details ) .,0
1413,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",0
1414,Using biLMs for supervised NLP tasks,0
1415,"Given a pre-trained biLM and a supervised architecture fora target NLP task , it is a simple process to use the biLM to improve the task model .",0
1416,We simply run the biLM and record all of the layer representations for each word .,0
1417,"Then , we let the end task model learn a linear combination of these representations , as described below .",0
1418,First consider the lowest layers of the supervised model without the biLM .,0
1419,"Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , unified manner .",0
1420,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",0
1421,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",0
1422,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",0
1423,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",0
1424,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",0
1425,"For example , seethe SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",0
1426,"Finally , we found it beneficial to add a moderate amount of dropout to ELMo and in some cases to regularize the ELMo weights by adding kwk 2 2 to the loss .",0
1427,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,0
1428,Pre-trained bidirectional language model architecture,0
1429,"The pre-trained biLMs in this paper are similar to the architectures in and , but modified to support joint training of both directions and add a residual connection between LSTM layers .",0
1430,"We focus on large scale biLMs in this work , as highlighted the importance of using biLMs over forward - only LMs and large scale training .",0
1431,"To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in .",0
1432,The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer .,0
1433,The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation .,0
1434,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",0
1435,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",0
1436,"After training for 10 epochs on the 1B Word Benchmark , the average forward and backward perplexities is 39.7 , compared to 30.0 for the forward CNN - BIG - LSTM .",0
1437,"Generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .",0
1438,"Once pretrained , the biLM can compute representations for any task .",0
1439,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .",0
1440,This can be seen as a type of domain transfer for the biLM .,0
1441,"As a result , inmost cases we used a fine - tuned biLM in the downstream task .",0
1442,See supplemental material for details .,0
1443,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,0
1444,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",0
1445,This is a very general result across a diverse set model architectures and language understanding tasks .,0
1446,In the remainder of this section we provide high - level sketches of the individual task results ; seethe supplemental material for full experimental details .,0
1447,Evaluation,0
1448,Question answering,1
1449,The Stanford Question Answering Dataset ( SQuAD ) contains 100K + crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph .,0
1450,Our baseline model is an improved version of the Bidirectional Attention Flow model in .,0
1451,"It adds a self - attention layer after the bidirectional attention component , simplifies some of the pooling operations and substitutes the LSTMs for gated recurrent units ( GRU s ; .",0
1452,"After adding ELMo to the baseline model , test set F 1 improved by 4.7 % from 81.1 % to 85.8 % , a 24.9 % relative error reduction over the baseline , and improving the overall single model state - of - the - art by 1.4 % .",1
1453,"A 11 member ensemble pushes F 1 to 87.4 , the overall state - of - the - art at time of submission to the leaderboard .",0
1454,The increase of 4.7 % with ELMo is also significantly larger then the 1.8 % improvement from adding CoVe to a baseline model .,1
1455,"Due to the small test sizes for NER and SST - 5 , we report the mean and standard deviation across five runs with different random seeds .",0
1456,"The "" increase "" column lists both the absolute and relative improvements over our baseline .",0
1457,Textual entailment,1
1458,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",0
1459,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,0
1460,"Our baseline , the ESIM sequence model from , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .",0
1461,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",1
1462,"A five member ensemble pushes the overall accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",0
1463,Semantic role labeling,1
1464,"A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",0
1465,"modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",0
1466,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,0
1467,Our baseline model is the end - to - end span - based neural model of .,0
1468,It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,0
1469,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",1
1470,Named entity extraction,1
1471,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",0
1472,"Following recent state - of - the - art systems , the baseline model uses pre-trained word embeddings , a character - based CNN representation , two biLSTM layers and a conditional random field ( CRF ) loss , similar to .",0
1473,"As shown in , our ELMo enhanced biLSTM - CRF achieves 92. 22 % F 1 averaged over five runs .",1
1474,"The key difference between our system and the previous state of the art from is that we allowed the task model to learn a weighted average of all biLM layers , whereas Peters et al .",0
1475,only use the top biLM layer .,0
1476,"As shown in Sec. 5.1 , using all layers instead of just the last layer improves performance across multiple tasks .",0
1477,Sentiment analysis,0
1478,The fine - grained sentiment classification task in the Stanford Sentiment Treebank ( SST - 5 ; involves selecting one of five labels ( from very negative to very positive ) to describe a sentence from a movie review .,0
1479,The sentences contain diverse linguistic phenomena such as idioms and complex syntac -,0
1480,Analysis,0
1481,This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations .,0
1482,"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regardless of whether they are produced from a biLM or MT encoder , and that ELMo representations provide the best overall performance .",0
1483,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers , consistent with MT encoders .",0
1484,It also shows that our biLM consistently provides richer representations then CoVe .,0
1485,"Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec. 5.2 ) , training set size ( Sec. 5.4 ) , and visualize the ELMo learned weights across the tasks ( Sec. 5.5 ) .",0
1486,Alternate layer weighting schemes,0
1487,There are many alternatives to Equation 1 for combining the biLM layers .,0
1488,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an .",0
1489,"The choice of the regularization parameter is also important , as large values such as = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , = 0.001 ) allow the layer weights to vary .",0
1490,"compares these alternatives for SQuAD , SNLI and SRL .",0
1491,"Including representations from all layers improves overall performance over just using the last layer , and including contextual representations from the last layer improves performance over the baseline .",0
1492,"For example , in the case of SQuAD , using just the last biLM layer improves development F 1 by 3.9 % over the baseline .",0
1493,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to = 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( = 1 vs. = 0.001 ) .",0
1494,"A small is preferred inmost cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ( not shown ) .",0
1495,The overall trend is similar with CoVe but with smaller increases over the baseline .,0
1496,"For SNLI , averaging all layers with = 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",0
1497,SRL F 1 increased a marginal 0.1 % to 82.2 for the = 1 case compared to using the last layer only .,0
1498,Where to include ELMo ?,0
1499,All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,0
1500,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves overall results for some tasks .",0
1501,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",0
1502,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",0
1503,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement .:",0
1504,All - words fine grained WSD F 1 .,0
1505,"For CoVe and the biLM , we report scores for both the first and second layer biLSTMs .",0
1506,the task - specific context representations are likely more important than those from the biLM .,0
1507,What information is captured by the biLM 's representations ?,0
1508,Since adding,0
1509,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",0
1510,"Intuitively , the biLM must be disambiguating the meaning of words using their context .",0
1511,"Consider "" play "" , a highly polysemous word .",0
1512,"The top of lists nearest neighbors to "" play "" using GloVe vectors .",0
1513,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",0
1514,"In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM 's context representation of "" play "" in the source sentence .",0
1515,"In these cases , the biLM is able to disambiguate both the part of speech and word sense in the source sentence .",0
1516,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,0
1517,"To isolate the information encoded by the biLM , the representations are used to directly make predictions fora fine grained word sense disambiguation ( WSD ) task and a POS tagging task .",0
1518,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",0
1519,"Word sense disambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",0
1520,"To do so , we first use the biLM to compute representations for all words in Sem - Cor 3.0 , our training corpus , and then take the average representation for each sense .",0
1521,"At test time , we again use the biLM to compute representations fora given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",0
1522,compares WSD results using the evaluation framework from across the same suite of four test sets in .,0
1523,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",0
1524,This is competitive with a state - of - the - art WSD - specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags .,0
1525,"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher overall performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .",0
1526,POS tagging,0
1527,"To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) .",0
1528,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",0
1529,"Similar to WSD , the biLM representations are competitive with carefully tuned , task specific biLSTMs .",0
1530,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",0
1531,"CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder .",0
1532,"Implications for supervised tasks Taken together , these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks .",0
1533,"In addition , the biLM 's representations are more transferable to WSD and POS tagging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks .",0
1534,Sample efficiency,0
1535,"Adding ELMo to a model increases the sample efficiency considerably , both in terms of number of parameter updates to reach state - of - the - art performance and the overall training set size .",0
1536,"For example , the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo .",0
1537,"After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach In addition , ELMo - enhanced models use smaller training sets more efficiently than models without ELMo.",0
1538,compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1 % to 100 % .,0
1539,Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance .,0
1540,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",0
1541,visualizes the softmax - normalized learned layer weights .,0
1542,"At the input layer , the task model favors the first biLSTM layer .",0
1543,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",0
1544,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",0
1545,Visualization of learned weights,0
1546,Contextual vs. sub - word information,0
1547,"In addition to the contextual information captured in the biLM 's biLSTM layers , ELMo representations also contain sub - word information in the fully character based context insensitive type layer , x LM k .",0
1548,"To analyze the relative contribution of the contextual information compared to the sub -word information , we ran an additional ablation that replaced the GloVe vectors with just the biLM character based x LM k layer without the biLM biLSTM layers .",0
1549,"summarizes the results for SQuAD , SNLI and SNLI .",0
1550,"Replacing the Glo Ve vectors with the biLM character layer gives a slight improvement for all tasks ( e.g. from 80.8 to 81.4 F 1 for SQuAD ) , but overall the improvements are small compared to the full ELMo model .",0
1551,"From this , we conclude that most of the gains in the downstream tasks are due to the contextual information and not the sub-word information .",0
1552,Are pre-trained vectors necessary with,0
1553,ELMo?,0
1554,All of the results presented in Sec.4 include pretrained word vectors in addition to ELMo representations .,0
1555,"However , it is natural to ask whether pre-trained vectors are still necessary with high quality contextualized representations .",0
1556,"As shown in the two right hand columns of , adding Glo Ve to models with ELMo generally provides a marginal improvement over ELMo only models ( e.g. 0.2 % F 1 improvement for SRL from 84.5 to 84.7 ) .",0
1557,Conclusion,0
1558,"We have introduced a general approach for learning high - quality deep context - dependent representations from biLMs , and shown large improve- ments when applying ELMo to abroad range of NLP tasks .",0
1559,"Through ablations and other controlled experiments , we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin - context , and that using all layers improves overall task performance .",0
1560,title,0
1561,Word Sense Disambiguation using a Bidirectional LSTM,1
1562,abstract,0
1563,"In this paper we present a clean , yet effective , model for word sense disambiguation .",0
1564,Our approach leverage a bidirectional long short - term memory network which is shared between all words .,0
1565,This enables the model to share statistical strength and to scale well with vocabulary size .,0
1566,"The model is trained end - to - end , directly from the raw text to sense labels , and makes effective use of word order .",0
1567,"We evaluate our approach on two standard datasets , using identical hyperparameter settings , which are in turn tuned on a third set of held out data .",0
1568,"We employ no external resources ( e.g. knowledge graphs , part - of - speech tagging , etc ) , language specific features , or hand crafted rules , but still achieve statistically equivalent results to the best state - of - the - art systems , that employ no such limitations .",0
1569,*,0
1570,Introduction,0
1571,Words are in general ambiguous and can have several related or unrelated meanings depending on context .,0
1572,"For instance , the word rock can refer to both a stone and a music genre , but in the sentence "" Without the guitar , there would be no rock music "" the sense of rock is no longer ambiguous .",0
1573,"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .",1
1574,From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense .,0
1575,"However , it may not be so obvious that this is a difficult task .",0
1576,"To see this , consider instead the phrase "" Solid rock "" where changing the order of words completely changes the meaning , or "" Hard rock crushes heavy metal "" where individual words seem to indicate stone but together they actually define the word token as music .",0
1577,"With this in mind , our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling .",0
1578,"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .",1
1579,"However , though much progress has been made in the area , many current WSD systems suffer from one or two of the following deficits .",0
1580,( 1 ) Disregarding the order of words in the context which can lead to problems as described above .,0
1581,"( 2 ) Relying on complicated and potentially language specific hand crafted features and resources , which is a big problem particularly for resource poor languages .",0
1582,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .",1
1583,Using word embeddings has previously been shown to improve WSD .,0
1584,"However , these works did not consider the order of words or their operational effect on each other .",0
1585,The main contributions of this work include :,0
1586,"A purely learned approach to WSD that achieves results on par with state - of - the - art resource heavy systems , employing e.g. knowledge graphs , parsers , part - of - speech tagging , etc .",0
1587,Parameter sharing between different word types to make more efficient use of labeled data and make full vocabulary scaling plausible without the number of parameters exploding .,0
1588,Empirical evidence that highlights the importance of word order for WSD .,0
1589,"A WSD system that , by using no explicit window , is allowed to combine local and global information when deducing the sense .",0
1590,Background,0
1591,In this section we introduce the most important underlying techniques for our proposed model .,0
1592,Bidirectional LSTM,0
1593,Long short - term memory ( LSTM ) is a gated type of recurrent neural network ( RNN ) .,0
1594,LSTMs were introduced by to enable RNNs to better capture long term dependencies when used to model sequences .,0
1595,This is achieved by letting the model copy the state between timesteps without forcing the state through a non-linearity .,0
1596,The flow of information is instead regulated using multiplicative gates which preserves the gradient better than e.g. the logistic function .,0
1597,"The bidirectional variant of LSTM , ( BLSTM ) is an adaptation of the LSTM where the state at each time step consist of the state of two LSTMs , one going left and one going right .",0
1598,"For WSD this means that the state has information about both preceding words and succeeding words , which in many cases are absolutely necessary to correctly classify the sense .",0
1599,Word embeddings by GloVe,0
1600,Word embeddings is away to represent words as real valued vectors in a semantically meaningful space .,0
1601,"Global Vectors for Word Representation ( GloVe ) , introduced by Pennington et al. is a hybrid approach to embedding words that combine a log - linear model , made popular by , with counting based co-occurrence statistics to more efficiently capture global statistics .",0
1602,"Word embeddings are trained in an unsupervised fashion , typically on large amounts of data , and is able to capture fine grained semantic and syntactic information about words .",0
1603,These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model .,0
1604,The Model,0
1605,"Given a document and the position of the target word , i.e. the word to disambiguate , the model computes a probability distribution over the possible senses corresponding to that word .",0
1606,"The architecture of the model , depicted in , consist of a softmax layer , a hidden layer , and a BLSTM .",0
1607,See Section 2.1 for more details regarding the BLSTM .,0
1608,"The BLSTM and the hidden layer share parameters overall word types and senses , while the softmax is parameterized byword type and selects the corresponding weight matrix and bias vector for each word type respectively .",0
1609,This structure enables the model to share statistical strength across different word types while remaining computationally efficient even fora large total number of senses and realistic vocabulary sizes .,0
1610,Model definition,0
1611,The input to the BLSTM at position n in document Dis computed as,0
1612,"Here , v ( w n ) is the one - hot representation of the word type corresponding tow n ?",0
1613,D. A one - hot representation is a vector with dimension V consisting of | V | ? 1 zeros and a single one which index indicate the word type .,0
1614,This will have the effect of picking the column from W x corresponding to that word type .,0
1615,The resulting vector is referred to as a word embedding .,0
1616,"Further , W x can be initialized using pre-trained word embeddings , to leverage large unannotated datasets .",0
1617,"In this work Glo Ve vectors are used for this purpose , see Section 4.1 for details .",0
1618,"The model output ,",0
1619,"is the predicted distribution over senses for the word at position n , where W ay wn and bay wn are the weights and biases for the softmax layer corresponding to the word type at position n .",0
1620,"Hence , each word type will have its own softmax parameters , with dimensions depending on the number of senses of that particular word .",0
1621,"Further , the hidden layer a is computed as",0
1622,] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM at word n.,0
1623,W ha and b ha are the weights and biases for the hidden layer .,0
1624,Loss function,0
1625,"The parameters of the model , ? = { W x , ?",0
1626,"BLST M , W ha , b ha , { W ay w , bay w } ?w? V , } , are fitted by minimizing the cross entropy error",0
1627,over a set of sense labeled tokens with indices I ?,0
1628,"{ 1 , . . . , | C |} within a training corpus C , each labeled with a target sense ti , ?i ?",0
1629,I.,0
1630,Dropword,0
1631,Dropword is a regularization technique very similar to word dropout introduced by .,0
1632,Both methods are word level generalizations of dropout but in word dropout the word is set to zero while in dropword it is replaced with a < dropped > tag .,0
1633,The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained .,0
1634,"This process is repeated overtime , so that the words dropped changeover time .",0
1635,The motivation for doing dropword is to decrease the dependency on individual words in the training context .,0
1636,"This technique can be generalized to other kinds of sequential inputs , not only words .",0
1637,"To evaluate our proposed model we perform the lexical sample task of SensEval 2 ( SE2 ) and SensEval 3 ( SE3 ) , part of the SensEval workshops organized by Special Interest Group on the Lexicon at ACL .",0
1638,"For both instances of the task training and test data are supplied , and the task consist of disambiguating one indicated word in a context .",0
1639,"The words to disambiguate are sampled from the vocabulary to give a range of low , medium and high frequency words , and a gold standard sense label is supplied for training and evaluation .",0
1640,Experimental settings,0
1641,"The hyperparameter settings used during the experiments , presented in , were tuned on a separate validation set with data picked from the SE2 training set .",0
1642,"The source code , implemented using TensorFlow , has been released as open source 1 .",1
1643,Hyperparameter,0
1644,"Range : Hyperparameter settings used for both experiments and the ranges that were searched during tuning . "" - "" indicates that no tuning were performed on that parameter .",0
1645,Embeddings,0
1646,The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .,1
1647,"Words not included in this set are initialized from N ( 0 , 0.1 ) .",1
1648,To keep the input noise proportional to the embeddings it is scaled by ?,0
1649,"i which is the standard deviation in embedding dimension i for all words in the embeddings matrix , W x . ?",0
1650,i is updated after each weight update .,0
1651,Data preprocessing,0
1652,The only preprocessing of the data that is conducted is replacing numbers with a < number > tag .,0
1653,This result in a vocabulary size of | V | = 50817 for SE2 and | V | = 37998 for SE3 .,0
1654,Words not present in the training set are considered unknown during test .,0
1655,"Further , we limit the size of the context to max 140 words centered around the target word to facilitate faster training .",0
1656,Results,0
1657,The results of our experiments and the state - of - the - art are shown in .,0
1658,100 JHU ( R ) was developed by and achieved the best score on the English lexical sample task of SE2 with a F 1 score of 64.2 .,0
1659,"Their system utilized a rich feature space based on raw words , lemmas , POS tags , bag - of - words , bi-gram , and tri-gram collocations , etc. as inputs to an ensemble classifier .",0
1660,htsa 3 by was the winner of the SE3 lexical sample task with a F 1 score of 72.9 .,0
1661,"This system was based mainly on raw words , lemmas , and POS tags .",0
1662,These were used as inputs to a regularized least square classifier .,0
1663,"IMS + adapted CW is a more recent system , by , that uses separately trained word embeddings as input .",0
1664,"However , it also relies on a rich set of other features including POS tags , collocations and surrounding words to achieve their reported result .",0
1665,Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .,1
1666,"Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .",1
1667,"Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .",1
1668,We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .,1
1669,F1 score,0
1670,Conclusions & future work,0
1671,"We presented a BLSTM based model for WSD that was able to effectively exploit word order and achieve results on state - of - the - art level , using no external resources or handcrafted features .",0
1672,"As a consequence , the model is largely language independent and applicable to resource poor languages .",0
1673,"Further , the system was designed to generalize to full vocabulary WSD by sharing most of the parameters between words .",0
1674,"For future work we would like to provide more empirical evidence for language independence by evaluating on several different languages , and do experiments on large vocabulary all words WSD , where every word in a sentence is disambiguated .",0
1675,"Further , we plan to experiment with unsupervised pre-training of the BLSTM , encouraged by the substantial improvement achieved by incorporating word embeddings .",0
1676,title,0
1677,Semi-supervised Word Sense Disambiguation with Neural Models,1
1678,abstract,0
1679,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,1
1680,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",1
1681,"However , a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text .",0
1682,"In this paper , we study WSD with a sequence learning neural net , LSTM , to better capture the sequential and syntactic patterns of the text .",0
1683,"To alleviate the lack of training data in all - words WSD , we employ the same LSTM in a semi-supervised label propagation classifier .",0
1684,"We demonstrate state - of - the - art results , especially on verbs .",0
1685,This work is licensed under a Creative Commons Attribution 4.0 International License .,0
1686,License details :,0
1687,Introduction,0
1688,Word sense disambiguation ( WSD ) is a long - standing problem in natural language processing ( NLP ) with broad applications .,0
1689,"Supervised , unsupervised , and knowledge - based approaches have been studied for WSD .",0
1690,"However , for all - words WSD , where all words in a corpus need to be annotated with word senses , it has proven extremely challenging to beat the strong baseline , which always assigns the most frequent sense of a word without considering the context .",0
1691,"Given the good performance of published supervised WSD systems when provided with significant training data on specific words , it appears lack of sufficient labeled training data for large vocabularies is the central problem .",0
1692,One way to leverage unlabeled data is to train a neural network language model ( NNLM ) on the data .,0
1693,Word embeddings extracted from such a NNLM ( often Word2 Vec ) can be incorporated as features into a WSD algorithm .,0
1694,show that this can substantially improve WSD performance and indeed that competitive performance can be attained using word embeddings alone .,0
1695,"In this paper , we describe two novel WSD algorithms .",1
1696,The first is based on a Long Short Term Memory ( LSTM ) ) .,1
1697,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .",1
1698,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,1
1699,"This allows us to better estimate the distribution of word senses , obtaining more accurate decision boundaries and higher classification accuracy .",0
1700,The best performance was achieved by using an LSTM language model with label propagation .,0
1701,Our algorithm achieves state - of - art performance on many SemEval all - words tasks .,0
1702,It also outperforms the most -frequent - sense and Word2 Vec baselines by 10 % ( see Section 5.2 for details ) .,0
1703,Related Work,0
1704,"The development of large lexical resources , such as WordNet and BabelNet , has enabled knowledge - based algorithms which show promising results on allwords prediction tasks .",0
1705,"WSD algorithms based on supervised learning are generally believed to perform better than knowledgebased WSD algorithms , but they need large training sets to perform well .",0
1706,Acquiring large training sets is costly .,0
1707,"In this paper , we show that a supervised WSD algorithm can perform well with ?",0
1708,20 training examples per sense .,0
1709,"In the past few years , much progress has been made on using neural networks to learn word embeddings , to construct language models , perform sentiment analysis , machine translation and many other NLP applications .",0
1710,A number of different ways have been studied for using word embeddings in WSD .,0
1711,There are some common elements :,0
1712,Context embeddings .,0
1713,"Given a window of text w n?k , ... , w n , ... , w n+k surrounding a focus word w n ( whose label is either known in the case of example sentences or to be determined in the case of classification ) , an embedding for the context is computed as a concatenation or weighted sum of the embeddings of the words w i , i = n.",0
1714,Context embeddings of various kinds are used in both and .,0
1715,Sense embeddings .,0
1716,Embeddings are computed for each word sense in the word sense inventory ( e.g. WordNet ) .,0
1717,"In , equations are derived relating embeddings for word senses with embeddings for undisambiguated words .",0
1718,The equations are solved to compute the sense embeddings .,0
1719,"In , sense embeddings are computed first as weighted sums of the embeddings of words in the WordNet gloss for each sense .",0
1720,"These are used in an initial bootstrapping WSD phase , and then refined by a neural network which is trained on this bootstrap data . Embeddings as SVM features .",0
1721,"Context embeddings , or features computed by combining context embeddings with sense embeddings , can be used as additional features in a supervised WSD system e.g. the SVMbased IMS .",0
1722,Indeed found that using embeddings as the only features in IMS gave competitive WSD performance .,0
1723,Nearest neighbor classifier .,0
1724,"Another way to perform classification is to find the word sense whose sense embedding is closest , as measured by cosine similarity , to the embedding of the classification context .",0
1725,"This is used , for example , in the bootstrapping phase of . Retraining embeddings .",0
1726,A feedforward neural network can be used to jointly perform WSD and adjust embeddings .,0
1727,"In our work , we start with a baseline classifier which uses 1000 - dimensional embeddings trained on a 100 billion word news corpus using Word2 Vec .",0
1728,"The vocabulary consists of the most frequent 1 , 000 , 000 words , without lemmatization or case normalization .",0
1729,Sense embeddings are computed by averaging the context embeddings of sentences which have been labeled with that sense .,0
1730,"To classify a word in a context , we assign the word sense whose embedding has maximal cosine similarity with the embedding of the context .",0
1731,This classifier has similar performance to the best classifier in when SemCor is used as a source of labeled sentences .,0
1732,"The Word2 Vec embeddings are trained using a bag of words model , i.e. without considering word order in the training context , and word order is also not considered in the classification context .",0
1733,In Section 3 we show that using a more expressive language model which takes account of word order yields significant improvements .,0
1734,Semi-supervised learning has previously been applied successfully to word sense disambiguation .,0
1735,In bootstrapping was used to learn a high precision WSD classifier .,0
1736,"A low recall classifier was learned from a small set of labeled examples , and the labeled set then extended with those sentences from an unlabeled corpus which the classifier could label with high confidence .",0
1737,"The classifier was then retrained , and this iterative training process continued to convergence .",0
1738,Additional heuristics helped to maintain the stability of the bootstrapping process .,0
1739,The method was evaluated on a small data set .,0
1740,"In , a label propagation algorithm was proposed for word sense disambiguation and compared to bootstrapping and a SVM supervised classifier .",0
1741,"Label propagation can achieve better performance because it assigns labels to optimize a global objective , whereas bootstrapping propagates labels based on local similarity of examples .",0
1742,In Section 4 we describe our use of label propagation to improve on nearest neighbor for classification .,0
1743,Supervised WSD with LSTM,0
1744,Neural networks with long short - term memory ( LSTM ) units ) make good language models which take into account word order .,0
1745,We train a LSTM language model to predict the held - out word in a sentence .,0
1746,"As shown in , we first replace the held - out word with a special symbol $ , and then , after consuming the remaining words in the sentence , project the h dimensional hidden layer to a p dimensional context layer , and finally predict the held out word with softmax .",0
1747,"By default , the LSTM model has 2048 hidden units , 512 dimensional context layer and 512 dimensional word embeddings .",0
1748,"We also studied other settings , see Section 5.2.2 for details .",0
1749,"We train the LSTM on a news corpus of about 100 billion tokens , with a vocabulary of 1 , 000 , 000 words .",0
1750,Words in the vocabulary are neither lemmatized nor case normalized .,0
1751,Our LSTM model is different from that of Kgebck and Salomonsson .,0
1752,"We train a LSTM language model , which predicts a held - out word given the surrounding context , with a large amount of unlabeled text as training data .",0
1753,"The huge training dataset allows us to train a high - capacity model ( 2048 hidden units , 512 dimensional embeddings ) , which achieves high precision without overfitting .",0
1754,"In our experiments , this directional LSTM model was faster and easier to train than a bidirectional LSTM , especially given our huge training dataset .",0
1755,Kgebck and Salomonsson 's LSTM directly predicts the word senses and it is trained with a limited number of word sense - labeled examples .,0
1756,"Although regularization and dropout are used to avoid overfitting the training data , the bidirectional LSTM is small with only 74 + 74 neurons and 100 dimensional word embeddings .",0
1757,"Because our LSTM is generally applicable to any word , it achieves high performance on all - words WSD tasks ( see Section 5 for details ) , which is the focus of this paper .",0
1758,Kgebck and Salomonsson 's LSTM is only evaluated on lexical sample WSD tasks of SemEval 2 and 3 .,0
1759,Ht+3 Ht +4,0
1760,The behavior of the LSTM can be intuited by its predictions .,0
1761,shows the top 10 words predicted by an LSTM language model for the word ' stock ' in sentences containing various senses of ' stock ' .,0
1762,"In our initial experiments , we computed similarity between two contexts by the overlap between their bags of predicted words .",0
1763,For example the top predictions for the query overlap most with the LSTM predictions for ' sense # 1 ' - we predict that ' sense# 1 ' is the correct sense .,0
1764,"This bag of predictions , while easily interpretable , is just a discrete approximation to the internal state of the LSTM when predicting the held out word .",0
1765,We therefore directly use the LSTM 's context layer from which the bag of predictions was computed as a representation of the context ( see ) .,0
1766,"Given context vectors extracted from the LSTM , our supervised WSD algorithms classify a word in a context by finding the sense vector which has maximum cosine similarity to the context vector ) .",0
1767,"We find the sense vectors foods , food , vegetables , meats , recipes , cheese , meat , chicken , pasta , milk sense # 3 query",0
1768,"In addition , they will receive stock in the reorganized company , which will be named Ranger Industries Inc.",0
1769,"shares , positions , equity , jobs , awards , representation , stock , investments , roles , funds ?:",0
1770,Top predictions of ' stock ' in 5 sentences of different word senses by averaging context vectors of all training sentences of the same sense .,0
1771,"We observed in a few cases that the context vector is far from the held - out word 's embedding , especially when the input sentence is not informative .",0
1772,"For example , the LSTM language model will predict "" night "" for the input sentence "" I fell asleep at . "" when we holdout "" work "" .",0
1773,"Currently , we treat the above cases as outliers .",0
1774,"We would like explore alternative solutions , e.g. , forcing the model to predict words that are close to one sense vector of the held - out word , in further work .",0
1775,"As can be seen in SemEval all - words tasks and Tables 6 , this LSTM model has significantly better performance than the Word2Vec models .",0
1776,The non-parametric nearest neighbor algorithm described in Section 3 has the following drawbacks :,0
1777,"It assumes a spherical shape for each sense cluster , being unable to accurately model the decision boundaries given the limited number of examples .",0
1778,"It has no training data for , and does not model , the sense prior , omitting an extremely powerful potential signal .",0
1779,To overcome these drawbacks we present a semi-supervised method which augments the labeled example sentences with a large number of unlabeled sentences from the web .,0
1780,Sense labels are then propagated from the labeled to the unlabeled sentences .,0
1781,Adding a large number of unlabeled sentences allows the decision boundary between different senses to be better approximated .,0
1782,A label - propagation graph consists of ( a ) vertices with a number of labeled seed nodes and ( b ) undirected weighted edges .,0
1783,"Label propagation ( LP ) ( Talukdar and Crammer , 2009 ) iteratively computes a distribution of labels on the graph 's vertices to minimize a weighted combination of :",0
1784,The discrepancy between seed labels and their computed labels distributions .,0
1785,The disagreement between the label distributions of connected vertices .,0
1786,"A regularization term which penalizes distributions which differ from the prior ( by default , a uniform distribution ) .",0
1787,"We construct a graph for each lemma with labeled vertices for the labeled example sentences , and unlabeled vertices for sentences containing the lemma , drawn from some additional corpus .",0
1788,"Vertices for sufficiently similar sentences ( based on criteria discussed below ) are connected by an edge whose weight is the cosine similarity between the respective context vectors , using the LSTM language model .",0
1789,"To classify an occurrence of the lemma , we create an additional vertex for the new sentence and run LP to propagate the sense labels from the seed vertices to the unlabeled vertices .",0
1790,illustrates the graph configuration .,0
1791,Spatial proximity represents similarity of the sentences attached to each vertex and the shape of each node represents the word sense .,0
1792,Filled nodes represent seed nodes with known word senses .,0
1793,"Unfilled nodes represent sentences with no word sense label , and the ?",0
1794,represents the word we want to classify .,0
1795,"With too many edges , sense labels propagate too far , giving low precision .",0
1796,"With too few , sense labels do not propagate sufficiently , giving low recall .",0
1797,We found that the graph has about the right density for common senses when we ranked vertex pairs by similarity and connected the pairs above the 95 percentile .,0
1798,"This may still leave rare senses sparsely connected , so we additionally added edges to ensure that every vertex is connected to at least 10 other vertices .",0
1799,"Our experiments show that this setting achieves good performance on WSD , and the performance is stable when the percentile ranges between 85 to 98 .",0
1800,"Since it requires running LP for every classification , the algorithm is slow compared to the nearest neighbor algorithm .",0
1801,Experiments,0
1802,We evaluated the LSTM algorithm with and without label propagation on standard SemEval all - words tasks using WordNet as the inventory .,0
1803,Our proposed algorithms achieve state - of - art performance on many SemEval all - words WSD tasks .,0
1804,In order to assess the effects of training corpus size and language model capacity we also evaluate our algorithms using the New Oxford American Dictionary ( NOAD ) inventory with or MASC 1 .,0
1805,Sem Eval Tasks,1
1806,"In this section , we study the performance of our classifiers on Senseval2 , Senseval3 , , SemEval - 2013 Task 12 ) and SemEval - 2015 task 13 2 .",0
1807,We focus the study on all - words WSD tasks .,0
1808,"For a fair comparison with related works , the classifiers are evaluated on all words ( both polysemous and monosemous ) .",0
1809,"Following related works , we use SemCor or OMSTI for training .",0
1810,"In our LP classifiers , unlabeled data for each lemma consists either of 1000 sentences which contain the lemma , randomly sampled from the web , or all OMSTI sentences ( without labels ) which contain the lemma . shows the Sem - Eval results .",0
1811,Our proposed algorithms achieve the highest all - words F 1 scores except for Sem - Eval 2013 .,1
1812,"only disambiguates nouns , and it outperforms our algorithms on Sem - Eval 2013 by 4 % , but is ranked behind our algorithms on Senseval - 3 and SemEval - 7 tasks with an F1 score more than 6 % lower than our algorithms .",0
1813,"Unified WSD has the highest F 1 score on Nouns ( Sem - Eval - 7 Coarse ) , but our algorithms outperform Unified WSD on other part - of - speech tags .",1
1814,Settings,0
1815,"For a fair comparison of Word2Vec and LSTM , we do not use pre-trained word - embeddings as in , but instead train the Word2 Vec and LSTM models on a 100 billion word news corpus 3 with a vocabulary of the most frequent 1,000,000 words .",0
1816,"Our self - trained word embeddings have similar performance to the pre-trained embeddings , as shown in .",0
1817,The Word2 Vec word vectors are of dimension 1024 .,0
1818,"The LSTM model has 2048 hidden units , and inputs are 512 - dimensional word vectors .",0
1819,We train the LSTM model by minimizing sampled softmax loss with Adagrad .,0
1820,The learning rate is 0.1 .,0
1821,"We experimented with other learning rates , and observed no significant performance difference after the training converges .",0
1822,We also downsample frequent terms in the same way as .,0
1823,Word2 Vec vectors Vs. LSTM,1
1824,"To better compare LSTM with word vectors we also build a nearest neighbor classifier using Word2 Vec word embeddings and SemCor example sentences , Word2 Vec ( T : SemCor ) .",0
1825,"It performs similar to IMS + Word2 Vec ( T: SemCor ) , a SVM - based classifier studied in .",1
1826,shows that the LSTM classifier outperforms the Word2 Vec classifier across the board .,1
1827,Sem Cor Vs. OMSTI,1
1828,"Contrary to the results observed in , the LSTM classifier trained with OMSTI performs worse than that trained with SemCor .",1
1829,It seems that the larger size of the OMSTI training data set is more than offset by noise present in its automatically generated labels .,0
1830,"While the SVM classifier studied in maybe able to learn a model which copes with this noise , our naive nearest neighbor classifiers do not have a learned model and deal less well with noisy labels .",1
1831,Label propagation,0
1832,We use the implementation of DIST EXPANDER .,0
1833,We test the label propagation algorithm with SemCor or OMSTI as labeled data sets and OMSTI or 1000 random sentences from the web per lemma as unlabeled data .,0
1834,The algorithm performs similarly on the different data sets .,0
1835,NOAD Eval,1
1836,"Many dictionary lemmas and senses have no examples in SemCor or OSTMI , giving rise to losses in allwords WSD when these corpora are used as training data .",0
1837,The above SemEval scores do not distinguish errors caused by missing training data for certain labels or inaccurate classifier .,0
1838,"To better study the proposed algorithms , we train the classifiers with the New Oxford American Dictionary ( NOAD ) , in which there are example sentences for each word sense .",0
1839,Word Sense Inventory,0
1840,The NOAD focuses on American English and is based on the Oxford Dictionary of English ( ODE ) .,0
1841,It distinguishes between coarse ( core ) and fine - grained ( sub ) word senses in the same manner as ODE .,0
1842,"Previous investigations using the ODE have shown that coarse - grained word senses induced by the ODE inventory address problems with WordNet 's fine - grained inventory , and that the inventory is useful for word sense disambiguation .",0
1843,"For our experiments , we use NOAD 's core senses , and we also use lexicographer - curated example sentences from the Semantic English Language Database ( SELD ) 4 , provided by Oxford University Press .",0
1844,"We manually annotated all words of the English language SemCor corpus and MASC corpora with NOAD word senses in order to evaluate performance 5 . shows the total number of polysemes ( more than one core sense ) and average number of senses per polyseme in NOAD / SELD ( hereafter , NOAD ) , SemCor and MASC .",0
1845,Both SemCor and MASC individually cover around 45 % of NOAD polysemes and 62 % of senses of those polysemes .,0
1846,gives the number of labeled sentences of these datasets .,0
1847,"Note that although NOAD has more labeled sentences than SemCor or MASC , the average numbers of sentences per sense of these datasets are similar .",0
1848,"This is because NOAD has labeled sentences for each word sense , whereas SemCor ( MASC ) only covers a subset of lemmas and senses ) .",0
1849,The last column of shows that each annotated word in SemCor and MASC has an average of more than 4 NOAD corse senses .,0
1850,"Hence , a random guess will have a precision around 1 / 4 .",0
1851,"In the default setting , we use NOAD example sentences as labeled training data and evaluate on Sem - Cor and MASC .",0
1852,We evaluate all polysemous words in the evaluation corpus .,0
1853,LSTM classifier,1
1854,We compare our algorithms with two baseline algorithms :,0
1855,Most frequent sense :,1
1856,Compute the sense frequency ( from a labeled corpus ) and label word w with w 's most frequent sense .,0
1857,"Word 2 Vec : a nearest - neighbor classifier with Word2 Vec word embedding , which has similar performance to cutting - edge algorithms studied in on SemEval tasks .",0
1858,compares the F 1 scores of the LSTM and baseline algorithms .,0
1859,"LSTM outperforms Word2Vec by more than 10 % overall words , where most of the gains are from verbs and adverbs .",1
1860,"The results suggest that syntactic information , which is well modeled by LSTM but ignored by Word2 Vec , is important to distinguishing word senses of verbs and adverbs .",0
1861,Change of training data,1
1862,"By default , the WSD classifier uses the NOAD example sentences as training data .",0
1863,"We build a larger training dataset by adding labeled sentences from SemCor and MASC , and study the change of F1 scores in .",0
1864,"Across all part of speech tags and datasets , F1 scores increase after adding more training data .",0
1865,We further test our algorithm by using SemCor ( or MASC ) as training data ( without NOAD examples ) .,0
1866,The SemCor ( or MASC ) trained classifier is on a par with the NOAD trained classifier on F1 score .,1
1867,"However , the macro F1 score of the former is much lower than the latter , as shown in , because of the limited coverage of rare senses and words in SemCor and MASC .",0
1868,Change of language model capacity,1
1869,"In this experiment , we change the LSTM model capacity by varying the number of hidden units hand the dimensions of the input embeddings p and measuring F1 . shows strong positive correlation between F1 and the capacity of the language model .",0
1870,"However , larger models are slower to train and use more memory .",0
1871,"To balance the accuracy and resource usage , we use the second best LSTM model ( h = 2048 and p = 512 ) by default .",1
1872,Semi-supervised WSD,1
1873,We evaluate our semi-supervised WSD classifier in this subsection .,0
1874,We construct the graph as described in Section 4 and run LP to propagate sense labels from the seed vertices to the unlabeled vertices .,0
1875,We evaluate the performance of the algorithm by comparing the predicted labels and the gold labels on eval nodes .,0
1876,"As can be observed from , LP did not yield clear benefits when using the Word2 Vec language model .",1
1877,"We did see significant improvements , 6.3 % increase on SemCor and 7.3 % increase on MASC , using LP with the LSTM language model .",1
1878,We hypothesize that this is because LP is sensitive to the quality of the graph distance metric .,0
1879,Change of seed data :,1
1880,"As can be seen in , LP substantially improves classifier F1 when the training datasets are SemCor + NOAD or MASC + NOAD .",1
1881,"As discussed in Section 4 , the improvement may come from explicitly modeling the sense prior .",0
1882,We did not see much performance lift by increasing the number of unlabeled sentences per lemma .,0
1883,Change of graph density :,1
1884,"By default , we construct the LP graph by connecting two nodes if their affinity is above 95 % percentile .",1
1885,We also force each node to connect to at least 10 neighbors to prevent isolated nodes .,0
1886,shows the performance of the LP algorithm by changing the percentile threshold .,0
1887,"The F1 scores are relatively stable when the percentile ranges between 85 to 98 , but decrease when the percentile drops to 80 .",1
1888,"Also , it takes longer to run the LP algorithm on a denser graph .",0
1889,We pick the 95 percentile in our default setting to achieve both high F1 scores and short running time . :,0
1890,F1 scores of the LSTM LP trained on NOAD with varying graph density .,0
1891,Conclusions and Future Work,0
1892,"In this paper , we have presented two WSD algorithms which combine ( 1 ) LSTM neural network language models trained on a large unlabeled text corpus , with ( 2 ) labeled data in the form of example sentences , and , optionally , ( 3 ) unlabeled data in the form of additional sentences .",0
1893,Using an LSTM language model gave better performance than one based on Word2 Vec embeddings .,0
1894,"The best performance was achieved by our semi-supervised WSD algorithm which builds a graph containing labeled example sentences augmented with a large number of unlabeled sentences from the web , and classifies by propagating sense labels through this graph .",0
1895,Several unanswered questions suggest lines of future work .,0
1896,"Since our general approach is amenable to incorporating any language model , further developments in NNLMs may permit increased performance .",0
1897,"We would also like to better understand the limitations of language modeling for this task : we expect there are situations - e.g. , in idiomatic phrases - where per-word predictions carry little information .",0
1898,"We believe our model should generalize to languages other than English , but have not yet explored this .",0
1899,Character - level LSTMs may provide robustness to morphology and diacritics and may prove useful even in English for spelling errors and out of vocabulary words .,0
1900,title,0
1901,Neural Sequence Learning Models for Word Sense Disambiguation,1
1902,abstract,0
1903,Word Sense Disambiguation models exist in many flavors .,0
1904,"Even though supervised ones tend to perform best in terms of accuracy , they often lose ground to more flexible knowledge - based solutions , which do not require training by a word expert for every disambiguation target .",0
1905,"To bridge this gap we adopt a different perspective and rely on sequence learning to frame the disambiguation problem : we propose and study in depth a series of end - to - end neural architectures directly tailored to the task , from bidirectional Long Short - Term Memory to encoder - decoder models .",0
1906,"Our extensive evaluation over standard benchmarks and in multiple languages shows that sequence learning enables more versatile all - words models that consistently lead to state - of - the - art results , even against word experts with engineered features .",0
1907,Introduction,0
1908,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",1
1909,"Indeed , by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications , from Information Retrieval and Extraction to Machine Translation .",0
1910,"Recently , WSD has also been leveraged to build continuous vector representations for word senses .",0
1911,"Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory , the majority of WSD approaches to date can be grouped into two main categories : supervised ( or semisupervised ) and knowledge - based .",0
1912,"Supervised models have been shown to consistently outperform knowledge - based ones in all standard benchmarks , at the expense , however , of harder training and limited flexibility .",0
1913,"First of all , obtaining reliable sense - annotated corpora is highly expensive and especially difficult when non-expert annotators are involved ( de Lacalle and , and as a consequence approaches based on unlabeled data and semisupervised learning are emerging .",0
1914,"Apart from the shortage of training data , a crucial limitation of current supervised approaches is that a dedicated classifier ( word expert ) needs to be trained for every target lemma , making them less flexible and hampering their use within endto - end applications .",0
1915,"In contrast , knowledge - based systems do not require sense - annotated data and often draw upon the structural properties of lexicosemantic resources .",0
1916,"Such systems construct a model based only on the underlying resource , which is then able to handle multiple target words at the same time and disambiguate them jointly , whereas word experts are forced to treat each disambiguation target in isolation .",0
1917,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",1
1918,"From this standpoint , WSD amounts to translating a sequence of words into a sequence of potentially sense - tagged tokens .",0
1919,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .",1
1920,"Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .",1
1921,The contributions of this paper are twofold .,0
1922,"First , we show that neural sequence learning represents a novel and effective alternative to the traditional way of modeling supervised WSD , enabling a single all - words model to compete with a pool of word experts and achieve state - of - the - art results , while also being easier to train , arguably more versatile to use within downstream applications , and directly adaptable to different languages without requiring additional sense - annotated data ( as we show in Section 6.2 ) ; second , we carryout an extensive experimental evaluation where we compare various neural architectures designed for the task ( and somehow left underinvestigated in previous literature ) , exploring different configurations and training procedures , and analyzing their strengths and weaknesses on all the standard benchmarks for all - words WSD .",0
1923,Related Work,0
1924,The literature on WSD is broad and comprehensive : new models are continuously being developed and tested over a wide variety of standard benchmarks .,0
1925,"Moreover , the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks ( Pilehvar and .",0
1926,"As regards supervised WSD , traditional approaches are generally based on extracting local features from the words surrounding the target , and then training a classifier for each target lemma .",0
1927,"In their latest developments , these models include more complex features based on word embeddings .",0
1928,The recent upsurge of neural networks has also contributed to fueling WSD research : rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w ; their instance - based system then compares that representation with those of example sentences annotated with the candidate meanings of w .,0
1929,"Similarly , Context2 Vec makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data .",0
1930,"Finally , present a supervised classifier based on bidirectional LSTM for the lexical sample task .",0
1931,All these contributions have shown that supervised neural models can achieve state - of - the - art performances without taking advantage of external resources or language - specific features .,0
1932,"However , they all consider each target word as a separate classification problem and , to the best of our knowledge , very few attempts have been made to disambiguate a text jointly using sequence learning .",0
1933,"Sequence learning , especially using LSTM , has become a well - established standard in numerous NLP tasks .",0
1934,"In particular , sequence - to - sequence models have grown increasingly popular and are used extensively in , e.g. , Machine Translation , Sentence Representation , Syntactic Parsing , Conversation Modeling , Morphological Inflection and Text Summarization .",0
1935,"In line with this trend , we focus on the ( so far unexplored ) context of supervised WSD , and investigate state - of - the - art all - words approaches that are based on neural sequence learning and capable of disambiguating all target content words within an input text , a key feature in several knowledge - based approaches .",0
1936,y 3 y 2 y 1 y 4 y 5 : Bidirectional LSTM sequence labeling architecture for WSD ( 2 hidden layers ) .,0
1937,We use the notation of Navigli ( 2009 ) for word senses : w i p is the i - th sense of w with part of speech p.,0
1938,Sequence Learning for Word Sense Disambiguation,0
1939,In this section we define WSD in terms of a sequence learning problem .,0
1940,"While in its classical formulation ) WSD is viewed as a classification problem fora given word win context , with word senses of w being the class labels , here we consider a variable - length sequence of input symbols x = x 1 , ... , x T and we aim at predicting a sequence of output symbols y = y 1 , ... , y T .",0
1941,1 Input symbols are word tokens drawn from a given vocabulary V .,0
1942,"2 Output symbols are either drawn from a pre-defined sense inventory S ( if the corresponding input symbols are open - class content words , i.e. , nouns , verbs , adjectives or adverbs ) , or from the same input vocabulary V ( e.g. , if the corresponding input symbols are function words , like prepositions or determiners ) .",0
1943,"Hence , we can define a WSD model in terms of a function that maps sequences of symbols",0
1944,"Here all - words WSD is no longer broken down into a series of distinct and separate classification tasks ( one per target word ) but rather treated directly at the sequence level , with a single model handling all disambiguation decisions .",0
1945,"In what follows , we describe three different models for accomplishing this : a traditional LSTMbased model ( Section 3.1 ) , a variant that incorporates an attention mechanism ( Section 3.2 ) , and an encoder - decoder architecture ( Section 3.3 ) .",0
1946,"In general x and y might have different lengths , e.g. , if x contains a multi-word expression ( European Union ) which is mapped to a unique sense identifier ( European Union 1 n ) .",0
1947,2 V generalizes traditional vocabularies used in WSD and includes both word lemmas and inflected forms .,0
1948,Bidirectional LSTM,0
1949,Tagger,0
1950,The most straightforward way of modeling WSD as formulated in Section 3 is that of considering a sequence labeling architecture that tags each symbol xi ?,0
1951,V in the input sequence with a label y j ?,0
1952,O .,0
1953,"Even though the formulation is rather general , previous contributions have already shown the effectiveness of recurrent neural networks for WSD .",0
1954,"We follow the same line and employ a bidirectional LSTM architecture : in fact , important clues for disambiguating a target word could be located anywhere in the context ( not necessarily before the target ) and fora model to be effective it is crucial that it exploits information from the whole input sequence at every time step .",0
1955,Architecture .,0
1956,A sketch of our bidirectional LSTM tagger is shown in .,0
1957,It consists of :,0
1958,An embedding layer that converts each word,0
1959,One or more stacked layers of bidirectional LSTM .,0
1960,The hidden state vectors hi and output vectors oi at the i th time step are then obtained as the concatenations of the forward and backward pass vectors,0
1961,A fully - connected layer with softmax activation that turns the output vector oi at the i th time step into a probability distribution over the output vocabulary O.,0
1962,Training .,0
1963,"The tagger is trained on a dataset of N labeled sequences {( x k , y k ) }",0
1964,"N k=1 directly obtained from the sentences of a sense - annotated corpus , where each x k is a sequence of word tokens , and each y k is a sequence containing both word tokens and sense labels .",0
1965,Ideally y k is a copy of x k where each content word is sense - tagged .,0
1966,"This is , however , not the casein many real - world datasets , where only a subset of the content words is annotated ; hence the architecture is designed to deal with both fully and partially annotated sentences .",0
1967,"Apart from sentence splitting and tokenization , no preprocessing is required on the training data .",0
1968,Attentive Bidirectional,0
1969,LSTM Tagger,0
1970,"The bidirectional LSTM tagger of Section 3.1 exploits information from the whole input sequence x , which is encoded in the hidden state hi .",0
1971,"However , certain elements of x might be more discriminative than others in predicting the output label at a given time step ( e.g. , the syntactic subject and object when predicting the sense label of a verb ) .",0
1972,"We model this hunch by introducing an attention mechanism , already proven to be effective in other NLP tasks , into the sequence labeling architecture of Section 3.1 .",0
1973,"The resulting attentive bidirectional LSTM tagger augments the original architecture with an attention layer , where a context vector c is computed from all the hidden states h 1 , ... , h T of the bidirectional LSTM .",0
1974,"The attentive tagger first reads the entire input sequence x to construct c , and then exploits c to predict the output label y j at each time step , by concatenating it with the output vector o j of the bidirectional LSTM ( ) .",0
1975,"We follow previous work and compute c as the weighted sum of the hidden state vectors h 1 , ... , h T . Formally , let H ?",0
1976,"Rn T be the matrix of hidden state vectors [ h 1 , ... , h T ] , where n is the hidden state dimension and T is the input sequence length ( cf. Section 3 ) .",0
1977,c is obtained as follows :,0
1978,where ? ?,0
1979,"Rn is a parameter vector , and a ? R T is the vector of normalized attention weights .",0
1980,Sequence - to - Sequence Model,0
1981,"The attentive tagger of Section 3.2 performs a two - pass procedure by first reading the input sequence x to construct the context vector c , and then predicting an output label y j for each element in x .",0
1982,"In this respect , the attentive architecture can effectively be viewed as an encoder for x .",0
1983,"A further generalization of this model would then be a complete encoder - decoder architecture where WSD is treated as a sequence - to - sequence mapping ( sequence - to- sequence WSD ) , i.e. , as the "" translation "" of word sequences into sequences of potentially sense - tagged tokens .",0
1984,Softmax Layer,0
1985,x 1 x 2 x 4,0
1986,x 5,0
1987,x 1 x 2 x 3 x 4 x 5 ch 2 h 3 he later the report he Softmax Layer,0
1988,x 1 x 2 x 4,0
1989,x 5,0
1990,x 1 x 2 x 3 x 4 x 5 y 3 he later the report he,0
1991,"In the sequence - to - sequence framework , a variable - length sequence of input symbols x is represented as a sequence of vectors x = x 1 , ... , x T by converting each symbol x i ?",0
1992,"x into a real - valued vector x i via an embedding layer , and then fed to an encoder , which generates a fixed - dimensional vector representation of the sequence .",0
1993,"Traditionally , the encoder function is a Recurrent Neural Network ( RNN ) such that :",0
1994,where ht ?,0
1995,"Rn is the n-dimensional hidden state vector at time t , c ?",0
1996,"Rn is a vector generated from the whole sequence of input states , and f and q are non-linear functions .",0
1997,3,0
1998,"A decoder is then trained to predict the next output symbol y t given the encoded input vector c and all the previously predicted output symbols y 1 , ... , y t?1 .",0
1999,"More formally , the decoder defines a probability over the output sequence y = y 1 , ... , y T by decomposing the joint probability into ordered conditionals :",0
2000,"Typically a decoder RNN defines the hidden state at time t ass t = g (s t?1 , {c , y t?1 }) and then feeds st to a softmax layer in order to obtain a conditional probability over output symbols . :",0
2001,"Encoder - decoder architecture for sequence - to - sequence WSD , with 2 bidirectional LSTM layers and an attention layer .",0
2002,"In the context of WSD framed as a sequence learning problem , a sequence - to - sequence model takes as input a training set of labeled sequences ( cf. Section 3.1 ) and learns to replicate an input sequence x while replacing each content word with its most suitable word sense from S .",0
2003,"In other words , sequence - to - sequence WSD can be viewed as the combination of two sub - tasks :",0
2004,"A memorization task , where the model learns to replicate the input sequence token by token at decoding time ;",0
2005,The actual disambiguation task where the model learns to replace content words across the input sequence with their most suitable senses from the sense inventory S.,0
2006,"In the latter stage , multi-word expressions ( such as nominal entity mentions or phrasal verbs ) are replaced by their sense identifiers , hence yielding an output sequence that might have a different length than x .",0
2007,Architecture .,0
2008,The encoder - decoder architecture generalizes over both the models in Sections 3.1 and 3.2 .,0
2009,"In particular , we include one or more bidirectional LSTM layers at the core of both the encoder and the decoder modules .",0
2010,"The encoder utilizes an embedding layer ( cf. Section 3.1 ) to convert input symbols into embedded representations , feeds it to the bidirectional LSTM layer , and then constructs the context vector c , either by simply letting c = h T ( i.e. , the hidden state of the bidirectional LSTM layer after reading the whole input sequence ) , or by computing the weighted sum described in Section 3.2 ( if an attention mechanism is employed ) .",0
2011,"In either case , the context vector c is passed over to the decoder , which generates the output symbols sequentially based on c and the current hidden state st , using one or more bidirectional LSTM layers as in the encoder module .",0
2012,"Instead of feeding c to the decoder only at the first time step , we condition each output symbol y ton c , allowing the decoder to peek into the input at every step , as in .",0
2013,"Finally , a fully - connected layer with softmax activation converts the current output vector of the last LSTM layer into a probability distribution over the output vocabulary O .",0
2014,The complete encoder - decoder architecture ( including the attention mechanism ) is shown in .,0
2015,Multitask Learning with Multiple Auxiliary Losses,0
2016,Several recent contributions have shown the effectiveness of multitask learning in a sequence learning scenario .,0
2017,"In MTL the idea is that of improving generalization performance by leveraging training signals contained in related tasks , in order to exploit their commonalities and differences .",0
2018,"MTL is typically carried out by training a single architecture using multiple loss functions and a shared representation , with the underlying intention of improving a main task by incorporating joint learning of one or more related auxiliary tasks .",0
2019,"From a practical point of view , MTL works by including one task - specific output layer per additional task , usually at the outermost level of the architecture , while keeping the remaining hidden layers common across all tasks .",0
2020,"In line with previous approaches , and guided by the intuition that WSD is strongly linked to other NLP tasks at various levels , we also design and study experimentally a multitask augmentation of the models described in Section 3 .",0
2021,"In particular , we consider two auxiliary tasks :",0
2022,"Part - of - speech ( POS ) tagging , a standard auxiliary task extensively studied in previous work .",0
2023,"Predicting the part - of - speech tag fora given token can also be informative for word senses , and help in dealing with cross - POS lexical ambiguities ( e.g. , book a flight vs.",0
2024,reading a good book ) ;,0
2025,"Coarse - grained semantic labels ( LEX ) based on the WordNet lexicographer files , 4 i.e. , 45 coarse - grained semantic categories manually associated with all the synsets in WordNet on the basis of both syntactic and logical groupings ( e.g. , noun.location , or verb.motion ) .",0
2026,"These very coarse semantic labels , recently employed in a multitask setting by Alonso and Plank , group together related senses and help the model to generalize , especially over senses less covered at training time .",0
2027,We follow previous work and define an auxiliary loss function for each additional task .,0
2028,"The overall loss is then computed by summing the main loss ( i.e. , the one associated with word sense labels ) and all the auxiliary losses taken into account .",0
2029,"As regards the architecture , we consider both the models described in Sections 3.2 and 3.3 and modify them by adding two softmax layers in addition to the one in the original architecture .",0
2030,"illustrates this for the attentive tagger of Section 3.2 , considering both POS and LEX as auxiliary tasks .",0
2031,At the j th time step the model predicts a sense label y j together with a part - of - speech tag POS j and a coarse semantic label LEX j .,0
2032,5,0
2033,Experimental Setup,0
2034,In this section we detail the setup of our experimental evaluation .,0
2035,We first describe the training corpus and all the standard benchmarks for all - words WSD ; we then report technical details on the architecture and on the training process for all the models described throughout Section 3 and their multitask augmentations ( Section 4 ) . Evaluation Benchmarks .,0
2036,"We evaluated our models on the English all - words WSD task , considering both the fine - grained and coarsegrained benchmarks ( Section 6.1 ) .",0
2037,"As regards fine - grained WSD , we relied on the evaluation framework of , which includes five standardized test sets from the Senseval / Sem Eval series : Senseval - 2 , Senseval - 3 , , SemEval-2013 and SemEval - 2015 .",0
2038,"Due to the lack of a reasonably large development set for our setup , we considered the smallest among these test sets , i.e. , SE07 , as development set and excluded it from the evaluation of Section 6.1 .",0
2039,"As for coarse - grained WSD , we used the SemEval - 2007 task 7 test set , which is not included in the standardized framework , and mapped the original sense inventory from WordNet 2.1 to WordNet 3.0 .",0
2040,"6 Finally , we carried out an experiment on multilingual WSD using the Italian , German , French and Spanish data of SE13 .",0
2041,"For these benchmarks we relied on BabelNet At testing time , given a target word w , our models used the probability distribution over O , computed by the softmax layer at the corresponding time step , to rank the candidate senses of w ; we then simply selected the top ranking candidate as output of the model .",0
2042,Architecture Details .,0
2043,"To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .",1
2044,For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units ( 1024 units per direction ) .,1
2045,"As regards multilingual all - words WSD ( Section 6.2 ) , we experimented , instead , with two different configurations of the embedding layer : the pre-trained bilingual embeddings by for all the language pairs of interest ( EN - IT , EN - FR , EN - DE , and EN - ES ) , and the pre-trained multilingual 512 - dimensional embeddings for 12 languages by .",0
2046,Training .,0
2047,We used SemCor 3.0 as training corpus for all our experiments .,0
2048,"Widely known and utilized in the WSD literature , SemCor is one of the largest corpora annotated manually with word senses from the sense inventory of WordNet for all openclass parts of speech .",0
2049,We used the standardized version of SemCor as provided in the evaluation framework 9 which also includes coarse - grained POS tags from the universal tagset .,0
2050,"All models were trained fora fixed number of epochs E = 40 using Adadelta ( Zeiler , 2012 ) with learning rate 1.0 and batch size 32 .",0
2051,"After each epoch we evaluated our models on the development set , and then compared the best iterations ( E * ) on the development set with the reported state of the art in each benchmark .",0
2052,Experimental Results,0
2053,"Throughout this section we identify the models based on the LSTM tagger ( Sections 3.1 - 3.2 ) by the label BLSTM , and the sequence - to - sequence models ( Section 3.3 ) by the label Seq2Seq. shows the performance of our models on the standardized benchmarks for all - words finegrained WSD .",0
2054,"We report the F1 - score on each in - dividual test set , as well as the F1- score obtained on the concatenation of all four test sets , divided by part - of - speech tag .",1
2055,We compared against the best supervised and knowledge - based systems evaluated on the same framework .,0
2056,"As supervised systems , we considered Context2 Vec and It Makes Sense , both the original implementation and the best configuration reported by , which also integrates word embeddings using exponential decay .",1
2057,10 All these supervised systems were trained on the standardized version of Sem - Cor .,0
2058,"As knowledge - based systems we considered the embeddings - enhanced version of Lesk by + emb ) , UKB ( UKB gloss w2 w ) , and Babelfy .",0
2059,All these systems relied on the Most Frequent Sense ( MFS ) baseline as back - off strategy .,0
2060,"11 Overall , both BLSTM and Seq2Seq achieved results that are either state - of - the - art or statistically equivalent ( unpaired t- test , p < 0.05 ) to the best supervised system in each benchmark , performing on par with word experts tuned over explicitly engineered features .",1
2061,"Interestingly enough , BLSTM models tended consistently to outperform their Seq2Seq counterparts , suggesting that an encoder - decoder architecture , despite being more powerful , might be suboptimal for WSD .",1
2062,"Furthermore , introducing LEX ( cf. Section 4 ) as auxiliary task was generally helpful ; on the other hand , POS did not seem to help , corroborating previous findings ( Alonso .",0
2063,English All - words WSD,1
2064,"The overall performance by part of speech was consistent with the above analysis , showing that our models outperformed all knowledgebased systems , while obtaining results that are superior or equivalent to the best supervised mod -",0
2065,"We are not including , as their models are not available and not replicable on the standardized test sets , being based on proprietary data .",0
2066,"11 Since each system always outputs an answer , F- score equals both precision and recall , and statistical significance can be expressed with respect to any of these measures . : F-scores ( % ) for multilingual WSD .",0
2067,els .,0
2068,"It is worth noting that RNN - based architectures outperformed classical supervised approaches when dealing with verbs , which are shown to be highly ambiguous .",1
2069,The performance on coarse - grained WSD followed the same trend ) .,0
2070,"Both BLSTM and Seq2Seq outperformed UKB and IMS trained on SemCor , as well as recent supervised approaches based on distributional semantics and neural architectures .",1
2071,Multilingual All - words WSD,1
2072,All the neural architectures described in this paper can be readily adapted to work with different languages without adding sense - annotated data in the target language .,0
2073,"In fact , as long as the first layer ( cf. is equipped with bilingual or multilingual embeddings where word vectors in the training and target language are defined in the same space , the training process can be left unchanged , even if based only on English data .",0
2074,"The underlying assumption is that words that are translations of each other ( e.g. , house in English and casa in Italian ) are mapped to word embeddings that are as close as possible in the vector space .",0
2075,"In order to assess this , we considered one of our best models ( BLSTM + att.+ LEX ) and replaced the monolingual embeddings with bilingual and multilingual embeddings ( as specified in Section 5 ) , leaving the rest of the architecture unchanged .",0
2076,"We then trained these architectures on the same English training data , and ran the resulting models on the multilingual benchmarks of SemEval - 2013 for Italian , French , German and Spanish .",0
2077,"While doing this , we exploited BabelNet 's inter-resource mappings to convert WordNet sense labels ( used at training time ) into BabelNet synsets compliant with the sense inventory of the task .",0
2078,"F - score figures show that bilingual and multilingual models , despite being trained only on English data , consistently outperformed the MFS baseline and achieved results that are competitive with the best participating systems in the task .",1
2079,"We also note that the overall F- score performance did not change substantially ( and slightly improved ) when moving from bilingual to multilingual models , despite the increase in the number of target languages treated simultaneously .",1
2080,Discussion and Error Analysis,0
2081,"All the neural models evaluated in Section 6.1 utilized the MFS back - off strategy for instances unseen at training time , which amounted to 9.4 % overall for fine - grained WSD and 10.5 % for coarse - grained WSD .",0
2082,"Back - off strategy aside , 85 % of the times the top candidate sense fora target instance lay within the 10 most probable entries in the probability distribution over O computed by the softmax layer .",0
2083,"12 In fact , our sequence models learned , on the one hand , to associate a target word with its candidate senses ( something word experts are not required to learn , as they only deal with a single word type at a time ) ; on the other , they tended to generate softmax distributions reflecting the semantics of the surronding context .",0
2084,"For example , in the sentence :",0
2085,( a ),0
2086,"The two justices have been attending federalist society events for years , our model correctly disambiguated justices with the WordNet sense justice 3 n ( public official ) rather than justice 1 n ( the quality of being just ) , and the corresponding softmax distribution was heavily biased towards words and senses related to persons or groups ( commissioners , defendants , jury , cabinet , directors ) .",0
2087,"On the other hand , in the sentence :",0
2088,"( b ) Xavi Hernandez , the player of Barcelona , has 106 matches , the same model disambiguated matches with the wrong WordNet sense match 1 n ( tool for starting a fire ) .",0
2089,This suggests that the signal carried by discriminative words like player vanishes rather quickly .,0
2090,"In order to enforce global coherence further , recent contributions have proposed more sophisticated models where recurrent architectures are combined with Conditional Random Fields .",0
2091,"Finally , a number of errors were connected to shorter sentences with limited context for disambiguation : in fact , we noted that the average pre -",0
2092,"We refer hereto the same model considered in Section 6.2 ( i.e. , BLSTM + att.+ LEX ) .",0
2093,"cision of our model , without MFS back - off , increased by 6.2 % ( from 74.6 % to 80.8 % ) on sentences with more than 20 word tokens .",0
2094,Conclusion,0
2095,"In this paper we adopted anew perspective on supervised WSD , so far typically viewed as a classification problem at the word level , and framed it using neural sequence learning .",0
2096,"To this aim we defined , analyzed and compared experimentally different end - to - end models of varying complexities , including augmentations based on an attention mechanism and multitask learning .",0
2097,"Unlike previous supervised approaches , where a dedicated model needs to be trained for every content word and each disambiguation target is treated in isolation , sequence learning approaches learn a single model in one pass from the training data , and then disambiguate jointly all target words within an input text .",0
2098,"The resulting models consistently achieved state - of - the - art ( or statistically equivalent ) figures in all benchmarks for all - words WSD , both fine - grained and coarse - grained , effectively demonstrating that we can overcome the so far undisputed and long - standing word - expert assumption of supervised WSD , while retaining the accuracy of supervised word experts .",0
2099,"Furthermore , these models are sufficiently flexible to allow them , for the first time in WSD , to be readily adapted to languages different from the one used at training time , and still achieve competitive results ( as shown in Section 6.2 ) .",0
2100,"This crucial feature could potentially pave the way for crosslingual supervised WSD , and overcome the shortage of sense - annotated data in multiple languages that , to date , has prevented the development of supervised models for languages other than English .",0
2101,"As future work , we plan to extend our evaluation to larger sense - annotated corpora as well as to different sense inventories and different languages .",0
2102,"We also plan to exploit the flexibility of our models by integrating them into downstream applications , such as Machine Translation and Information Extraction .",0
2103,title,0
2104,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,1
2105,abstract,0
2106,The first stage of every knowledge base question answering approach is to link entities in the input question .,0
2107,We investigate entity linking in the context of a question answering task and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity .,0
2108,We use the Wikidata knowledge base and available question answering datasets to create benchmarks for entity linking on question answering data .,0
2109,"Our approach outperforms the previous state - of - the - art system on this data , resulting in an average 8 % improvement of the final score .",0
2110,We further demonstrate that our model delivers a strong performance across different entity categories .,0
2111,Introduction,0
2112,Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,0
2113,"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB .",1
2114,"In , two entity mentions are detected and linked to the knowledge base referents .",0
2115,This step is crucial for QA since the correct answer must be connected via some path over KB to the entities mentioned in the question .,0
2116,The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .,1
2117,Multiple EL systems are freely available and can be readily applied what are taylor swift 's albums ?,0
2118,"Taylor Swift Q462 album Q24951125 Red , 1989 , etc. :",0
2119,"An example question from a QA dataset that shows the correct entity mentions and their relationship with the correct answer to the question , Qxxx stands fora knowledge base identifier for question answering ( e.g. DBPedia Spotlight 1 , AIDA 2 ) .",0
2120,"However , these systems have certain drawbacks in the QA setting : they are targeted at long well - formed documents , such as news texts , and are less suited for typically short and noisy question data .",0
2121,"Other EL systems focus on noisy data ( e.g. S - MART , , but are not openly available and hence limited in their usage and application .",0
2122,Multiple error analyses of QA systems point to entity linking as a major external source of error .,0
2123,PERFORMER,0
2124,INSTANCE OF,0
2125,"The QA datasets are normally collected from the web and contain very noisy and diverse data , which poses a number of challenges for EL .",0
2126,"First , many common features used in EL systems , such as capitalization , are not meaningful on noisy data .",0
2127,"Moreover , a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation .",0
2128,The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems .,0
2129,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .",1
2130,"This ensures that any token n-gram can be considered as a potential entity mention , which is important to be able to link entities of different categories , such as movie titles and organization names .",0
2131,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .",1
2132,Each level of granularity is handled by a separate component of the model .,1
2133,"A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",1
2134,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",1
2135,This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,1
2136,Contributions,0
2137,The two main contributions of our work are :,0
2138,( i ) We construct two datasets to evaluate EL for QA and present a set of strong baselines : the existing EL systems that were used as a building block for QA before and a model that uses manual features from the previous work on noisy data .,0
2139,( ii ) We design and implement an entity linking system that models contexts of variable granularity to detect and disambiguate entity mentions .,0
2140,"To the best of our knowledge , we are the first to present a unified end - to - end neural model for entity linking for noisy data that operates on different context levels and does not rely on manual features .",0
2141,Our architecture addresses the challenges of entity linking on question answering data and outperforms state - of - the - art EL systems .,0
2142,Code and datasets,0
2143,Our system can be applied on any QA dataset .,0
2144,The complete code as well as the scripts that produce the evaluation data can be found here : https://github.com/UKPLab/ starsem2018-entity-linking.,1
2145,"Several benchmarks exist for EL on Wikipedia texts and news articles , such as ACE and CoNLL - YAGO .",0
2146,"These datasets contain multi-sentence documents and largely cover three types of entities : Location , Person and Organization .",0
2147,"These types are commonly recognized by named entity recognition systems , such as Stanford NER Tool .",0
2148,"Therefore in this scenario , an EL system can solely focus on entity disambiguation .",0
2149,"In the recent years , EL on Twitter data has emerged as a branch of entity linking research .",0
2150,"In particular , EL on tweets was the central task of the NEEL shared task from 2014 to 2016 .",0
2151,Tweet s share some of the challenges with QA data : in both cases the input data is short and noisy .,0
2152,"On the other hand , it significantly differs with respect to the entity types covered .",0
2153,"The data for the NEEL shared task was annotated with 7 broad entity categories , that besides Location , Organization and Person include Fictional Characters , Events , Products ( such as electronic devices or works of art ) and Things ( abstract objects ) .",0
2154,"One can see on the diagram that the distribution is mainly skewed towards 3 categories : Location , Person and Organization .",0
2155,Figure 2 also shows the entity categories present in two QA datasets .,0
2156,The distribution over the categories is more diverse in this case .,0
2157,The WebQuestions dataset includes the Fictional Character and Thing categories which are almost absent from the NEEL dataset .,0
2158,"A more even distribution can be observed in the GraphQuestion dataset that features many Events , Fictional Characters and Professions .",0
2159,This means that a successful system for EL on question data needs to be able to recognize and to link all categories of entities .,0
2160,"Thus , we aim to show that comprehensive modeling of different context levels will result in a better generalization and performance across various entity categories .",0
2161,Existing Solutions,0
2162,The early machine learning approaches to EL focused on long well - formed documents .,0
2163,These systems usually rely on an off - theshelf named entity recognizer to extract entity mentions in the input .,0
2164,"As a consequence , such approaches can not handle entities of types other than those that are supplied by the named entity recognizer .",0
2165,"Named entity recognizers are normally trained to detect mentions of Locations , Organizations and Person names , whereas in the context of QA , the system also needs to cover movie titles , songs , common nouns such as ' president ' etc .",0
2166,"To mitigate this , Cucerzan ( 2012 ) has introduced the idea to perform mention detection and entity linking jointly using a linear combination of manually defined features .",0
2167,have adopted the same idea and suggested a probabilistic graphical model for the joint prediction .,0
2168,This is essential for linking entities in questions .,0
2169,"For example in "" who does maggie grace play in taken ? "" , it is hard to distinguish between the usage of the word ' taken ' and the title of a movie ' Taken ' without consulting a knowledge base .",0
2170,were among the first to use neural networks to embed the mention and the entity fora better prediction quality .,0
2171,"Later , have employed convolutional neural networks to extract features from the document context and mixed them with manually defined features , though they did not integrate it with mention detection .",0
2172,continued the work in this direction recently and applied convolutional neural networks to cross - lingual EL .,0
2173,The approaches that were developed for Twitter data present the most relevant work for EL on QA data .,0
2174,have created anew dataset of around 1500 tweets and suggested a Structured SVM approach that handled mention detection and entity disambiguation together .,0
2175,describe the winning system of the NEEL 2014 competition on EL for short texts :,0
2176,"The system adapts a joint approach similar to , but uses the MART gradient boosting algorithm instead of the SVM and extends the feature set .",0
2177,The current state - of - the - art system for EL on noisy data is S - MART which extends the approach from to make structured predictions .,0
2178,The same group has subsequently applied S - MART to extract entities fora QA system .,0
2179,"Unfortunately , the described EL systems for short texts are not available as stand - alone tools .",0
2180,"Consequently , the modern QA approaches mostly rely on off - the - shelf entity linkers that were designed for other domains .",0
2181,have employed the Freebase online API that was since deprecated .,0
2182,A number of question answering systems have relied on DBPedia Spotlight to extract entities .,0
2183,"DB - Pedia Spotlight uses document similarity vectors , word embeddings and manually defined features such as entity frequency .",0
2184,We are addressing this problem in our work by presenting an architecture specifically targeted at EL for QA data .,0
2185,The Knowledge Base,0
2186,"Throughout the experiments , we use the Wikidata 3 open - domain KB .",0
2187,"Among the previous work , the common choices of a KB include Wikipedia , DBPedia and Freebase .",0
2188,"The entities in Wikidata directly correspond to the Wikipedia articles , which enables us to work with data that was previously annotated with DBPedia .",0
2189,Freebase was discontinued and is no longer up - todate .,0
2190,"However , most entities in Wikidata have been annotated with identifiers from other knowledge sources and databases , including Freebase , which establishes a link between the two KBs .",0
2191,Entity Linking Architecture,0
2192,The overall architecture of our entity linking system is depicted in .,0
2193,From the input question x we extract all possible token n-grams N up to ax = what are taylor swift 's albums ?,0
2194,Step 1 . consider all n-grams,0
2195,Step 2 . entity candidates for an n -gram C = entity candidates ( n ),0
2196,wikidata,0
2197,Full text search,0
2198,"Step 3 . score the n - gram with the model p n , p c = M (x , n , C )",0
2199,"Step 4 . compute the global assignment of entities G = global assignment ( p n , p c , n , x |n ?",0
2200,N ) :,0
2201,Architecture of the entity linking system certain length as entity mention candidates ( Step 1 ) .,0
2202,"For each n-gram n , we look it up in the knowledge base using a full text search over entity labels .",0
2203,That ensures that we find all entities that contain the given n-gram in the label .,0
2204,"For example fora unigram ' obama ' , we retrieve ' Barack Obama ' , ' Michelle Obama ' etc .",0
2205,This step produces a set of entity disambiguation candidates C for the given ngram n .,0
2206,We sort the retrieved candidates by length and cutoff after the first 1000 .,0
2207,That ensures that the top candidates in the list would be those that exactly match the target n-gram n.,0
2208,"In the next step , the list of n-grams N and the corresponding list of entity disambiguation candidates are sent to the entity linking model ( Step 3 ) .",0
2209,The model jointly performs the detection of correct mentions and the disambiguation of entities .,0
2210,Variable Context Granularity,0
2211,Network,0
2212,"The neural architecture ( Variable Context Granularity , VCG ) aggregates and mixes contexts of different granularities to perform a joint mention detection and entity disambiguation .",0
2213,shows the layout of the network and its main components .,0
2214,"The input to the model is a list of question tokens x , a token n-gram n and a list of candidate entities C .",0
2215,"Then the model is a function M (x , n , C ) that produces a mention detection score p n for each n-gram and a ranking score p c for each of the candidates c ? C : p n , p c = M (x , n , C ) .",0
2216,Dilated Convolutions,0
2217,"To process sequential input , we use dilated convolutional networks ( DCNN ) .",0
2218,have recently shown that DCNNs are faster and as effective as recurrent models on the task of named entity recognition .,0
2219,We define two modules : DCNN wand DCNN c for processing token - level and character - level input respectively .,0
2220,"Both modules consist of a series of convolutions applied with an increasing dilation , as described in .",0
2221,The output of the convolutions is averaged and transformed by a fully - connected layer .,0
2222,Context components,0
2223,The token component corresponds to sentence - level features normally defined for EL and encodes the list of question tokens x into a fixed size vector .,0
2224,"It maps the tokens in x to d w - dimensional pre-trained word embeddings , using a matrix W ?",0
2225,"R | V w | d w , where | V w | is the size of the vocabulary .",0
2226,We use 50 - dimensional GloVe embeddings pre-trained on a 6 billion tokens corpus .,0
2227,The word embeddings are concatenated with d p -dimensional position embeddings P w ?,0
2228,R 3 d p that are used to denote the tokens that are part of the target n-gram .,0
2229,The concatenated embeddings are processed by DCNN w to get a vector o s .,0
2230,The character component processes the target token n-gram non the basis of individual characters .,0
2231,"We add one token on the left and on the right to the target mention and map the string of characters to dz - character embeddings , Z ?",0
2232,R | V z | d z .,0
2233,We concatenate the character embeddings with d p -dimensional position embeddings P z ?,0
2234,R | x |d p and process them with DCNN c to get a feature vector on .,0
2235,We use the character component with the same learned parameters to encode the label of a candidate entity from the KB as a vector o l .,0
2236,The parameter sharing between mention encoding and entity label encoding ensures that the representation of a mention is similar to the entity label .,0
2237,The KB structure is the highest context level included in the model .,0
2238,The knowledge base structure component models the entities and relations that are connected to the candidate entity c.,0
2239,"First , we map a list of relations r of the candidate entity to d r - dimensional pre-trained relations embeddings , using a matrix R ?",0
2240,"R | V r |d r , where | V r | is the number of relation types in the KB .",0
2241,We transform the relations embeddings with a single fullyconnected layer fr and then apply a max pooling operation to get a single relation vector or per entity .,0
2242,"Similarly , we map a list of entities that are immediately connected to the candidate entity e w 1 p 1 w 2 p 2 w3 p 3 w 4 p 4",0
2243,". to d e -dimensional pre-trained entity embeddings , using a matrix E ?",0
2244,"R | V e | d e , where | V e | is the number of entities in the KB .",0
2245,The entity embeddings are transformed by a fully - connected layer f e and then also pooled to produce the output o e .,0
2246,The embedding of the candidate entity itself is also transformed with f e and is stored as o d .,0
2247,"To train the knowledge base embeddings , we use the TransE algorithm .",0
2248,"Finally , the knowledge base lexical component takes the labels of the relations in r to compute lexical relation embeddings .",0
2249,"For each r ? r , we tokenize the label and map the tokens x r to word embeddings , using the word embedding matrix W.",0
2250,"To get a single lexical embedding per relation , we apply max pooling and transform the output with a fully - connected layer f rl .",0
2251,The lexical relation embeddings for the candidate entity are pooled into the vector o rl .,0
2252,Context Aggregation,0
2253,The different levels of context are aggregated and are transformed by a sequence of fully - connected layers into a final vector o c for the n-gram n and the candidate entity c.,0
2254,The vectors for each candidate are aggregated into a matrix O = [ o c | c ?,0
2255,C ] .,0
2256,We apply element - wise max pooling on O to get a single summary vector s for all entity candidates for n.,0
2257,"To get the ranking score p c for each entity candidate c , we apply a single fully - connected layer g con the concatenation of o c and the summary vector s: p c = g c ( o c s ) .",0
2258,"For the mention detection score for the n-gram , we separately concatenate the vectors for the token context o sand the character context on and transform them with an array of fully - connected layers into a vector o t .",0
2259,We concatenate o t with the summary vector sand apply another fully - connected layer to get the mention detection score p n = ? ( g n ( o t s ) ) .,0
2260,Global entity assignment,0
2261,The first step in our system is extracting all possible overlapping n-grams from the input texts .,0
2262,We assume that each span in the input text can only refer to a single entity and therefore resolve overlaps by computing a global assignment using the model scores for each n-gram ( Step 4 in ) .,0
2263,"If the mention detection score p n is above the 0.5 - threshold , the n-gram is predicted to be a correct entity mention and the ranking scores p care used to disambiguate it to a single entity candidate .",0
2264,N-grams that have p n lower than the threshold are filtered out .,0
2265,"We follow in computing the global assignment and hence , arrange all n-grams selected as mentions into non-overlapping combinations and use the individual scores p n to compute the probability of each combination .",0
2266,The combination with the highest probability is selected as the final set of entity mentions .,0
2267,"We have observed in practice a similar effect as descirbed by , namely that DCNNs are able to capture dependencies between different entity mentions in the same context and do not tend to produce overlapping mentions .",0
2268,Composite Loss Function,0
2269,Our model jointly computes two scores for each n-gram : the mention detection score p n and the disambiguation score p c .,0
2270,We optimize the parameters of the whole model jointly and use the loss function that combines penalties for the both scores for all n-grams in the input question :,0
2271,"where tn is the target for mention detection and is either 0 or 1 , t c is the target for disambiguation and ranges from 0 to the number of candidates | C |.",0
2272,"For the mention detection loss M , we include a weighting parameter ?",0
2273,for the negative class as the majority of the instances in the data are negative :,0
2274,The disambiguation detection loss Dis a maximum margin loss :,0
2275,where m is the margin value .,0
2276,"We set m = 0.5 , whereas the ?",0
2277,weight is optimized with the other hyper - parameters .,0
2278,Architecture comparison,0
2279,Datasets,0
2280,We compile two new datasets for entity linking on questions that we derive from publicly available question answering data : WebQSP and GraphQuestions .,0
2281,WebQSP contains questions that were originally collected for the WebQuestions dataset from web search logs .,0
2282,They were manually annotated with SPARQL queries that can be executed to retrieve the correct answer to each question .,0
2283,"Additionally , the annotators have also selected the main entity in the question that is central to finding the answer .",0
2284,The annotations and the query use identifiers from the Freebase knowledge base .,0
2285,We extract all entities that are mentioned in the question from the SPARQL query .,0
2286,"For the main entity , we also store the correct span in the text , as annotated in the dataset .",0
2287,"In order to be able to use Wikidata in our experiments , we translate the Freebase identifiers to Wikidata IDs .",0
2288,"The second dataset , GraphQuestions , was created by collecting manual paraphrases for automatically generated questions .",0
2289,The dataset is meant to test the ability of the system to understand different wordings of the same question .,0
2290,"In particular , the paraphrases include various references to the same entity , which creates a challenge for an entity linking system .",0
2291,The following PR F1,0
2292,Heuristic baseline 0.286 0.621 0.392 Simplified VCG 0.804 0.654 0.721 VCG 0.823 0.646 0.724,0
2293,"GraphQuestions does not contain main entity annotations , but includes a SPARQL query structurally encoded in JSON format .",0
2294,The queries were constructed manually by identifying the entities in the question and selecting the relevant KB relations .,0
2295,We extract gold entities for each question from the SPARQL query and map them to Wikidata .,0
2296,We split the WebQSP training set into train and development subsets to optimize the neural model .,0
2297,We use the GraphQuestions only in the evaluation phase to test the generalization power of our model .,0
2298,The sizes of the constructed datasets in terms of the number of questions and the number of entities are reported in .,0
2299,"In both datasets , each question contains at least one correct entity mention .",0
2300,Experiments,0
2301,Evaluation Methodology,0
2302,"We use precision , recall and F 1 scores to evaluate and compare the approaches .",0
2303,We follow and and define the scores on a per-entity basis .,0
2304,"Since there are no mention boundaries for the gold entities , an extracted entity is considered correct if it is present in the set of the gold entities for the given question .",0
2305,We compute the metrics in the micro and macro setting .,0
2306,The macro values are computed per entity class and averaged afterwards .,0
2307,"For the WebQSP dataset , we additionally perform a separate evaluation using only the information on the main entity .",0
2308,"The main entity has the information on the boundary offsets of the correct mentions and therefore for this type of evaluation , we enforce that the extracted mention has to over-emb .",0
2309,size filter size d w dz d ed rd p DCNN w DCNN c ? 50 25 50 50 5 64 64 0.5 :,0
2310,Best configuration for the VCG model lap with the correct mention .,0
2311,QA systems need at least one entity per question to attempt to find the correct answer .,0
2312,"Thus , evaluating using the main entity shows how the entity linking system fulfills this minimum requirement .",0
2313,Baselines,0
2314,Existing systems,1
2315,"In our experiments , we compare to DBPedia Spotlight that was used in several QA systems and represents a strong baseline for entity linking 4 .",1
2316,"In addition , we are able to compare to the state - of - the - art S - MART system , since their output on the WebQSP datasets was publicly released 5 .",1
2317,"The S - MART system is not openly available , it was first trained on the NEEL 2014 Twitter dataset and later adapted to the QA data .",0
2318,We also include a heuristics baseline that ranks candidate entities according to their frequency in Wikipedia .,1
2319,This baseline represents a reasonable lower bound fora Wikidata based approach .,0
2320,Simplified VCG,1
2321,"To test the effect of the end - toend context encoders of the VCG network , we define a model that instead uses a set of features commonly suggested in the literature for EL on noisy data .",0
2322,"In particular , we employ features that cover ( 1 ) frequency of the entity in Wikipedia , ( 2 ) edit distance between the label of the entity and the token n-gram , ( 3 ) number of entities and relations immediately connected to the entity in the KB , ( 4 ) word overlap between the input question and the labels of the connected entities and relations , ( 5 ) length of the n-gram .",1
2323,"We also add an average of the word embeddings of the question tokens and , separately , an average of the embeddings of tokens of entities and relations connected to the entity candidate .",0
2324,We train the simplified VCG model by optimizing the same loss function in Section 3.3 on the same data .,0
2325,Practical considerations,0
2326,"The hyper-parameters of the model , such as the dimensionality of the layers and the size of embed -",0
2327,We use the online end - point :,0
2328,http://www.,0
2329,dbpedia-spotlight.org /api,0
2330,5 https://github.com/scottyih/STAGG lists the main selected hyperparameters for the VCG model 6 and we also report the results for each model 's best configuration on the development set in .,0
2331,We observe that our model achieves the most gains in precision compared to the baselines and the previous stateof - the - art for QA data .,0
2332,VCG constantly outperforms the simplified VCG baseline that was trained by optimizing the same loss function but uses manually defined features .,0
2333,"Thereby , we confirm the advantage of the mixing context granularities strategy that was suggested in this work .",0
2334,"Most importantly , the VCG model achieves the best macro result which indicates that the model has a consistent performance on different entity classes .",0
2335,We further evaluate the developed VCG architecture on the GraphQuestions dataset against the DBPedia Spotlight .,0
2336,We use this dataset to evaluate VCG in an out - of - domain setting : neither our system nor DBPedia Spotlight were trained on it .,0
2337,Results,0
2338,The results for each model are presented in .,0
2339,We can see that GraphQuestions provides a much more difficult benchmark for EL .,0
2340,The VCG model shows the overall F- score result that is better than the DBPedia Spotlight baseline by a wide margin .,1
2341,It is notable that again our model achieves higher precision values as compared to other approaches and manages to keep a satisfactory level of recall .,1
2342,Analysis,0
2343,"In order to better understand the performance difference between the approaches and the gains of the VCG model , we analyze the results per entity class ( see ) .",0
2344,"We see that the S - MART system is slightly better in the disambiguation of Locations , Person names and a similar category of Fictional Character names , while it has : Ablation experiments for the VCG model on WEBQSP a considerable advantage in processing of Professions and Common Nouns .",0
2345,"Our approach has an edge in such entity classes as Organization , Things and Products .",0
2346,"The latter category includes movies , book titles and songs , which are particularly hard to identify and disambiguate since any sequence of words can be a title .",0
2347,VCG is also considerably better in recognizing Events .,0
2348,We conclude that the future development of the VCG architecture should focus on the improved identification and disambiguation of professions and common nouns .,0
2349,"To analyze the effect that mixing various context granularities has on the model performance , we include ablation experiment results for the VCG model ( see ) .",0
2350,We report the same scores as in the main evaluation but without individual model components that were described in Section 3 .,0
2351,We can see that the removal of the KB structure information encoded in entity and relation embeddings results in the biggest performance drop of almost 10 percentage points .,0
2352,The character - level information also proves to be highly important for the final state - of - the - art performance .,0
2353,These aspects of the model ( the comprehensive representation of the KB structure and the character - level information ) are two of the main differences of our approach to the previous work .,0
2354,"Finally , we see that excluding the token - level input and the lexical information about the related KB relations also decrease the results , albeit less dramatically .",0
2355,Conclusions,0
2356,We have described the task of entity linking on QA data and its challenges .,0
2357,The suggested new approach for this task is a unifying network that models contexts of variable granularity to extract features for mention detection and entity disambiguation .,0
2358,This system achieves state - of - the - art results on two datasets and outperforms the previous best system used for EL on QA data .,0
2359,The results further verify that modeling different types of context helps to achieve a better performance across various entity classes ( macro f-score ) .,0
2360,"Most recently , and have attempted to incorporate entity linking into a QA model .",0
2361,This offers an exciting future direction for the Variable Context Granularity model .,0
2362,title,0
2363,Knowledge - based Word Sense Disambiguation using Topic Models,1
2364,abstract,0
2365,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .,1
2366,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,1
2367,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .",0
2368,"As a result , our system is able to utilize the whole document as the context for a word to be disambiguated .",0
2369,The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions for a document are replaced by synset proportions .,0
2370,We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic - normal prior for document distribution over synsets .,0
2371,"We evaluate the proposed method on Senseval - 2 , Senseval - 3 , SemEval - 2007 , SemEval-2013 and SemEval - 2015 English All - Word WSD datasets and show that it outperforms the state - of - the - art unsupervised knowledge - based WSD system by a significant margin .",0
2372,Introduction,0
2373,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,1
2374,"WSD is an important problem in natural language processing ( NLP ) , both in its own right and as a steppingstone to more advanced tasks such as machine translation , information extraction and retrieval , and question answering .",0
2375,"WSD , being AI - complete ( Navigli 2009 ) , is still an open problem after over two decades of research .",0
2376,"Following Navigli ( 2009 ) , we can roughly distinguish between supervised and knowledge - based ( unsupervised ) approaches .",0
2377,Supervised methods require senseannotated training data and are suitable for lexical sample WSD tasks where systems are required to disambiguate a restricted set of target words .,0
2378,"However , the performance of supervised systems is limited in the all - word WSD tasks as labeled data for the full lexicon is sparse and difficult to obtain .",0
2379,"As the all - word WSD task is more challenging and has , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",0
2380,All rights reserved .,0
2381,"more practical applications , there has been significant interest in developing unsupervised knowledge - based systems .",0
2382,These systems only require an external knowledge source ( such as WordNet ) but no labeled training data .,0
2383,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",1
2384,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",1
2385,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,1
2386,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,1
2387,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",1
2388,"This makes our model similar to the correlated topic model , with the difference that our priors are not learned but fixed .",0
2389,"In particular , the values of these priors are determined using the knowledge from WordNet .",0
2390,"We evaluate our system on a set of five benchmark datasets , show that the proposed model outperforms stateof - the - art knowledge - based WSD system .",0
2391,Related Work,0
2392,Lesk ) is a classical knowledge - based WSD algorithm which disambiguates a word by selecting a sense whose definition overlaps the most with the words in its context .,0
2393,Many subsequent knowledge - based systems are based on the Lesk algorithm .,0
2394,extended,0
2395,Lesk by utilizing the definitions of words in the context and weighing the words by term frequency - inverse document frequency ( tf - idf ) .,0
2396,further enhanced,0
2397,Lesk by using word embeddings to calculate the similarity between sense definitions and words in the context .,0
2398,The above methods only use the words in the context for disambiguating the target word .,0
2399,"However , Chaplot , show that sense of a word depends on not just the words in the context but also on their senses .",0
2400,"Since the senses of the words in the context are also unknown , they need to be optimized jointly .",0
2401,"In the past decade , many graph - based unsupervised WSD methods have been developed which typically leverage the underlying structure of Lexical Knowledge Base such as Word - Net and apply well - known graph - based techniques to efficiently select the best possible combination of senses in the context .",0
2402,and build a subgraph of the entire lexicon containing vertices useful for disambiguation and then use graph connectivity measures to determine the most appropriate senses .,0
2403,"and construct a sentence - wise graph , where , for each word every possible sense forms a vertex .",0
2404,Then graph - based iterative ranking and centrality algorithms are applied to find most probable sense .,0
2405,"More recently , presented an unsupervised WSD approach based on personalized page rank over the graphs generated using Word Net .",0
2406,The graph is created by adding content words to the Word - Net graph and connecting them to the synsets in which they appear in as strings .,0
2407,"Then , the Personalized PageRank ( PPR ) algorithm is used to compute relative weights of the synsets according to their relative structural importance and consequently , for each content word , the synset with the highest PPR weight is chosen as the correct sense .",0
2408,"Chaplot , Bhattacharyya , and Paranjape ( 2015 ) present a graph - based unsupervised WSD system which maximizes the total joint probability of all the senses in the context by modeling the WSD problem as a Markov Random Field constructed using the WordNet and a dependency parser and using a Maximum A Posteriori ( MAP ) Query for inference .",0
2409,Babelfy is another graph - based approach which unifies WSD and Entity Linking .,0
2410,"It performs WSD by performing random walks with restart over BabelNet , which is a semantic network integrating WordNet with various knowledge resources .",0
2411,The WSD systems which try to jointly optimize the sense of all words in the context have a common limitation that their computational complexity scales exponentially with the number of content words in the context due to pairwise comparisons between the content words .,0
2412,"Consequently , practical implementations of these methods either use approximate or sub-optimal algorithms or reduce the size of context .",0
2413,"Chaplot , Bhattacharyya , and Paranjape ( 2015 ) limit the context to a sentence and reduce the number of pairwise comparisons by using a dependency parser to extract important relations .",0
2414,"Agirre , Lpez de Lacalle , and Soroa ( 2014 ) limit the size of context to a window of 20 words around the target word .",0
2415,"Moro , Raganato , and Navigli ( 2014 ) employ a densest subgraph heuristic for selecting high - coherence semantic interpretations of the input text .",0
2416,"In contrast , the proposed approach scales linearly with the number of words in the context while optimizing sense of all the words in the context jointly .",0
2417,"As a result , the whole document is utilized as the context for disambiguation .",0
2418,Our work is also related to who were the first to apply LDA techniques to WSD .,0
2419,"In their approach , words senses that share similar paths in the WordNet hierarchy are typically grouped in the same topic .",0
2420,"However , they observe that WordNet is perhaps not the most optimal structure for WSD .",0
2421,"Highly common , polysemous words such as man and time could potentially be associated with many different topics making decipherment of sense difficult .",0
2422,"Even rare words that differ only subtly in their sense ( e.g. , quarterback - the position and quarterback - the player himself ) could potentially only share the root node in WordNet and hence never have a chance of being on the same topic .",0
2423,"also employ the idea of global context for the task of WSD using topic models , but rather than using topic models in an unsupervised fashion , they embed topic features in a supervised WSD model .",0
2424,Word Net,0
2425,Most WSD systems use a sense repository to obtain a set of possible senses for each word .,0
2426,"WordNet is a comprehensive lexical database for the English language ( Miller 1995 ) , and is commonly used as the sense repository in WSD systems .",0
2427,"It provides a set of possible senses for each content word ( nouns , verbs , adjectives and adverbs ) in the language and classifies this set of senses by the POS tags .",0
2428,"For example , the word "" cricket "" can have 2 possible noun senses : ' cricket # n#1 : leaping insect ' and ' cricket # n # 2 : a game played with a ball and bat ' , and a single possible verb sense , ' cricket # v# 1 : ( play cricket ) ' .",0
2429,"Furthermore , Word",0
2430,Net also groups words which share the same sense into an entity called synset ( set of synonyms ) .,0
2431,Each synset also contains a gloss and example usage of the words present in it .,0
2432,"For example , ' aim # n# 2 ' , ' object # n# 2 , ' objective # n# 1 ' , ' tar - get # n # 5 ' share a common synset having gloss "" the goal intended to be attained "" .",0
2433,Word Net also contains information about different types of semantic relationships between synsets .,0
2434,"These relations include hypernymy , meronymy , hyponymy , holonymy , etc.",0
2435,shows a graph of a subset of the WordNet where nodes denote the synset and edges denote different semantic relationship between synsets .,0
2436,"For instance , ' plan of action # n # 1 ' is a meronym of ' goal # n# 1 ' , and ' pur-pose # n # 1 ' , ' aim # n# 1 ' and ' destination # n # 1 ' are hyponyms of ' goal # n # 1 ' , as shown in the figure .",0
2437,"These semantic relationships in the WordNet can be used to compute the similarity between different synsets using various standard relatedness measures ( Pedersen , Patwardhan , and Michelizzi 2004 ) .",0
2438,"Note that although WordNet is the most widely used sense repository , the sense distinctions can be too fine - grained in many scenarios .",0
2439,"This makes it difficult for expert annotators to agree on a correct sense , resulting in a very low interannotator agreement ( 72 % ) in standard WSD datasets .",0
2440,"Nevertheless , we will use WordNet for our experiments for a fair comparison with previous work .",0
2441,Methods,0
2442,Problem Definition,0
2443,"First , we formally define the task of all - word Word Sense Disambiguation by illustrating an example .",0
2444,"Consider a sentence , where we want to disambiguate all the content words ( nouns , verbs , adjectives and adverbs ) :",0
2445,They were troubled by insects while playing cricket .,0
2446,"The sense xi of each content word ( given it s part - of - speech tag ) w i can take k i possible values from the set y i = {y 1 i , y 2 i , . . . , y ki i } ( see ) .",0
2447,"In particular , the word w 5 = "" cricket "" can either meany 1 5 = "" a leaping insect "" or y 2 5 = "" a game played with a ball and bat played by two teams of 11 players . """,0
2448,"In this example , the second sense is more appropriate .",0
2449,The problem of mapping each content word in any given text to its correct sense is called the all - word WSD task .,0
2450,The set of possible senses for each word is given by a sense repository like WordNet .,0
2451,Semantics,0
2452,"In this subsection , we describe the semantic ideas underlying the proposed method and how they are incorporated in the proposed model :",0
2453,using the whole document as the context for WSD : modeled using Latent Dirichlet Allocation .,0
2454,some words in each synset are more frequent than others : modeled using non-uniform priors for the synset distribution over words .,0
2455,some synsets tend to co-occur more than others : modeled using logistic normal distribution for synset proportions in a document .,0
2456,"Wherever possible , we give examples to motivate the semantic ideas and illustrate their importance .",0
2457,Document context,0
2458,The sense of a word depends on other words in its context .,0
2459,"In the WordNet , the context of a word is defined to be the discourse that surrounds a language unit and helps to determine its interpretation .",0
2460,It is very difficult to determine the context of any given word .,0
2461,Most WSD systems use the sentence in which the word occurs as its context and each sentence is considered independent of others .,0
2462,"However , we know that a document or an article is about Here , the word ' chips ' could refer to potato chips , micro chips or poker chips .",0
2463,It is not possible to disambiguate this word without looking at other words in the document .,0
2464,"The presence of other words like ' casino ' , ' gambler ' , etc. in the document would indicate the sense of poker chips , while words like ' electronic ' and ' silicon ' indicate the sense of microchip .",0
2465,"Gale , Church , and Yarowsky ( 1992 ) also observed that words strongly tend to exhibit only one sense in a given discourse or document .",0
2466,"Thus , we hypothesize that the meaning of the word depends on words outside the sentence in which it occurs - as a result , we use the whole document containing the word as its context .",0
2467,Topic Models,0
2468,"In order to use the whole document as the context for a word , we would like to model the concepts involved in the document .",0
2469,"Topic models are suitable for this purpose , which aim to uncover the latent thematic structure in collections of documents .",0
2470,The most basic example of a topic model is Latent Dirichlet Allocation ( LDA ) .,0
2471,It is based on the key assumption that documents exhibit multiple topics ( which are nothing but distributions over some fixed vocabulary ) .,0
2472,"LDA has an implicit notion of word senses as words with several distinct meanings can appear in distinct topics ( e.g. , cricket the game in a "" sports "" topic and cricket the insect in a "" zoology "" topic ) .",0
2473,"However , since the sense notion is only implicit ( rather than a set of explicit senses for each word in WSD ) , it is not possible to directly apply LDA to the WSD task .",0
2474,"Therefore , we modify the basic LDA by representing documents by synset probabilities rather than topic probabilities and consequently , the words are generated by synsets rather than topics .",0
2475,We further modify this graphical model to incorporate the information in the WordNet as described in the following subsections .,0
2476,Synset distribution over words,0
2477,"Due to sparsity problems in large vocabulary size , the LDA model was extended to a "" smoothed "" LDA model by placing an exchangeable Dirichlet prior on topic distribution over words .",0
2478,"In an exchangeable Dirichlet distribution , each component of the parameter vector is equal to the same scalar .",0
2479,"However , such a uniform prior is not ideal for synset distribution over words since each synset contains only a fixed set of words .",0
2480,"For example , the synset defined as "" the place designated as the end ( as of a race or journey ) "" contains only ' goal ' , ' destination ' and ' finish ' .",0
2481,"Furthermore , some words in each synset are more frequent than others .",0
2482,"For example , in the synset defined as "" a person who participates in or is skilled at some game "" , the word ' player ' is more frequent than word ' participant ' , while in the synset defined as "" a theatrical performer "" , word ' actor ' is more frequent than word ' player ' .",0
2483,"Thus , we decide to have non-uniform priors for synset distribution over words .",0
2484,Document distribution over synsets,0
2485,The LDA model uses a Dirichlet distribution for the topic proportions of a document .,0
2486,"Under a Dirichlet , the components of the topic proportions vector are approximately independent ; this leads to the strong and unrealistic modeling assumption that the presence of one topic is not correlated with the presence of other topics .",0
2487,"Similarly , in our case , the presence of one synset is correlated with the presence of others .",0
2488,"For example , the synset representing the ' sloping land ' sense of the word ' bank ' is more likely to cooccur with the synset of ' river ' ( a large natural stream of water ) than the synset representing ' financial institution ' sense of the word ' bank ' .",0
2489,"Hence , we model the correlations between synsets using a logistic normal distribution for synset proportions in a document .",0
2490,Proposed Model,0
2491,"Following the ideas described in the previous subsection , we propose a probabilistic graphical model , which assumes that a corpus is generated according to the following process : where f ( ? ) = exp (? ) i exp (? i ) is the softmax function .",0
2492,Note that the prior for drawing word proportions for each sense is not symmetric : ?,0
2493,"sis a vector of length equal to word vocabulary size , having non - zero equal entries only for the words contained in synset sin Word Net .",0
2494,The graphical model corresponding to the generative process is shown in .,0
2495,illustrates a toy example of a possible word distribution in synsets and synset proportions in a document learned using the proposed model .,0
2496,Colors highlighting some of the words in the document denote the corresponding synsets they were sampled from .,0
2497,Priors,0
2498,We utilize the information in the WordNet for deciding the priors for drawing the word proportions for each synset and the synset proportions for each document .,0
2499,"The prior for distribution of synset s over words is chosen as the frequency of the words in the synset s , i.e. , ? sv = Frequency of word v in synset s.",0
2500,"The logistic normal distribution for drawing synset proportions has two priors , and ?.",0
2501,The parameter s gives the probability of choosing a synset s.,0
2502,"The frequency of the synset s would be the natural choice for s but since our method is unsupervised , we use a uniform for all synsets instead .",0
2503,The ?,0
2504,parameter is used to model the relationship between synsets .,0
2505,"Since , the inverse of covariance matrix will be used in inference , we directly choose ( i , j) th element of inverse of covariance matrix as follows : ? ? 1",0
2506,ij = Negative of similarity between synset i and synset j,0
2507,"The similarity between any two synsets in the WordNet can be calculated using a variety of relatedness measures given in WordNet : : Similarity library ( Pedersen , .",0
2508,"In this paper , we use the Lesk similarity measure as it is used in prior WSD systems .",0
2509,Lesk algorithm calculates the similarity between two synsets using the overlap between their definitions .,0
2510,Inference,0
2511,We use a Gibbs Sampler for sampling latent synsets z mn given the values of rest of the variables .,0
2512,"Given a corpus of M documents , the posterior over latent variables , i.e. the synset assignments z , logistic normal parameter ? , is as follows :",0
2513,"The word distribution p ( w mn | z mn , ? ) is multinomial in ?",0
2514,z mn and the conjugate distribution p (? s |? s ) is Dirichlet in ? s .,0
2515,"Thus , p ( w| z , ? ) p (?|? ) can be collapsed to p ( w | z , ? ) by integrating out ?",0
2516,s for all senses s to obtain : ?,0
2517,m follows a normal distribution which is not conjugate of the multinomial distribution :,0
2518,"Thus , p ( z | ? ) p ( ? | , ? ) ca n't be collapsed .",0
2519,"In typical logistic - normal topic models , a block - wise Gibbs sampling algorithm is used for alternatively sampling topic assignments and logistic - normal parameters .",0
2520,"However , since in our case the logistic - normal priors are fixed , we can sample synset assignments directly using the following equation :",0
2521,"Here , n SV sv , n SM sm and n S s correspond to standard counts :",0
2522,Experiments & Results,0
2523,"For evaluating our system , we use the English all - word WSD task benchmarks of the SensEval - 2 , ) , SemEval- 2013 and. ) standardized all the above datasets into a unified format with gold standard keys in WordNet 3.0 .",0
2524,"We use the standardized version of all the datasets and use the same experimental setting as ( Raganato , Camacho - Collados , and Navigli 2017 ) for fair comparison with prior methods .",0
2525,"In we compare our overall F 1 scores with different unsupervised systems described in Section 2 which include Banerjee03 , Basile14 , Agirre14 and Moro14 .",0
2526,"In addition to knowledge - based systems which do not require any labeled training corpora , we also report F 1 scores of the state - of - the - art supervised systems trained on SemCor and OMSTI ( Taghipour and Ng 2015 ) for comparison .",0
2527,"Zhong10 use a Support Vector Machine over a set of features which include surrounding words in the context , their PoS tags , and local collocations .",0
2528,Mel - maud16 learn context embeddings of a word and classify a test word instance with the sense of the training set word whose context embedding is the most similar to the context embedding of the test instance .,0
2529,"We also provide the F 1 scores of MFS baseline , i.e. labeling each word with its most frequent sense ( MFS ) in labeled datasets , ) and OM - STI ( Taghipour and Ng 2015 ) .",0
2530,"The proposed method , denoted by WSD - TM in the tables referring to WSD using topic models , outperforms the state - of - the - art WSD system by a significant margin ( pvalue < 0.01 ) by achieving an overall F1 - score of 66.9 as compared to Moro14 's score of 65.5 .",1
2531,"We also observe that the performance of the proposed model is not much worse than the best supervised system , Melamud16 ( 69.4 ) .",1
2532,In we report the F 1 scores on different parts of speech .,0
2533,The proposed system outperforms all previous knowledgebased systems overall parts of speech .,1
2534,This indicates that using document context helps in disambiguating words of all PoS tags .,0
2535,Discussions,0
2536,"In this section , we illustrate the benefit of using the whole document as the context for disambiguation by illustrat - ing an example .",0
2537,Consider an excerpt from the SensEval 2 dataset shown in .,0
2538,Highlighted words clearly indicate that the domain of the document is Biology .,0
2539,"While most of these words are monosemous , let 's consider disambiguating the word ' cell ' , which is highly polysemous , having 7 possible senses as shown in .",0
2540,"As shown in , the correct sense of cell ( ' cell # 2 ' ) has the highest similarity with senses of three monosemous words ' scientist ' , ' researcher ' and ' protein ' .",0
2541,"The word ' cell ' occurs 21 times in the document , and several times , the other words in the sentence are not adequate to disambiguate it .",0
2542,"Since our method uses the whole document as the context , words such as ' scientists ' , ' researchers ' and ' protein ' help in disambiguating ' cell ' , which is not possible otherwise .",0
2543,The proposed model also overcomes several limitations of topic models based on Latent Dirichlet Allocation and its variants .,0
2544,"Firstly , LDA requires the specification of the number of topics as a hyper - parameter which is difficult to tune .",0
2545,The proposed model does n't require the total number of synsets to be specified as the total number of synsets are equal to the number of synsets in the sense repository which is fixed .,0
2546,"Secondly , topics learned using LDA are often not meaningful as the words inside some topics are unrelated .",0
2547,"However , synsets are always meaningful as they contain only synonymous words .",0
2548,This is ensured in the proposed by using a non-uniform prior for word distribution in synsets .,0
2549,Conclusion,0
2550,"In this paper , we propose a novel knowledge - based WSD system based on a logistic normal topic model which incorporates semantic information about synsets as its priors .",0
2551,"The proposed model scales linearly with the number of words in the context , which allows our system to use the whole document as the context for disambiguation and outperform state - of - the - art knowledge - based WSD system on a set of benchmark datasets .",0
2552,One possible avenue for future research is to use this model for supervised WSD .,0
2553,This could be done by using sense tags from the SemCor corpus as training data in a supervised topic model similar to the one presented by .,0
2554,Another possibility would be to add another level to the hierarchy of the document generating process .,0
2555,This would allow us to bring back the notion of topics and then to define topic - specific sense distributions .,0
2556,The same model can also be extended to other problems such named - entity disambiguation .,0
2557,title,0
2558,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,1
2559,abstract,0
2560,"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora .",1
2561,The limited quantity of such corpora however restricts the coverage and the performance of these systems .,0
2562,"In this article , we propose anew method that solves these issues by taking advantage of the knowledge present in WordNet , and especially the hypernymy and hyponymy relationships between synsets , in order to reduce the number of different sense tags that are necessary to disambiguate all words of the lexical database .",0
2563,"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data .",1
2564,"In addition , we exhibit results that significantly outperform the state of the art when our method is combined with an ensembling technique and the addition of the WordNet Gloss Tagged as training corpus .",0
2565,Introduction,0
2566,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",0
2567,"Various approaches have been proposed to achieve WSD , and they are generally ordered by the type and the quantity of resources they use :",0
2568,"Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as main resources , and use algorithms such as lexical similarity measures or graph - based measures .",0
2569,"Supervised methods , on the other hand , exploit sense annotated corpora as training instances that can be used by a multiclass classifier such as SVM , or more recently by a neural network .",0
2570,Semi-supervised methods generally use unannotated data to artificially increase the quantity of sense annotated data and hence improve supervised methods .,0
2571,Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns ( for instance ) .,0
2572,"State of the art classifiers used to combine a set of specific features such as the parts of speech tags of surrounding words , local collocations and pretrained word embeddings , but they are now replaced by recurrent neural networks which learn their own representation of words .",0
2573,One of the major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora .,0
2574,"Indeed , while the lexical database WordNet , the sense inventory of reference used inmost works on WSD , contains more than 200 000 different word - sense pairs 1 , the SemCor , the corpus which is used the most in the training of supervised systems , only represents approximately 34 000 of them .",0
2575,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing , but in this work , the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet : the hypernymy and hyponymy relationships .",0
2576,Our method is based on three observations :,0
2577,"1 . A sense , its hypernym and it s hyponyms share a common idea or concept , but on different levels of abstraction .",0
2578,2 .,0
2579,"In general , a word can be disambiguated using the hypernyms of its senses , and not necessarily the senses themselves .",0
2580,3 .,0
2581,"Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .",0
2582,Contributions :,0
2583,We propose a method for reducing the vocabulary of senses of Word Net by selecting the minimal set of senses required for differentiating the meaning of every word .,1
2584,"By using this technique , and converting the sense tags present in sense annotated corpora to the most generalized sense possible , we are able to greatly improve the coverage and the generalization ability of supervised systems .",0
2585,"We start by presenting the state of the art of supervised neural architectures for word sense disambiguation , then we describe our new method for sense vocabulary reduction .",0
2586,Our method is then evaluated by measuring its contribution to a state of the art neural WSD system evaluated on classic WSD evaluation campaigns .,0
2587,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,1
2588,Neural Word Sense Disambiguation,0
2589,"The neural approaches for WSD fall into two categories : approaches based on a neural model that learns to classify a sense directly , and approaches based on a neural language model that learns to predict a word , and is then used to find the closest sense to the predicted word .",0
2590,Language Model Based WSD,0
2591,"The core of these approaches is a powerful neural language model able to predict a word with consideration for the words surrounding it , thanks to a recurrent neural network trained on a massive quantity of unannotated data .",0
2592,The main works that implement these kind of model are and .,0
2593,"Once the language model is trained , its predictions are used to produce sense vectors as the average of the word vectors predicted by the language model in the places where the words are sense annotated .",0
2594,"At test time , the language model is used to predict a vector according to the surrounding context , and the sense closest to the predicted vector is assigned to each word .",0
2595,These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner .,0
2596,"However , sense annotated corpora are still indispensable to construct the sense vectors , and the quantity of data needed for training the language model ( 100 billion tokens for , 2 billion tokens for ) makes these systems more difficult to train than those relying on sense annotated data only .",0
2597,Classification Based WSD,0
2598,"In these systems , the main neural network directly classifies and attributes a sense to each input word .",0
2599,"Sense annotations are simply seen as tags put on every word , like a POS - tagging task for instance .",0
2600,"These models are more similar to classical supervised models such as , except that the input features are not manually selected , but trained as part of the neural network ( using pre-trained word embeddings or not ) .",0
2601,"In addition , we can distinguish two separate branches of these types of neural networks :",0
2602,1 .,0
2603,"Those in which we have several distinct and small neural networks ( or classifiers ) for every different word in the dictionary ) , each of them being able to manage a particular word and its particular senses .",0
2604,"For instance , one of the classifiers is specialized into choosing between the four possible senses of the noun "" mouse "" .",0
2605,"This type of approaches is particularly fitted for the lexical sample tasks , where a small and finite set of very ambiguous words have to be sense annotated in several contexts , but it can also be used in all - words word sense disambiguation tasks .",0
2606,2 .,0
2607,Those in which we have a bigger and unique neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used .,0
2608,"The advantage of the first branch of approaches is that in order to disambiguate a word , limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words .",0
2609,"To put things in perspective , the number of senses of each word in WordNet ranges from 1 to 59 , whereas the total number of senses considering all words is 206 941 .",0
2610,The other approach however has an interesting property : all senses reside in the same vector space and hence share features in the hidden layers of the network .,0
2611,"This allows the model to predict a commonsense for two different words ( i.e. synonyms ) , but it also offers the possibility to predict a sense fora word not present in the dictionary ( e.g. neologism , spelling mistake ... ) , and let the user or the underlying system to decide afterwards what to do with this prediction .",0
2612,"In practice , this ability of merging multiple sense tags together is especially useful when working with WordNet : indeed , this lexical database is based on the notion of synonym sets or "" synsets "" , group of senses with the same meaning and definition .",0
2613,"Disambiguating with synset tags instead of sense tags is a common practice , as it effectively decreases the output vocabulary of the classifier that considers all senses in WordNet from 206 941 to 117 659 , and one can unambiguously retrieve the sense tag given a synset tag and the tagged word ( because every sense of a word belong to a different synset ) .",0
2614,"In this work , we go further into this direction and we present a method based on the hy-pernymy and hyponymy relationships present in WordNet , in order to merge synset tags together and reduce even more the output vocabulary of such neural WSD systems .",0
2615,Sense Vocabulary,0
2616,Reduction,0
2617,We can draw three issues of the current situation regarding supervised WSD systems :,0
2618,1 .,0
2619,The training of systems that directly predict a tag in the set of all WordNet senses becomes slower and take more memory the larger the output vocabulary is .,0
2620,"This output layer size going up to 206 941 if we consider all word - senses , and 117 659 if we consider all synsets .",0
2621,2 .,0
2622,"Due to the small number of manually sense annotated corpora available , a target word may never be observed during the training , and therefore the system would not be able to annotate it .",0
2623,3 .,0
2624,"For the same reason , a word may have been observed , but not all of its senses .",0
2625,"In this case the system is able to annotate the word , but if the expected sense has never been observed , the output will be wrong , regardless of the architecture of the supervised system .",0
2626,"In the SemCor for instance , the largest manually sense annotated corpus available , words are annotated with 33 760 different sense keys , which corresponds to approximately 16 % of the sense inventory of WordNet 2 .",0
2627,"Grouping together multiple senses is hence a good way to overcome all these issues : by considering that multiple tags refer in fact to the same concept , the output vocabulary decreases , the ability of the trained system to generalize improves , and also its coverage .",0
2628,"Moreover , it reflects more accurately our intuition of what a sense is : clearly the notions of "" tree "" ( with a trunk and leaves , not the mathematical graph ) and "" plant "" ( the living organism , not the industrial building ) forms a group in our mind such that observing one sense in a context should help disambiguating the other .",0
2629,From Senses to Synsets : A First Sense Vocabulary Reduction,0
2630,Word Net is a lexical database organized in sets of synonyms called synsets .,0
2631,A synset is technically a group of one or more word - senses that have the same definition and consequently the same meaning .,0
2632,"For instance , the first senses of "" eye "" , "" optic "" and "" oculus "" all refer to a common synset which definition is "" the organ of sight "" .",0
2633,"Training a WSD supervised system to predict synset tags instead of word - sense tags is a common practice , and it can be seen as a form of output vocabulary reduction based on the knowledge that is present in WordNet . :",0
2634,"Word - sense to synset vocabulary reduction applied on the first three senses of the words "" help "" , "" aid "" and "" assist "" Illustrated in , the word - sense to synset vocabulary reduction clearly helps to improve the coverage of supervised systems .",0
2635,"Indeed , if the verb "" help "" is observed in the annotated data in its first sense , and consequently with the tag "" v02553283 "" , the context surrounding the target word can be used to later annotate the verb "" assist "" or "" aid "" with the same valid tag .",0
2636,"Once applied , the number of different labels needed to coverall senses of WordNet drops from 206 941 to 117 659 ( approximately 43 % of reduction ) , and considering the SemCor , the corpus contains 26 215 different synsets , which accounts now for 22 % of this total .",0
2637,The vocab - ulary size was reduced and the coverage improved .,0
2638,"Going a little further , other information from WordNet can help the system to generalize .",0
2639,"In the next section , we describe anew method for taking advantage of the hypernymy and hyponymy relationships in order to accomplish this same idea .",0
2640,Sense Vocabulary,0
2641,Reduction through Hypernymy and Hyponymy Relationships,0
2642,"According to , hypernymy and hyponymy are two semantic relationships which correspond to a particular case of sense inclusion : the hyponym of a term is a specialization of this term , whereas its hypernym is a generalization .",0
2643,"For instance , a "" mouse "" is a type of "" rodent "" which is in turn a type of "" animal "" .",0
2644,"In WordNet , these relationships bind nearly every nouns 3 together in a tree structure that goes from the generic root , the node "" entity "" to the most specific leaves , for instance the node "" white - footed mouse "" .",0
2645,"These relationships are also present on several verbs : so for instance "" add "" is away of "" compute "" which in turn is away of "" reason "" which is away of "" think "" .",0
2646,"For the sake of WSD , just like grouping together the senses of a same synset helps to better generalize , we hypothesize that grouping together the synsets of a same hypernymy relationship also helps in the same way .",0
2647,The general idea of our method is that the most specialized concepts in WordNet are often superfluous in order to perform WSD .,0
2648,"Indeed , consider a small subset of Word - Net that only consists of the word "" mouse "" , its first sense ( the small rodent ) , its fourth sense ( the electronic device ) , and all of their hypernyms .",0
2649,This is illustrated in .,0
2650,"We can see that every concept that is more specialized than the concepts "" artifact "" and "" liv - ing_thing "" could be removed , and we could map every tag of "" mouse# 1 "" to the tag of "" liv - ing_ thing# 1 "" and we could still be able to disambiguate this word , but with a benefit : all other "" living things "" and animals in the sense annotated data could be tagged with the same sense , give examples of what is an animal and then show how to differentiate the small rodent to the hand - operated electronic device .",0
2651,"In order to achieve this goal of mapping every sense to its most generic sense still allowing to differentiate the meanings of the words , we have to consider certain difficulties that are not present with the word - sense to synset vocabulary reduction .",0
2652,"First , contrary to the synonymy relationship which is symmetric ( i.e. if A is a synonym of B then B is a synonym of A ) , the hypernymy relationship is not .",0
2653,"For instance , all mice are animals , but not all animals are mice .",0
2654,"In addition , two different senses of a word necessarily have two different synsets , but they may have the same direct hypernym , and they generally have the same inherited hypernym at a certain point .",0
2655,"For instance , we can distinguish the sense 1 of "" mouse "" which is a type of "" animal "" from the sense 4 which is a type of "" electronic device "" , but we can not distinguish them if we go too far into the hypernymy hierarchy , because both of them area type of "" physical entity "" .",0
2656,"Finally , we could think of removing a synset from the vocabulary of Word Net because it is not useful locally ( from the point of view of a specific word ) , but it could be necessary to diferentiate the meanings of another word .",0
2657,Our method thus works in two steps :,0
2658,1 .,0
2659,"We mark as "" necessary "" all synsets that are the lowest nodes of the hypernymy hierarchies of the senses of all word that can still allow to discriminate the different senses of the word .",0
2660,2 .,0
2661,"We transform our sense vocabulary by mapping every synset to the lowest synset in its hypernymy hierarchy that is marked as "" necessary "" .",0
2662,The result of this method is that the most specific synsets of the tree that are not useful for discriminating are automatically removed from the vocabulary .,0
2663,"In other words , the set of synsets that is left in the vocabulary is the smallest subset of all synsets that are necessary to distinguish every sense of every word of WordNet .",0
2664,"When applied on WordNet , the number of synset in the vocabulary now drops from 117 659 to 39 147 ( approximately 66 % of reduction ) , and applied on the SemCor , it now contains 12 779 different synsets , which counts for 32 % of coverage .",0
2665,"Again , the vocabulary size has drastically decreased , and the coverage really improved .",0
2666,"Note that if we narrow down our computation to consider only polysemic words in WordNet , the full vocabulary of all reduced synsets of WordNet is 23 148 , and the SemCor contains 9 461 of them represented , and that is a coverage of approximately 40 % .",0
2667,Experiments,0
2668,"In order to evaluate our vocabulary reduction method , we applied it on a classification based neural network ( subsection 2.2 ) capable of classifying a word in all possible synsets of WordNet .",0
2669,Our architecture is very similar to 's BiLSTM model except for the input and output vocabulary used .,0
2670,"Indeed , in their system , they have chosen to include their input vocabulary in their output vocabulary , so their network is able to predict both a sense tag when the target word has an entry in WordNet ( nouns , verbs , adjectives and averbs ) , and a word tag for every other word .",0
2671,"In our architecture , we chose to only predicts sense tags , in order to keep the output vocabulary the smallest possible .",0
2672,Then we systematically trained two models :,0
2673,1 .,0
2674,A baseline model that predicts a tag belonging to all the synset tags seen during training ( thus using the common wordsense to synset vocabulary reduction ) .,0
2675,2 .,0
2676,"A second system trained under the same conditions , but with our vocabulary reduction through hypernyms algorithm applied on the training corpus .",0
2677,Neural Architecture,0
2678,The architecture of our neural network relies on 3 layers :,0
2679,1 .,0
2680,"The input layer , which takes directly the words in a vector form , from a pre-trained word embeddings model .",0
2681,2 .,0
2682,"The hidden layer , composed of bidirectional LSTM units .",0
2683,3 .,0
2684,"The output layer , which represents for each word in input , a probability distribution overall senses in the output vocabulary used , thanks to a classical softmax function .",0
2685,"The cost function to minimize during the training is the cross entropy between the output layer and a one - hot vector , i.e. a vector for which all coordinates are set to 0 except for the coordinate at the index of the target sense which is 1 .",0
2686,"In consequence , the cost function is ? log q [ s ] , where q [ s ] is the output of the network at the index s of the target sense .",0
2687,"Our model always predicts a sense in output , for every input word , even for words that do not convey directly a meaning ( e.g. stopwords , articles , etc. ) or words that were not annotated in the training set .",0
2688,"However , we assign a special tag < skip > to these cases , allowing us to ignore the predictions made by the model and to not take it into account during the back - propagation step of the training .",0
2689,This behavior is the main difference between our architecture and the one introduced by .,0
2690,"In their model , the gradient is computed overall words of a sentence , and those that do not have a sense in WordNet are annotated with their surface form .",0
2691,"In input of our network , we used the GloVe vectors pretrained on Wikipedia 2014 and Gigaword 5 4 .",0
2692,"The dimension of the vectors is 300 , the vocabulary size is 400 000 and all words are lowercased .",0
2693,These vectors are also used as input in the network described by .,0
2694,"For the hidden layer of recurrent units , we chose LSTM cells of size 1000 for each direction .",0
2695,This is approximately the same size that was used in ( 1024 per direction ) and ) ( a single layer of size 2048 ) .,0
2696,"Finally , we applied the regularization method Dropout between the hidden layer and the output layer , with a parameter set to 50 % , in order to avoid overfitting during the training and to make the model more robust .",0
2697,"We implemented our neural network using PyTorch 5 , and our code is available at the following URL :",0
2698,https://github.com/getalp/disambiguate,0
2699,Training,0
2700,We compared our sense vocabulary reduction method on two training sets :,0
2701,"The first is the SemCor , the most popular corpus that is used for training most WSD supervised systems .",0
2702,The second is the concatenation of the SemCor and the WordNet Gloss Tagged 6 .,0
2703,"The latter is a corpus distributed as part of WordNet since its version 3.0 , and it consists of all the definitions ( glosses ) of every synset of WordNet , with every word manually or semi-automatically sense annotated .",0
2704,"We used the version of these corpora given as part of the UFSAC 2.1 resource 7 , a set of gathered publicly available sense annotated corpora converted into a clean and unified format .",0
2705,We performed every training for 20 epochs .,0
2706,"That is , the whole training set has been read 20 times .",0
2707,At the beginning of each epoch we shuffled the training set .,0
2708,"We evaluated our model at the end of every epoch on a development set , and we kept only the one which obtained the best F1 WSD score .",0
2709,"The development set was composed of 4 000 random sentences taken from the WordNet Gloss Tagged for the models trained on the SemCor , and 4 000 random sentences extracted from the training set for the other models .",0
2710,"We trained with mini-batches of 100 sentences , truncated to 80 words , and padded with zero vectors from the end , and we used Adam ( Kingma and Ba , 2014 ) , with the same default parameters described in their article as the optimization method , except for the learning rate that we set to 0.0001 ( 10 times smaller than the default value ) .",0
2711,All models have been trained on Nvidia 's Titan X GPUs .,0
2712,"The approximate training times of individual models , depending on the training corpus and if the vocabulary reduction method was applied , are displayed in the following",0
2713,Disambiguation,0
2714,"In order to disambiguate an input sequence of words using the trained model , we followed the following steps :",0
2715,"First , each word of the sequence is lowercased and transformed into a vector using the pre-trained word embeddings model , then the sequence of vectors is given as input to our model .",0
2716,"Then , we annotate each word with the one among its possible senses which has the maximum probability .",0
2717,"We first map each sense to its synset in the case of the baseline model , or map each sense to its reduced synset , in the case of the sense vocabulary reduced model , according to the method described in subsection 3.2 , then we select the one which has the maximum value in the output layer of the model .",0
2718,"Finally , if no sense is assigned , because no instance of the word has been observed in the training data , a back - off is performed .",0
2719,We chose the most common one which is to assign the first sense in WordNet .,0
2720,Evaluation,0
2721,"We evaluated our models on all evaluation corpora commonly used in WSD , that is the WSD tasks of the evaluation campaigns Sen - s Eval / Sem Eval .",0
2722,"We used the fine - grained evaluation corpora from the evaluation framework of , which consists of SensEval 2 , SensEval 3 , , SemEval 2013 task 12 and SemEval 2015 task 13 , as well as their "" ALL "" corpus consisting of the concatenation of all previous ones .",0
2723,We also compared our result on the coarse - grained task 7 of SemEval 2007 which is not present in the evaluation framework .,0
2724,"We used the version of these corpora from the UFSAC 2.1 resource 8 , the sense inventory used for the sense annotations is Word - Net 3.0 . : F1 scores ( % ) obtained by our systems against the state of the art on the English WSD tasks of the evaluation campaigns SensEval 2 ( SE2 ) , SensEval 3 ( SE3 ) , SemEval 2007 ( SE07 ) task 7 and 17 , SemEval 2013 ( SE13 ) task 12 , SemEval 2015 ( SE15 ) task 13 and the corpus composed of the concatenation of all previous ones ( ALL ) except SE07 task",0
2725,7 . Results in bold are the best results from using the sense vocabulary reduction or not .,0
2726,Results in red are to our knowledge the best results obtained on the task .,0
2727,"Our results are the mean scores of 20 individual systems , with the standard deviation given in parenthesis .",0
2728,Results prefixed by a star ( * ) was obtained on the development corpus used during the training .,0
2729,"For each evaluation , we trained 20 separated models , and we give two scores :",0
2730,"First , the mean of the F 1 scores obtained by the models , along with its standard deviation .",0
2731,"Then , the score obtained by an ensemble of the models .",0
2732,"For the ensemble , we averaged the predictions of all individual models through a geometric mean , a common practice that is used for instance in machine translation .",0
2733,Results,0
2734,"The scores obtained by our systems using a single trained model compared to the state - of the - art systems , along with the first sense baseline are present in table",0
2735,2 .,0
2736,The scores obtained by our ensemble of models are given in .,0
2737,"In subsection 3.2 , we showed that our vocabulary reduction method improves the coverage of supervised systems overall WordNet vocabulary .",1
2738,"In , we can see that this coverage improvement holds true on the evaluation tasks , for both training sets .",1
2739,"On the total of 7 253 words to annotate for the corpus "" ALL "" , the baseline system trained on the SemCor only is incapable of annotating 491 of them , and with the vocabulary reduction applied this number drops to 91 .",0
2740,"When adding the WordNet Gloss Tagged to the training set , this number is 126 for the baseline system , and with the vocabulary reduction , only 12 words can not be annotated .",0
2741,"Now if we look at the results in , the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant ( regarding the "" ALL "" column ) .",1
2742,"However we can notice a very large gap on the SemEval 2013 task , especially when the SemCor is used alone for training .",1
2743,"This can be explained by the fact that this corpus is only composed of nouns , and our method for vocabulary reduction targets this part of speech principally .",0
2744,"This is also the task where the coverage was improved the most by our method , as it can be seen in .",0
2745,"In comparison with the other works , our systems trained on the SemCor alone expose results comparable with the best system of , which is trained on the same corpus and augmented with a semi-supervised method .",0
2746,"When we add the WordNet Gloss Tagged to the training data however , we obtain systematically state of the art results on all tasks except on SensEval 3 .",1
2747,"Once again , the sense reduction method does not consistently improves or decreases the score on every task , and in overall ( task "" ALL "" ) , the result is roughly the same as without sense reduction applied .",1
2748,"Finally , in we show the results of our system ensembling 20 models by averaging the output of their last layer .",0
2749,"As we can see , ensembling is a very efficient method in WSD as it improves systematically all our results .",1
2750,"Interestingly , with ensembles , the scores are significantly higher when applying the vocabulary reduction algorithm .",1
2751,"One possible interpretation is that individual models might be more frequently "" lost "" in the sense that with the sense vocabulary reduction applied , a lot of words are annotated with the same tag , and it can make the trained model "" unsure "" about the decisions it make .",0
2752,Ensemble of models tends to prevent this problem by favoring the most probable decisions of the models .,0
2753,title,0
2754,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,1
2755,abstract,0
2756,"Contextualized word embeddings ( CWE ) such as provided by ELMo ( Peters et al. , 2018 ) , Flair NLP ( Akbik et al. , 2018 ) , or BERT ( Devlin et al. , 2019 area major recent innovation in NLP .",0
2757,CWEs provide semantic vector representations of words depending on their respective context .,0
2758,"Their advantage over static word embeddings has been shown fora number of tasks , such as text classification , sequence tagging , or machine translation .",0
2759,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .",1
2760,We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,1
2761,We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets .,0
2762,"We further show that the pre-trained BERT model is able to place polysemic words into distinct ' sense ' regions of the embedding space , while ELMo and Flair NLP do not seem to possess this ability .",0
2763,Synonymy and Polysemy of Word Representations,0
2764,"Lexical semantics is characterized by a high degree of polysemy , i.e. the meaning of a word changes depending on the context in which it is currently used .",0
2765,Word Sense Disambiguation ( WSD ) is the task to identify the correct sense of the usage of a word from a ( usually ) fixed inventory of sense identifiers .,0
2766,"For the English language , WordNet is the most commonly used sense inventory providing more than 200K word - sense pairs .",0
2767,"To train and evaluate WSD systems , a number of shared task datasets have been published in the SemEval workshop series .",0
2768,"In the lexical sample task , a training set and a test set is provided .",0
2769,The relatively large data contains one sense - annotated word per training / test instance .,0
2770,The all - words task only provides a small number of documents as test data where each ambiguous word is annotated with its sense .,0
2771,"To facilitate the comparison of WSD systems , some efforts have been made to provide a comprehensive evaluation framework , and to unify all publicly available datasets for the English language .",0
2772,"WSD systems can be distinguished into three types - knowledge - based , supervised , and semisupervised approaches .",0
2773,"Knowledge - based systems utilize language resources such as dictionaries , thesauri and knowledge graphs to infer senses .",0
2774,Supervised approaches train a machine classifier to predict a sense given the target word and its context based on an annotated training data set .,0
2775,Semisupervised approaches extend manually created training sets by large corpora of unlabeled data to improve WSD performance .,0
2776,All approaches rely on someway of context representation to predict the correct sense .,0
2777,"Context is typically modeled via dictionary resources linked with senses , or as some feature vector obtained from a machine learning model .",0
2778,A fundamental assumption in structuralist linguistics is the distinction between signifier and signified as introduced by Ferdinand de Saussure in the early 20 th century .,0
2779,"Computational linguistic approaches , when using character strings as the only representatives for word meaning , implicitly assume identity between signifier and signified .",0
2780,Different word senses are simply collapsed into the same string representation .,0
2781,"In this respect , word counting and dictionary - based approaches to analyze natural language texts have been criticized as pre-Saussurean .",0
2782,"In contrast , the distributional hypothesis not only states that meaning is dependent on context .",0
2783,It also states that words occurring in the same contexts tend to have a similar meaning .,0
2784,"Hence , a more elegant way of representing meaning has been introduced by using the contexts of a word as an intermediate semantic representation that mediates between signifier and signified .",0
2785,"For this , explicit vector representations , such as TF - IDF , or latent vector representations , with reduced dimensionality , have been widely used .",0
2786,Latent vector representations of words are commonly called word embeddings .,0
2787,"They are fixed length vector representations , which are supposed to encode semantic properties .",0
2788,"The seminal neural word embedding model , for instance , can be trained efficiently on billions of sentence contexts to obtain semantic vectors , one for each word type in the vocabulary .",0
2789,It allows synonymous terms to have similar vector representations that can be used for modeling virtually any downstream NLP task .,0
2790,"Still , a polysemic term is represented by one single vector only , which represents all of its different senses in a collapsed fashion .",0
2791,"To capture polysemy as well , the idea of word embeddings has been extended to encode word sense embeddings .",0
2792,first introduced a neural model to learn multiple embeddings for one word depending on different senses .,0
2793,"The number of senses can be defined by a given parameter , or derived automatically in a non-paramentric version of the model .",0
2794,"However , employing sense embeddings in any downstream NLP task requires a reliable WSD system in an earlier stage to decide how to choose the appropriate embedding from the sense inventory .",0
2795,Recent efforts to capture polysemy for word embeddings give upon the idea of a fixed word sense inventory .,0
2796,"Contextualized word embeddings ( CWE ) do not only create one vector representation for each type in the vocabulary , they also they produce distinct vectors for each token in a given context .",0
2797,The contextualized vector representation is supposed to represent word meaning and context information .,0
2798,This enables downstream tasks to actually distinguish the two levels of the signifier and the signified allowing for more realistic modeling of natural language .,0
2799,The advantage of such contextually embedded token representations compared to static word embeddings has been shown fora number of tasks such as text classification and sequence tagging .,0
2800,Contribution :,0
2801,We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,1
2802,"To learn the semantic capabilities of CWEs , we employ a simple , yet interpretable approach to WSD using a k -nearest neighbor classification ( kNN ) approach .",1
2803,We compare the performance of three different CWE models on four standard benchmark datasets .,0
2804,"Our evaluation yields that not all contextualization approaches are equally effective in dealing with polysemy , and that the simple kNN approach suffers severely from sparsity in training datasets .",0
2805,"Yet , by using kNN , we include provenance into our model , which allows to investigate the training sentences that lead to the classifier 's decision .",0
2806,"Thus , we are able to study to what extent polysemy is captured by a specific contextualization approach .",0
2807,"For two datasets , we are able to report new state - of - the - art ( SOTA ) results .",0
2808,Related Work,0
2809,Neural Word Sense Disambiguation,0
2810,"Several efforts have been made to induce different vectors for the multiplicity of senses a word can express. , , or induce so - called sense embeddings in a pre-training fashion .",0
2811,"While induce sense embeddings in an unsupervised way and only fix the maximum number of senses per word , require a pre-labeled sense inventory such as WordNet .",0
2812,"Then , the sense embeddings are mapped to their corresponding synsets .",0
2813,Other approaches include the re-use of pre-trained word embeddings in order to induce new sense embeddings .,0
2814,then also use induced sense embeddings for the downstream task of WSD .,0
2815,Camacho - Collados and Pilehvar ( 2018 ) provide an extensive overview of different word sense modeling approaches .,0
2816,"For WSD , ( semi- ) supervised approaches with recurrent neural network architectures represent the current state of the art .",0
2817,Two major approaches were followed .,0
2818,"First , and , for instance , compute sentence context vectors for ambiguous target words .",0
2819,"In the prediction phase , they select nearest neighbors of context vectors to determine the target word sense .",0
2820,also use unlabeled sentences in a semisupervised label propagation approach to overcome the sparse training data problem of the WSD task .,0
2821,"Second , Kgebck and Salomonsson ( 2016 ) employ a recurrent neural network to classify sense labels for an ambiguous target word given its surrounding sentence context .",0
2822,"In contrast to earlier approaches , which relied on feature engineering , their architecture only uses pretrained Glo Ve word embeddings to achieve SOTA results on two English lexical sample datasets .",0
2823,"For the all - words WSD task , also employ a recurrent neural network .",0
2824,"But instead of single target words , they sequentially classify sense labels for all tokens in a sentence .",0
2825,"They also introduce an approach to collapse the sense vocabulary from WordNet to unambiguous hypersenses , which increases the label to sample ratio for each label , i.e. sense identifier .",0
2826,"By training their network on the large sense annotated datasets SemCor and the Princeton Annotated Gloss Corpus based on WordNet synset definitions , they achieve the highest performance so far on most all - words WSD benchmarks .",0
2827,"A similar architecture with an enhanced sense vocabulary compression was applied in , but instead of GloVe embeddings , BERT wordpiece embeddings are used as input for training .",0
2828,Especially the BERT embeddings further improved the performance yielding new state - of - the - art results .,0
2829,Contextualized Word Embeddings,0
2830,The idea of modeling sentence or context - level semantics together with word - level semantics proved to be a powerful innovation .,0
2831,"For most downstream NLP tasks , CWEs drastically improved the performance of neural architectures compared to static word embeddings .",0
2832,"However , the contextualization methodologies differ widely .",0
2833,"We , thus , hypothesize that they are also very different in their ability to capture polysemy .",0
2834,"Like static word embeddings , CWEs are trained on large amounts of unlabeled data by some vari - ant of language modeling .",0
2835,"In our study , we investigate three most prominent and widely applied approaches :",0
2836,"Flair , ELMo",0
2837,"( Peters et al. , 2018 ) , and BERT .",0
2838,"Flair : For the contextualization provided in the Flair NLP framework , take a static pre-trained word embedding vector , e.g. the Glo Ve word embeddings , and concatenate two context vectors based on the left and right sentence context of the word to it .",0
2839,"Context vectors are computed by two recurrent neural models , one character language model trained from left to right , one another from right to left .",0
2840,Their approach has been applied successfully especially for sequence tagging tasks such as named entity recognition and part - of - speech tagging .,0
2841,"ELMo : Embeddings from language models ( ELMo ) approaches contextualization similar to Flair , but instead of two character language models , two stacked recurrent models for words are trained , again one left to right , and another right to left .",0
2842,"For CWEs , outputs from the embedding layer , and the two bidirectional recurrent layers are not concatenated , but collapsed into one layer by a weighted , element - wise summation .",0
2843,"BERT : In contrast to the previous two approaches , Bidirectional Encoder Representations from Transformers ( BERT ) does not rely on the merging of two uni-directional recurrent language models with a ( static ) word embedding , but provides contextualized token embeddings in an end - to - end language model architecture .",0
2844,"For this , a self - attention based transformer architecture is used , which , in combination with a masked language modeling target , allows to train the model seeing all left and right contexts of a target word at the same time .",0
2845,Self- attention and non-directionality of the language modeling task result in extraordinary performance gains compared to previous approaches .,0
2846,"According to the distributional hypothesis , if the same word regularly occurs in different , distinct contexts , we may assume polysemy of its meaning ) .",0
2847,Contextualized embeddings should be able to capture this property .,0
2848,"In the following experiments , we investigate this hypothesis on the example of the introduced models .",0
2849,SE - 2 ( Tr ) SE - 2 ( Te ) SE -3 ( Tr ) SE -3 ( Te ) S7-T7 ( coarse ) S7-T17,0
2850,Nearest Neighbor Classification for WSD,0
2851,We employ a rather simple approach to WSD using non-parametric nearest neighbor classification ( k NN ) to investigate the semantic capabilities of contextualized word embeddings .,0
2852,"Compared to parametric classification approaches such as support vector machines or neural models , k NN has the advantage that we can directly investigate the training examples that lead to a certain classifier decision .",0
2853,The k NN classification algorithm ) assigns a plurality vote of a sample 's nearest labeled neighbors in its vicinity .,0
2854,"In the most simple case , one - nearest neighbor , it predicts the label from the nearest training instance by some defined distance metric .",0
2855,"Although complex weighting schemes for kNN exist , we stick to the simple non-parametric version of the algorithm to be able to better investigate the semantic properties of different contextualized embedding approaches .",0
2856,"As distance measure for kNN , we rely on cosine distance of the CWE vectors .",0
2857,Our approach considers only senses fora target word that have been observed during training .,0
2858,We call this approach localized nearest neighbor word sense disambiguation .,0
2859,"We use spa Cy 1 for pre-processing and the lemma of a word as the target word representation , e.g. ' danced ' , ' dances ' and ' dancing ' are mapped to the same lemma ' dance ' .",0
2860,"Since BERT uses wordpieces , i.e. subword units of words instead of entire words or lemmas , we re-tokenize the lemmatized sentence and average all wordpiece CWEs that belong to the target word .",0
2861,"Moreover , for the experiments with BERT embeddings 2 , we follow the heuristic by and concatenate the averaged wordpiece vectors of the last four layers .",0
2862,1 https://spacy.io/,0
2863,2 We use the bert - large - uncased model .,0
2864,"We test different values for our single hyperparameter k ? { 1 , . . . , 10 , 50 , 100 , 500 , 1000 } .",0
2865,"Like words in natural language , word senses follow a power - law distribution .",0
2866,"Due to this , simple baseline approaches for WSD like the most frequent sense ( MFS ) baseline are rather high and hard to beat .",0
2867,Another effect of the skewed distribution are imbalanced training sets .,0
2868,"Many senses described in Word Net only have one or two example sentences in the training sets , or are not present at all .",0
2869,This is severely problematic for larger k and the default implementation of kNN because of the majority class dominating the classification result .,0
2870,"To deal with sense distribution imbalance , we modify the majority voting of k NN to k = min (k , | V s | ) where V sis the set of CWEs with the least frequent training examples fora given word sense s.",0
2871,Datasets,0
2872,"We conduct our experiments with the help of four standard WSD evaluation sets , two lexical sample tasks and two all - words tasks .",0
2873,"As lexical sample tasks , SensEval - 2 and provide a training data set and test set each .",0
2874,"The all - words tasks of SemEval 2007 Task 7 and Task 17 solely comprise test data , both with a substantial overlap of their documents .",0
2875,The two sets differ in granularity :,0
2876,"While ambiguous terms in Task 17 are annotated with one Word Net sense only , in Task 7 annotations are coarser clusters of highly similar WordNet senses .",0
2877,"For training of the allwords tasks , we use a ) the SemCor dataset , and b) the Princeton WordNet gloss corpus ( WNGT ) separately to investigate the influence of different training sets on our approach .",0
2878,"For all experiments , we utilize the suggested datasets as provided by the UFSAC Model SE - 2 SE - 3 S7 - T7 ( coarse ) S7-T17 ( fine ) , i.e. the respective training data .",0
2879,A concise overview of the data can be found in .,0
2880,"From this , we can observe that the SE - 2 and SE - 3 training data sets , which were published along with the respective test sets , provide many more examples per word and sense than SemCor or WNGT .",0
2881,Experimental Results,0
2882,We conduct two experiments to determine whether contextualized word embeddings can solve the WSD task .,0
2883,"In our first experiment , we compare different pre-trained embeddings with k = 1 .",0
2884,"In our second experiment , we test multiple values of k and the BERT pre-trained embeddings 4 in order to estimate an optimal k.",0
2885,"Further , we qualitatively examine the results to analyze , which cases can be typically solved by our approach and where it fails .",0
2886,Contextualized Embeddings,1
2887,"To compare different CWE approaches , we use k = 1 nearest neighbor classification .",0
2888,shows a high variance in performance .,0
2889,Simple k NN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE - 2 ( cp. Table 3 ) .,1
2890,BERT also outperforms all others on the SE - 3 task .,1
2891,"However , we observe a major performance drop of our approach for the two all - words WSD tasks in which no training data is provided along with the test set .",1
2892,"For S7 - T7 and S7 - T17 , the content and structure of the out - of - domain SemCor and WNGT training datasets differ drastically from those in the test data , which prevents yielding near stateof - the - art results .",0
2893,"In fact , similarity of contextualized embeddings largely relies on semantically and structurally similar sentence contexts of polysemic target words .",1
2894,"Hence , the more example sentences can be used fora sense , the higher are the chances 3 Unification of Sense Annotated Corpora and Tools .",0
2895,We are using Version 2.1 : https://github.com/getalp/,0
2896,UFSAC 4 BERT performed best in experiment one .,0
2897,that a nearest neighbor expresses the same sense .,0
2898,"As can be seen in , the SE - 2 and SE - 3 training datasets provide more CWEs for each word and sense , and our approach performs better with a growing number of CWEs , even with a higher average number of senses per word as is the casein SE - 3 .",1
2899,"Thus , we conclude that the nearest neighbor approach suffers specifically from data sparseness .",0
2900,The chances increase that aspects of similarity other than the sense of the target word in two compared sentence contexts drive the kNN decision .,0
2901,"Moreover , CWEs actually do not organize well in spherically shaped form in the embedding space .",1
2902,"Although senses might be actually separable , the nonparametric kNN classification is unable to learn a complex decision boundary focusing only on the most informative aspects of the CWE .",0
2903,Nearest Neighbors,1
2904,K- Optimization :,1
2905,"To attenuate for noise in the training data , we optimize fork to obtain a more robust nearest neighbor classification .",0
2906,shows our best results using the BERT embeddings along with results from related works .,0
2907,"For SensEval - 2 and SensEval - 3 , we achieve a new state - of - the - art result .",1
2908,We observe convergence with higher k values since our k normalization heuristic is activated .,0
2909,"For the S7 - T * , we also achieve minor improvements with a higher k , but still drastically lack behind the state of the art .",1
2910,Senses in CWE space :,1
2911,We investigate how well different CWE models encode information such as distinguishable senses in their vector space .,1
2912,"shows T - SNE plots ( van der Maaten and Hinton , 2008 ) of six different senses of the word "" bank "" in the SE - 3 training dataset encoded by the three different CWE methods .",0
2913,"For visualization purposes , we exclude senses with a frequency of less than two .",0
2914,The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot .,1
2915,"In the ELMo embedding space , the major senses are slightly more separated in different regions of the point cloud .",1
2916,"Only in the BERT embedding space , some senses form clearly separable clusters .",0
2917,"Also within the larger clusters , single senses are spread mostly in separated regions of the cluster .",0
2918,"Hence , we conclude that BERT embeddings actually seem to encode some form of sense knowledge , which also explains why kNN can be successfully applied to them .",0
2919,"Moreover , we can see why a more powerful parametric classification approach such as employed by is able to learn clear decision boundaries .",0
2920,Such clear decision boundaries seem to successfully solve the data sparseness issue of kNN .,0
2921,Error analysis :,0
2922,"From a qualitative inspection of true positive and false positive predictions , we are able to infer some semantic properties of the BERT embedding space and the used training corpora .",0
2923,"shows selected examples of polysemic words in different test sets , including their nearest neighbor from the respective training set .",0
2924,"Not only vocabulary overlap in the context as in ' along the bank of the river ' and ' along the bank of the river Greta ' ( 2 ) allows for correct predictions , but also semantic overlap as in ' little earthy bank ' and ' huge bank [ of snow ] '",0
2925,( 3 ) .,0
2926,"On the other hand , vocabulary overlap , as well as semantic relatedness as in ' land bank ' ( 5 ) can lead to false predictions .",0
2927,Another interesting example for the latter is the confusion between ' grass bank ' and ' river bank ' ( 6 ) where the nearest neighbor sentence in the training set shares some military context with the target sentence .,0
2928,"The correct sense ( bank% 1 : 17:01 :: Sloping Land ) and the predicted sense ( bank% 1 : 17:00 : : A Long Ridge or Pile [ of earth ] ) share high semantic similarity , too .",0
2929,"In this example , they might even be used interchangeably .",0
2930,Apparently this context yields higher similarity than any of the other training sentences containing ' grass bank ' explicitly .,0
2931,"In Example ( 10 ) , the targeted sense is an action , i.e. a verb sense , while the predicted sense is a noun , i.e. a different word class .",0
2932,"In general , this could be easily fixed by restricting the classifier decision to the desired POS .",0
2933,"However , while example ( 12 ) is still a false positive , it nicely shows that the simple kNN approach is able to distinguish senses byword class even though BERT never learned POS classes explicitly .",0
2934,"This effect has been investigated in - depth by , who found that each BERT layer learns different structural aspects of natural language .",0
2935,"Example ( 12 ) also emphasizes the difficulty of distinguishing verb senses itself , i.e. the correct sense label in this example is watch % 2:39:00 : : look attentively whereas the predicted label and the nearest neighbor is watch % 2:41:00 : : follow with the eyes or the mind ; observe .",0
2936,"Verb senses in WordNet are very fine grained and thus harder to distinguish automatically and by humans , too .",0
2937,Post - evaluation experiment,0
2938,"In order to address the issue of mixed POS senses , we run a further experiment , which restricts words to their lemma and their POS tag .",0
2939,shows that including the POS restriction increases the F 1 scores for S7 - T7 and S7 - T17 .,0
2940,This can be explained by the number of different POS tags that can be found in the different corpora ( c.f. ) .,0
2941,"The results are more stable with respect to their relative performance , i.e. SemCor and WNGT reach comparable scores on S7 - T17 .",0
2942,"Also , the results for SE - 2 and SE - 3 did not change drastically .",0
2943,This can be explained by the average number of POS tags a certain word is labeled with .,0
2944,This variety is much stronger in the S7-T * tasks compared to SE - *.,0
2945,Conclusion,0
2946,"In this paper , we tested the semantic properties of contextualized word embeddings ( CWEs ) to address word sense disambiguation .",0
2947,5,0
2948,"To test their capabilities to distinguish different senses of a particular word , by placing their contextualized vector ( 2 ) Soon after setting off we came to a forested valley along the banks [ Sloping Land ] of the Gwaun .",0
2949,"In my own garden the twisted hazel , corylus avellana contorta , is underplanted with primroses , bluebells and wood anemones , for that is how I remember them growing , as they still do , along the banks [ Sloping Land ] of the rive Greta",0
2950,"( 3 ) In one direction only a little earthy bank separates me from the edge of the ocean , while in the other the valley goes back for miles and miles .",0
2951,"The lake has been swept clean of snow by the wind , the sweepings making a huge bank on our side that we have to negotiate .",0
2952,"( 4 ) However , it can be possible for the documents to be signed after you have sent a payment by cheque provided that you arrange for us to hold the cheque and not pay it into the bank until we have received the signed deed of covenant .",0
2953,The purpose of these stubs in a paying - in book is for the holder to have a record of the amount of money he had deposited in his bank .,0
2954,"( 5 ) He continued : assuming current market conditions do not deteriorate further , the group , with conservative borrowings , a prime land bank and a good forward sales position can look forward to another year of growth .",0
2955,"Crest Nicholson be the exception , not have much of a land bank and rely on its skill inland buying .",0
2956,"( 6 ) The marine said , get down behind that grass bank , sir , and he immediately lobbed a mills grenade into the river .",0
2957,The guns were all along the riverbank [ Sloping Land ] as far as I could see .,0
2958,"Sem Cor S7-T17 ( 7 ) Some 30 balloon shows are held annually in the U.S. , including the world 's largest convocation of ersatz Phineas Foggs - the nine - day Albuquerque International Balloon Fiesta that attracts some 800 , 000 enthusiasts and more than 500 balloons , some of which are fetchingly shaped to resemble Carmen Miranda , Garfield or a 12 - story - high condom .",0
2959,"Homes and factories and schools and a big wide federal highway , instead of peaceful corn to rest your eyes on while you tried to rest your heart , while you tried not to look at the balloon [ Large Tough Nonrigid Bag ] and the bandstand and the uniforms and the flash of the instruments ..",0
2960,"representation into different regions of the shared vector space , we used a k-nearest neighbor approach , which allows us to investigate their properties on an example basis .",0
2961,"For experimentation , we used pre-trained models from Flair NLP , ELMo , and BERT ) .",0
2962,"Further , we tested our hypothesis on four standard benchmark datasets for word sense disambiguation .",0
2963,We conclude that WSD can be surprisingly effective using solely CWEs .,0
2964,We are even able to report improvements over state - of the - art results for the two lexical sample tasks of SenseEval - 2 and SensEval - 3 .,0
2965,"Further , experiments showed that CWEs in general are able to capture senses , i.e. words , when used in a different sense , are placed in different regions .",0
2966,"This effect appeared strongest using the BERT pre-trained model , where example instances even form clusters .",0
2967,"This might give rise to future directions of investigation , e.g. unsupervised word sense - induction using clustering techniques . :",0
2968,Percentage of senses with a certain POS tag in the corpora .,0
2969,"Since the publication of the BERT model , a number of extensions based on transformer architectures and language model pre-training have been released .",0
2970,"In future work , we plan to evaluate also XLM ( Lample and Conneau , 2019 ) , RoBERTa and XLNet ( Yang et al. , 2019 ) with our approach .",0
2971,"In our qualitative error analysis , we observed many near - misses , i.e. the target sense and the predicted sense are not particularly faraway .",0
2972,We will investigate if more powerful classification algorithms for WSD based on contextualized embeddings are able to solve this issue even in cases of extremely sparse training data .,0
2973,title,0
2974,Learning Distributed Representations of Texts and Entities from Knowledge Base,1
2975,abstract,0
2976,We describe a neural network model that jointly learns distributed representations of texts and knowledge base ( KB ) entities .,0
2977,"Given a text in the KB , we train our proposed model to predict entities that are relevant to the text .",0
2978,Our model is designed to be generic with the ability to address various NLP tasks with ease .,0
2979,We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia .,0
2980,"We evaluated the model on three important NLP tasks ( i.e. , sentence textual similarity , entity linking , and factoid question answering ) involving both unsupervised and supervised settings .",0
2981,"As a result , we achieved state - of - the - art results on all three of these tasks .",0
2982,Our code and trained models are publicly available for further academic research .,0
2983,1,0
2984,Our model Skip - gram Europe Eastern Europe ( 0.67 ) Western Europe ( 0.66 ) Central Europe ( 0.64 ) Asia ( 0.64 ) North America ( 0.64 ) Asia ( 0.85 ) Western Europe ( 0.78 ) North America ( 0.76 ) Central Europe ( 0.75 ) Americas ( 0.73 ) Golf Golf course ( 0.76 ) PGA Tour ( 0.74 ) LPGA ( 0.74 ) Professional golfer ( 0.73 ) U.S. Open ( 0.71 ) Tennis ( 0.74 ) LPGA ( 0.72 ) PGA Tour ( 0.69 ) Golf course ( 0.68 ) Nicklaus Design ( 0.66 ) Tea Coffee ( 0.82 ) Green tea ( 0.81 ),0
2985,Black tea ( 0.80 ) Camellia sinensis ( 0.78 ) Spice ( 0.76 ) Coffee ( 0.78 ) Green tea ( 0.76 ) Black tea ( 0.75 ) Camellia sinensis ( 0.74 ) Spice ( 0.73 ) Smartphone Tablet computer ( 0.93 ) Mobile device ( 0.89 ) Personal digital assistant ( 0.88 ) Android ( operating system ) ( 0.86 ) iPhone ( 0.85 ) Tablet computer ( 0.91 ) Personal digital assistant ( 0.84 ) Mobile device ( 0.84 ) Android ( operating system ) ( 0.82 ) Feature phone ( 0.82 ) Scarlett Johansson Kirsten Dunst ( 0.85 ) Anne Hathaway ( 0.85 ) Cameron Diaz ( 0.85 ) Natalie Portman ( 0.85 ) Jessica Biel ( 0.84 ) Anne Hathaway ( 0.79 ) Natalie Portman ( 0.78 ) Kirsten Dunst ( 0.78 ) Cameron Diaz ( 0.78 ),0
2986,Kate Beckinsale ( 0.77 ) The Lord of the Rings The Hobbit ( 0.85 ) J. R. R. Tolkien ( 0.84 ) The Silmarillion ( 0.81 ),0
2987,The Fellowship of the Ring ( 0.80 ),0
2988,The Lord of the Rings ( film series ) ( 0.78 ) The Hobbit ( 0.77 ) J. R. R. Tolkien ( 0.76 ) The Silmarillion ( 0.71 ),0
2989,The Fellowship of the Ring ( 0.70 ) Elvish languages ( 0.69 ) Table,0
2990,7 : Examples of top five similar entities with their cosine similarities in our learned entity representations with those of the skip - gram model .,0
2991,Introduction,0
2992,"Methods capable of learning distributed representations of arbitrary - length texts ( i.e. , fixed - length continuous vectors that encode the semantics of texts ) , such as sentences and paragraphs , have recently attracted considerable attention ( Le and .",0
2993,These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec and Glo Ve .,0
2994,Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base ( KB ) such as Wikipedia and Freebase .,0
2995,These methods encode information of entities in the KB into a continuous vector space .,0
2996,"They are shown to be effective for various KB - related tasks such as entity search , entity linking , and link prediction .",0
2997,"In this paper , we describe a novel method to bridge these two different approaches .",0
2998,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .",1
2999,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .",1
3000,We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,1
3001,"Note that , KB entities have been conventionally used to model semantics of texts .",1
3002,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .",1
3003,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .",1
3004,Based on 2 Entity annotations in Wikipedia can be viewed as supervised data of relevant entities because Wikipedia instructs its contributors to create annotations only where they are relevant in its manual :,0
3005,https://en.wikipedia.org/,0
3006,"wiki/Wikipedia:Manual_of_Style ar Xiv:1705.02494v3 [ cs.CL ] 7 Nov 2017 this fact , we hypothesize that we can use the annotations of relevant entities as the supervised data of learning text representations .",0
3007,"Furthermore , we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities , which can be beneficial for various KB - related tasks .",1
3008,"In order to test this hypothesis , we conduct three experiments involving both the unsupervised and the supervised tasks .",0
3009,"First , we use standard semantic textual similarity datasets to evaluate the quality of the learned text representations of our method in an unsupervised fashion .",0
3010,"As a result , our method clearly outperformed the state - of - the - art methods .",0
3011,"Furthermore , to test the effectiveness of our method to perform KB - related tasks , we address the following two important problems in the supervised setting : entity linking ( EL ) and factoid question answering ( QA ) .",0
3012,"In both tasks , we adopt a simple multi -layer perceptron ( MLP ) classifier with the learned representations as features .",0
3013,"We tested our method using two standard datasets ( i.e. , CoNLL 2003 and TAC 2010 ) for the EL task and a popular factoid QA dataset based on the quiz bowl quiz game for the factoid QA task .",0
3014,"As a result , our method outperformed recent state - of - the - art methods on both the EL and the factoid QA tasks .",0
3015,"Additionally , there have also been proposed methods that map words and entities into the same continuous vector space .",0
3016,"Our work differs from these works because we aim to map texts ( i.e. , sentences and paragraphs ) and entities into the same vector space .",0
3017,Our contributions are summarized as follows :,0
3018,We propose a neural network model that jointly learns vector representations of texts and KB entities .,0
3019,We train the model using a large amount of entity annotations extracted directly from Wikipedia .,0
3020,We demonstrate that our proposed representations are surprisingly effective for various NLP tasks .,0
3021,"In particular , we apply the proposed model to three different NLP tasks , namely semantic textual similarity , entity linking , and factoid question answering , and achieve stateof - the - art results on all three tasks .",0
3022,The Lord of the Rings is an epic high - fantasy novel written by English author J. R. R. Tolkien .,0
3023,Entity Annotations :,0
3024,"The Lord of the Rings , Epic ( genre ) , High fantasy , J. R. R. Tolkien :",0
3025,An example of a sentence with entity annotations .,0
3026,We release our code and trained models to the community at https://github.com/ studio-ousia /ntee to facilitate further academic research .,1
3027,Our Approach,0
3028,"In this section , we propose our approach of learning distributed representations of texts and entities in KB .",0
3029,Model,0
3030,"Given a text t ( a sequence of words w 1 , ... , w N ) , we train our model to predict entities e 1 , ... , en that appear int .",0
3031,"Formally , the probability that represents the likelihood of an entity e appearing int is defined as the following softmax function :",0
3032,"where E KB is a set of all entities in KB , and v e ?",0
3033,Rd and v t ?,0
3034,"Rd are the vector representations of the entity e and the text t , respectively .",0
3035,We compute v t using the element - wise sum of word vectors int with L 2 normalization and a fully connected layer .,0
3036,Let us denote v s as a vector of the sum of word vectors,0
3037,where W ?,0
3038,"R dd is a weight matrix , and b ?",0
3039,Rd is a bias vector .,0
3040,"Here , we initialize v wand v e using the pre-trained representations described in the next section .",0
3041,The loss function of our model is defined as follows :,0
3042,where ?,0
3043,denotes a set of pairs each of which consists of a text t and its entity annotations E tin KB .,0
3044,One problem in training our model is that the denominator in Eq. ( 1 ) is computationally very expensive because it involves summation overall entities in KB .,0
3045,"We address this problem by replacing E KB in Eq. ( 1 ) with E * , which is the union of the positive entity e and the randomly chosen k negative entities that do not appear int .",0
3046,This method can be viewed as negative sampling with a uniform negative distribution .,0
3047,"In addition , because the length of a text t is arbitrary in our model , we test the following two settings : t as a paragraph , and t as a sentence 3 .",0
3048,Parameters,0
3049,"The parameters to be learned by our model are the vector representations of words and entities in our vocabulary V , the weight matrix W , and the bias vector b.",0
3050,"Consequently , the total number of parameters in our model is",0
3051,We initialize the representations of words and entities using pre-trained representations to reduce the training time .,0
3052,We use the skip - gram model of Word2vec with negative sampling trained with Wikipedia articles .,0
3053,"In order to create a corpus for the skip - gram model from Wikipedia , we simply replace the name of each entity annotation in Wikipedia articles with the unique identifier of the entity the annotation refers to .",0
3054,This simple method enables us to easily train the distributed representations of words and entities simultaneously .,0
3055,We used a Wikipedia dump generated in July 2016 4 .,0
3056,"For the hyper- parameters of the skip - gram model , we used standard parameters such as the context window size being 10 , and the size of negative samples being 5 .",0
3057,We used the Python Word2vec implementation in Gensim 5 .,0
3058,"Additionally , the entity representations were normalized to unit length before they were used as the pre-trained representations .",0
3059,Corpus,0
3060,"We trained our model by using the English DBpedia abstract corpus , an open corpus of Wikipedia texts with entity annotations manually created by Wikipedia contributors .",0
3061,6,0
3062,It was extracted from the first introductory sections of 4.4 million Wikipedia articles .,0
3063,We train our model by iterating over the texts and their entity annotations in the corpus .,0
3064,"We used words that appear five times or more and entities that appear three times or more in the corpus , and simply ignored the other words and entities .",0
3065,"As a result , our vocabulary V consisted of 705,168 words and 957,207 entities .",0
3066,"Further , the number of valid words and entity annotations were approximately 382 million and 28 million , respectively .",0
3067,"Additionally , we also introduce one heuristic method to generate entity annotations .",0
3068,"For each text , we add a pseudo - annotation that points to the entity of which the KB page is the source of the text .",0
3069,"Because every KB page describes its corresponding entity , it typically contains many mentions referring to the entity .",0
3070,"However , because hyper - linking to the web page itself does not make sense , these kinds of mentions can not be observed as annotations in Wikipedia .",0
3071,"Therefore , we use the aforementioned heuristic method to address this problem .",0
3072,Other Details,0
3073,Our model has several hyper - parameters .,0
3074,"Following , the number of dimensions we used was d = 300 .",0
3075,"The mini-batch size was fixed at 100 , the size of negative samples k was set to 30 , and the training consisted of one epoch .",0
3076,The model was implemented using Python and Theano .,0
3077,The training took approximately six days using a NVIDIA K80 GPU .,0
3078,We trained the model using stochastic gradient descent ( SGD ) and its learning rate was controlled by RMSprop .,0
3079,"In order to evaluate our model presented in the previous section , we conduct experiments on three important NLP tasks using the representations learned by our model .",0
3080,"First , we conduct an experiment on a semantic textual similarity task in order to evaluate the quality of the learned text representations .",0
3081,"Next , we conduct experiments on two important NLP problems ( i.e. , EL and factoid QA ) in order to test the effectiveness of our proposed representations as features for downstream NLP tasks .",0
3082,"Finally , we further qualitatively analyze the learned representations .",0
3083,Note that we separately describe how we address each task using our representations in the subsection of each experiment .,0
3084,Semantic Textual Similarity,0
3085,Semantic textual similarity aims to test how well a model reflects human judgments of the semantic similarity between two sentence pairs .,0
3086,The task has been used as a standard method to evaluate the quality of distributed representations of sentences in past work .,0
3087,Setup,0
3088,We followed an existing method for our experimental setup .,0
3089,We used the public quiz bowl dataset proposed in .,0
3090,"Following past work , we only used questions belonging to the history and literature categories , and only used answers that appeared at least six times .",0
3091,"For questions referring to the same answer , we sampled 20 % of each for the development set and test sets , and the remaining 60 % for the training set .",0
3092,"As a result , we obtained 1,535 training , 511 development , and 511 test questions for history , and 2,524 training , 840 development , and 840 test questions for literature .",0
3093,"The number of possible answers was 303 and 424 in the history and literature categories , respectively .",0
3094,Baselines,0
3095,We use two types of baselines : two conventional bag - of - words ( BOW ) models and two state - of - theart neural network models .,0
3096,The details of these models are as follows :,0
3097,BOW is a conventional approach using a logistic regression ( LR ) classifier trained with binary BOW features to predict the correct answer .,1
3098,BOW - DT is based on the BOW baseline augmented with the feature set with dependency relation indicators .,1
3099,QANTA is an approach based on a recursive neural network to derive the distributed representations of questions .,1
3100,The method also uses the LR classifier with the derived representations as features .,0
3101,FTS - BRNN is based on the bidirectional recurrent neural network ( RNN ) with gated recurrent units ( GRU ) .,1
3102,"Similar to QANTA , the method adopts the LR classifier with the derived representations as features .",0
3103,shows the results of our methods compared with those of the baseline methods .,0
3104,"The results of BOW , BOW - DT , and QANTA were obtained from .",0
3105,"We also include the result reported in ( denoted by QANTAfull ) , which used a significantly larger dataset than ours for training and testing .",0
3106,Results,0
3107,The experimental results show that our NTEE model achieved the best performance compared to the other proposed models and all the baseline methods on both the history and the literature datasets . :,0
3108,Accuracies of the proposed method and the state - of - the - art methods for the factoid QA task .,0
3109,"In particular , despite the simplicity of the neural network architecture of our method compared to the state - of - the - art methods ( i.e. , QANTA and FTS - BRNN ) , our method clearly outperformed these methods .",1
3110,This demonstrates the effectiveness of our proposed representations as background knowledge for the QA task .,0
3111,We also conducted a brief error analysis using the test set of the history dataset .,0
3112,"Our observations indicated that our method mostly performed perfect in terms of predicting the types of target answers ( e.g. , locations , events , and people ) .",1
3113,"However , our method erred in delicate cases such as predicting Henry II of England instead of Henry I of England , and Syracuse , Sicily instead of Sicily .",0
3114,Entity Linking,0
3115,Entity Linking ( EL ) is the task of resolving ambiguous mentions of entities to their referent entities in KB .,0
3116,EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search .,0
3117,"The task is challenging because of the ambiguity in the meaning of entity mentions ( e.g. , "" Washington "" can refer to the state , the capital of the US , the first US president George Washington , and so forth ) .",0
3118,The key to improve the performance of EL is to accurately model the semantic context of entity mentions .,0
3119,"Because our model learns the likelihood of an entity appearance in a given text , it can naturally be used for modeling the context of EL .",0
3120,Our Method,0
3121,"Following past work ) , we address this task as a classification problem that selects the most relevant answer from the possible answers observed in the dataset .",0
3122,We adopt the same neural network architecture described in Section 3.2.2 ( see ) .,0
3123,"We use the following three features : the vector of the entity v e 14 , the vector of the question v t ( computed using Eq. ( 2 ) ) , and the dot product of v e and v t .",0
3124,Note that we do not include other features in this task .,0
3125,"The hyper - parameters used in our model ( i.e. , the number of units in the hidden layer and the dropout probability ) are shown in .",0
3126,We tuned these parameters using the development set of each dataset .,0
3127,"Unlike the EL task , we updated all parameters including representations of words and entities for training our QA method .",0
3128,We used stochastic gradient descent ( SGD ) to train the model .,0
3129,"The minibatch size was fixed at 100 , and the learning rate was controlled by RMSprop .",0
3130,We used the accuracy on the development set of each dataset to detect the best epoch .,0
3131,"Similar to the EL task , we tested the four models to initialize the representations v t and v e , i.e. , the NTEE , the fixed NTEE , the SG - proj , and the SGproj - dbp models .",0
3132,"Further , the representations of the NTEE model and the fixed NTEE model were those that were trained with the sentences because of their overall superior accuracy compared to those trained with paragraphs .",0
3133,Mention Disambiguation,0
3134,We address the mention disambiguation task using a multi - layer perceptron ( MLP ) with a single hidden layer .,0
3135,shows the architecture of our neural network model .,0
3136,The model selects an entity from among the entity candidates for each mention min a document t.,0
3137,"For each entity candidate e , we input the vector of the entity v e 9 , the vector of the document v t ( computed with Eq. ( 2 ) ) , the dot product of v e and v t 10,11 , and the small number of features for EL described below .",0
3138,"On top of these features , we stack a hidden layer with nonlinearity using rectified linear units ( ReLU ) and dropout .",0
3139,We also add an output layer onto the hidden layer and select the most relevant entity using softmax over the entity candidates .,0
3140,"Similar to past work , we include a small number of features in our model .",0
3141,"First , we use the following three standard EL features : the entity popularity of e , the prior probability of m referring toe , and the maximum prior probability of e of all mentions int .",0
3142,"In addition , we optionally add features representing string similarities between the title of e and the surface of m .",0
3143,"These similarities include whether the title of e exactly equals or contains the surface of m , and whether the title of e starts or ends with the surface of m .",0
3144,We tuned the following two hyper - parameters using the micro-accuracy on the development set of each dataset : the number of units in the hidden layer and the dropout probability .,0
3145,The results are listed in .,0
3146,"Further , we trained the model by using stochastic gradient descent ( SGD ) .",0
3147,"The learning rate was controlled by RMSprop , and the mini-batch size was set to 100 .",0
3148,We also used the micro-accuracy on the development set to locate the best epoch for testing .,0
3149,We tested the NTEE model and the fixed NTEE model to initialize the parameters of representations v t and v e .,0
3150,"Furthermore , we also tested two simple methods using the pre-trained representations ( i.e. , skip - gram ) .",0
3151,"The first method is that the representations of words and entities are initialized using the pre-trained representations presented in Section 2.2 , and the other parameters are initialized randomly ( denoted by SG - proj ) .",0
3152,The second method is the same method as in SG - proj except the training corpus of the pre-trained representations is augmented using the DBpedia abstract corpus ( denoted by SGproj - dbp ) .,0
3153,"Regarding the NTEE and the fixed NTEE models , sentences ( rather than paragraphs ) were used to train the proposed representations because of the superior performance of this approach on both the CoNLL and TAC 2010 datasets .",0
3154,"Further , we did not update our representations of words ( v w ) and entities ( v e ) in the training of our EL method , because updating them did not generally improve the performance .",0
3155,"Additionally , we used a vector filled with zeros as representations of entities that were not contained in our vocabulary .",0
3156,Factoid Question Answering,0
3157,Question Answering ( QA ) has been one of the central problems in NLP research for the last few decades .,0
3158,"Factoid QA is one of the typical types of QA that aims to predict an entity ( e.g. , events , authors , and actors ) that is discussed in a given question .",0
3159,Quiz bowl is a popular trivia quiz game in which players are asked questions consisting of 4 - 6 sentence questions describing entities .,0
3160,The dataset of the quiz bowl has been frequently used for evaluating factoid QA methods in recent literature on QA .,0
3161,"In this section , we demonstrate that our proposed representations can be effectively used as background knowledge for the QA task .",0
3162,Qualitative Analysis,0
3163,"In order to investigate what happens inside our model , we conducted a qualitative analysis using our proposed representations trained with sentences .",0
3164,"We first inspected the word representations of our model and our pre-trained representations ( i.e. , the skip - gram model ) by computing the top five similar words of five words ( i.e. , her , dry , spanish , tennis , moon ) using cosine similarity .",0
3165,The results are presented in .,0
3166,"Interestingly , our model is somewhat more specific than the skip - gram model .",0
3167,"For example , there is only one word she whose cosine similarity to the word her is more than 0.5 in our model , whereas all the corresponding similar words in the skip - gram model ( i.e. , she , his , herself , him , and mother ) satisfy that condition .",0
3168,We observe a similar trend for the similar words of dry .,0
3169,"Furthermore , all the words similar to tennis are strictly re-lated to the sport itself in our model , whereas the corresponding similar words of the skip - gram model contain broader words such as ball sports ( e.g. , badminton and volleyball ) .",0
3170,A similar trend can be observed for the similar words of spanish and moon .,0
3171,"Similarly , we also compared our entity representations with those of the pre-trained representations by computing the top five similar entities of six entities ( i.e. , Europe , Golf , Tea , Smartphone , Scarlett Johansson , and The Lord of the Rings ) with respect to cosine similarity .",0
3172,contains the results .,0
3173,"For the entities Europe and Golf , we observe similar trends to our word representations .",0
3174,"Particularly , in our model , the most similar entities of Europe and Golf are Eastern Europe and Golf course , respectively , whereas those of the skip - gram model are Asia and Tennis , respectively .",0
3175,"However , the similar entities of most entities ( e.g. , Tea , Smartphone , Scarlett Johansson and The Lord of the Rings ) appear to be similar between our model and the skipgram model .",0
3176,Related Work,0
3177,"Various neural network models that learn distributed representations of arbitrary - length texts ( e.g. , paragraphs and sentences ) have recently been proposed .",0
3178,These models aimed to produce general - purpose text representations that can be used with ease in various downstream NLP tasks .,0
3179,"Although most of these models learn text representations from an unstructured text corpus ( Le and , there have also been proposed models that learn text representations by leveraging structured linguistic resources .",0
3180,"For instance , trained their model using a large number of noisy phrase pairs retrieved from the Paraphrase Database ( PPDB ) .",0
3181,use several public dictionaries to train the model by mapping definition texts in a dictionary to representations of the words explained by these texts .,0
3182,"To our knowledge , our work is the first work to learn generic text representations with the supervision of entity annotations .",0
3183,Several methods have also been proposed for extending the word embedding methods .,0
3184,"For example , proposed a method to train word embedding with dependency - based con - .",0
3185,These models are typically based on the skip - gram model and directly model the semantic relatedness between KB entities .,0
3186,Our work differs from these studies because we aim to learn representations of arbitrary - length texts in addition to entities .,0
3187,"Another related approach is the relational embedding ( or knowledge embedding ) , which encodes entities as continuous vectors and relations as some operations on the vector space , such as vector addition .",0
3188,These models typically learn representations from large KB graphs consisting of entities and relations .,0
3189,"Similarly , the universal schema jointly learned continuous representations of KB relations , entities , and surface text patterns for the relation extraction task .",0
3190,"Finally , recently proposed a method to jointly learn the embeddings of words and entities from Wikipedia using the skip - gram model and applied it to EL .",0
3191,"Our method differs from their method in that their method does not directly model arbitrary - length texts ( i.e. , paragraphs and sentences ) , which we proved to be highly effective for various tasks in this paper .",0
3192,"Moreover , we also showed that the joint embedding of texts and entities can be applied not only to EL but also for wider applications such as semantic textual similarity and factoid QA .",0
3193,Conclusions,0
3194,"In this paper , we presented a novel model capable of jointly learning distributed representations of texts and entities from a large number of entity annotations in Wikipedia .",0
3195,Our aim was to construct the proposed general - purpose model such that it enables practitioners to address various NLP tasks with ease .,0
3196,"We achieved state - of - the - art results on three important NLP tasks ( i.e. , semantic textual similarity , entity linking , and factoid question answering ) , which clearly demonstrated the effectiveness of our model .",0
3197,"Furthermore , the qualitative analysis showed that the characteristics of our learned representations apparently differ from those of the conventional word embedding model ( i.e. , the skip - gram model ) , which we plan to investigate in more detail in the future .",0
3198,"Moreover , we make our code and trained models publicly available for future research .",0
3199,Future work includes analyzing our model more extensively and exploring the effectiveness of our model in terms of other NLP tasks .,0
3200,"We also aim to test more expressive neural network models ( e.g. , LSTM ) to derive our text representations .",0
3201,"Furthermore , we believe that one of the promising directions would be to incorporate the rich structural data of the KB such as relationships between entities , links between entities , and the hierarchical category structure of entities .",0
3202,title,0
3203,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,1
3204,abstract,0
3205,"Deep contextualized embeddings trained using unsupervised language modeling ( e.g. , ELMo and BERT ) are successful in a wide range of NLP tasks .",0
3206,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .",1
3207,Our model is based on the bidirectional transformer encoder and produces contextualized embeddings for words and entities in the input text .,0
3208,The embeddings are trained using a new masked entity prediction task that aims to train the model by predicting randomly masked entities in entityannotated texts .,0
3209,We trained the model using entity - annotated texts obtained from Wikipedia .,0
3210,We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .,1
3211,"As a result , we achieved stateof - the - art or competitive results on several standard NED datasets .",0
3212,Introduction,0
3213,Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .,0
3214,"This task is challenging owing to the ambiguity between entity names ( e.g. , "" World Cup "" ) and the entities they refer to ( e.g. , FIFA World Cup and Rugby World Cup ) .",0
3215,"Deep contextualized word embedding models , e.g. , ELMo and BERT , have recently achieved stateof - the - art results on many tasks .",0
3216,"Unlike conventional word embedding models that assign a single , fixed embedding per word , these models produce a contextualized embedding for each word in the input text using a pretrained neural network encoder .",0
3217,"The encoder can be a recurrent neural network or transformer , and is usually trained using an unsupervised objective based on language modeling .",0
3218,"For instance , proposed Masked Language Model ( MLM ) , which aims to train the embeddings by predicting randomly masked words in the text .",0
3219,"In this paper , we describe a new contextualized embedding model for words and entities for NED .",1
3220,"Following , the proposed model is based on the bidirectional transformer encoder .",1
3221,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",1
3222,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",1
3223,We trained the model using texts and their entity annotations retrieved from Wikipedia .,1
3224,We evaluated the proposed model by addressing NED using an NED model based on trained contextualized embeddings .,0
3225,The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .,0
3226,"As a result , we achieved state - of - the - art or competitive results on various standard NED datasets .",0
3227,We will release our code and trained embeddings for further research .,0
3228,Background and Related Work,0
3229,Neural network - based approaches have recently achieved strong results on NED .,0
3230,"A key component of these approaches is an embedding model of words and entities trained using a large knowledge base ( e.g. , Wikipedia ) .",0
3231,Such embedding models enable us to design NED models that capture the contextual information required to address NED .,0
3232,"These models are typically based on conventional word embedding models ( e.g. , skip - gram ) that assign a fixed embedding to each word and entity .",0
3233,"In this study , we aim to test the effectiveness of the pretrained contextualized embeddings for NED .",0
3234,Contextualized,0
3235,Embeddings of Words and Entities,0
3236,"In this section , we introduce our contextualized embedding model for words and entities .",0
3237,shows the architecture of the proposed model .,0
3238,Our model adopts a multi - layer bidirectional transformer encoder 1 with input representations described later in this section .,0
3239,"Given a sequence of tokens consisting of words and entities , the model first represents the sequence as a sequence of input embeddings , one for each token , and then the model generates a contextualized output embedding for each token .",0
3240,Both input and output embeddings have H dimensions .,0
3241,"Hereafter , we denote the number of words and that of entities in the vocabulary of our model by V wand V e , respectively .",0
3242,Input Representation,0
3243,"Similar to the approach adopted in , the input representation of a given token ( i.e. , word or entity ) is constructed by summing the following three embeddings of H dimensions :",0
3244,Token embedding is the embedding of the corresponding token .,0
3245,The matrices of the word and entity token embeddings are represented as A ? R VwH and B ?,0
3246,"R VeH , respectively .",0
3247,"Token type embedding represents the type of token , namely word type ( denoted by C word ) or entity type ( denoted by C entity ) .",0
3248,Position embedding represents the position of the token in a word sequence .,0
3249,"A word and an entity appearing at i -th position in the sequence are represented as Di and E i , respectively .",0
3250,"If an entity name contains multiple words , we compute its position embedding by averaging the embeddings of the corresponding positions ( e.g. , New York City in ) .",0
3251,"Following , we insert special word tokens [ CLS ] and [ SEP ] to the word sequence as the first and last words , respectively .",0
3252,Masked Entity Prediction,0
3253,"To train the embeddings , we propose masked entity prediction ( MEP ) , a new task based on MLM .",0
3254,"In particular , we mask some percentage of the input entities at random ; then , we train the embeddings to predict masked entities based on words and non-masked entities .",0
3255,We represent masked entities using the special [ MASK ] entity token .,0
3256,We adopt a model equivalent to the one used to predict words in MLM .,0
3257,"Specifically , we predict the original entity of a masked entity by applying the softmax function overall entities in our vocabulary : ?",0
3258,where b o ? R,0
3259,"Ve is the output bias , and m ? R H is derived as :",0
3260,"is the gelu activation function , and layer norm ( ) is the layer normalization function ( Lei .",0
3261,Training,0
3262,We used the same model configuration adopted in the BERT LARGE model .,0
3263,"In particular , we used the bidirectional transformer encoder with H = 1024 hidden dimensions , 24 hidden layers , 16 self - attention heads , and the gelu activation function .",0
3264,"We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .",0
3265,"Furthermore , we initialized the parameters of our model that were common with BERT ( i.e. , parameters in the transformer encoder and the embeddings for words ) using the uncased version of the pretrained BERT LARGE model .",0
3266,2,0
3267,"Other parameters , namely the parameters in the MEP and the embeddings for entities , were initialized randomly .",0
3268,The model was trained via iterations over Wikipedia pages in a random order for two epochs .,0
3269,"We generated input sequences by splitting the content of each page into sequences consisting of ? 512 words and their entity annotations ( i.e. , hyperlinks ) .",0
3270,"We used the December 2018 version of Wikipedia , consisting of approximately 3.5 billion words and 11 million entity annotations .",0
3271,We masked 30 % of all entities in each sequence at random .,0
3272,"The input text was lowercased and tokenized to words 3 using the BERTs sub - word tokenizer with its vocabulary consisting of V w = 30 , 000 words .",0
3273,"Similar to Ganea and Hofmann , we built an entity vocabulary consisting of V e = 221 , 965 entities that were contained in the entity candidates in the NED datasets described in Section 4.1 .",0
3274,"We used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .",0
3275,The batch size was set to 252 .,0
3276,We trained the model by maximizing the log likelihood of MEP 's predictions .,0
3277,"To stabilize training , we updated only parameters that were initialized randomly ( i.e. , fixed the parameters initialized using BERT ) at the first epoch , and updated all parameters at the second epoch .",0
3278,The training took approximately six days using eight Tesla V100 GPUs .,0
3279,NED Based on Pre-trained Contextualized Embeddings,0
3280,"In this section , we address NED based on the proposed contextualized embeddings .",0
3281,Experimental Setup,0
3282,"Our experimental setup described in this section follows past work ( Ganea and Hofmann , 2017 ; .",0
3283,"In particular , we test the NED models using in - domain and out - domain scenarios .",0
3284,"In the in - domain scenario , we use the train and test b sets of the AIDA - CoNLL dataset to train and test the models , respectively .",0
3285,"In the out - domain scenario , we test the generalization ability of the models on five test sets : MSNBC ( MSB ) , AQUAINT ( AQ ) , ACE2004 ( ACE ) , which were cleaned by , WNED - CWEB ( CWEB ) , and WNED - WIKI ( WW ) , which were obtained from ClueWeb and Wikipedia .",0
3286,"For all datasets in both scenarios , we use the standard KB + YAGO entity candidates and their associated prior probabilities ( p ( e |m ) ) ( Ganea and Hofmann , 2017 ) , and use only the top 30 candidates based onp ( e |m ) .",0
3287,We consider only mentions that refer to valid entities in Wikipedia .,0
3288,"We report the accuracy for the indomain scenario , and the micro F1 score ( averaged per mention ) for the out - domain scenario .",0
3289,Model Inputs,0
3290,"For each mention in the input document , we create an input sequence consisting of ( 1 ) a masked entity corresponding to the mention , ( 2 ) words in the document 4 , and optionally ( 3 ) entities obtained from pseudo entity annotations .",0
3291,Pseudo entity annotations are created by treating all mentions except the target mention in the document as entity annotations referring to their entity candidates .,0
3292,"For each mention , we create a pseudo entity annotation for each entity candidate of the mention .",0
3293,"For efficiency , we ignore a candidate if itsp ( e |m ) is less than 0.1 .",0
3294,Note that the contextual information obtained from entities appearing in the same document has been considered as critical in improving NED and a main focus of the past literature on NED .,0
3295,"Because the transformer encoder is based on a neural attention mechanism to compute the embedding of a token by automatically attending relevant tokens in the input document , we assume that the trained transformer encoder can selectively attend to relevant entities if we input noisy and likely irrelevant entities based on the pseudo annotations .",0
3296,Model,0
3297,Our NED model is based on our pre-trained contextualized embeddings .,0
3298,"For each entity mention with its K entity candidates , our NED model first takes the input sequence described above , and computes the vector m ?",0
3299,R H corresponding to the mention using Eq. ( 2 ) .,0
3300,"Then , the model predicts the referent entity using the softmax function over the entity candidates :",0
3301,where B * ?,0
3302,R KH and b * o ?,0
3303,"R K consists of the entity token embeddings and the output bias values corresponding to the entity candidates , respectively .",0
3304,"Note that B * and b * o are the subsets of B and b o , respectively .",0
3305,"In the in - domain scenario , we fine - tuned the model by maximizing the log likelihood of the NED predictions on the training set of the AIDA - CoNLL dataset .",0
3306,"During the training , we fixed the entity token embeddings ( B and B * ) and output bias ( b o and b * o ) , and updated all other parameters .",0
3307,"We set the batch size to 32 , and used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .",0
3308,The training consisted of two epochs .,0
3309,5,0
3310,"In the outdomain scenario , we did not perform fune - tuning .",0
3311,Results,0
3312,The results of the in - domain scenario are shown in .,0
3313,"6 Several models , including our models , report the 95 % confidence intervals obtained over five runs .",0
3314,"As shown , our models outperformed all previously proposed models .",1
3315,"Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .",1
3316,Methods,0
3317,Accuracy 88.7 89.0 91.0 91.5 Ganea and Hofmann 92.22 0.14 93.0 93.07 0.27 Our 94.0 0.28 Our ( + pseudo entities ),0
3318,94.3 0.25 The results of the out - domain scenario are shown in .,0
3319,"Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .",1
3320,"Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .",1
3321,"Note that unlike all the past models shown in except , our models used in the out - domain scenario were trained only on entity annotations retrieved from Wikipedia .",0
3322,"Therefore , our models can be easily applied to any languages in which Wikipedia is available .",0
3323,Conclusions,0
3324,"In this work , we proposed a contextualized embedding model of words and entities for NED .",0
3325,We also introduced MEP to train the model using entityannotated texts as inputs .,0
3326,We trained the model using Wikipedia and evaluated the effectiveness of the model by addressing NED using various standard NED datasets .,0
3327,The experimental results show the competitiveness of our model across a wide range of NED datasets .,0
3328,"In the future , we intend to improve our NED model by modeling the global coherence of the disambiguated entities .",0
3329,title,0
3330,Incorporating Glosses into Neural Word Sense Disambiguation,1
3331,abstract,0
3332,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,1
3333,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,1
3334,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",0
3335,"In this paper , we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge .",0
3336,"Therefore , we propose GAS : a gloss - augmented WSD neural network which jointly encodes the context and glosses of the target word .",0
3337,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",0
3338,We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information .,0
3339,The experimental results show that our model outperforms the state - of - theart systems on several English all - words WSD datasets 1 .,0
3340,Introduction,0
3341,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,0
3342,There are several lines of research on WSD .,0
3343,Knowledge - based methods focus on exploiting lexical resources to infer the senses of word in the context .,0
3344,Supervised methods usually train multiple classifiers with manual designed features .,0
3345,"Although supervised methods can achieve the state - of - the - art performance , there are still two major challenges .",0
3346,"Firstly , supervised methods usually train a dedicated classifier for each word individually ( often called word expert ) .",0
3347,So it can not easily scale up to all - words WSD task which requires to disambiguate all the polysemous word in texts,0
3348,2 .,0
3349,"Recent neural - based methods solve this problem by building a unified model for all the polysemous words , but they still ca n't beat the best word expert system .",0
3350,"Secondly , all the neural - based methods always only consider the local context of the target word , ignoring the lexical resources like which are widely used in the knowledge - based methods .",0
3351,"The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .",0
3352,"Recent studies have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of To this end , our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words .",0
3353,We further consider extending the original gloss through its semantic relations in our framework .,0
3354,"As shown in , the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation .",0
3355,"Therefore , we integrate not only the original gloss but also :",0
3356,"The hypernym ( green node ) and hyponyms ( blue nodes ) for the 2nd sense bed 2 of bed , which means a plot of ground in which plants are growing , rather than the bed for sleeping in .",0
3357,"The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc .",0
3358,the related glosses of hypernyms and hyponyms into the neural network .,0
3359,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",1
3360,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,1
3361,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",1
3362,The main contributions of this paper are listed as follows :,0
3363,"To the best of our knowledge , our model is the first to incorporate the glosses into an end - to - end neural WSD model .",0
3364,"In this way , our model can benefit from not only massive labeled data but also rich lexical knowledge .",0
3365,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",0
3366,We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context .,0
3367,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,0
3368,The experimental results on several English all - words WSD benchmark datasets show that our model outperforms the state - of - theart systems .,0
3369,Related Work,0
3370,"Knowledge - based , supervised and neural - based methods have already been applied to WSD task ) .",0
3371,Knowledge - based WSD methods mainly exploit two kinds of knowledge to disambiguate polysemous words :,0
3372,"1 ) The gloss , which defines a word sense meaning , is mainly used in Lesk algorithm and its variants .",0
3373,"2 ) The structure of the semantic network , whose nodes are synsets 3 and edges are semantic relations , is mainly used in graph - based algorithms .",0
3374,Supervised methods usually involve each target word as a separate classification problem ( often called word expert ) and train classifiers based on manual designed features .,0
3375,"Although word expert supervised WSD methods perform best in terms of accuray , they are less flexible than knowledge - based methods in the allwords WSD task .",0
3376,"To deal with this problem , recent neural - based methods aim to build a unified classifier which shares parameters among all the polysemous words .",0
3377,leverages the bidirectional long short - term memory network which shares model parameters among all the polysemous words .,0
3378,transfers the WSD problem into a neural sequence labeling task .,0
3379,"However , none of the neural - based methods can totally beat the best word expert supervised methods on English all - words WSD datasets .",0
3380,"What 's more , all of the previous supervised methods and neural - based methods rarely take the lexical resources like WordNet into consideration .",0
3381,Recent studies on sense embeddings have proved that lexical resources are helpful .,0
3382,trains word sense embeddings through learning sentence level embeddings from glosses using a convolutional neural networks .,0
3383,extends word embeddings to sense embeddings by using the constraints and semantic relations in Word Net .,0
3384,They achieve an improvement of more than 1 % in WSD performance when using sense embeddings as WSD features for SVM classifier .,0
3385,This work shows that integrating structural information of lexical resources can help to word expert supervised methods .,0
3386,"However , sense embeddings can only indirectly help to WSD ( as SVM classifier features ) .",0
3387,shows that the coarse - grained semantic labels in WordNet can help to WSD in a multi - task learning framework .,0
3388,"As far as we know , there is no study directly integrates glosses or semantic relations of the Word - Net into an end - to - end model .",0
3389,"In this paper , we focus on how to integrate glosses into a unified neural WSD system .",0
3390,Memory network is initially proposed to solve question answering problems .,0
3391,"Recent researches show that memory network obtains the state - of - the - art results in many NLP tasks such as sentiment classification and analysis , poetry generation , spoken language understanding , etc .",0
3392,"Inspired by the success of memory network used in many NLP tasks , we introduce it into WSD .",0
3393,We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses .,0
3394,Incorporating Glosses into Neural Word Sense Disambiguation,0
3395,"In this section , we first give an overview of the proposed model GAS : a gloss - augmented WSD neural network which integrates the context and the glosses of the target word into a unified framework .",0
3396,"After that , each individual module is described in detail .",0
3397,Architecture of GAS,0
3398,The overall architecture of the proposed model is shown in .,0
3399,It consists of four modules :,0
3400,Context Module :,0
3401,The context module encodes the local context ( a sequence of surrounding words ) of the target word into a distributed vector representation .,0
3402,Gloss Module :,0
3403,"Like the context module , the gloss module encodes all the glosses of the target word into a separate vector representations of the same size .",0
3404,"In other words , we can get |s t | word sense representations according to |s t | 4 senses of the target word , where |s t | is the sense number of the target word wt .",0
3405,Memory Module :,0
3406,The memory module is employed to model the semantic relationship between the context embedding and gloss embedding produced by context module and gloss module respectively .,0
3407,Scoring Module :,0
3408,"In order to benefit from both labeled contexts and gloss knowledge , the scoring module takes the context embedding from context module and the last step result from the memory module as input .",0
3409,Finally it generates a probability distribution overall the possible senses of the target word .,0
3410,Detailed architecture of the proposed model is shown in .,0
3411,The next four sections will show detailed configurations in each module .,0
3412,Context Module,0
3413,"Context module encodes the context of the target word into a vector representation , which is also called context embedding in this paper .",0
3414,We leverage the bidirectional long short - term memory network ( Bi - LSTM ) for taking both the preceding and following words of the target word into consideration .,0
3415,"The input of this module [ x 1 , . . . , x t?1 , x t+1 , . . . , x Tx ] is a sequence of words surrounding the target word x t , where T x is the length of the context .",0
3416,After applying a lookup operation over the pre-trained word embedding matrix M ?,0
3417,"R DV , we transfer a one hot vector xi into a D-dimensional vector .",0
3418,"Then , the forward LSTM reads the segment ( x 1 , . . . , x t?1 ) on the left of the target word x t and calculates a sequence of forward hidden states",0
3419,The backward LSTM reads the,0
3420,Scoring Module,0
3421,"The scoring module calculates the scores for all the related senses {s 1 t , s 2 t , . . . , s pt } corresponding to the target word x t and finally outputs a sense probability distribution overall senses .",0
3422,The overall score for each word sense is determined by gloss attention ?,0
3423,"TM i from the last pass in the memory module , where TM is the number of passes in the memory module .",0
3424,The e TM ( ? TM without Softmax ) is regarded as the gloss score .,0
3425,"Meanwhile , a fully - connected layer is employed to calculate the context score .",0
3426,where W xt ?,0
3427,"R | st|2 n , b xt ? R | st | , |s t | is the number of senses for the target word x t and n is the number of hidden units in the LSTM .",0
3428,"It 's noteworthy that in Equation 11 , each ambiguous word x t has its corresponding weight matrix W xt and bias b xt in the scoring module .",0
3429,"In order to balance the importance of background knowledge and labeled data , we introduce a parameter ? ?",0
3430,RN 8 in the scoring module which is jointly learned during the training process .,0
3431,The probability distribution ?,0
3432,overall the word senses of the target word is calculated as :,0
3433,where ?,0
3434,"xt is the parameter for word x t , and ? xt ?",0
3435,"[ 0 , 1 ] .",0
3436,"During training , all model parameters are jointly learned by minimizing a standard crossentropy loss between ?",0
3437,and the true label y.,0
3438,Gloss Module,0
3439,"The gloss module encodes each gloss of the target word into a fixed size vector like the context vector c , which is also called gloss embedding .",0
3440,We further enrich the gloss information by taking semantic relations and their associated glosses into consideration .,0
3441,This module contains a gloss reader layer and a relation fusion layer .,0
3442,Gloss reader layer generates a vector representations fora gloss .,0
3443,Relation fusion layer aims at modeling the semantic relations of each gloss in the expanded glosses list which consists of related glosses of the original gloss .,0
3444,Our model GAS with extended glosses is denoted as GAS ext .,0
3445,"GAS only encodes the original gloss , while GAS ext encodes the expanded glosses from hypernymy and hyponymy relations ( details in ) .",0
3446,Original Gloss,0
3447,"Extended Glosses g i : Detailed architecture of our proposed model , which consists of a context module , a gloss module , a memory module and a scoring module .",0
3448,The context module encodes the adjacent words surrounding the target word into a vector c.,0
3449,The gloss module encodes the original gloss or extended glosses into a vector g i .,0
3450,"In the memory module , we calculate the inner relationship ( as attention ) between context c and each gloss g i and then update the memory as mi at pass i .",0
3451,"In the scoring module , we make final predictions based on the last pass attention of memory module and the context vector c. Note that GAS only uses the original gloss , while GAS ext uses the entended glosses through hypernymy and hyponymy relations .",0
3452,"In other words , the relation fusion layer ( grey dotted box ) only belongs to GAS ext .",0
3453,"segment ( x Tx , . . . , x t+1 ) on the right of the target word x t and calculates a sequence of backward hidden states ( ? ? h Tx , . . . , ? ? h t+1 ) .",0
3454,The context vector c is finally concatenated as,0
3455,where : is the concatenation operator .,0
3456,Gloss Reader Layer,0
3457,Gloss reader layer contains two parts : gloss expansion and gloss encoder .,0
3458,Gloss expansion is to enrich the original gloss information through its hypernymy and hyponymy relations in WordNet .,0
3459,Gloss encoder is to encode each gloss into a vector representation .,0
3460,Gloss Expansion :,0
3461,We only expand the glosses of nouns and verbs via their corresponding hypernyms and hyponyms .,0
3462,There are two reasons :,0
3463,One is that most of polysemous words ( about 80 % ) are nouns and verbs ; the other is that the most frequent relations among word senses for nouns and verbs are the hypernymy and hyponymy relations 5 .,0
3464,The original gloss is denoted as g 0 .,0
3465,Breadthfirst search method with a limited depth K is employed to extract the related glosses .,0
3466,"The glosses of hypernyms within K depth are denoted as [ g ?1 , g ?2 , . . . , g ?L 1 ].",0
3467,"The glosses of hyponyms within K depth are denoted as [ g + 1 , g +2 , . . . , g +L",0
3468,2 ] 6 . Note that g + 1 and g ? 1 are the glosses of the nearest word sense .,0
3469,Gloss Encoder :,0
3470,We denote the j -th 7 gloss in the expanded glosses list for i th sense of the target word as a sequence of G words .,0
3471,"Like the context encoder , the gloss encoder also leverages Bi - LSTM units to process the words sequence of the gloss .",0
3472,The gloss representation g i j is computed as the concatenation of the last hidden states of the forward and backward LSTM .,0
3473,"where j ? [?L 1 , . . . , ? 1 , 0 , + 1 , . . . , + L 2 ] and : is the concatenation operator .",0
3474,Relation Fusion Layer,0
3475,Relation fusion layer models the hypernymy and hyponymy relations of the target word sense .,0
3476,A forward LSTM is employed to encode the hypernyms ' glosses of i th sense,0
3477,.,0
3478,"In order to highlight the original gloss g i 0 , the enhanced i th sense representation is concatenated as the final state of the forward and backward LSTM .",0
3479,Memory Module,0
3480,"The memory module has two inputs : the context vector c from the context module and the gloss vectors {g 1 , g 2 , . . . , g |st | } from the gloss module , where |s t | is the number of word senses .",0
3481,We model the inner relationship between the context and glosses by attention calculation .,0
3482,"Since onepass attention calculation may not fully reflect the relationship between the context and glosses ( details in Section 4.4.2 ) , the memory module adopts a repeated deliberation process .",0
3483,"The process repeats reading gloss vectors in the following passes , in order to highlight the correct word sense for the following scoring module by a more accurate attention calculation .",0
3484,"After each pass , we update Since one synset has one or more direct hypernyms and hyponyms , L1 >= K and L2 >= K. 7 Since GAS do n't have gloss expansion , j is always 0 and gi = g i 0 .",0
3485,See more in .,0
3486,the memory to refine the states of the current pass .,0
3487,"Therefore , memory module contains two phases : attention calculation and memory update .",0
3488,Attention Calculation :,0
3489,"For each pass k , the attention e k i of gloss g i is generally computed as",0
3490,where m k ?1 is the memory vector in the ( k ? 1 ) th pass while c is the context vector .,0
3491,"The scoring function f calculates the semantic relationship of the gloss and context , taking the vector set ( g i , m k?1 , c ) as input .",0
3492,"In the first pass , the attention reflects the similarity of context and each gloss .",0
3493,"In the next pass , the attention reflects the similarity of adapted memory and each gloss .",0
3494,A dot product is applied to calculate the similarity of each gloss vector and context ( or memory ) vector .,0
3495,We treat c as m 0 .,0
3496,"So , the attention ?",0
3497,k i of gloss g i at pass k is computed as a dot product of g i and m k?1 :,0
3498,Memory Update :,0
3499,"After calculating the attention , we store the memory state in u k which is a weighted sum of gloss vectors and is computed as",0
3500,where n is the hidden size of LSTM in the context module and gloss module .,0
3501,"And then , we update the memory vector m k from last pass memory m k?1 , context vector c , and memory state u k .",0
3502,We propose two memory update methods :,0
3503,Linear : we update the memory vector m k by a linear transformation from m k?1,0
3504,where H ?,0
3505,R 2 n2 n .,0
3506,Concatenation : we get anew memory for kth pass by taking both the gloss embedding and context embedding into consideration,0
3507,"where : is the concatenation operator , W ?",0
3508,R n6 n and b ?,0
3509,R 2 n .,0
3510,Experiments and Evaluation,0
3511,Dataset,0
3512,Evaluation Dataset : we evaluate our model on several English all - words WSD datasets .,0
3513,"For fair comparison , we use the benchmark datasets proposed by which includes five standard all - words fine - grained WSD datasets from the Senseval and SemEval competitions .",0
3514,"They are Senseval - 2 ( SE2 ) , Senseval - 3 task 1 ( SE3 ) , SemEval - 07 task 17 ( SE7 ) , SemEval - 13 task 12 ( SE13 ) , and SemEval - 15 task 13 ( SE15 ) .",0
3515,"Following by , we choose SE7 , the smallest test set as the development ( validation ) set , which consists of 455 labeled instances .",0
3516,"The last four test sets consist of 6798 labeled instances with four types of target words , namely nouns , verbs , adverbs and adjectives .",0
3517,We extract word sense glosses from WordNet3.0 because maps all the sense annotations 9 from its original version to 3.0 .,0
3518,Training Dataset :,0
3519,"We choose SemCor 3.0 as the training set , which was also used by , , , , etc .",0
3520,"It consists of 226,036 sense annotations from 352 documents , which is the largest manually annotated corpus for WSD .",0
3521,Note that all the systems listed in are trained on SemCor 3.0 .,0
3522,Implementation Details,0
3523,"We use the validation set ( SE7 ) to find the optimal settings of our framework : the hidden state size n , the number of passes | T M | , the optimizer , etc .",0
3524,"We use pre-trained word embeddings with 300 dimensions 10 , and keep them fixed during the training process .",1
3525,"We employ 256 hidden units in both the gloss module and the context module , which means n = 256 .",1
3526,"Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [ - 0.1 , 0.1 ] is used for others .",1
3527,We assign gloss expansion depth K the value of 4 .,1
3528,"We also experiment with the number of passes | T M | from 1 to 5 in our framework , finding | T M | = 3 performs best .",1
3529,We use Adam optimizer in the training process with 0.001 initial learning rate .,1
3530,"In order to avoid overfitting , we use dropout regularization and set drop rate to 0.5 .",1
3531,Training runs for up to 100 epochs with early stopping if the validation loss does n't improve within the last 10 epochs .,1
3532,Systems to be Compared,0
3533,"In this section , we describe several knowledgebased methods , supervised methods and neuralbased methods which perform well on the English all - words WSD datasets for comparison . : F1-score ( % ) for fine - grained English all - words WSD on the test sets .",0
3534,Bold font indicates best systems .,0
3535,The * represents the neural network models using external knowledge .,0
3536,"The fives blocks list the MFS baseline , two knowledge - based systems , two supervised systems ( feature - based ) , three neuralbased systems and our models , respectively .",0
3537,.,0
3538,Knowledge - based Systems,0
3539,Lesk ext+emb :,0
3540,Basile et al.,0
3541,is a variant of Lesk algorithm by using a word similarity function defined on a distributional semantic space to calculate the gloss - context overlap .,0
3542,This work shows that glosses are important to WSD and enriching gloss information via its semantic relations can help to WSD .,0
3543,Babelfy : exploits the semantic network structure from BabelNet and builds a unified graph - based architecture for WSD and Entity Linking .,0
3544,Supervised Systems,0
3545,The supervised systems mentioned in this paper refers to traditional feature - based systems which train a dedicated classifier for every word individually ( word expert ) .,0
3546,"IMS : Zhi and Ng ( 2010 ) selects a linear Support Vector Machine ( SVM ) as its classifier and makes use of a set of features surrounding the target word within a limited window , such as POS tags , local words and local collocations .",0
3547,IMS +emb : selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat inmost of WSD datasets .,0
3548,Neural - based Systems,0
3549,Neural - based systems aim to build an end - to - end unified neural network for all the polysemous words in texts .,0
3550,Bi- LSTM : leverages a bidirectional LSTM network which shares model parameters among all words .,0
3551,Note that this model is equivalent to our model if we remove the gloss module and memory module of GAS .,0
3552,"Bi-LSTM +att.+ LEX and it s variant Bi- LSTM +att.+ LEX+P OS : transfers WSD into a sequence learning task and propose a multi - task learning framework for WSD , POS tagging and coarse - grained semantic labels ( LEX ) .",0
3553,"These two models have used the external knowledge , for the LEX is based on lexicographer files in WordNet .",0
3554,"Moreover , we introduce MFS baseline , which simply selects the most frequent sense in the training data set .",0
3555,Results and Discussion,0
3556,English all - words results,1
3557,"In this section , we show the performance of our proposed model in the English all - words task .",0
3558,Table 1 shows the F1 - score results on the four test sets mentioned in Section 4.1 .,0
3559,The systems in the first four blocks are implemented by except for the single Bi - LSTM model .,0
3560,The last block lists the performance of our proposed model GAS and its variant GAS ext which extends the gloss module in GAS .,0
3561,GAS and GAS ext achieves the state - of - theart performance on the concatenation of all test datasets .,1
3562,Although there is no one system al - Context :,0
3563,He plays a pianist in the film Glosses Pass 1 Pass 2 Pass 3 Pass 4 Pass 5 g 1 : participate in games or sport g 2 : perform music on a instrument g 3 : act a role or part : F1-score ( % ) of different passes from 1 to 5 on the test data sets .,0
3564,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,0
3565,". ways performs best on all the test sets 11 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets .",1
3566,"Compared with other three neural - based methods in the fourth block , we can find that our best model outperforms the previous best neural network models on every individual test set .",1
3567,"The IMS +emb , which trains a dedicated classifier for each word individually ( word expert ) with massive manual designed features including word embeddings , is hard to beat for neural networks models .",0
3568,"However , our best model can also beat IMS + emb on the SE3 , SE13 and SE15 test sets .",1
3569,Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results .,1
3570,"Compared with the Bi - LSTM baseline which only uses labeled data , our proposed model greatly improves the WSD task by 2.2 % F1 - score with the help of gloss knowledge .",1
3571,"Furthermore , compared with the GAS which only uses original gloss as the background knowledge , GAS ext can further improve the performance with the help of the extended glosses through the semantic relations .",1
3572,This proves that incorporating extended glosses through its hypernyms and hyponyms into the neural network models can boost the performance for Because the source of the four datasets are extremely different which belongs to different domains .,0
3573,WSD .,0
3574,Multiple Passes Analysis,0
3575,"To better illustrate the influence of multiple passes , we give an example in .",0
3576,"Consider the situation that we meet an unknown word x 12 , we lookup from the dictionary and find three word senses and their glosses corresponding to x .",0
3577,We try to figure out the correct meaning of x according to its context and glosses of different word senses by the proposed memory module .,0
3578,"In the first pass , the first sense is excluded , for there are no relevance between the context and g 1 .",0
3579,"But the g 2 and g 3 may need repeated deliberation , for word pianist is similar to the word music and role in the two glosses .",0
3580,"By re-reading the context and gloss information of the target word in the following passes , the correct word sense g 3 attracts much more attention than the other two senses .",0
3581,Such rereading process can be realized by multi-pass operation in the memory module .,0
3582,"Furthermore , shows the effectiveness of multi-pass operation in the memory module .",0
3583,"It shows that multiple passes operation performs better than one pass , though the improvement is not significant .",0
3584,"The reason of this phenomenon is that for most target words , one main word sense accounts for the majority of their appearances .",0
3585,"Therefore , inmost circumstances , one - pass inference can lead to the correct word senses .",0
3586,Case studies in show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for wordplay .,0
3587,"In Table 3 , with the increasing number of passes , the F1 - score increases .",0
3588,"However , when the number of passes is larger than 3 , the F1- score stops increasing or even decreases due to over-fitting .",0
3589,It shows that appropriate number of passes can boost the performance as well as avoid over - fitting of the model .,0
3590,Conclusions and Future Work,0
3591,"In this paper , we seek to address the problem of integrating the glosses knowledge of the ambiguous word into a neural network for WSD .",0
3592,We further extend the gloss information through its semantic relations in WordNet to better infer the context .,0
3593,"In this way , we not only make use of labeled context data but also exploit the background knowledge to disambiguate the word sense .",0
3594,Results on four English all - words WSD data sets show that our best model outperforms the existing methods .,0
3595,There is still one challenge left for the future .,0
3596,"We just extract the gloss , missing the structural properties or graph information of lexical resources .",0
3597,"In the next step , we will consider integrating the rich structural information into the neural network for Word Sense Disambiguation .",0
3598,title,0
3599,Segmentation Is All You Need,0
3600,abstract,0
3601,Region proposal mechanisms are essential for existing deep learning approaches to object detection in images .,1
3602,"Although they can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases is unacceptably low .",0
3603,"This is mainly because bounding box annotations contain much environment noise information , and non-maximum suppression ( NMS ) is required to select target boxes .",0
3604,"Therefore , in this paper , we propose the first anchorfree and NMS - free object detection model , called weakly supervised multimodal annotation segmentation ( WSMA - Seg ) , which utilizes segmentation models to achieve an accurate and robust object detection without NMS .",0
3605,"In WSMA - Seg , multimodal annotations are proposed to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .",0
3606,"In addition , we propose a multi-scale pooling segmentation ( MSP - Seg ) as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors .",0
3607,Introduction,0
3608,Object detection in images is one of the most widely explored tasks in computer vision .,0
3609,"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection .",1
3610,"Although such mechanisms can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases ( e.g. , complex occlusion ( ) , poor illumination ( ) , and large - scale small objects ( ) ) is unacceptably low .",0
3611,"Specifically , detecting objects under extreme cases via region proposal mechanisms encounters two challenges :",0
3612,"First , the performance of region proposal mechanisms highly depends on the purity of bounding boxes ; however , the annotated bounding boxes in extreme cases usually contain much more environment noise than those in normal cases .",0
3613,"This inevitably increases the difficulty of model learning and decreases the resulting confidence scores of bounding boxes , which consequently weakens the detection performance .",0
3614,"Second , non-maximum suppression ( NMS ) operations are used in region proposal mechanisms to select target boxes by setting an intersection over union ( IoU ) threshold to filter other bounding boxes .",0
3615,"However , it is very hard ( and sometimes even impossible ) to find an appropriate threshold to adapt to the very complex situations in extreme cases .",0
3616,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .",1
3617,"It consists of two phases , namely , a training and a testing phase .",0
3618,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .",1
3619,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .",1
3620,"WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .",0
3621,"Furthermore , it is obvious that the performance of the proposed WSMA - Seg approach greatly depends on the segmentation performance of the underlying segmentation model .",0
3622,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and consequently enhances the detection accuracy of WSMA - Seg .",0
3623,The contributions of this paper are briefly as follows :,0
3624,"We propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach to achieve an accurate and robust object detection without NMS , which is the first anchor-free and NMS - free object detection approach .",0
3625,We propose multimodal annotations to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .,0
3626,We propose a multi-scale pooling segmentation ( MSP - Seg ) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg .,0
3627,"We have conducted extensive experimental studies on the Rebar Head , WIDER Face , and MS COCO datasets ; the results show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors on all testing datasets .",0
3628,Weakly Supervised Multimodal Annotation Segmentation,0
3629,"In this section , we introduce our approach to object detection using weakly supervised multimodal annotation segmentation ( WSMA - Seg ) .",0
3630,WSMA - Seg generally consists of two phases : a training phase and a testing phase .,0
3631,"In the training phase , as shown in , WSMA - Seg first converts the weakly supervised bounding box annotations to pixel - level segmentation - like masks with three channels , representing interior , boundary , and boundary on interior masking information , respectively ; the resulting annotations are called multimodal annotations ; then , multimodal annotations are used as labels to train an underlying segmentation model to learn corresponding multimodal heatmaps for the training images .",0
3632,"In the testing phase , as shown in , we first send the given testing image into the well - trained segmentation model to obtain multimodal heatmaps ; then , the resulting three heatmaps are converted into an instance - aware segmentation map based on a pixel - level logic operation ; finally , a contour tracing operation is conducted to generate contours for objects using the segmentation map , and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours .",0
3633,The rest of this section will introduce the main ingredients of WSMA - Seg .,0
3634,Generating Multimodal Annotations,0
3635,"Pixel - level segmentation annotations are much more representative than bounding box annotations , so they can resolve some extreme cases that are challenging for bounding box annotations .",0
3636,"However , creating well - designed pixel - level segmentation masks is very time - consuming , which is about 15 times of creating bounding box annotations .",0
3637,"Therefore , in this work , we propose a methodology to automatically convert bounding box annotations to segmentation - like multimodal annotations , which are pixel - level geometric segmentation - like multichannel annotations .",0
3638,"Here , "" geometric segmentationlike "" means that the multimodal annotations are not strict segmentation annotations ; rather , they are annotations generated from simple geometries , e.g. , inscribed ellipses of bounding boxes .",0
3639,"This is motivated by the finding in that pixel - level segmentation information is not fully utilized by segmentation models ; we thus believe that well - designed pixel - level segmentation annotations may not be essential to achieve a reasonable performance ; rather , pixel - level geometric annotations should be enough .",0
3640,"Furthermore , to generate a bounding box for each object in the image , an instance - aware segmentation is required ; to achieve this , multimodal annotations are designed to have multiple channels to introduce additional information .",0
3641,"Specifically , as shown in , multimodal annotations use three channels to represent pixellevel masking information regarding the interior , the boundary , and the boundary on the interior of geometries .",0
3642,These three different pixel - level masks are generated as follows :,0
3643,"Given an image with bounding box annotations , we first obtain an inscribed ellipse for each bounding box , then the interior mask ( channel 0 ) is obtained by setting the values of pixels on the edge of or inside the ellipses to 1 , and setting the values of other pixels to 0 .",0
3644,"Then , the boundary mask ( channel 1 ) is obtained by setting the values of pixels on the edge of or within the inner width w of the ellipses to 1 , and setting the rest to 0 .",0
3645,"Similarly , the boundary on the interior mask ( channel 2 ) is generated by setting the values of pixels on the edge of or within the inner width w of the area of the elliptical overlap to 1 .",0
3646,Multi - Scale Pooling Segmentation,0
3647,It is obvious that the performance of the proposed WSMA - Seg approach greatly depends on the segmentation performance of the underlying segmentation model .,0
3648,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and to consequently enhance the detection accuracy of WSMA - Seg .",0
3649,"As shown in , MSP - Seg is an improved segmentation model of Hourglass .",0
3650,"The main improvement of MSP - Seg is to introduce a multi-scale block on the skip connections , performing multi-scale pooling operations to the output feature maps of residual blocks .",0
3651,"Specifically , as shown in , multi-scale pooling utilizes four pooling kernals with sizes 1 1 , 3 3 , 5 5 , and 7 7 to simultaneously conduct average pooling operations on the previous feature maps generated by residual blocks on skip connections .",0
3652,"Then , four feature maps generated by different pooling channels are concatenated to form anew feature map whose number of channels is four times of the previous feature maps .",0
3653,"Here , to ensure that the four feature maps have the same size , the stride is set to 1 , and zero - padding is conducted .",0
3654,"Finally , we apply 1 1 convolution to restore the number of channels , and element - wise addition to merge the feature maps .",0
3655,"As shown in , by using multimodal annotations as labels , MSP - Seg is trained to learn three heatmaps for each image , which are called interior heatmap , boundary heatmap , and boundary on interior heatmap , respectively .",0
3656,"Intuitively , multi-scale pooling is capable of enhancing the segmentation accuracy , because it combines features of different scales to obtain more representative feature maps .",0
3657,"Please note that , as a highly accurate segmentation model , MSP - Seg can be widely applied to various segmentation tasks .",0
3658,Object Detection Using Segmentation Results and Contour Tracing,0
3659,"After obtaining a well - trained segmentation model , we are now able to conduct object detection .",0
3660,"As shown in , given a test image as the input of the segmentation model , WSMA - Seg first generates three heatmaps , i.e. , interior , boundary , and boundary on interior heatmaps , which are denoted as I , B , and O , respectively .",0
3661,"These three heatmaps are then converted to binary heatmaps , where the values of pixels in interested area are set to 1 , and the rest is set to 0 .",0
3662,This conversion is conducted following the approach in .,0
3663,"Furthermore , a pixel - level operation , I ? ( B ? O ) , is used to merge three heatmaps into an instance - aware segmentation map .",0
3664,"Finally , a contour tracing operation is conducted to generate contours for objects using the instanceaware segmentation map , and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours .",0
3665,One conventional way to trace a contour is to use scan - based - following algorithm .,0
3666,"However , in the case of a large image with many objects ( which is common in detection tasks ) , scan - based - following algorithm is very time consuming .",0
3667,"Therefore , motivated by the work in , we propose a modified run - data - based ( RDB ) following algorithm , which greatly reduces the time and memory costs of the contour tracing operation .",0
3668,Pseudocode of the RDB following algorithm is shown in Algorithm 1 and an example is shown in . 3 .,0
3669,"Differently from the pixel - following algorithm that requires to scan the entire image to find the starting point and tracing contour pixels along the clockwise direction to generate the results recurrently , the RDB following algorithm only needs to save two lines of pixel values and to scan the whole image once , which significantly reduces the memory consumption and increases the speed .",0
3670,"Specifically , RDB following algorithm first initialize two variables ledge and r edge with null value , then scans the binary instance - aware segmentation map row by row from the top - left corner to the bottom - right corner to find contours ( lines 1 - 3 ) .",0
3671,"If a pixel 's value is 1 and it s left pixel 's value is 0 , then this pixel is on the left side of a contour , so it is assigned to ledge ; similarly , if a pixel 's value is 1 and it s right pixel 's value is 0 , then this pixel is on the right side of a contour , so it is assigned tor edge ( lines 4 - 9 ) .",0
3672,"When both ledge and r edge are found , we check if there exists a pair of ledge and r edge on above line whose x - coordinates are the same as or greater / smaller by 1 than the corresponding x - coordinates of ledge and r edge ; if so , we add ledge and r edge to the same contour set as ledge and r edge ; otherwise , we create anew contour set and add ledge and r edge to it ( lines .",0
3673,if there exists a pair of r edge and ledge in row j ? 1 and 12:,0
3674,x coord,0
3675,Of ( r edge ) ?,0
3676,x coord Of ( r edge ) ? 1 and 13 :,0
3677,x coord,0
3678,Of ( l edge ) ? x coord,0
3679,Of ( l edge ) ?,0
3680,1 then,0
3681,14:,0
3682,Add ledge and r edge to the same contour set as r edge and ledge 15 :,0
3683,"To show the strength of our proposed WSMA - Seg approach in object detection , extensive experimental studies have been conducted on three benchmark datasets , namely , the Rebar Head 3 , WIDER Face 4 , and MS COCO datasets , each of which containing many extreme cases .",0
3684,The important parameters of WSMA - Seg are as follows :,0
3685,"Stack is the number of the stacked hourglass networks ( see for more details about hourglass ) , Base is a pre-defined basic number , and the number of channels is always an integer multiple of Base , and Depth is the number of down - samplings .",0
3686,Stem represents three consecutive 3 3 convolution operations with stride = 1 before the first stack .,0
3687,Rebar Head Detection,1
3688,"We first conduct experiments on the Rebar Head detection dataset , which consists of 250 training images ( including a total of 30942 rebar heads ) and 200 testing images .",0
3689,The orignal resolution of the whole image is 2000 2666 .,0
3690,"Performing object detection on this dataset is very challenging , because it only contains a few training samples and also encounters very severe occlusion situations ( see ) .",0
3691,"In addition , the target rebar heads are very small : the average area of each box is 7 , 000 pixels , taking up only 0.13 % of the whole image .",0
3692,The images are also poorly annotated and rich in diverse illuminations .,0
3693,"Two state - of - the - art anchor- based models , Faster R - CNN and Cascade R - CNN , are selected as the baselines .",0
3694,shows the detection performances of our proposed WSMA - Seg and baselines on this dataset .,0
3695,"As shown in , our proposed method with Stack = 2 , Base = 40 , Depth = 5 has achieved the best performance among all solutions in terms of F1 Score .",1
3696,"In addition , the number of parameters needed for WSMA - Seg is much less than the baselines ( only 1 / 7 of Cascade RCNN and 1 / 4 of Faster RCNN ) , while the number of training epochs for WSMA - Seg is also less than those of the baselines .",0
3697,"Therefore , we can conclude that , compared to the state - of - the - art baselines , WSMA - Seg is much simpler , more effective , and more efficient .",1
3698,WIDER Face Detection,1
3699,"We further conduct experiments on the WIDER Face detection dataset , which consists of 32 , 203 images and 393 , 703 faces .",0
3700,"Face detections in this dataset are extremely challenging due to a high degree of variability in scale , pose , and occlusion .",0
3701,WIDER Face results in a much lower detection accuracy compared to other face detection datasets .,1
3702,"WIDER Face has defined three levels of difficulties ( i.e. , Easy , Medium , and Hard ) , based on the detection accuracies of EdgeBox .",0
3703,"Furthermore , the dataset also treats occlusion as an additional attribute and is partitioned into three categories : no occlusion , partial occlusion , and heavy occlusion .",0
3704,"Specifically , a face is categorized as partial occlusion when 1 % to 30 % of the total face area is occluded , and a face with the occluded area over 30 % is categorized as heavy occlusion .",0
3705,"The size of the training set is 12879 , that of the validation set is 3226 , and that of the testing set is 16098 .",0
3706,"Twelve state - of - the - art approaches are selected as baselines , namely , Two - stage CNN , Cascade R - CNN , and LDCF + , multitask Cascade CNN , ScaleFace , MSCNN , HR , Face R - CNN , Face Attention Networks , and PyramidBox .",0
3707,The experimental results in terms of F1 score are shown in .,0
3708,"The results show that our proposed WSMA - Seg outperforms the state - of - the - art baselines in all three categories , reaching 94.70 , 93.41 , and 87.23 in Easy , Medium , and Hard categories , respectively .",1
3709,MS COCO Detection,1
3710,"Finally , we conduct experimental studies on the MS COCO detection dataset , which is one of the most popular large - scale detection datasets .",0
3711,Our results are obtained using the test - dev split ( 20 k images ) with a host of the detection method .,0
3712,"We have constructed the training set with 82081 samples , the validation set with 40137 samples , and the testing set with 20288 samples .",0
3713,We use the metrics as used in to characterize the performance .,0
3714,Four types of metrics are defined and described as follows :,0
3715,Average Precision ( AP ) :,0
3716,- AP : AP at IoU=.50:.05:.95 ( primary challenge metric ) - AP .50 : AP at IoU=.50 ( PASCAL VOC metric ) - AP at IoU=.75 ( strict metric ),0
3717,AP Across Scales :,0
3718,- AP s : AP for small objects : area < 32 2 - AP m : AP for medium objects : 32 2 < area < 96 2 - AP l :,0
3719,AP for large objects : area > 96 2,0
3720,Average Recall ( AR ) :,0
3721,- AR 1 : AR given 1 detection per image - AR 10 : AR given 10 detections per image - AR 100 : AR given 100 detections per image,0
3722,AR Across Scales :,0
3723,- AR s :,0
3724,AR for small objects : area < 32 2 - AR m : AR for medium objects : 32 2 < area < 96 2 - AR l :,0
3725,AR for large objects : area > 96 2,0
3726,"Seven state - of - the - art solutions are selected as baselines , and the experimental results for four types of metrics are shown in .",0
3727,"The results show that our WSMA - Seg approach outperforms all state - of - the - art baselines in terms of most metrics , including the most challenging metrics , AP , AP s , AR 1 , and AR s .",1
3728,"For the other metrics , the performance of our proposed approach is also close to those of the best baselines .",1
3729,This proves that the proposed WSMA - Seg approach generally achieves more accurate and robust object detection than the state - of - the - art approaches without NMS .,1
3730,Conclusion,0
3731,"In this work , we have proposed a novel approach to object detection in images , called weakly supervised multimodal annotation segmentation ( WSMA - Seg ) , which is anchor-free and NMS - free .",0
3732,We observed that NMS is one of the bottlenecks of existing deep learning approaches to object detection in images .,0
3733,The need to tune hyperparameters on NMS has seriously hindered the scalability of high - performance detection frameworks .,0
3734,"Therefore , to realize WSMA - Seg , we proposed to use multimodal annotations to achieve an instance - aware segmentation based on weakly supervised bounding boxes , and developed a run-data - based following algorithm to trace contours of objects .",0
3735,"In addition , a multi-scale pooling segmentation ( MSP - Seg ) model was proposed as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets concluded that the proposed WSMA - Seg approach is superior to the state - of - the - art detectors .",0
3736,title,0
3737,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",1
3738,abstract,0
3739,"We present an algorithm for simultaneous face detection , landmarks localization , pose estimation and gender recognition using deep convolutional neural networks ( CNN ) .",0
3740,"The proposed method called , HyperFace , fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features .",0
3741,It exploits the synergy among the tasks which boosts up their individual performances .,0
3742,"Additionally , we propose two variants of HyperFace :",0
3743,"( 1 ) HyperFace - ResNet that builds on the ResNet - 101 model and achieves significant improvement in performance , and ( 2 ) Fast - HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm .",0
3744,Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks .,0
3745,INTRODUCTION,0
3746,"D ETECTION and analysis of faces is a challenging problem in computer vision , and has been actively researched for applications such as face verification , face tracking , person identification , etc .",0
3747,"Although recent methods based on deep Convolutional Neural Networks ( CNN ) have achieved remarkable results for the face detection task , , , it is still difficult to obtain facial landmark locations , head pose estimates and gender information from face images containing extreme poses , illumination and resolution variations .",0
3748,"The tasks of face detection , landmark localization , pose estimation and gender classification have generally been solved as separate problems .",0
3749,"Recently , it has been shown that learning correlated tasks simultaneously can boost the performance of individual tasks , , .",0
3750,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .",1
3751,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,1
3752,We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,0
3753,"Lower layers respond to edges and corners , and hence contain better localization properties .",0
3754,They are more suitable for learning landmarks localization and pose estimation tasks .,0
3755,"R. On the other hand , deeper layers are class - specific and suitable for learning complex tasks such as face detection and gender recognition .",0
3756,It is evident that we need to make use of all the intermediate layers of a deep CNN in order to train different tasks under consideration .,0
3757,We refer the set of intermediate layer features as hyperfeatures .,1
3758,We borrow this term from which uses it to denote a stack of local histograms for multilevel image coding .,0
3759,"Since a CNN architecture contains multiple layers with hundreds of feature maps in each layer , the overall dimension of hyperfeatures is too large to be efficient for learning multiple tasks .",0
3760,"Moreover , the hyperfeatures must be associated in away that they efficiently encode the arXiv:1603.01249v3 [ cs. CV ] 6 Dec 2017 features common to the multiple tasks .",0
3761,This can be handled using feature fusion techniques .,0
3762,Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly .,0
3763,Recent advances in deep learning have shown that CNNs are capable of estimating an arbitrary complex function .,0
3764,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .",1
3765,"In order to learn the tasks , we train them simultaneously using multiple loss functions .",1
3766,"In this way , the features get better at understanding faces , which leads to improvements in the performances of individual tasks .",0
3767,The deep CNN combined with the fusion - CNN can be learned together in an end -toend fashion .,1
3768,"We also study the performance of face detection , landmarks localization , pose estimation and gender recognition tasks using off - the - shelf Region - based CNN ( R - CNN ) approach .",0
3769,"Although R - CNN for face detection has been explored in DP2 MFD , we provide a comprehensive study of all these tasks based on R - CNN .",0
3770,"Furthermore , we study the multitask approach without fusing the intermediate layers of CNN .",0
3771,Detailed experiments show that the multi-task learning method performs better than methods based on individual learning .,0
3772,Fusing the intermediate layer features provides additional performance boost .,0
3773,This paper makes the following contributions .,0
3774,"Region Proposals ( IRP ) and Landmarks - based Non - Maximum Suppression ( L - NMS ) , which leverage the multi-task information obtained from the CNN to improve the overall performance .",0
3775,3 ) We study the performance of R - CNN - based approaches for individual tasks and the multi-task approach without intermediate layer fusion .,0
3776,4 ) We achieve significant improvement in performance on challenging unconstrained datasets for all of these four tasks .,0
3777,This paper is organized as follows .,0
3778,Section 2 reviews related work .,0
3779,Section 3 describes the proposed HyperFace framework in detail .,0
3780,"Section 4 describes the implementation of R - CNN , Multitask Face and HF - ResNet approaches .",0
3781,Section 5 provides the results of HyperFace and HF - ResNet along with R - CNN baselines on challenging datasets .,0
3782,"Finally , Section 6 concludes the paper with a brief summary and discussion .",0
3783,RELATED WORK,0
3784,Multi - Task Learning :,0
3785,Multi - task learning ( MTL ) was first analyzed in detail by Caruana .,0
3786,"Since then , several approaches have adopted MTL for solving different problems in computer vision .",0
3787,"One of the earlier approaches for jointly addressing the tasks of face detection , pose estimation , and landmark localization was proposed in and later extended in .",0
3788,This method is based on a mixture of trees with a shared pool of parts in the sense that every facial landmark is modeled as apart and uses global mixtures to capture the topological changes due to viewpoint variations .,0
3789,A joint cascade - based method was recently proposed in for simultaneously detecting faces and landmark points on a given image .,0
3790,This method yields improved detection performance by incorporating a face alignment step in the cascade structure .,0
3791,Multi - task learning using CNNs has also been studied recently .,0
3792,"Eigen and Fergus proposed a multi-scale CNN for simultaneously predicting depth , surface normals and semantic labels from an image .",0
3793,They apply CNNs at three different scales where the output of the smaller scale network is fed as input to the larger one .,0
3794,"UberNet adopts a similar concept of simultaneously training low - , mid - and high - level vision tasks .",0
3795,It fuses all the intermediate layers of a CNN at three different scales of the image pyramid for multi-task training on diverse sets .,0
3796,"Gkioxari et al. train a CNN for person pose estimation and action detection , using features only from the last layer .",0
3797,The use of MTL for face analysis is somewhat limited .,0
3798,Zhang et al.,0
3799,"used MTLbased CNN for facial landmark detection along with the tasks of discrete head yaw estimation , gender recognition , smile and glass detection .",0
3800,"In their method , the predictions for all theses tasks were pooled from the same feature space .",0
3801,"Instead , we strategically design the network architecture such that the tasks exploit low level as well as high level features of the network .",0
3802,We also jointly predict the task of face detection and landmark localization .,0
3803,These two tasks always go hand - in - hand and are used inmost end - to - end face analysis systems .,0
3804,Feature Fusion : Fusing intermediate layers from CNN to bring both geometry and semantically rich features together has been used by quite a few methods .,0
3805,Hariharan et al.,0
3806,proposed,0
3807,"Hypercolumns to fuse pool2 , conv4 and fc7 layers of AlexNet for image segmentation .",0
3808,"Yang and Ramanan proposed DAG - CNNs , which extract features from multiple layers to reason about high , mid and low - level features for image classification .",0
3809,Sermanet et al .,0
3810,"merge the 1st stage output of CNN to the classifier input after sub-sampling , for the application of pedestrian detection .",0
3811,Face detection :,0
3812,Viola - Jones detector is a classic method which uses cascaded classifiers on Haar - like features to detect faces .,0
3813,"This method provides realtime face detection , but works best for full , frontal , and well lit faces .",0
3814,"Deformable Parts Model ( DPM ) - based face detection methods have also been proposed in the literature where a face is essentially defined as a collection of parts , .",0
3815,"It has been shown that in unconstrained face detection , features like HOG or Haar wavelets do not capture the discriminative facial information at different illumination variations or poses .",0
3816,"To overcome these limitations , various deep CNN - based face detection methods have been proposed in the literature , , , , .",0
3817,These methods have produced state - of - the - art results on many challenging publicly available face detection datasets .,0
3818,"Some of the other recent face detection methods include NPDFaces , PEP - Adapt , and .",0
3819,Landmarks localization :,0
3820,Fiducial points extraction or landmarks localization is one of the most important steps in face recognition .,0
3821,Several approaches have been proposed in the literature .,0
3822,"These include both regression - based , , , , , and model - based , , methods .",0
3823,"While the former learns the shape increment given a mean initial shape , the latter trains an appearance model to predict the keypoint locations .",0
3824,"CNN - based landmark localization methods have also been proposed in recent years , , and have achieved remarkable performance .",0
3825,"Although much work has been done for localizing landmarks for frontal faces , limited attention has been given to profile faces which occur more often in real world scenarios .",0
3826,Jourabloo and Liu recently proposed PIFA that estimates 3 D landmarks for large pose face alignment by integrating a 3D point distribution model with a cascaded coupled - regressor .,0
3827,"Similarly , 3DDFA fits a dense 3D model by estimating its parameters using a CNN .",0
3828,Zhu et al.,0
3829,proposed a cascaded compositional learning approach that combines shape prediction from multiple domain specific regressors .,0
3830,Pose estimation :,0
3831,The task of head pose estimation is to infer the orientation of person 's head relative to the camera view .,0
3832,It is useful in face verification for matching face similarity across different orientations .,0
3833,"Non-linear manifoldbased methods have been proposed in , , to classify face images based on pose .",0
3834,A survey of various head pose estimation methods is provided in .,0
3835,Gender recognition :,0
3836,Previous works on gender recognition have focused on finding good discriminative features for classification .,0
3837,"Most previous methods use one or more combination of features such as LBP , SURF , HOG or SIFT .",0
3838,"In recent years , attribute - based methods for face recognition have gained a lot of traction .",0
3839,"Binary classifiers were used in for each attribute such as male , longhair , white etc .",0
3840,Separate features were computed for different attributes and they were used to train individual SVMs for each attribute .,0
3841,"CNN - based methods have also been proposed for learning attribute - based representations in , .",0
3842,HYPERFACE,0
3843,"We propose a single CNN model for simultaneous face detection , landmark localization , pose estimation and gender classification .",0
3844,"The network architecture is deep in both vertical and horizontal directions , i.e. , it has both top - down and lateral connections , as shown in .",0
3845,"In this section , we provide a brief overview of the system and then discuss the different components in detail .",0
3846,The proposed algorithm called HyperFace consists of three modules .,0
3847,The first one generates class independent region - proposals from the given image and scales them to 227 227 pixels .,0
3848,The second module is a CNN which takes in the resized candidate regions and classifies them as face or non-face .,0
3849,"If a region gets classified as a face , the network additionally provides facial landmarks locations , estimated head pose and gender information .",0
3850,The third module is a post-processing step which involves Iterative Region Proposals ( IRP ) and Landmarks - based Non -Maximum Suppression ( L - NMS ) to boost the face detection score and improve the performance of individual tasks .,0
3851,HyperFace Architecture,0
3852,We start with Alexnet for image classification .,0
3853,The network consists of five convolutional layers along with three fully connected layers .,0
3854,We initialize the network with the weights of R - CNN Face network trained for face detection task as described in Section 4 .,0
3855,"All the fully connected layers are removed as they encode image - classification specific information , which is not needed for pose estimation and landmarks extraction .",0
3856,We exploit the following two observations to create our network .,0
3857,1 ) The features in CNN are distributed hierarchically in the network .,0
3858,"While the lower layer features are effective for landmarks localization and pose estimation , the higher layer features are suitable for more complex tasks such as detection or classification .",0
3859,"2 ) Learning multiple correlated tasks simultaneously builds a synergy and improves the performance of individual tasks as shown in , .",0
3860,"Hence , in order to simultaneously learn face detection , landmarks , pose and gender , we need to fuse the features from the intermediate layers of the network ( hyperfeatures ) , and learn multiple tasks on top of it .",0
3861,"Since the adjacent layers are highly correlated , we do not consider all the intermediate layers for fusion .",0
3862,"We fuse the max 1 , conv 3 and pool 5 layers of Alexnet , using a separate network .",0
3863,A naive way for fusion is directly concatenating the features .,0
3864,"Since the feature maps for these layers have different dimensions 27 27 96 , 13 13 384 , 6 6 256 , respectively , they can not be easily concatenated .",0
3865,"We therefore add conv 1 a and conv 3 a convolutional layers to pool 1 , conv 3 layers to obtain consistent feature maps of dimensions 6 6 256 at the output .",0
3866,We then concatenate the output of these layers along with pool 5 to form a 6 6 768 dimensional feature maps .,0
3867,The dimension is still quite high to train a multi -task framework .,0
3868,"Hence , a 1 1 kernel convolution layer ( conv all ) is added to reduce the dimensions [ 51 ] to 6 6 192 .",0
3869,"We add a fully connected layer ( f call ) to conv all , which outputs a 3072 dimensional feature vector .",0
3870,"At this point , we split the network into five separate branches corresponding to the different tasks .",0
3871,"We add f c detection , f c landmarks , f c visibility , f c pose and f c gender fully connected layers , each of dimension 512 , to f call .",0
3872,"Finally , a fully connected layer is added to each of the branch to predict the individual task labels .",0
3873,"After every convolution or a fully connected layer , we deploy the Rectified Linear Unit ( ReLU ) .",0
3874,We do not include any pooling operation in the fusion network as it provides local invariance which is not desired for the face landmark localization task .,0
3875,Taskspecific loss functions are then used to learn the weights of the network .,0
3876,Training,0
3877,We use the AFLW dataset for training the HyperFace network .,0
3878,"It contains 25 , 993 faces in 21 , 997 real - world images with full pose , expression , ethnicity , age and gender variations .",0
3879,"It provides annotations for 21 landmark points per face , along with the face bounding - box , face pose ( yaw , pitch and roll ) and gender information .",0
3880,"We randomly selected 1000 images for testing , and used the rest for training the network .",0
3881,"Different loss functions are used for training the tasks of face detection , landmark localization , pose estimation and gender classification .",0
3882,Face Detection :,0
3883,We use the Selective Search algorithm in R - CNN to generate region proposals for faces in an image .,0
3884,A region having an Intersection over Union ( IOU ) overlap of more than 0.5 with the ground truth bounding box is considered a positive sample ( l = 1 ) .,0
3885,The candidate regions with IOU overlap less than 0.35 are treated as negative instances ( l = 0 ) .,0
3886,All the other regions are ignored .,0
3887,We use the softmax loss function given by for training the face detection task .,0
3888,where p is the probability that the candidate region is a face .,0
3889,The probability values p and 1 ?,0
3890,pare obtained from the last fully connected layer for the detection task .,0
3891,Landmarks Localization :,0
3892,We use 21 point markups for face landmarks locations as provided in the AFLW dataset .,0
3893,"Since the faces have full pose variations , some of the landmark points are invisible .",0
3894,The dataset provides the annotations for the visible landmarks .,0
3895,"We consider boundingbox regions with IOU overlap greater than 0.35 with the ground truth for learning this task , while ignoring the rest .",0
3896,"A region can be characterized by {x , y , w , h} where ( x , y) are the co-ordinates of the center of the region and w , h are the width and height of the region respectively .",0
3897,"Each visible landmark point is shifted with respect to the region center ( x , y ) , and normalized by ( w , h) as given by ( 2 )",0
3898,"where ( x i , y i ) 's are the given ground truth fiducial coordinates .",0
3899,"The ( a i , bi ) 's are treated as labels for training the landmark localization task using the Euclidean loss weighted by the visibility factor .",0
3900,The loss in predicting the landmark location is computed from ( 3 ),0
3901,"where ( x i ,? i ) is the i th landmark location predicted by the network , relative to a given region , N is the total number of landmark points ( 21 for AFLW ) .",0
3902,"The visibility factor vi is 1 if the i th landmark is visible in the candidate region , else it is 0 .",0
3903,This implies that there is no loss corresponding to invisible points and hence they do not take part during back - propagation .,0
3904,Learning Visibility :,0
3905,We also learn the visibility factor in order to test the presence of the predicted landmark .,0
3906,"For a given region with overlap higher than 0.35 , we use a simple Euclidean loss to train the visibility as shown in ( 4 )",0
3907,wherev i is the predicted visibility of i th landmark .,0
3908,"The true visibility vi is 1 if the i th landmark is visible in the candidate region , else it is 0 .",0
3909,Pose Estimation :,0
3910,"We use the Euclidean loss to train the head pose estimates of roll ( p 1 ) , pitch ( p 2 ) and yaw ( p 3 ) .",0
3911,"We compute the loss fora candidate region having an overlap more than 0.5 with the ground truth , from ( 5 )",0
3912,"where ( p 1 , p 2 , p 3 ) are the estimated pose labels .",0
3913,Gender Recognition :,0
3914,Predicting gender is a two class problem similar to face detection .,0
3915,"For a candidate region with overlap of 0.5 with the ground truth , we compute the softmax loss given in",0
3916,"where g = 0 if the gender is male , or else g =",0
3917,1 .,0
3918,"Here , ( p 0 , p 1 ) is the two dimensional probability vector computed from the network .",0
3919,The total loss is computed as the weighted sum of the five individual losses as shown in loss,0
3920,"where ti is the i th element from the set of tasks T = { D , L , V , P , G}. The weight parameter ?",0
3921,ti is decided based on the importance of the task in the overall loss .,0
3922,We choose,0
3923,for our experiments .,0
3924,Higher weights are assigned to landmark localization and pose estimation tasks as they need spatial accuracy .,0
3925,Testing,0
3926,"From a given test image , we first extract the candidate region proposals using .",0
3927,"For each region , we predict the task labels by a forward - pass through the HyperFace network .",0
3928,"Only those regions , whose detection scores are above a certain threshold , are classified as face and processed for subsequent tasks .",0
3929,The predicted landmark points are scaled and shifted to the image co-ordinates using ( 8 ),0
3930,"where ( x i ,? i ) are the predicted locations of the i th landmark from the network , and {x , y , w , h} are the region parameters defined in .",0
3931,Points obtained with predicted visibility less than a certain threshold are marked invisible .,0
3932,"The pose labels obtained from the network are the estimated roll , pitch and yaw for the face region .",0
3933,The gender is assigned according to the label with maximum predicted probability .,0
3934,There are two major issues while using proposal - based face detection .,0
3935,"First , the proposals might not be able to capture small and difficult faces , hence reducing the overall recall of the system .",0
3936,"Second , the proposal boxes might not be well localized with the actual face region .",0
3937,It is a common practice to use bounding - box regression as a post processing step to improve the localization of the detected face box .,0
3938,This adds an additional burden of training regressors to learn the transformation from the detected candidate box to the annotated face box .,0
3939,"Moreover , the localization is still weak since the regressors are usually linear .",0
3940,"Recently , Gidaris and Komodakis proposed LocNet which tries to solve these limitations by refining the detection bounding box .",0
3941,"Given a set of initial bounding box proposals , it generates new sets of bounding boxes that maximize the likelihood of each row and column within the box .",0
3942,It allows an accurate inference of bounding box under a simple probabilistic framework .,0
3943,"Instead of using the probabilistic framework , we solve the above mentioned issues in an iterative way using the predicted landmarks .",0
3944,"The fact that we obtain landmark locations along with the detections , enables us to improve the post -processing step so that all the tasks benefit from it .",0
3945,We propose two novel methods : Iterative Region Proposals ( IRP ) and Landmarks - based Non - Maximum Suppression ( L - NMS ) to improve the performance .,0
3946,IRP improves the recall by generating more candidate proposals by using the predicted landmarks information from the initial set of region proposals .,0
3947,"On the other hand , L - NMS improves the localization by re-adjusting the detected bounding boxes according to the predicted landmarks and performing NMS on top of them .",0
3948,No additional training is required for these methods .,0
3949,Iterative Region Proposals ( IRP ) :,0
3950,We use a fast version of Selective Search which extracts around 2000 regions from an image .,0
3951,We call this version F ast SS .,0
3952,It is quite possible that some faces with poor illumination or small size fail to get captured by any candidate region with a high overlap .,0
3953,The network would fail to detect that face due to low score .,0
3954,"In these situations , it is desirable to have a candidate box which precisely captures the face .",0
3955,"Hence , we generate anew candidate bounding box from the predicted landmark points using the FaceRectCalculator provided by , and pass it again through the network .",0
3956,"The new region , being more localized yields a higher detection score and improves the corresponding tasks output , thus increasing the recall .",0
3957,"This procedure can be repeated ( say T time ) , so that boxes at a given step will be more localized to faces as compared to the previous step .",0
3958,"From our experiments , we found that the localization component saturates in just one step ( T = 1 ) , which shows the strength of the predicted landmarks .",0
3959,The pseudo - code of IRP is presented in Algorithm,0
3960,"1 . The usefulness of IRP can be seen in , which shows a low - resolution face region cropped from the top - right image in .",0
3961,for stage = 1 to T do fids ?,0
3962,get hyperf ace f iducials ( new boxes ) new boxes ?,0
3963,F aceRectCalculator ( fids ) deteced boxes ?,0
3964,[ deteced boxes | new boxes ] : end final scores ?,0
3965,get hyperf ace scores ( detected boxes ),0
3966,Algorithm 1 Iterative Region Proposals,0
3967,Landmarks - based Non-Maximum Suppression ( L - NMS ) :,0
3968,The traditional approach of non-maximum suppression involves selecting the top scoring region and discarding all the other regions with overlap more than a certain threshold .,0
3969,This method can fail in the following two scenarios :,0
3970,"1 ) If a region corresponding to the same detected face has less overlap with the highest scoring region , it can be detected as a separate face .",0
3971,"2 ) The highest scoring region might not always be localized well for the face , which can create some discrepancy if two faces are close together .",0
3972,"To overcome these issues , we perform NMS on anew region whose bounding box is defined by the boundary co-ordinates as [ min ix i , mini y i , maxi x i , maxi y i ] of the landmarks for the given region .",0
3973,"In this way , the candidate regions would get close to each other , thus decreasing the ambiguity of the overlap and improving the localization .",0
3974,Algorithm 2,0
3975,Landmarks - based NMS,0
3976,1 : Get detected boxes from Algorithm 1 2 : fids ?,0
3977,get hyperf ace f iducials ( detected boxes ) precise boxes ?,0
3978,"[ min x , min y , max x , max y ] ( fids ) 4 : faces ?",0
3979,"n ms ( precise boxes , overlap ) 5 : for each face in faces do top -k boxes ?",0
3980,Get top -k scoring boxes final fids ?,0
3981,median ( f ids ( top - k boxes ) ) final pose ?,0
3982,median ( pose ( top - k boxes ) ) final gender ?,0
3983,median ( gender ( t op -k boxes ) ) final visibility ?,0
3984,median ( visibility ( top -k boxes ) ) final bounding box ?,0
3985,F aceRectCalculator ( final fids ) : end,0
3986,"We apply landmarks - based NMS to keep the top -k boxes , based on the detection scores .",0
3987,The detected face corresponds to the region with maximum score .,0
3988,"The landmark points , pose estimates and gender classification scores are decided by the median of the top k boxes obtained .",0
3989,"Hence , the predictions do not rely only on one face region , but considers the votes from top -k regions for generating the final output .",0
3990,"From our experiments , we found that the best results are obtained with the value of k being 5 .",0
3991,The pseudocode for L - NMS is given in Algorithm,0
3992,2 .,0
3993,NETWORK ARCHITECTURES,0
3994,"To emphasize the importance of multitask approach and fusion of the intermediate layers of CNN , we study the performance of simpler CNNs devoid of such features .",0
3995,"We evaluate four R - CNN - based models , one for each task of face detection , landmark localization , pose estimation and gender recognition .",0
3996,"We also build a separate Multitask Face model which performs multitask learning just like HyperFace , but does not fuse the information from the intermediate layers .",0
3997,These models are described as follows :,0
3998,R- CNN,0
3999,Face :,0
4000,This model is used for face detection task .,0
4001,The network architecture is shown in .,0
4002,"For training R - CNN Face , we use the region proposals from AFLW training set , each associated with a face label based on the overlap with the ground truth .",0
4003,The loss is computed as per ( 1 ) .,0
4004,The model parameters are initialized using the Alexnet weights trained on the Imagenet dataset .,0
4005,"Once trained , the learned parameters from this network are used to initialize other models including Multitask Face and HyperFace as the standard Imagenet initialization does n't converge well .",0
4006,We also perform a linear bounding box regression to localize the face co-ordinates .,0
4007,R - CNN,0
4008,Fiducial :,0
4009,This model is used for locating the facial landmarks .,0
4010,The network architecture is shown in ( b ) .,0
4011,"It simultaneously learns the visibility of the points to account for the invisible points attest time , and thus can be used as a standalone fiducial extractor .",0
4012,The loss functions for landmarks localization and visibility of points are computed using ( c ) presents the network architecture .,0
4013,"Similar to R - CNN Fiducial , only region proposals with overlap >",0
4014,0.5 with the ground truth bounding box are used for training .,0
4015,The training loss is computed using ( 5 ) .,0
4016,R - CNN,0
4017,Gender :,0
4018,This model is used for face gender recognition task .,0
4019,The network architecture is shown in .,0
4020,It has the same training set as R - CNN Fiducial and R - CNN Pose .,0
4021,The training loss is computed using .,0
4022,"Multitask Face : Similar to HyperFace , this model is used to simultaneously detect face , localize landmarks , estimate pose and predict its gender .",0
4023,The only difference between Multitask Face and HyperFace is that HyperFace fuses the intermediate layers of the network whereas Multitask Face combines the tasks using the common fully connected layer at the end of the network as shown in .,0
4024,"Since it provides the landmarks and face score , it leverages iterative region proposals and landmark - based NMS post-processing algorithms during evaluation .",0
4025,The performance of all the above models for their respective tasks are evaluated and discussed in details in Section 5 .,0
4026,HyperFace - ResNet,0
4027,"The CNN architectures have improved a lot over the years , mainly due to an increase in number of layers , effective convolution kernel size , batch normalization and skip connections .",0
4028,"Recently ,",0
4029,"He et al. proposed a deep residual network architecture with more than 100 layers , that achieves state - of - the - art results on the ImageNet challenge .",0
4030,"Hence , we propose a variant of HyperFace that is built using the ResNet - 101 model instead of AlexNet .",0
4031,"The proposed network called HyperFace - ResNet ( HF - ResNet ) significantly improves upon its AlexNet baseline for all the tasks of face detection , landmarks localization , pose estimation and gender recognition .",0
4032,shows the network architecture for HF - ResNet .,0
4033,"Similar to HyperFace , we fuse the geometrically rich features from the lower layers and semantically strong features from the deeper layers of ResNet , such that multi-task learning can leverage from their synergy .",0
4034,"Taking inspiration from , we fuse the features using hierarchical elementwise addition .",0
4035,"Starting with ' res2c ' features , we first reduce its resolution using a 3 3 convolution kernel with stride of 2 .",0
4036,It is then passed through the a 1 1 convolution layer that increases the number of channels to match the next level features ( ' re s 3 b 3 ' in this case ) .,0
4037,Element - wise addition is applied between the two to generate anew set of fused features .,0
4038,The same operation is applied in a cascaded manner to fuse ' res4 b22 ' and ' res5c ' features of the ResNet - 101 model .,0
4039,"Finally , average pooling is carried out to generate 2048 - dimensional feature vector that is shared among all the tasks .",0
4040,Task - specific sub- networks are branched out separately in a similar way as HyperFace .,0
4041,Each convolution layer is followed by a Batch - Norm + Scale layer and ReLU activation unit .,0
4042,We do not use dropout in HF - ResNet .,0
4043,The training loss functions are the same as described in Section 3.2 .,0
4044,HF - ResNet is slower than HyperFace since it performs more convolutions .,0
4045,This makes it difficult to be used with Selective Search algorithm which generates more than 2000 region proposals to be processed .,0
4046,"Hence , we use a faster version of region proposals using high recall SSD face detector .",0
4047,"It produces 200 proposals , needing just 0.05s .",0
4048,This considerably reduces the total runtime for HF - ResNet to less than 1s .,0
4049,The fast version of HyperFace is discussed in Section 5.6 .,0
4050,EXPERIMENTAL RESULTS,0
4051,"We evaluated the proposed HyperFace method , along with HF - ResNet , Multask Face , R - CNN Face , R - CNN Fiducial , R - CNN Pose and R - CNN Gender on six challenging datasets :",0
4052,"Annotated Face in - the - Wild ( AFW ) for evaluating face detection , landmarks localization , and pose estimation tasks 300 - W Faces in - the - wild ( IBUG ) for evaluating 68 point landmarks localization .",0
4053,Annotated Facial Landmarks in the Wild ( AFLW ) for evaluating landmarks localization and pose estimation tasks Face Detection Dataset and Benchmark ( FDDB ) and PASCAL faces for evaluating the face detection results Large - scale CelebFaces Attributes ( CelebA ) and LFWA for evaluating gender recognition results .,0
4054,"Our method was trained on randomly selected 20 , 997 images from the AFLW dataset using Caffe .",0
4055,The remaining 1000 images were used for testing .,0
4056,Face Detection,1
4057,"We present face detection results for AFW , PASCAL and FDDB datasets .",0
4058,The AFW dataset was collected from Flickr and the images in this dataset contain large variations in appearance and viewpoint .,0
4059,In total there are 205 images with 468 faces in this dataset .,0
4060,"The FDDB dataset consists of 2,845 images containing 5,171 faces collected from news articles on the Yahoo website .",0
4061,This dataset is the most widely used benchmark for unconstrained face detection .,0
4062,"The PASCAL faces dataset was collected from the test set of PASCAL person layout dataset , which is a subset from PASCAL VOC .",0
4063,This dataset contains 1335 faces from 851 images with large appearance variations .,0
4064,"For improved face detection performance , we learn a SVM classifier on top off c detection features using the training splits from the FDDB dataset .",0
4065,"Some of the recent published methods compared in our evaluations include DP2MFD , Faceness , Head - Hunter , JointCascade , CCF , Squares ChnFtrs - 5 , CascadeCNN , Structured Models , DDFD , NPDFace , PEP - Adapt , TSM , as well as three commercial systems Face ++ , Picasa and Face.com .",0
4066,"The precision - recall curves of different detectors corresponding to AFW and PASCAL faces datasets are shown in and ( b ) , respectively .",0
4067,compares the performance of different detectors using the Receiver Operating Characteristic ( ROC ) curves on the FDDB dataset .,0
4068,"As can be seen from these figures , both HyperFace and HF - ResNet outperform all the reported academic and commercial detectors on the AFW and PASCAL datasets .",1
4069,"HyperFace achieves a high mean average precision ( m AP ) of 97.9 % and 92.46 % , for AFW and PASCAL datasets respectively .",1
4070,HF - ResNet further improves the m AP to 99.4 % and 96.2 %,1
4071,respectively .,0
4072,"The FDDB dataset is very challenging for HyperFace and any other R - CNN - based face detection methods , as the dataset contains many small and blurred faces .",0
4073,"First , some of these faces do not get included in the region proposals from selective search .",0
4074,"Second , re-sizing small faces to the input size of 227 227 adds distortion to the face resulting in low detection score .",0
4075,"In spite of these issues , HyperFace performance is comparable to recently published deep learning - based face detection methods such as DP2MFD and Faceness on the FDDB dataset 1 with m AP of 90.1 % .",1
4076,"It is interesting to note the performance differences between R - CNN Face , Multitask Face and HyperFace for the face detection tasks .",0
4077,clearly show that multitask CNNs ( Multitask Face and HyperFace ) outperform R - CNN Face by a wide margin .,1
4078,The boost in the performance gain is mainly due to the following two reasons .,0
4079,"First , multitask learning approach helps the network to learn improved features for face detection which is evident from their mAP values on the AFW dataset .",0
4080,"Using just the linear bounding 1 . http://vis-www.cs.umass.edu/fddb/results.html box regression and traditional NMS , the HyperFace obtains a m AP of 94 % ( ) while R - CNN Face achieves a m AP of 90.3 % .",0
4081,"Second , having landmark information associated with detection boxes makes it easier to localize the bounding box to a face , by using IRP and L - NMS algorithms .",0
4082,"On the other hand , HyperFace and Multi- task Face perform comparable to each other for all the face detection datasets which suggests that the network does not gain much by fusing intermediate layers for the face detection task ..",0
4083,Landmarks Localization cumulative error distribution curves on the AFW dataset .,0
4084,The numbers in the legend are the fraction of testing faces that have average error below ( 5 % ) of the face size .,0
4085,Landmarks Localization,1
4086,We evaluate the performance of different landmarks localization algorithms on AFW and AFLW datasets .,0
4087,Both of these datasets contain faces with full pose variations .,0
4088,"Some of the methods compared include Multiview Active Appearance Model - based method ( Multi. AAM ) , Constrained Local Model ( CLM ) , Oxford facial landmark detector , Zhu , FaceDPL , JointCascade , CDM , RCPR , ESR , SDM and 3DDFA .",0
4089,"Although both of these datasets provide ground truth bounding boxes , we do not use them for evaluating on HyperFace , HF - ResNet , Multitask Face and R - CNN Fiducial .",0
4090,Instead we use the respective algorithms to detect both the face and its fiducial points .,0
4091,"Since , the R - CNN Fiducial can not detect faces , we provide it with the detections from the HyperFace .",0
4092,compares the performance of different landmark localization methods on the AFW dataset using the protocol defined in .,0
4093,"In this figure , ( * ) indicates that models that are evaluated on near frontal faces or use hand - initialization .",0
4094,"The dataset provides six keypoints for each face which are : left eye center , right eye center , nose tip , mouth left , mouth center and mouth right .",0
4095,"We compute the error as the mean distance between the predicted and ground truth keypoints , normalized by the face size .",0
4096,The plots for comparison were obtained from .,0
4097,"For the AFLW dataset , we calculate the error using all the visible keypoints .",0
4098,"For AFW , we adopt the same protocol as defined in .",0
4099,"The only difference is that our AFLW testset consists of only 1000 images with 1132 face samples , since we use the rest of the images for training .",0
4100,"To be consistent with the protocol , we randomly create a subset of 450 samples from our testset whose absolute yaw angles within [ 0 , 30 ] , [ 30 , 60 ] and [ 60 , 90 ] are 1 / 3 each .",0
4101,compares the performance of different landmark localization methods .,0
4102,"We obtain the comparison plots from where the evaluations for RCPR , ESR and SDM are carried out after adapting the algorithms to face profiling .",0
4103,"provides the Normalized Mean Error ( NME ) for AFLW dataset , for each of the pose group .",0
4104,"As can be seen from the figures , R - CNN Fiducial , Multitask Face , HyperFace and HF - ResNet outperform many recent state - of - the - art landmark localization methods including FaceDPL , 3DDFA and SDM .",0
4105,shows that HyperFace performs consistently accurate overall pose angles .,1
4106,"This clearly suggests that while most of the methods work well on frontal faces , HyperFace is able to predict landmarks for faces with full pose variations .",0
4107,"Moreover , we find that R - CNN Fiducial and Multitask Face attain similar performance .",1
4108,The HyperFace has an advantage over them as it uses the intermediate layers for fusion .,0
4109,The local information is contained well in the lower layers of CNN and becomes invariant as depth increases .,0
4110,Fusing the layers brings out that hidden information which boosts the performance for the landmark localization task .,0
4111,"Additionally , we observe that HF - ResNet significantly improves the performance over HyperFace for both AFW and AFLW datasets .",1
4112,The large margin in performance can be attributed to the larger depth for the HF - ResNet model .,0
4113,We also evaluate our models on the challenging subset of the 300 - W landmarks localization dataset ( IBUG ) .,0
4114,The dataset contains 135 test images with wide variations in expression and illumination .,0
4115,The head - pose angle varies from ?60 to 60 in yaw .,0
4116,"Since the dataset contains 68 landmarks points instead of 21 used in AFLW training , the model can not be directly applied for evaluating IBUG .",0
4117,We retrain the network for predicting 68 facial key - points as a separate task in conjunction with the proposed tasks in hand .,0
4118,"We implement it by adding two fully - connected layers in a cascade manner to the shared feature space ( fcfull ) , having dimensions 512 and 136 , respectively .",0
4119,"Following the protocol described in , we use 3 , 148 faces with 68 - point annotations for training .",0
4120,The network is trained end - to - end for the localization of 68 - points landmarks along with the other tasks mentioned in Section 3.2 .,0
4121,We use the standard Euclidean loss function for training .,0
4122,"For evaluation , we compute the average error of all 68 landmarks normalized by the inter-pupil distance .",0
4123,compares the Normalized Mean Error ( NME ) obtained by HyperFace and HF - ResNet with other recently published methods .,0
4124,"We observe that HyperFace achieves a comparable NME of 10.88 , while HF - ResNet achieves the state - of - theart result on IBUG with NME of 8.18 .",1
4125,This shows the effectiveness of the proposed models for 68 - point landmarks localization .,0
4126,Method,0
4127,Normalized Mean Error CDM 19.54 RCPR 17.26 ESR 17.00 SDM 15.40 LBF 11.98 LDDR 11.49 CFSS 9.98 3DDFA 10.59 TCDCN 8 .,0
4128,Pose Estimation,1
4129,"We evaluate R - CNN Pose , Multitask Face and HyperFace on the AFW and AFLW datasets for the pose estimation task .",0
4130,The detection boxes used for evaluating the landmark localization task are used here as well for initialization .,0
4131,"For the AFW dataset , we compare our approach with Multi .",0
4132,"AAM , Multiview HoG , FaceDPL 2 and face.com .",0
4133,Note that multiview AAMs are initialized using the ground truth bounding boxes ( denoted by *) .,0
4134,shows the cumulative error distribution curves on AFW dataset .,0
4135,The curve provides the fraction of faces for which the estimated pose is within some error tolerance .,0
4136,"As can be seen from the figure , both HyperFace and HF - ResNet outperform existing methods by a large margin .",1
4137,"For the AFLW dataset , we do not have pose estimation evaluation for any previous method .",0
4138,"Hence , we show the performance of our method for different pose angles : roll , pitch and yaw in ( a ) , ( b ) and ( c ) respectively .",0
4139,"It can be seen that the network is able to learn roll , and pitch information better than yaw .",0
4140,"The performance traits of R - CNN Pose , Multitask Face , HyperFace and HF - ResNet for pose estimation task are similar to that of the landmarks localization task .",0
4141,R - CNN Pose and Multitask Face perform comparable to each other whereas HyperFace achieves a boosted performance due to 2 .,0
4142,Available at : http://www.ics.uci.edu/ ? dramanan / software / face / face journal.pdf the intermediate layers fusion .,0
4143,It shows that tasks which rely on the structure and orientation of the face work well with features from lower layers of the CNN .,0
4144,"HF - ResNet further improves the performance for roll , pitch as well as yaw .",1
4145,Gender Recognition,1
4146,We present the gender recognition performance on CelebA and LFWA datasets since these datasets come with gender information .,0
4147,"The CelebA and LFWA datasets contain labeled images selected from the Celeb - Faces and LFW datasets , respectively .",0
4148,The Celeb,0
4149,"A dataset contains 10,000 identities and there are 200,000 images in total .",0
4150,"The LFWA dataset has 13,233 images of 5,749 identities .",0
4151,"We compare our approach with Face - Tracer , PANDA - w , PANDA - 1 , with ANet and .",0
4152,The gender recognition performance of different methods is reported in .,0
4153,"On the LFWA dataset , our method outperforms PANDA and FaceTracer , and is equal to .",1
4154,"On the Celeb A dataset , our method performs comparably to .",0
4155,"Unlike which uses 180 , 000 images for training and validation , we only use 20 , 000 images from validation set of CelebA to fine - tune the network .",0
4156,"Similar to the face detection task , we find that gender recognition performs better for HyperFace and Multitask Face as compared to R - CNN Gender proving that learning related tasks together improves the discriminating capability of the individual tasks .",0
4157,"Again , we do not see much difference in the performance of Multitask Face and HyperFace suggesting intermediate layers do not contribute much for the gender recognition task .",0
4158,HF - ResNet achieves state - of - the - art results on both CelebA and LFWA datasets .,1
4159,"provides an experimental analysis of the postprocessing methods : IRP and L - NMS , for face detection task on the AFW dataset .",0
4160,Fast SS denotes the quick version of selective search which produces around 2000 region proposals and takes 2s per image to compute .,0
4161,"On the other hand , Quality SS refers to its slow version which outputs more than 10 , 000 region proposals consuming more than 10s for one image .",0
4162,The HyperFace with a linear bounding box regression and traditional NMS achieves a m AP of 94 % .,1
4163,Just by replacing them with L - NMS provides a boost of 1.2 % .,0
4164,"In this case , bounding - box is constructed using the landmarks information rather linear regression .",0
4165,"Additionaly , we can see from the figure that although Quality SS generates more region proposals , it performs worse than Fast SS with iterative region proposals .",0
4166,IRP adds 300 new regions fora typical image consuming less than 0.5 s which makes it highly efficient as compared to Quality SS .,0
4167,Effect of Post - Processing,0
4168,Fast - HyperFace,0
4169,The Hyperface method is tested on a machine with 8 cores and GTX TITAN - X GPU .,0
4170,The overall time taken to perform all the four tasks is 3s per image .,0
4171,"The limitation is not because of CNN , but due to Selective Search algorithm which takes approximately 2s to generate candidate region proposals .",0
4172,One forward pass through the HyperFace network for 200 proposals takes merely 0.1s .,0
4173,We also propose a fast version of HyperFace which uses a high recall fast face detector instead of Selective Search to generate candidate region proposals .,0
4174,We implement a face detector using Single Shot Detector ( SSD ) framework .,0
4175,"The SSD - based face detector takes a 512 512 dimensional input image and generates face boxes in less than 0.05s , with confidence probability scores ranging from 0 to 1 .",0
4176,We use a probability threshold of 0.01 to select high recall detection boxes .,0
4177,"Unlike traditional SSD , we do not use non-maximum suppression on the detector output , so that we have more number of region proposals .",0
4178,"Typically , the SSD face detector generates 200 proposals per image .",0
4179,"These proposals are directly passed through HyperFace to generate face detection scores , localize face landmarks , estimate pose and recognize gender for every face in the image .",0
4180,"Fast - HyperFace consumes a total time of 0.15s ( 0.05 s for SSD face detector , and 0.1s for HyperFace ) on a GTX TITAN X GPU .",0
4181,"The Fast - HyperFace achieves a m AP of 97.6 % on AFW face detection task , which is comparable to the HyperFace m AP of 97.9 % .",0
4182,"Thus , Fast - HyperFace improves the speed by a factor of 12 with negligible degradation in performance .",0
4183,DISCUSSION,0
4184,We present some observations based on our experiments .,0
4185,"First , all the face related tasks benefit from using the multitask learning framework .",0
4186,"The gain is mainly due to the network 's ability to learn more discriminative features , and post-processing methods which can be leveraged by having landmarks as well as detection scores fora region .",0
4187,"Secondly , fusing intermediate layers improves the performance for structure dependent tasks of pose estimation and landmarks localization , as the features become invariant to geometry in deeper layers of CNN .",0
4188,The HyperFace exploits these observations to improve the performance for all the four tasks .,0
4189,We also visualize the features learned by the HyperFace network .,0
4190,shows the network activation fora few selected feature maps out of 192 from the conv all layer .,0
4191,It can be seen that some feature maps are dedicated solely fora single task while others can be used to predict different tasks .,0
4192,"For example , feature map 27 and 186 can be used for face detection and gender recognition , respectively .",0
4193,The former distinguishes the face and non-face regions whereas the latter outputs high activation for the female faces .,0
4194,"Similarly , feature map 19 shows high activation near eyes and mouth regions , while feature map 96 gives a rough contour of the face orientation .",0
4195,These features can be used for landmark localization and pose estimation tasks .,0
4196,"Several qualitative results of our method on the AFW , PASCAL and FDDB datasets are shown in .",0
4197,"As can be seen from this figure , our method is able to simultaneously perform all the four tasks on images containing extreme pose , illumination , and resolution variations with cluttered background .",0
4198,CONCLUSION,0
4199,"In this paper , we presented a multi-task deep learning method called HyperFace for simultaneously detecting faces , localizing landmarks , estimating head pose and identifying gender .",0
4200,Extensive experiments using various publicly available unconstrained datasets demonstrate the effectiveness of our method on all four tasks .,0
4201,"In future , we will evaluate the performance of our method on other applications such as simultaneous human detection and human pose estimation , object recognition and pedestrian detection .",0
4202,title,0
4203,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,1
4204,abstract,0
4205,"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection .",1
4206,The MS - CNN consists of a proposal sub-network and a detection sub-network .,0
4207,"In the proposal sub-network , detection is performed at multiple output layers , so that receptive fields match objects of different scales .",0
4208,These complementary scale - specific detectors are combined to produce a strong multi-scale object detector .,0
4209,"The unified network is learned end - to - end , by optimizing a multi - task loss .",0
4210,"Feature upsampling by deconvolution is also explored , as an alternative to input upsampling , to reduce the memory and computation costs .",0
4211,"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects .",1
4212,Introduction,0
4213,"Classical object detectors , based on the sliding window paradigm , search for objects at multiple scales and aspect ratios .",0
4214,"While real - time detectors are available for certain classes of objects , e.g. faces or pedestrians , it has proven difficult to build detectors of multiple object classes under this paradigm .",0
4215,"Recently , there has been interest in detectors derived from deep convolutional neural networks ( CNNs ) .",0
4216,"While these have shown much greater ability to address the multiclass problem , less progress has been made towards the detection of objects at multiple scales .",0
4217,"The R - CNN samples object proposals at multiple scales , using a preliminary attention stage , and then warps these proposals to the size ( e.g. 224224 ) supported by the CNN .",0
4218,"This is , however , very inefficient from a computational standpoint .",0
4219,The development of an effective and computationally efficient region proposal mechanism is still an open problem .,0
4220,"The more recent Faster - RCNN addresses the issue with a region proposal network ( RPN ) , which enables end - to - end training .",0
4221,"However , the RPN generates proposals of multiple scales by sliding a fixed set of filters over a fixed set of convolutional feature maps .",0
4222,"This creates an inconsistency between the sizes of objects , which are variable , and filter receptive fields , which are fixed .",0
4223,"As shown in , a fixed receptive field can not cover the multiple scales at which objects .",0
4224,"In natural images , objects can appear at very different scales , as illustrated by the yellow bounding boxes .",0
4225,"A single receptive field , such as that of the RPN ( shown in the shaded area ) , can not match this variability .",0
4226,appear in natural scenes .,0
4227,"This compromises detection performance , which tends to be particularly poor for small objects , like that in the center of .",0
4228,"In fact , handle such objects by upsampling the input image both at training and testing time .",0
4229,This increases the memory and computation costs of the detector .,0
4230,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",1
4231,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .",1
4232,Both of them are learned end - to - end and share computations .,1
4233,"However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .",1
4234,"The intuition is that lower network layers , such as "" conv - 3 , "" have smaller receptive fields , better matched to detect small objects .",0
4235,"Conversely , higher layers , such as "" conv - 5 , "" are best suited for the detection of large objects .",0
4236,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,1
4237,"This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .",0
4238,A second contribution of this work is the use of feature upsampling as an alternative to input upsampling .,0
4239,"This is achieved by introducing a deconvolutional layer that increases the resolution of feature maps ( see ) , enabling small objects to produce larger regions of strong response .",0
4240,This is shown to reduce memory and computation costs .,0
4241,"While deconvolution has been explored for segmentation and edge detection , it is , as far as we know , for the first time used to speedup and improve detection .",0
4242,"When combined with efficient context encoding and hard negative mining , it results in a detector that advances the state - of - the - art detection on the KITTI and Caltech benchmarks .",0
4243,"Without image upsampling , the MS - CNN achieves speeds of 10 fps on KITTI ( 1250375 ) and 15 fps on Caltech ( 640480 ) images .",0
4244,Related Work,0
4245,One of the earliest methods to achieve real - time detection with high accuracy was the cascaded detector of .,0
4246,"This architecture has been widely used to implement sliding window detectors for faces , pedestrians and cars .",0
4247,Two main streams of research have been pursued to improve its speed : fast feature extraction and cascade learning .,0
4248,"In , a set of efficient Haar features was proposed with recourse to integral images .",0
4249,The aggregate feature channels ( ACF ) of made it possible to compute HOG features at about 100 fps .,0
4250,"On the learning front , proposed the soft - cascade , a method to transform a classifier learned with boosting into a cascade with certain guarantees in terms of false positive and detection rate .",0
4251,introduced a Lagrangian formulation to learn cascades that achieve the optimal trade - off between accuracy and computational complexity .,0
4252,"extended this formulation for cascades of highly heterogeneous features , ranging from ACF set to deep CNNs , with widely different complexity .",0
4253,The main current limitation of detector cascades is the difficulty of implementing multiclass detectors under this architecture .,0
4254,"In an attempt to leverage the success of deep neural networks for object classification , proposed the R - CNN detector .",0
4255,This combines an object proposal mechanism and a CNN classifier .,0
4256,"While the R - CNN surpassed previous detectors by a large margin , its speed is limited by the need for object proposal generation and repeated CNN evaluation .",0
4257,"has shown that this could be ameliorated with recourse to spatial pyramid pooling ( SPP ) , which allows the computation of CNN features once per image , increasing the detection speed by an order of magnitude .",0
4258,"Building on SPP , the Fast - RCNN introduced the ideas of back - propagation through the ROI pooling layer and multi-task learning of a classifier and a bounding box regressor .",0
4259,"However , it still depends on bottomup proposal generation .",0
4260,"More recently , the Faster - RCNN has addressed the generation of object proposals and classifier within a single neural network , leading to a significant speedup for proposal detection .",0
4261,"Another interesting work is YOLO , which outputs object detections within a 77 grid .",0
4262,"This network runs at ? 40 fps , but with some compromise of detection accuracy .",0
4263,"For object recognition , it has been shown beneficial to combine multiple losses , defined on intermediate layers of a single network .",0
4264,"GoogLe Net proposed the use of three weighted classification losses , applied at layers of intermediate heights , showing that this type of regularization is useful for very deep models .",0
4265,The deeply supervised network architecture of extended this idea to a larger number of layers .,0
4266,"The fact that higher layers convey more semantic information motivated to combine features from intermediate layers , leading to more accurate semantic segmentation .",0
4267,A similar idea was shown useful for edge detection in .,0
4268,"Similar to , the proposed MS - CNN is learned with losses that account for intermediate layer outputs .",0
4269,"However , the aim is not to simply regularize the learning , as in , or provide detailed information for higher outputs , as in .",0
4270,"Instead , the goal is to produce a strong individual object detector at each intermediate output layer .",0
4271,Multi-scale Object Proposal Network,0
4272,"In this section , we introduce the proposed network for the generation of object proposals .",0
4273,Multi-scale Detection,0
4274,The coverage of many object scales is a critical problem for object detection .,0
4275,"Since a detector is basically a dot -product between a learned template and an image region , the template has to be matched to the spatial support of the object to recognize .",0
4276,There are two main strategies to achieve this goal .,0
4277,"The first is to learn a single classifier and rescale the image multiple times , so that the classifier can match all possible object sizes .",0
4278,"As illustrated in ( a ) , this strategy requires feature computation at multiple image scales .",0
4279,"While it usually produces the most accurate detection , it tends to be very costly .",0
4280,An alternative approach is to apply multiple classifiers to a single input image .,0
4281,"This strategy , illustrated in , avoids the repeated computation of feature maps and tends to be efficient .",0
4282,"However , it requires an individual classifier for each object scale and usually fails to produce good detectors .",0
4283,Several approaches have been proposed to achieve a good trade - off between accuracy and complexity .,0
4284,"For example , the strategy of ( c ) is to rescale the input a few times and learn a small number of model templates .",0
4285,Another possibility is the feature approximation of .,0
4286,"As shown in , this consists of rescaling the input a small number of times and interpolating the missing feature maps .",0
4287,This has been shown to achieve considerable speed - ups fora very modest loss of classification accuracy .,0
4288,"The implementation of multi-scale strategies on CNN - based detectors is slightly different from those discussed above , due to the complexity of CNN features .",0
4289,"As shown in ( e ) , the R - CNN of simply warps object proposal patches to the natural scale of the CNN .",0
4290,"This is somewhat similar to ( a ) , but features are computed for patches rather than the entire image .",0
4291,"The multi-scale mechanism of the RPN , shown in , is similar to that of .",0
4292,"However , multiple sets of templates of the same size are applied to all feature maps .",0
4293,This can lead to a severe scale inconsistency for template matching .,0
4294,"As shown in , the single scale of the feature maps , dictated by the ( 228228 ) receptive field of the CNN , can be severely mismatched to small ( e.g. 3232 ) or large ( e.g. 640640 ) objects .",0
4295,This compromises object detection performance .,0
4296,Inspired by previous evidence on the benefits of the strategy of ( c ) over that of,0
4297,Architecture,0
4298,The detailed architecture of the MS - CNN proposal network is shown in .,0
4299,The network detects objects through several detection branches .,0
4300,The results by all detection branches are simply declared as the final proposal detections .,0
4301,"The network has a standard CNN trunk , depicted in the center of the figure , and a set of output branches , which emanate from different layers of the trunk .",0
4302,These branches consist of a single detection layer .,0
4303,"Note that a buffer convolutional layer is introduced on the branch that emanates after layer "" conv4 - 3 "" .",0
4304,"Since this branch is close to the lower layers of the trunk network , it affects their gradients more than the other detection branches .",0
4305,This can lead to some instability during learning .,0
4306,The buffer convolution prevents the gradients of the detection branch from being back - propagated directly to the trunk layers .,0
4307,"During training , the parameters",0
4308,W of the multi-scale proposal network are learned from a set of training samples,0
4309,", where X i is a training image patch , and Y i = ( y i , bi ) the combination of its class label y i ?",0
4310,"{ 0 , 1 , 2 , , K} and bounding box coordinates",0
4311,.,0
4312,This is achieved with a multi- task loss,0
4313,"where M is the number of detection branches , ?",0
4314,"m the weight of loss l m , and S = { S 1 , S 2 , , S M } , where S m contains the examples of scale m.",0
4315,Note that only a subset,0
4316,"Sm of the training samples , selected by scale , contributes to the loss of detection layer m .",0
4317,"Inspired by the success of joint learning of classification and bounding box regression , the loss of each detection layer combines these two objectives",0
4318,"where p ( X ) = ( p 0 ( X ) , , p K ( X ) ) is the probability distribution over classes ,",0
4319,the smoothed bounding box regression loss of .,0
4320,The bounding box loss is only used for positive samples and the optimal parameters W * = arg min W L ( W ) are learned by stochastic gradient descent .,0
4321,Sampling,0
4322,This section describes the assembly of training samples,0
4323,"In what follows , the superscript m is dropped for notional simplicity .",0
4324,An anchor is centered at the sliding window on layer m associated with width and height corresponding to filter size .,0
4325,More details can be found in .,0
4326,Sgt is the ground truth and IoU the intersection over union between two bounding boxes .,0
4327,"In this case , Y = ( y i * , bi * ) , where i * = arg max i? Sgt IoU ( b , bi ) and ( X , Y ) are added to the positive set S + .",0
4328,All the positive samples in,0
4329,"we considered three sampling strategies : random , bootstrapping , and mixture .",0
4330,Random sampling consists of randomly selecting negative samples according to a uniform distribution .,0
4331,"Since the distribution of hard and easy negatives is heavily asymmetric too , most randomly collected samples are easy negatives .",0
4332,"It is well known that hard negatives mining helps boost performance , since hard negatives have the largest influence on the detection accuracy .",0
4333,"Bootstrapping accounts for this , by ranking the negative samples according to their objectness scores , and then collecting top | S ? | negatives .",0
4334,"Mixture sampling combines the two , randomly sampling half of S ?",0
4335,and sampling the other half by bootstrapping .,0
4336,"In our experiments , mixture sampling has very similar performance to bootstrapping .",0
4337,"To guarantee that each detection layer only detects objects in a certain range of scales , the training set for the layer consists of the subset of S that covers the corresponding scale range .",0
4338,"For example , the samples of smallest scale are used to train the detector of "" det - 8 "" in .",0
4339,"It is possible that no positive training samples are available fora detection layer , resulting in | S ? |/| S + | ? ?.",0
4340,This can make learning unstable .,0
4341,"To address this problem , the cross - entropy terms of positives and negatives are weighted as follows",0
4342,Implementation Details,0
4343,"Learning is initialized with the model generated by the first learning stage of the proposal network , described in Section 3.4 .",0
4344,"The learning rate is set to 0.0005 , and reduced by a factor of 10 times after every 10,000 iterations .",0
4345,"Learning stops after 25,000 iterations .",0
4346,The joint optimization of ( 6 ) is solved by back - propagation throughout the unified network .,0
4347,Bootstrapping is used and ? =,0
4348,"1 . Following , the parameters of layers "" conv 1 - 1 "" to "" conv2 - 2 "" are fixed during learning , for faster training .",0
4349,Object Detection Network,0
4350,"Although the proposal network could work as a detector itself , it is not strong , since it s sliding windows do not cover objects well .",0
4351,"To increase detection accu- racy , a detection network is added .",0
4352,"Following , a ROI pooling layer is first used to extract features of a fixed dimension ( e.g. 77512 ) .",0
4353,"The features are then fed to a fully connected layer and output layers , as shown in .",0
4354,"A deconvolution layer , described in Section 4.1 , is added to double the resolution of the feature maps .",0
4355,The multi - task loss of ( 1 ) is extended to,0
4356,where l M+1 and S M +1 are the loss and training samples for the detection sub-network .,0
4357,S M+1 is collected as in .,0
4358,"As in ( 2 ) , l M+1 combines a crossentropy loss for classification and a smoothed L 1 loss for bounding box regression .",0
4359,The detection sub -network shares some of the proposal sub-network parameters W and adds some parameters W d .,0
4360,"The parameters are optimized jointly , i.e. ( W * , W * d ) = arg min L(W , W d ) .",0
4361,"In the proposed implementation , ROI pooling is applied to the top of the "" conv4 - 3 "" layer , instead of the "" conv5 - 3 "" layer of , since "" conv4 - 3 "" feature maps performed better in our experiments .",0
4362,"One possible explanation is that "" conv4 - 3 "" corresponds to higher resolution and is better suited for location - aware bounding box regression .",0
4363,CNN,0
4364,Feature Map Approximation,0
4365,Input size has a critical role in CNN - based object detection accuracy .,0
4366,"Simply forwarding object patches , at the original scale , through the CNN impairs performance ( especially for small ones ) , since the pre-trained CNN models have a natural scale ( e.g. 224224 ) .",0
4367,"While the R - CNN naturally solves this problem through warping , it is not explicitly addressed by the Fast - RCNN or Faster - RCNN .",0
4368,"To bridge the scale gap , these methods simply upsample input images ( by ?2 times ) .",0
4369,"For datasets , such as KITTI , containing large amounts of small objects , this has limited effectiveness .",0
4370,"Input upsampling also has three side effects : large memory requirements , slow training and slow testing .",0
4371,It should be noted that input upsampling does not enrich the image details .,0
4372,"Instead , it is needed because the higher convolutional layers respond very weakly to small objects .",0
4373,"For example , a 3232 object is mapped into a 44 patch of the "" conv4 - 3 "" layer and a 22 patch of the "" conv5 - 3 "" layer .",0
4374,This provides limited information for 77 ROI pooling .,0
4375,"To address this problem , we consider an efficient way to increase the resolution of feature maps .",0
4376,"This consists of upsampling feature maps ( instead of the input ) using a deconvolution layer , as shown in .",0
4377,"This strategy is similar to that of , shown in , where input rescaling is replaced by feature rescaling .",0
4378,"In , a feature approximator is learned by least squares .",0
4379,"In the CNN world , a better solution is to use a deconvolution layer , similar to that of .",0
4380,"Unlike input upsampling , feature upsampling does not incur in extra costs for memory and computation .",0
4381,"Our experiments show that the addition of a deconvolution layer significantly boosts detection performance , especially for small objects .",0
4382,"To the best of our knowledge , this is the first application of deconvolution to jointly improve the speed and accuracy of an object detector .",0
4383,Context Embedding,0
4384,Context has been shown useful for object detection and segmentation .,0
4385,Context information has been modeled by a recurrent neural network in and acquired from multiple regions around the object location in .,0
4386,"In this work , we focus on context from multiple regions .",0
4387,"As shown in , features from an object ( green cube ) and a context ( blue cube ) region are stacked together immediately after ROI pooling .",0
4388,The context region is 1.5 times larger than the object region .,0
4389,An extra convolutional layer without padding is used to reduce the number of model parameters .,0
4390,"It helps compress redundant context and object information , without loss of accuracy , and guarantees that the number of model parameters is approximately the same .",0
4391,Experimental Evaluation,0
4392,The performance of the MS - CNN detector was evaluated on the KITTI and Caltech Pedestrian benchmarks .,1
4393,"These were chosen because , unlike VOC and ImageNet , they contain many small objects .",0
4394,Typical image sizes 8x4 2048 anchor 40x20 56x28 80x40 112x56 160x80 224x112 320x160 are 1250375 on KITTI and 640480 on Caltech .,0
4395,"KITTI contains three object classes : car , pedestrian and cyclist , and three levels of evaluation : easy , moderate and hard .",0
4396,"The "" moderate "" level is the most commonly used .",0
4397,"In total , 7,481 images are available for training / validation , and 7,518 for testing .",0
4398,"Since no ground truth is available for the test set , we followed , splitting the trainval set into training and validation sets .",0
4399,"In all ablation experiments , the training set was used for learning and the validation set for evaluation .",0
4400,"Following , a model was trained for car detection and another for pedestrian / cyclist detection .",0
4401,One pedestrian model was learned on Caltech .,0
4402,The model configurations for original input size are shown in .,0
4403,"The detector was implemented in C ++ within the Caffe toolbox , and source code is available at https://github.com/zhaoweicai/mscnn.",1
4404,All times are reported for implementation on a single CPU core ( 2.40 GHz ) of an Intel Xeon E5 - 2630 server with 64 GB of RAM .,1
4405,An NVIDIA Titan GPU was used for CNN computations .,1
4406,Proposal Evaluation,0
4407,We start with an evaluation of the proposal network .,0
4408,"Following , oracle recall is used as performance metric .",0
4409,"For consistency with the KITTI setup , aground truth is recalled if it s best matched proposal has IoU higher than 70 % for cars , and 50 % for pedestrians and cyclists .",0
4410,The roles of individual detection layers shows the detection accuracy of the various detection layers as a function of object height in pixels .,0
4411,"As expected , each layer has highest accuracy for the objects that match its scale .",0
4412,"While the individual recall across scales is low , the combination of all detectors achieves high recall for all object scales .",0
4413,The effect of input size shows that the proposal network is fairly robust to the size of input images for cars and pedestrians .,0
4414,"For cyclist , performance increases between heights 384 and 576 , but there are no gains beyond this .",0
4415,These results show that the network can achieve good proposal generation performance without substantial input upsampling .,0
4416,Detection sub-network improves proposal sub- network has shown that multi-task learning can benefit both bounding box regression and classification .,0
4417,"On the other hand showed that , even when features are shared between the two tasks , object detection does not improve object proposals too much .",0
4418,"shows that , for the MS - CNN , detection can substantially benefit proposal generation , especially for pedestrians .",0
4419,"Comparison with the state - of - the - art compares the proposal generation network to BING , Selective Search , EdgeBoxes , MCG , 3 DOP and RPN .",0
4420,The top row of the figure shows that the MS - CNN achieves a recall about 98 % with only 100 proposals .,0
4421,"This should be compared to the ? 2,000 proposals required by 3 DOP and the ? 10,000 proposals required by EdgeBoxbes .",0
4422,"While it is not surprising that the proposed network outperforms unsupervised proposal methods , such as , its large gains over supervised methods , that can even use 3D information , are significant .",0
4423,"The closest performance is achieved by RPN ( input upsampled twice ) , which has substantially weaker performance for pedestrians and cyclists .",0
4424,"When the input is not upsampled , RPN misses even more objects , as shown in .",0
4425,It is worth mentioning that the MS - CNN generates high quality proposals ( high overlap with the ground truth ) without any edge detection or segmentation .,0
4426,This is evidence for the effectiveness of bounding box regression networks .,0
4427,Object Detection Evaluation,0
4428,In this section we evaluate object detection performance .,0
4429,"Since the performance of the cyclist detector has large variance on the validation set , due to the low number of cyclist occurrences , only car and pedestrian detection are considered in the ablation experiments .",0
4430,The effect of input upsampling shows that input upsampling can be a crucial factor for detection .,0
4431,"A significant improvement is obtained by upsampling the inputs by 1.5?2 times , but we saw little gains beyond a factor of .",0
4432,Proposal performance comparison on KITTI validation set ( moderate ) .,0
4433,The first row is proposal recall curves and the second row is recall v.s.,0
4434,IoU for 100 proposals ..,0
4435,Results on the KITTI validation set .,0
4436,""" hXXX "" indicates an input of height "" XXX "" , "" 2 x "" deconvolution , "" ctx "" context encoding , and "" c "" dimensionality reduction convolution .",0
4437,"In columns "" Time "" and "" # params "" , entries before the "" / "" are for car model and after for pedestrian / cyclist model .",0
4438,2 . This is smaller than the factor of 3.5 required by .,0
4439,Larger factors lead to ( exponentially ) slower detectors and larger memory requirements .,0
4440,"Sampling strategy compares sampling strategies : random ( "" h576random "" ) , bootstrapping ( "" h 576 "" ) and mixture ( "" h576 - mixture "" ) .",0
4441,"For car , these three strategies are close to each other .",0
4442,"For pedestrian , bootstrapping and mixture are close , but random is much worse .",0
4443,Note that random sampling has many more false positives than the other two .,0
4444,CNN feature approximation,0
4445,Three methods were attempted for learning the deconvolution layer for feature map approximation :,0
4446,1 ) bilinearly interpolated weights ; 2 ) weights initialized by bilinear interpolation and learned with backpropagation ; 3 ) weights initialized with Gaussian noise and learned by backpropagation .,0
4447,"We found the first method to work best , confirming the findings of .",0
4448,Comparison to the state - of - the - art on KITTI benchmark test set ( moderate ) ..,0
4449,"As shown in , the deconvoltion layer helps inmost cases .",0
4450,"The gains are larger for smaller input images , which tend to have smaller objects .",0
4451,Note that the feature map approximation adds trivial computation and no parameters .,0
4452,Context embedding shows that there is again in encoding context .,0
4453,"However , the number of model parameters almost doubles .",0
4454,"The dimensionality reduction convolution layer significantly reduces this problem , without impairment of accuracy or speed .",0
4455,Object detection by the proposal network,0
4456,"The proposal network can work as a detector , by switching the class - agnostic classification to class - specific .",0
4457,"shows that , although not as strong as the unified network , it achieves fairly good results , which are better than those of some detectors on the KITTI leaderboard 1 .",0
4458,Comparison to the state - of - the - art,0
4459,"The results of model "" h768 - ctx - c "" were submitted to the KITTI leaderboard .",0
4460,A comparison to previous approaches is given in and .,0
4461,The MS - CNN set a new record for the detection of pedestrians and cyclists .,1
4462,"The columns "" Pedestrians - Mod "" and "" Cyclists - Mod "" show substantial gains ( 6 and 7 points respectively ) over 3 DOP , and much better performance than the Faster - RCNN , Regionlets , etc .",0
4463,"We also led a nontrivial margin over the very recent SDP + RPN , which used scale depen - dent pooling .",1
4464,"In terms of speed , the network is fairly fast .",0
4465,"For the largest input size , the MS - CNN detector is about 8 times faster than 3DOP .",0
4466,On the original images ( 1250375 ) detection speed reaches 10 fps .,0
4467,Pedestrian detection on Caltech The MS - CNN detector was also evaluated on the Caltech pedestrian benchmark .,1
4468,"The model "" h720 - ctx "" was compared to methods such as DeepParts , Com p ACT - Deep , CheckerBoard , LDCF , ACF , and SpatialPooling on three tasks : reasonable , medium and partial occlusion .",0
4469,"As shown in , the MS - CNN has state - of the - art performance . and ( c ) show that it performs very well for small and occluded objects , outperforming DeepParts , which explicitly addresses occlusion .",1
4470,"Moreover , it misses a very small number of pedestrians , due to the accuracy of the proposal network .",0
4471,The speed is approximately 8 fps ( 15 fps ) on upsampled 960720 ( original 640480 ),0
4472,Caltech images .,0
4473,Conclusions,0
4474,"We have proposed a unified deep convolutional neural network , denoted the MS - CNN , for fast multi-scale object detection .",0
4475,"The detection is preformed at various intermediate network layers , whose receptive fields match various object scales .",0
4476,"This enables the detection of all object scales by feedforwarding a single input image through the network , which results in a very fast detector .",0
4477,"CNN feature approximation was also explored , as an alternative to input upsampling .",0
4478,It was shown to result in significant savings in memory and computation .,0
4479,"Overall , the MS - CNN detector achieves high detection rates at speeds of up to 15 fps .",0
4480,Acknowledgement,0
4481,This work was partially funded by NSF grant IIS1208522 and a gift from KETI .,0
4482,We also thank NVIDIA for GPU donations through their academic program .,0
4483,title,0
4484,Aggregate Channel Features for Multi-view Face Detection,1
4485,abstract,0
4486,Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones .,1
4487,"While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild .",0
4488,"To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form .",0
4489,"We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multiscale version of features with better performance .",0
4490,"To deal with poses of faces in the wild , we propose a multi-view detection approach featuring score re-ranking and detection adjustment .",0
4491,"Following the learning pipelines in Viola - Jones framework , the multi-view face detector using aggregate channel features shows competitive performance against state - of - the - art algorithms on AFW and FDDB testsets , while runs at 42 FPS on VGA images .",0
4492,Introduction,0
4493,Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction .,1
4494,"In the past decade , the most influential work should be the face detection framework proposed by Viola and Jones .",0
4495,The Viola - Jones ( abbreviated as VJ below ) framework uses rectangular Haar - like features and learns the hypothesis using Adaboost algorithm .,0
4496,"Combined with the attentional cascade structure , the VJ detector achieved real - time face detection at that time .",0
4497,"Despite the great success of the VJ detector , the performance is still far from satisfactory due to the large appearance variance of faces in unconstrained settings .",0
4498,* Corresponding author ..,0
4499,An intuitive visualization of our multi-view face detector using aggregate channel features .,0
4500,The area with warmer color indicates more attention paid to by the detector .,0
4501,"To handle faces in the wild , many subsequences of VJ framework merged .",0
4502,"These methods mainly get the performance gains in two aspects , more complicated features and ( or ) more powerful learning algorithms .",0
4503,"As the combination of boosting and cascade has been proven to be quite effective in face detection , the bottleneck lies in the feature representation since complicated features adopted in the above literatures bring about limited performance gains at the cost of large computation cost .",0
4504,"Lately in another domain of pedestrian detection , a family of channel features has achieved record performances .",0
4505,Channel features compute registered maps of the original images like gradients and histograms of oriented gradients and then extract features on these extended channels .,0
4506,The classifier learning process follows the VJ framework pipeline .,0
4507,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .",1
4508,"Channel extension offers rich representation capacity , while simple feature form guarantees fast computation .",0
4509,"With these two superiorities , the aggregate channel features breakthrough the bottleneck in VJ framework and have the potential to make great advance in face detection .",1
4510,"As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation .",0
4511,"To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .",1
4512,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .",1
4513,"Although multi-view detection could effectively deal with diverse poses , additional issues come up as how to merge detections output by separately trained subview detectors , and how to deal with the offsets of location and scale between output detections and ground - truth .",0
4514,"We solve these problems by carefully designed post -processing including score re-ranking , detection merging and bounding box adjustment .",0
4515,"The detailed experimental exploration of aggregate channel features , along with our improvements on multiview detection , leads to large performance gain in face detection in the wild .",0
4516,"On two challenging face databases , AFW and FDDB , the proposed multi-view face detector shows competitive performance against state - of - the - art detectors in both detection accuracy and speed .",0
4517,The remaining parts of this paper are organized as follows .,0
4518,Section 2 revisits related work in face detection .,0
4519,Section 3 describes how we build the face detector using aggregate channel features .,0
4520,Section 4 addresses problems concerning multi-view face detection .,0
4521,Experimental results on AFW and FDDB are shown in section 5 and we conclude the paper in section 6 .,0
4522,Related work,0
4523,Face detection has drawn much attention since the early time of computer vision .,0
4524,"Although many solutions had been put forward , it was not until Viola and Jones proposed their milestone work that face detection saw surprising progress in the past decades .",0
4525,"The VJ face detector features in three aspects : fast feature computation via integral image representation , classifier learning using Adaboost , and the attentional cascade structure .",0
4526,"One main drawback of the VJ framework is that the features have limited repre-sentation capacity , while the feature pool size is quite large to compensate for that .",0
4527,"Typically , in a 24 24 detection window , the number of Haar - like features is 160,000 .",0
4528,"To address the problem , efforts are made in two directions .",0
4529,"Some focus on more complicated features like HoG , SURF .",0
4530,Some aim to speedup the feature selection in a heuristic way .,0
4531,"However , the problem has n't been solved perfectly .",0
4532,"In this paper , we mainly focus on the feature representation part and make a deep exploration into it , which is complementary to existing work on the learning algorithm and classifier structure in the VJ framework .",0
4533,Recently channel features have been proposed and shown record performance in pedestrian detection .,0
4534,"Due to the channel extension to diverse types like gradients and local histograms , the features show richer representation capacity for classification .",0
4535,"However , the features are extracted as rectangular sums at various locations and scales which we believe leads to a redundant feature pool .",0
4536,"During preparation of this paper , Mathias et al. independently discover the effectiveness of integral channel features in face detection domain .",0
4537,"In this paper , we adopt a novel variant of channel features called aggregate channel features , which extract features directly as pixel values in extended channels without computing rectangular sums at various locations and scales .",0
4538,The feature has powerful representation capacity and the feature pool size is only several thousands .,0
4539,"Through careful design in section 3 and implementation of multi-view detection in section 4 , the aggregate channel features based detector achieves state - of - theart performance on challenging databases .",0
4540,Proposed face detector,0
4541,"In this section , we make a full exploration of the aggregate channel features in the context of face detection .",0
4542,"We first give a brief introduction of the feature itself , including its computation , properties and advantages over traditional Haar - like features used in VJ framework .",0
4543,"Then the detailed experimental investigation is described in two parts , feature design and training design .",0
4544,"Before that , some guidelines concerning how we conduct the investigation are demonstrated .",0
4545,Each design part is divided into several separate experiments ended with a summary explaining the specific parameters used in our proposed face detector .,0
4546,Note that each experiment focuses on only one parameter and the others remain constant .,0
4547,"Through the well - designed experiments , the proposed face detector based on aggregate channel features is built step by step .",0
4548,Issues concerning the implementation of multi-view face detection which further improves the performance are discussed in the next section .,0
4549,Feature description,0
4550,Channel extension :,0
4551,The basic structure of the aggregate channel features is channel .,0
4552,The application of channel has along history since digital images were invented .,0
4553,"The most common type of channel should be the color channels of the image , with Gray - scale and RGB being typical ones .",0
4554,"Besides color channels , many different channel types have been invented to encode different types of information for more difficult problems .",0
4555,"Generally , channels can be defined as a registered map of the original image , whose pixels are computed from corresponding patches of original pixels .",0
4556,Different channels can be computed with linear or non-linear transformation of the original image .,0
4557,"To allow for sliding window detection , the transformations are constrained to be translationally invariant .",0
4558,Feature computation :,0
4559,"Based on the definition of channels , the computation of aggregate channel features is quite simple .",0
4560,"As shown in , given a color image , all defined channels are computed and subsampled by a preset factor .",0
4561,The aggregate pixels in all subsampled channels are then vectorized into a pixel look - up table .,0
4562,Note that an optional smoothing procedure can be done on each channel with a binomial filter both before computation and after subsampling .,0
4563,Classifier learning :,0
4564,The learning process is quite simple .,0
4565,Two changes are made compared with VJ framework .,0
4566,First is that weak classifier is changed from decision stump to depth - 2 decision tree .,0
4567,The more complex weak classifier shows stronger ability in seeking the discriminant intra and inter channel correlations for classification .,0
4568,Second difference is that soft - cascade structure is used .,0
4569,"Unlike the attentional cascade structure in VJ framework which has several cascade stages , a single - stage classifier is trained on the whole training data and a threshold is then set after each weak classifier picked by Adaboost .",0
4570,These two changes lead to more efficient training and detection .,0
4571,"Overall superiority : Compared with traditional Haarlike features used in VJ framework , aggregate channel features have the following differences and advantages :",0
4572,"1 ) The image channels are extended to more types in order to encode diverse information like color , gradients , local histograms and soon , therefore possess richer representation capacity .",0
4573,"2 ) Features are extracted directly as pixel values on downsampled channels rather than computing rectangu - lar sums with various locations and scales using integral images , leading to a faster feature computation and smaller feature pool size for boosting learning .",0
4574,"With the help of cascade structure , detection speed is accelerated more .",0
4575,"3 ) Due to its structure consistence with the overall image , when coupled with boosting method , the boosted classifier naturally encodes structured pattern information from large training data ( see for an illustration ) , which gives more accurate localization of faces in the image .",0
4576,Investigation guidelines,0
4577,All investigations are trained on the AFLW face database 1 and tested on the Annotated Faces in the Wild ( AFW ) testset 2 .,0
4578,"To make it clear , there are in total 36 , 112 positive samples and 108 , 336 negative samples selected from AFLW which are kept constant in all investigations .",0
4579,"Testset contains 205 natural images with faces that vary a lot in pose , appearance and illumination .",0
4580,"To alleviate the ground - truth offset caused by different annotation styles ) in training and testing set and make the evaluation more comparable , a lower Jaccard index 3 with threshold 0.3 is adopted in comparative evaluation .",0
4581,Practically the lower threshold wo n't cause errors being mistakenly corrected .,0
4582,"Note that in final evaluation of the proposed face detector ( section 5 ) , the AFW testset , together with another face benchmark FDDB database , are used as testbed and the evaluation metric follows the database protocol .",0
4583,Feature design,0
4584,"To fully exploit the power of aggregate channel features in face detection domain , a deep investigation into the design of the feature is done mainly on channel types , window size , subsampling method and feature scale .",0
4585,Results of comparative experiments are shown in .,0
4586,Channel types :,0
4587,"Three types of channels are used , which are color channel ( Gray - scale , RGB , HSV and LUV ) , gradient magnitude , and gradient histograms .",0
4588,The computation of the latter two channel types could be seen as a generalized version of HoG features .,0
4589,"Specifically , gradient magnitude is the biggest response on all three color channels , and oriented gradient histograms follow the idea of HoG in that :",0
4590,1 ) rectangular cell size in HoG equals the subsampling factor in aggregated channel features ; 2 ) each orientation bin results in one feature channel ( 6 orientation bins are used in this paper ) .,0
4591,( a ) ( c ) show how much each of these three types alone contributes to the performance of face detection .,0
4592,It can be seen that the gradient histograms contribute most to the performance among all three channel types .,0
4593,shows the performances of combinations of these three types computed on different color channels .,0
4594,Detection window size : Detection window size is the scale to which we resize all face and non-face samples and then train our detector .,0
4595,Larger window size includes more pixels in feature pool and thus may improve the face detection performance .,0
4596,"On the other hand , too large window will miss some small faces and diminish the detection efficiency .",0
4597,( e ) shows comparison of window size ranging from 32 to 112 with astride of 16 pixels .,0
4598,Subsampling :,0
4599,The factor for subsampling can be regarded as the perceptive scale for that it controls the scale at which the aggregation is done .,0
4600,Changing the factor from large to small leads to the feature representation shifting from coarse to fine and the feature pool size getting bigger .,0
4601,Experiments on different subsampling factors are shown in ( f ) .,0
4602,"In original aggregate channel features , the way to do subsampling is average pooling .",0
4603,"Following the idea in Convolutional Neural Networks , another two ways of subsampling , max pooling and stochastic pooling are tested in ( g ) .",0
4604,Smoothing :,0
4605,"As described in feature description , both pre and post smoothing is done in default setting of aggregate channel features .",0
4606,A binomial filter with a radius of 1 is used for smoothing .,0
4607,The smoothing procedure also has a great influence on the scale of the feature representation .,0
4608,"Concretely , pre-smoothing determines how far the local neighborhood is in which local correlations are encoded before channel computation , while post - smoothing determines the neighborhood size in which the computed channel features are integrated with each other .",0
4609,"In , the former corresponds to the ' local scale ' of the feature , while the latter represents the ' integration scale ' .",0
4610,We vary the filter radius used in pre and post smoothing and find that both using a radius of 1 gets the best results .,0
4611,( h ) ( i ) present the comparative results .,0
4612,Multi-scale :,0
4613,"In aggregate channel features , although hidden information at different scale could be extracted at a cost of more weak classifiers , it would be better to make the integrated channel features multi-scaled and thus make themselves more discriminant .",0
4614,Therefore the same or better classification performance can be achieved with fewer weak classifiers .,0
4615,"In this part , we implement three multiscale version of aggregate channel features in the aforementioned three kinds of scale , perceptive scale ( subsampling ) , local scale ( pre-smoothing ) and integration scale ( post- smoothing ) and compare their performaces .",0
4616,See results in ( j ) ( l ) .,0
4617,"Summary : The color channel , gradient magnitude and gradient histograms prove themselves a good match in aggregate channel features .",0
4618,"However , different choices of color channel used and on which gradients are computed have a great impact on performance .",0
4619,"According to the ex-periments , LUV channel and gradient magnitude and 6 - bin histograms computed on RGB color space ( in total 10 channels ) are the best choice for face detection .",0
4620,"Larger detection window size generally gets better performance , but will miss many small faces in testing and lead to inefficient detection .",0
4621,"In this work , we set the size to 80 80 as its optimal performance .",0
4622,"A subsampling factor of 4 is most reasonable according to the experiments , while different pooling methods show small differences .",0
4623,"However , max pooling and stochastic pooling are much slower than average pooling , therefore the average pooling becomes the best match for the sake of efficiency .",0
4624,"In this way , the resulting feature pool size of our face detector is ( 80 / 4 ) ( 80 / 4 ) 10 = 4000 , considerably smaller than that in VJ framework .",0
4625,"As for multi-scale version of aggregate channel features , multi-local - scale with an additional scale of radius 2 shows the best performance .",0
4626,The probable reason is that pre-smoothing controls the local scale of the neighborhood feature correlations and therefore matches the intuition inside multi-scale best .,0
4627,"Compared with other fine - tuning , the multi-scale version has a notable performance gain for that it makes up for the scale uniformity caused by subsampling to some extent .",0
4628,One main drawback is that it doubles the feature pool size and as a result slows down the detection speed somewhat .,0
4629,"Based on the trade - off , we implement two face detectors with different scale settings , one is singlescaled with faster speed and the other is multi-scaled with better accuracy .",0
4630,We evaluate and discuss the performances of these two versions in detail in section 5 .,0
4631,Training design,0
4632,"Besides careful design of the aggregate channel features , experiments on the training process which is similar to that in VJ framework are also carried out .",0
4633,The differences are that the weak classifier is changed into depth - 2 decision tree and soft - cascade structure is used .,0
4634,Details of the training design are as follows .,0
4635,Number of weak classifiers :,0
4636,"Given a feature pool size of 4 , 000 , we vary the number of weak classifiers contained in the soft - cascade .",0
4637,"In performances of various numbers of weak classifiers ranging from 32 to 8192 are displayed , which shows that apparently more classifiers generate better performance , and when the number gets larger the performance begins to saturate .",0
4638,"Since more classifiers slowdown the detection speed , there 's a tradeoff between accuracy and speed .",0
4639,Searching for the saturate point as the optimal is significant during training in such framework .,0
4640,Training data :,0
4641,"Empirically , more training data will get better performance given powerful representation capacity .",0
4642,"In this case , AFLW database is used as the only positive training data .",0
4643,"However , as images in AFLW database are very salient and the background has very less variance , negative samples cropped from the AFLW database ca n't represent the real world scenario well , which limits the face detection performance in the wild .",0
4644,"In this part , we further use PASCAL VOC 2007 database and randomly crop windows from images without person as the new negative samples .",0
4645,Experiments show that the new training data containing cluttered background significantly improve the performance with 4.1 % .,0
4646,"Summary : Based on observations above , we choose 2048 as the number of weak classifiers contained in the soft cascade .",0
4647,"As each weak classifier is a depth - 2 decision tree , it takes only two comparing operations to apply a weak classifier , which is quite fast .",0
4648,"During training , as negative data is large , we adopt a standard Bootstrap procedure to sample hard negative samples from PASCAL VOC 2007 in the implementation of the proposed face detector .",0
4649,Multi - view detection,0
4650,Human faces in real world usually have highly varied poses .,0
4651,"In AFLW database , the human pose is divided into three aspects : 1 in - plane rotation ' roll ' and 2 out - of - plane rotations ' yaw ' and ' pitch ' .",0
4652,"Because of this large variance in face pose , it is difficult to train a single view face detector to handle all the poses effectively .",0
4653,A multi-view detection is further examined in this part .,0
4654,"Due to the adoption of soft - cascade structure , a multi-view version of face detector wo n't cause too much computation burden .",0
4655,"Typically , we divide the out - of - plane rotation yaw into different views and let the classifier itself tolerate the pose variance in the other two types of rotations .",0
4656,Adopting multi-view detection also brings about many troublesome issues .,0
4657,"If handled improperly , the performance will differ greatly .",0
4658,"First , detectors of different view will each produce a set of candidate positive windows followed with a set of confidence scores .",0
4659,"For application purpose , we need to merge these detections from different views and also remove duplicated windows .",0
4660,A typical approach is Non -Maximum Suppression ( NMS ) .,0
4661,An issue rises on how to compare confidence scores from different classifiers and how to do window merging in the trade - off between high .,0
4662,"Illustration of different ground - truth annotation styles in databases , the view partition and symmetric detection adjustment .",0
4663,Rectangles with red and green color correspond to detections before and after adjustment .,0
4664,precision rate and high detection rate .,0
4665,"Second , as for detection evaluation , usually the overlap of bounding boxes is used as the criterion .",0
4666,"However , annotations in different data sets may not have a consistent style ) .",0
4667,This diversity suffers more in profile faces .,0
4668,"Since our face detector is trained and tested on different data sets , this issue impacts the performance a lot .",0
4669,"Third , detectors of different views need to be trained with different samples separately .",0
4670,How to divide the views therefore becomes another concerning problem .,0
4671,"In this section , we address the above three issues successfully by careful designs and therefore fully exploit the advantage of multi-view detection .",0
4672,View partition,0
4673,"In the scenario of detecting faces in the wild , pose variation caused by yaw is usually severer than pitch and roll .",0
4674,Therefore we divide the faces in AFLW database according to yaw angle .,0
4675,We have 6 subviews which are horizontally symmetric ( see 4 ( b ) ) because we flip each image in the training set .,0
4676,"Specifically , there are 6630 , 8446 , 9610 , 9610 , 8446 , 6630 images in views from 1 to 6 .",0
4677,"Benefitting from the symmetry of our model , we can only train three subview detectors of the right side for simplicity , and use these trained right - side detectors to generate the left - side detectors .",0
4678,Detections of all six detectors are then merged to get the final detections .,0
4679,"Though multi-view detection significantly improves the detection performance ( especially the recall rate ) , the post -processing of detections from different detectors becomes a trouble .",0
4680,"If handled improperly , the performance degrades a lot .",0
4681,Post - processing,0
4682,Difficulties in the post - processing of multi-view detection mainly reflect on the following aspects :,0
4683,1 ) different score distributions and ; 2 ) different bounding box styles .,0
4684,"Concretely , as each subview detector is trained separately , their output confidence scores usually have different distributions .",0
4685,"What 's more , due to the annotation rule in the AFLW database that the face 's nose is approximately at the center location of the bounding box ground - truth , as the subview changes , the bounding box shifts .",0
4686,This bounding box offset causes difficulty both in detection merging and final evaluation using Jaccard index metric .,0
4687,"To solve these annoying issues and make the best use of multi-view detection , we introduce the following methods for postprocessing .",0
4688,Score re-ranking :,0
4689,We propose the following three kinds of score re-ranking :,0
4690,"1 ) normalizing scores of different views to [ 0 , 1 ] ; 2 ) defining anew score that has uniform distribution and ; 3 ) taking overlapping detections into consideration .",0
4691,N ormalization :,0
4692,"After training a classifier , calculate the output range of the classifier and use the range to do normalization later so that output score has a range of [ 0 , 1 ] .",0
4693,"N ewScore : Originally , each weak classifier in the softcascade owns a score and final score is the sum of all scores .",0
4694,"Instead , we use the number of weak classifier that the image patch passed positively as the new score .",0
4695,Therefore the upper limit of the new score is 2048 in our case .,0
4696,"OverlapRerank : Given an image , multiple detections from multi-view detectors exist each with a score .",0
4697,"For each detection , we first calculate the number of overlapped detection it has ( overlap threshold is 0.65 ) and then multiply score of each detection with a factor of its overlapping number ranking 1 .",0
4698,Sumof Overlap :,0
4699,"Instead of using overlapping as a multiply factor , here we use the sum of overlapped detections ' scores as the current detection 's new score .",0
4700,Detection merging :,0
4701,"Apart from the Greedy * version of Non Maximum Suppression , we also use the detection combination introduced in .",0
4702,It averages the locations of overlapped detections rather than suppresses them .,0
4703,Detection adjustment :,0
4704,"As shown in ( a ) , different databases have different annotation styles of groundtruth .",0
4705,"Specifically , AFLW has square annotations with nose located approximately at the center .",0
4706,AFW uses tight rectangular bounding boxes as annotations with the eye - brow being the approximate upper bound .,0
4707,FDDB uses elliptical annotations bounding the whole head .,0
4708,"As our detector is trained on AFLW and tested on AFW and FDDB , there exist offsets in both detection position and scale .",0
4709,"According to observations , the offsets vary as face pose changes .",0
4710,Therefore we adopt a view - specific detection adjustment to alleviate the offsets .,0
4711,"Note that the adjustment is constant for all images and faces in the same database , see ( b ) for 1 A toy example : Det1 : score : 10 , n Overlap : 10 ; Det2 : score : 9 , n Overlap : 20 ; Det3 : score :",0
4712,"5 , n Overlap :",0
4713,5 . After score re-ranking : Det1 : score : 10 2 3 = 6.67 ; Det2 : score : 9 3 3 = 9 ; Det3 : score : 5,0
4714,details .,0
4715,"Summary : According to experimental results , OverlapRerank seems to be the best score re-ranking method .",0
4716,"The underlying reason maybe that true positives usually have many overlapped detections , while the false positives would only get a few responses .",0
4717,Therefore leveraging this overlapping information in score re-ranking can reduce many false positives .,0
4718,"However , in practice , overlap related methods and detection combination both cost much time to process , which is infeasible in a large majority of applications .",0
4719,We finally adopt N ormalization score reranking combined with Greedy * Non Maximum Suppression for the sake of detection speed .,0
4720,Experiments,0
4721,"In this section , we compare our method with state - of the - art methods on AFW and FDDB databases which contain challenging faces in the wild .",0
4722,"In AFW , we compare with three commercial systems ( Google Picasa , Face.com and Face + + ) and five academic methods ( Shen et al. , Zhu et al. , DPM , multiHOG and Kalal et al . ) .",0
4723,"In FDDB , we compare with one commercial system ( Olaworks ) and six academic methods ( Yan et al. , Boosted Exemplar et al. , SURF multiview , PEP - Adapt , XZJY and Zhu et al. ) listed on FDDB results page 1 .",0
4724,Evaluation on benchmark face database,0
4725,"As shown in , in AFW , our multi-scale detector achieves an ap value of 96.8 % , outperforming other academic methods by a large margin .",1
4726,"When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa .",1
4727,"Note that most of our false positives on AFW database are faces that have n't been annotated ( small , seriously occluded or artificial faces like mask and cartoon character ) .",0
4728,"When evaluated on FDDB database , we follow the evaluation protocol in and report the average discrete and continuous ROC of the ten subfolders .",0
4729,"For equality , we fix the number of false positives to 284 ( equivalent to an average of 1 False Positive Per Image ) and compare the true positive rate .",0
4730,"In discrete score where evaluation metric is the same as in AFW , our detector achieves 83.7 % , which is a little better than Yan et al ..",1
4731,"Note that the groundtruth in FDDB are elliptical faces , therefore the evaluation metric of an overlap ratio bigger than 0.5 can not reveal the true performance of the proposed detector well .",0
4732,"When using continuous score which takes the overlap ratio as the score , our method gets 61.9 % true positive rate at 1 FPPI for multiscale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan et al . detector outputs the same elliptical detections as the groundtruth , therefore having advantages with this metric ) .",1
4733,Our detector using single - scale features performs a little worse with the benefit of faster detection speed .,1
4734,Discussion,0
4735,Training efficiency :,0
4736,We implement the method with Piotr 's MATLAB toolbox on a PC with Intel Core i7-3770 CPU and 16 GB RAM .,0
4737,"With 21 , 328 positive images and 5 , 771 negative images in total 6 views , the training process takes about 5.3 mins fora single - scale subview detector containing 2048 weak classifiers and 10.2 mins for multi-scale version .",0
4738,Note that we use much fewer training data than SURF multiview whilst still outperforming their performance .,0
4739,Comparative results :,0
4740,"When inspecting detections of the proposed face detector and other algorithms on the testsets , some patterns can be found to explain why our detector outperforms others .",0
4741,One evident strength lies in detecting faces with extreme poses .,0
4742,"Because we adopt multi-view detection and train each subview detector separately , our detector handles pose variations very well .",0
4743,"Second is the outstanding illumination invariance of our detector , which is mainly owing to the extension of channel types to LUV color space and gradient - related channels .",0
4744,Detection speed :,0
4745,"Due to the simple form of aggregate channel features and fast computation of feature pyramid , detection is quite efficient .",0
4746,"For full yaw pose face detection in VGA image , the proposed detector using single - scale features runs at 20 FPS on a single thread and 62 FPS if 4 threads are used .",0
4747,"If only frontal faces are concerned , the detector runs at 34 FPS and 95 FPS after parallelization .",0
4748,"When it comes to the proposed detector using multi-scale features , the above four indices reduce to 15 , 42 , 21 and 55 FPS .",0
4749,"Considering the large performance gain and similar speed , the proposed method can replace Viola - Jones detector for face detection in the wild .",0
4750,Conclusion,0
4751,A novel feature representation called aggregate channel features possesses the merits of fast feature extraction and powerful representation capacity .,0
4752,"In this paper , we successfully apply the feature representation to face detection domain through a deep investigation into the feature design , and propose a multi-scale version of feature which further enriches the representation capacity .",0
4753,"Combined with our efforts into solving issues concerning multi-view detection , the proposed multi-view face detector shows state - of - the - art performance in both effectiveness and efficiency on faces in the wild .",0
4754,The proposed method appeals to real world application demands and has the potential to be embedded into low power devices .,0
4755,title,0
4756,Finding Tiny Faces,1
4757,abstract,0
4758,:,0
4759,"We describe a detector that can find around 800 faces out of the reportedly 1000 present , by making use of novel characterizations of scale , resolution , and context to find small objects .",0
4760,Detector confidence is given by the colorbar on the right : can you confidently identify errors ?,0
4761,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",1
4762,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .",1
4763,"While most recognition approaches aim to be scale - invariant , the cues for recognizing a 3 px tall face are fundamentally different than those for recognizing a 300 px tall face .",0
4764,We take a different approach and train separate detectors for different scales .,0
4765,"To maintain efficiency , detectors are trained in a multi-task fashion : they make use of features extracted from multiple layers of single ( deep ) feature hierarchy .",0
4766,"While training detectors for large objects is straightforward , the crucial challenge remains training detectors for small objects .",0
4767,"We show that context is crucial , and define templates that make use of massively - large receptive fields ( where 99 % of the template extends beyond the object of interest ) .",0
4768,"Finally , we explore the role of scale in pre-trained deep networks , providing ways to extrapolate networks tuned for limited scales to rather extreme ranges .",0
4769,We demonstrate state - of - the - art results on massively - benchmarked face datasets ( FDDB and WIDER FACE ) .,0
4770,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) .",0
4771,Introduction,0
4772,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",0
4773,We explore three aspects of the prob - : Different approaches for capturing scale - invariance .,0
4774,Traditional approaches build a single - scale template that is applied on a finely - discretized image pyramid ( a ) .,0
4775,"To exploit different cues available at different resolutions , one could build different detectors for different object scales ( b ) .",0
4776,Such an approach may fail on extreme object scales that are rarely observed in training ( or pre-training ) data .,0
4777,We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .,0
4778,"Finally , to improve performance on small faces , we model additional context , which is efficiently implemented as a fixed - size receptive field across all scale - specific templates ( d ) .",0
4779,"We define templates over features extracted from multiple layers of a deep model , which is analogous to foveal descriptors ( e ) .",0
4780,"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning .",1
4781,Scaleinvariance is a fundamental property of almost all current recognition and object detection systems .,0
4782,"But from a practical perspective , scale - invariance can not hold for sensors with finite resolution : the cues for recognizing a 300 px tall face are undeniably different that those for recognizing a 3 px tall face .",0
4783,Multi - task modeling of scales :,0
4784,"Much recent work in object detection makes use of scale - normalized classifiers ( e.g. , scanning - window detectors run on an image pyramid or region - classifiers run on "" ROI "" - pooled image features ) .",0
4785,"When resizing regions to a canonical template size , we ask a simple question - what should the size of the template be ?",0
4786,"On one hand , we want a small template that can detect small faces ; on the other hand , we want a large template that can exploit detailed features ( of say , facial parts ) to increase accuracy .",0
4787,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .",1
4788,Training a large collection of scale - specific detectors may suffer from lack of training data for individual scales and inefficiency from running a large number of detectors attest time .,0
4789,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .",1
4790,"While such a strategy results in detectors of high accuracy for large objects , finding small things is still challenging .",0
4791,How to generalize pre-trained networks ?,0
4792,We provide two remaining key insights to the problem of finding small objects .,0
4793,The first is an analysis of how best to extract scale - invariant features from pre-trained deep networks .,0
4794,We demonstrate that existing networks are tuned for objects of a characteristic size ( encountered in pre-training datasets such as ImageNet ) .,0
4795,"To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .",1
4796,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .",1
4797,Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,1
4798,How best to encode context ?,0
4799,Finding small objects is fundamentally challenging because there is little signal on the object to exploit .,0
4800,Hence we argue that one must use image evidence beyond the object extent .,0
4801,"This is often formulated as "" context "" .",0
4802,"In , we present a simple human experiment where users attempt to classify true and false positive faces ( as given by our detector ) .",0
4803,It is dramatically clear that humans need context to accurately classify small faces .,0
4804,"Though this observation is quite intuitive and highly explored in computer vision , it has been notoriously hard to quantifiably demonstrate the benefit of context in recognition .",0
4805,One of the challenges appears to be how to effectively encode large image regions .,0
4806,"We demonstrate that convolutional deep features extracted from multiple layers ( also known as "" hypercolumn "" features ) are effective "" foveal "" descriptors that capture both high - resolution detail and coarse low - resolution cues across large receptive field ( ) .",1
4807,We show that highresolution components of our foveal descriptors ( extracted from lower convolutional layers ) are crucial for such accurate localization in .,1
4808,Our contribution :,0
4809,"We provide an in - depth analysis of image resolution , object scale , and spatial context for the purposes of finding small faces .",0
4810,We demonstrate stateof - the - art results on massively - benchmarked face datasets ( FDDB and WIDER FACE ) .,0
4811,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) . :",0
4812,"On the left , we visualize a large and small face , both with and without context .",0
4813,"One does not need context to recognize the large face , while the small face is dramatically unrecognizable without its context .",0
4814,"We quantify this observation with a simple human experiment on the right , where users classify true and false positive faces of our proposed detector .",0
4815,Adding proportional context ( by enlarging the window by 3X ) provides a small improvement on large faces but is insufficient for small faces .,0
4816,Adding a fixed contextual window of 300 pixels dramatically reduces error on small faces by 20 % .,0
4817,This suggests that context should be modeled in a scale - variant manner .,0
4818,"We operationalize this observation with foveal templates of massively - large receptive fields ( around 300x300 , the size of the yellow boxes ) .",0
4819,Related work,0
4820,Scale - invariance :,0
4821,"The vast majority of recognition pipelines focus on scale - invariant representations , dating back to SIFT .",0
4822,"Current approaches to detection such as Faster RCNN subscribe to this philosophy as well , extracting scale - invariant features through ROI pooling or an image pyramid .",0
4823,"We provide an in - depth exploration of scale - variant templates , which have been previously proposed for pedestrian detection , sometimes in the context of improved speed .",0
4824,SSD is a recent technique based on deep features that makes use of scale - variant templates .,0
4825,Our work differs in our exploration of context for tiny object detection .,0
4826,Context :,0
4827,Context is key to finding small instances as shown in multiple recognition tasks .,0
4828,"In object detection , stacks spatial RNNs ( IRNN ) model context outside the region of interest and shows improvements on small object detection .",0
4829,"In pedestrian detection , uses ground plane estimation as contextual features and improves detection on small instances .",0
4830,"In face detection , simultaneously pool ROI features around faces and bodies for scoring detections , which significantly improve overall performance .",0
4831,Our proposed work makes use of large local context ( as opposed to a global contextual descriptor ) in a scale - variant way ( as opposed to ) .,0
4832,We show that context is mostly useful for finding low - resolution faces .,0
4833,Multi - scale representation :,0
4834,Multi - scale representation has been proven useful for many recognition tasks .,0
4835,"show that deep multi-scale descriptors ( known as "" hypercolumns "" ) are useful for semantic segmentation .",0
4836,demonstrate improvements for such models on object detection .,0
4837,pools multi -scale ROI features .,0
4838,"Our model uses "" hypercolumn "" features , pointing out that fine - scale features are most useful for localizing small objects ( Sec. 3.1 and ) .",0
4839,RPN :,0
4840,"Our model superficially resembles a regionproposal network ( RPN ) trained fora specific object class instead of a general "" objectness "" proposal generator .",0
4841,"The important differences are that we use foveal descriptors ( implemented through multi-scale features ) , we select a range of object sizes and aspects through cross-validation , and our models make use of an image pyramid to find extreme scales .",0
4842,"In particular , our approach for finding small objects make use of scale - specific detectors tuned for interpolated images .",0
4843,"Without these modifications , performance on small - faces dramatically drops by more than 10 % ( Table 1 ) .",0
4844,Exploring context and resolution,0
4845,"In this section , we present an exploratory analysis of the issues at play that will inform our final model .",0
4846,"To frame the discussion , we ask the following simple question : what is the best way to find small faces of a fixed - size ( 25 x 20 ) ?.",0
4847,"By explicitly factoring out scale - variation in terms of the desired output , we can explore the role of context and the canonical template size .",0
4848,"Intuitively , context will be crucial for finding small faces .",0
4849,"Canonical template size may seem like a strange dimension to explore - given that we want to find faces of size 25 x 20 , why define a template of any size other than 25x20 ?",0
4850,Our analysis gives a surprising answer of when and why this should be done .,0
4851,"To better understand the implications of our analysis , along the way we also ask the analogous question fora large object size : what is the best way to find large faces of a fixed - size ( 250x200 ) ?.",0
4852,Setup :,0
4853,"We explore different strategies for building scanning - window detectors for fixed - size ( e.g. , 25 x 20 ) faces .",0
4854,"We treat fixed - size object detection as a binary heatmap prediction problem , where the predicted heatmap at a pixel position ( x , y) specifies the confidence of a fixedsize detection centered at ( x , y ) .",0
4855,We train heatmap predictors using a fully convolutional network ( FCN ) defined over a state - of - the - art architecture ResNet .,0
4856,"We explore multi-scale features extracted from the last layer of each res-block , i.e. ( res2 cx , res3dx , res4 fx , res5 cx ) in terms of ResNet - 50 .",0
4857,"We will henceforth refer to these as ( res2 , res3 , res4 , res5 ) features .",0
4858,We discuss the remaining particulars of our training pipeline in Section 5 .,0
4859,Figure 4 :,0
4860,"Modeling additional context helps , especially for finding small faces .",0
4861,The improvement from adding context to a tight - fitting template is greater for small faces ( 18.9 % ) than for large faces ( 1.5 % ) .,0
4862,"Interestingly smaller receptive fields do better for small faces , because the entire face is visible .",0
4863,"The green box represents the actual face size , while dotted boxes represent receptive fields associated with features from different layers ( cyan = res2 , light - blue = res3 , dark - blue = res4 , black = res5 ) .",0
4864,"Same colors are used in presents an analysis of the effect of context , as given by the size of the receptive field ( RF ) used to make heatmap prediction .",0
4865,"Recall that for fixed - size detection window , we can choose to make predictions using features with arbitrarily smaller or larger receptive fields compared to this window .",0
4866,"Because convolutional features at higher layers tend to have larger receptive fields ( e.g. , res4 features span 291x291 pixels ) , smaller receptive fields necessitate the use of lower layer features .",0
4867,We see a number of general trends .,0
4868,"Adding context almost always helps , though eventually additional context for tiny faces ( beyond 300x300 pixels ) hurts .",0
4869,We verified that this was due to over-fitting ( by examining training and test performance ) .,0
4870,"Interestingly , smaller receptive fields do better for small faces , because the entire face is visible - it is hard to find large faces if one looks for only the tip of the nose .",0
4871,"More importantly , we analyze the impact of context by comparing performance of a "" tight "" RF ( restricted to the object extent ) to the bestscoring "" loose "" RF with additional context .",0
4872,"Accuracy for small faces improves by 18.9 % , while accuracy for large faces improves by 1.5 % , consistent with our human experiments ( that suggest that context is most useful for small instances ) .",0
4873,"Our results suggest that we can build multitask templates for detectors of different sizes with identical receptive fields ( of size 291x291 ) , which is particularly simple to implement as a multi-channel heatmap prediction problem ( where each scale - specific channel and pixel posi - : Foveal descriptor is crucial for accurate detection on small objects .",0
4874,The small template ( top ) performs 7 % worse with only res4 and 33 % worse with only res5 .,0
4875,"On the contrary , removing foveal structure does not hurt the large template ( bottom ) , suggesting high - resolution from lower layers is mostly useful for finding small objects !",0
4876,tion has it s own binary loss ) .,0
4877,"In , we compare between descriptors with and without foveal structure , which shows that high - resolution components of our foveal descriptors are crucial for accurate detection on small instances .",0
4878,Context,0
4879,Resolution,0
4880,We now explore a rather strange question .,0
4881,What if we train a template whose size intentionally differs from the target object to be detected ?,0
4882,"In theory , one can use a "" medium "" - size template ( 50 x 40 ) to find small faces ( 25 x 20 ) on a 2X upsampled ( interpolated ) test image .",0
4883,"actually shows the surprising result that this noticeably boosts performance , from 69 % to 75 % !",0
4884,"We ask the reverse question for large faces : can one find large faces ( 250 x200 ) by running a template tuned for "" medium "" faces ( 125 x 100 ) on test images downsampled by 2X ?",0
4885,"Once again , we see a noticeable increase in performance , from 89 % to 94 % !",0
4886,"One explanation is that we have different amounts of training data for different object sizes , and we expect better performance for those sizes with more training data .",0
4887,"A recurring observation in "" in - the -wild "" datasets such as WIDER FACE and COCO is that smaller objects greatly outnumber larger objects , in part because more small things can be labeled in a fixed - size image .",0
4888,We verify this for WIDER FACE in ( gray curve ) .,0
4889,"While imbalanced data may explain why detecting large faces is easier with medium templates ( because there are more mediumsized faces for training ) , it does not explain the result for small faces .",0
4890,"There exists less training examples of medium faces , yet performance is still much better using a mediumsize template .",0
4891,We find that the culprit lies in the distribution of object scales in the pre-trained dataset ( ImageNet ) .,0
4892,"reveals that 80 % of the training examples in Image Net contain objects of a "" medium "" size , between 40 to 140 px .",0
4893,"Specifically , we hypothesize that the pre-trained Image Net model ( used : The distribution of average object scales in the ImageNet dataset ( assuming images are normalized to 224x224 ) .",0
4894,More than 80 % categories have an average object size between 40 and 140 pixel .,0
4895,We hypothesize that models pre-trained on ImageNet are optimized for objects in that range .,0
4896,"for fine - tuning our scale - specific detectors ) is optimized for objects in that range , and that one should bias canonicalsize template sizes to lie in that range when possible .",0
4897,"We verify this hypothesis in the next section , where we describe a pipeline for building scale - specific detectors with varying canonical resolutions .",0
4898,Approach : scale-specific detection,0
4899,It is natural to ask a follow - up question : is there a general strategy for selecting template resolutions for particular object sizes ?,0
4900,"We demonstrate that one can make use of multi-task learning to "" brute - force "" train several templates at different resolution , and greedily select the ones that do the best .",0
4901,"As it turns out , there appears to be a general strategy consistent with our analysis in the previous section .",0
4902,"First , let us define some notation .",0
4903,"We use t ( h , w , ? ) to represent a template .",0
4904,"Such a template is tuned to detect objects of size ( h /? , w/? ) at resolution ?.",0
4905,"For example , the right - hand - side both t( 250 , 200 , 1 ) ( top ) and t ( 125 , 100 , 0.5 ) ( bottom ) to find 250x200 faces .",0
4906,"Given a training dataset of images and bounding boxes , we can define a set of canonical bounding box shapes that roughly covers the bounding box shape space .",0
4907,"In this paper , we define such canonical shapes by clustering , which is derived based on Jaccard distance d ( Eq. ( 1 ) ) :",0
4908,"where , s i = ( h i , w i ) and s j = ( h j , w j ) area pair of bounding box shapes and J represents the standard Jaccard similarity ( intersection over union overlap ) .",0
4909,"Now for each target object size s i = ( h i , w i ) , we ask :",0
4910,"To answer , we simply train separate multi -task models for each value of ? ? ?",0
4911,( some fixed set ) and take the max :,0
4912,Building templates at original resolution is not optimal .,0
4913,"For finding small ( 25 x 20 ) faces , building templates at 2 x resolution improves overall accuracy by 6.3 % ; while for finding large ( 250 x200 ) faces , building templates at 0.5 x resolution improves overall accuracy by 5.6 % .",0
4914,for each object size .,0
4915,We plot the performance of each resolution - specific multi-task model as a colored curve in .,0
4916,With optimal ?,0
4917,"i for each ( h i , w i ) , we retrain one multi -task model with "" hybrid "" resolutions ( referred to as HR ) , which in practice follows the upper envelope of all the curves .",0
4918,"Interestingly , there exist natural regimes for different strategies : to find large objects ( greater than 140 px in height ) , use 2X smaller canonical resolution .",0
4919,"To find small objects ( less than 40 px in height ) , use 2X larger canonical template resolution .",0
4920,"Otherwise , use the same ( 1X ) resolution .",0
4921,"Our results closely follow the statistics of ImageNet ) , for which most objects fall into this range .",0
4922,Pruning :,0
4923,The hybrid-resolution multitask model in the previous section is somewhat redundant .,0
4924,"For example , template ( 62 , 50 , 2 ) , the optimal template for finding 31x25 faces , is redundant given the existence of template ( 64 , 50 , 1 ) , the optimal template for finding 64x50 faces .",0
4925,Can we prune away such redundancies ?,0
4926,Yes !,0
4927,We refer the reader to the caption in for an intuitive description .,0
4928,"As shows , pruning away redundant templates led to some small improvement .",0
4929,"Essentially , our model can be reduced to a small set of scale - specific templates ( tuned for 40 - 140 px tall faces ) that can be run on a coarse image pyramid ( including 2X interpolation ) , combined with a set of scale - specific templates designed for finding small faces ( less than 20 px in height ) in 2X interpolated images .",0
4930,Architecture,0
4931,We visualize our proposed architecture in .,0
4932,We train binary multi-channel heatmap predictors to report object confidences fora range of face sizes ( 40 - 140 px in height ) .,0
4933,"We then find larger and smaller faces with a coarse image pyramid , which importantly includes a 2X upsampling stage with special - purpose heatmaps that are predicted only for this resolution ( e.g. , designed for tiny faces : Template resolution analysis .",0
4934,"X-axis represents target object sizes , derived by clustering .",0
4935,Left,0
4936,Y-axis shows AP at each target size ( ignoring objects with more than 0.5 Jaccard distance ) .,0
4937,"Natural regimes emerge in the figure : for finding large faces ( more than 140 px in height ) , build templates at 0.5 resolution ; for finding smaller faces ( less than 40 px in height ) , build templates at 2 X resolution .",0
4938,"For sizes in between , build templates at 1 X resolution .",0
4939,"Right Y-axis along with the gray curve shows the number of data within 0.5 Jaccard distance for each object size , suggesting that more small faces are annotated . :",0
4940,Pruning away redundant templates .,0
4941,Suppose we test templates built at 1X resolution ( A ) on a coarse image pyramid ( including 2X interpolation ) .,0
4942,"They will cover a larger range of scale except extremely small sizes , which are best detected using templates built at 2 X , as shown in .",0
4943,"Therefore , our final model can be reduced to two small sets of scale - specific templates : ( A ) tuned for 40 - 140 px tall faces and are run on a coarse image pyramid ( including 2X interpolation ) and ( B ) tuned for faces shorter than 20 px and are only run in 2X interpolated images .",0
4944,shorter than 20 pixels ) .,0
4945,"For the shared CNNs , we experimented with ResNet101 , ResNet50 , and VGG16 .",0
4946,"Though ResNet 101 performs the best , we included performance of all models in .",0
4947,"We see that all models achieve substantial improvement on "" hard "" set over prior art , including CMS - RCNN , which also models context , but in a proportional manner .",0
4948,Please refer to for visualization of ( Full ) and ( A+B ) .,0
4949,Method,0
4950,Easy Medium Hard w / regression 0.919 0.908 0.823 w / o regression 0.911 0.900,0
4951,0.798 : Comparison between testing with and without regression .,0
4952,We show performance on WIDER FACE validation set .,0
4953,Both models use ResNet - 101 architecture .,0
4954,Results suggest that regression helps slightly more on detecting small faces ( 2.4 % ) .,0
4955,Bounding ellipse regression,0
4956,Our bounding ellipse regression is formulated as Eq..,0
4957,t * ra = log ( r * a / ( h / 2 ) ) ( 4 ) t * r b = log ( r * b / ( w / 2 ) ),0
4958,( 5 ) t * ? = cot (? * ),0
4959,"where x * c , y * c , r * a , r * b , ? * represent center x - , y - coordinate , ground truth half axes , and rotation angle of the ground truth ellipse .",0
4960,"x c , y c , h , w represent the center x - , y - coordinate , height , and width of our predicted bounding box .",0
4961,"We learn the bounding ellipse linear regression offline , with the same feature used for training bounding box regression .",0
4962,Other hyper - parameters,0
4963,"We use a fixed learning rate of 10 ? 4 , a weight decay of 0.0005 , and a momentum of 0.9 .",0
4964,"We use a batch size of 20 images , and randomly crop one 500x500 region from the re-scaled version of each image .",0
4965,"In general , we train models for 50 epochs and then select the best - performing epoch on validation set . :",0
4966,Top 20 scoring false positives on validation set .,0
4967,Error type is labeled at the left bottom of each image .,0
4968,""" face ( bg ) "" represents background confusion and "" face ( loc ) "" represents inaccurate localization .",0
4969,""" ov "" represents overlap with ground truth bounding boxes , "" 1 - r "" represents the percentage of detections whose confidence is below the current one 's .",0
4970,"Our detector seems to find faces that were not annotated ( when prediction is on the face while "" ov "" equals to zero ) .",0
4971,Experiments,0
4972,WIDER FACE :,1
4973,We train a model with 25 templates on WIDER FACE 's training set and report the performance of our best model HR - ResNet101 ( A+B ) on the held - out test set .,0
4974,"As shows , our hybrid - resolution model ( HR ) achieves state - of - the - art performance on all difficulty levels , but most importantly , reduces error on the "" hard "" set by 2X .",1
4975,"Note that "" hard "" set includes all faces taller than 10 px , hence more accurately represents performance on the full testset .",0
4976,We visualize our performance under some challenging scenarios in .,0
4977,Please refer to the benchmark website for full evaluation and our Appendix A for more quantitative diagnosis .,0
4978,FDDB :,1
4979,We test our WIDER FACE - trained model on FDDB .,0
4980,"Our out - of - the - box detector ( HR ) outperforms all published results on the discrete score , which uses a standard 50 % intersection - over - union threshold to define correctness .",1
4981,"Because FDDB uses bounding ellipses while WIDER FACE using bounding boxes , we train a post -hoc linear regressor to transform bounding box predictions to ellipses .",0
4982,"With the post - hoc regressor , our detector achieves state - of - the - art performance on the continuous score ( measuring average bounding - box overlap ) as well .",1
4983,Our regressor is trained with 10 - fold cross validation .,1
4984,plots the performance of our detector both with and without the elliptical regressor ( ER ) .,0
4985,Qualitative results are shown in Please refer to our Appendix B fora formulation of our elliptical regressor .,0
4986,Run-time :,0
4987,"Our run - time is dominated by running a "" fully - convolutional "" network across a 2X - upsampled im - age .",0
4988,Our Resnet 101 - based detector runs at 1.4 FPS on 1080 p resolution and 3.1 FPS on 720 p resolution .,0
4989,"Importantly , our run-time is independent of the number of faces in an image .",0
4990,"This is in contrast to proposal - based detectors such as Faster R - CNN , which scale linearly with the number of proposals .",0
4991,Conclusion :,0
4992,"We propose a simple yet effective framework for finding small objects , demonstrating that both large context and scale - variant representations are crucial .",0
4993,We specifically show that massively - large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context ( necessary for detecting small objects ) and high - resolution image features ( helpful for localizing small objects ) .,0
4994,"We also explore the encoding of scale in existing pre-trained deep networks , suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale - variant fashion .",0
4995,"Finally , we use our detailed analysis of scale , resolution , and context to develop a state - of - the - art face detector that significantly outperforms prior work on standard benchmarks . :",0
4996,ROC curves on FDDB - test .,0
4997,Our pre-trained detector ( HR ) produces state - of - the - art discrete detections ( left ) .,0
4998,"By learning a post - hoc regressor that converts bounding boxes to ellipses , our approach ( HR - ER ) produces state - of the - art continuous overlaps as well ( right ) .",0
4999,We compare to only published results . :,0
5000,Qualitative results on WIDER FACE .,0
5001,We visualize one example for each attribute and scale .,0
5002,"Our proposed detector is able to detect faces at a continuous range of scales , while being robust to challenges such as expression , blur , illumination etc .",0
5003,Please zoom in to look for some very small detections .,0
5004,authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon .,0
5005,A.,0
5006,Error analysis,0
5007,Quantitative analysis,0
5008,We plot the distribution of error modes among false positives in and the impact of object characteristics on detection performance in and .,0
5009,Qualitative analysis,0
5010,We show top 20 scoring false positives in .,0
5011,B. Experimental details,0
5012,Multi - scale features,0
5013,"Inspired by the way trains "" FCN - 8s at - once "" , we scale the learning rate of predictor built on top of each layer by a fixed constant .",0
5014,"Specifically , we use a scaling factor of 1 for res4 , 0.1 for res3 , and 0.01 for res2 .",0
5015,"One more difference between our model and is that : instead of predicting at original resolution , our model predicts at the resolution of res3 feature ( downsampled by 8X comparing to input resolution ) .",0
5016,Input sampling,0
5017,"We first randomly re-scale the input image by 0.5 X , 1 X , or 2X .",0
5018,Then we randomly crop a 500x500 image region out of the re-scaled input .,0
5019,We pad with average RGB value ( prior to average subtraction ) when cropping outside image boundary .,0
5020,"Background confusion seems the dominating error mode among top - scoring detection , however , we found 15 out of 20 top - scoring false positives , as shown in , are in fact due to missed annotation .",0
5021,"Border cases Similar to , we ignore gradients coming from heatmap locations whose detection windows cross the image boundary .",0
5022,"The only difference is , we treat padded average pixels ( as described in Input sampling ) as outside image boundary as well .",0
5023,Online hard mining and balanced sampling,0
5024,We apply hard mining on both positive and negative examples .,0
5025,Our implementation is simpler yet still effective comparing to .,0
5026,We set a small threshold ( 0.03 ) on classification loss to filter out easy locations .,0
5027,Then we sample at most 128 locations for both positive and negative ( respectively ) from remaining ones whose losses are above the threshold .,0
5028,We compare training with and without hard mining on validation performance in . :,0
5029,Summary of sensitivity plot .,0
5030,We plot the maximum and minimum of AP N shown in .,0
5031,Our detector is mostly affected by object scale ( from 0.044 to 0.896 ) and blur ( from 0.259 to 0.798 ) .,0
5032,title,0
5033,Accurate Face Detection for High Performance,1
5034,abstract,0
5035,Face detection has witnessed significant progress due to the advances of deep convolutional neural networks ( CNNs ) .,0
5036,Its central issue in recent years is how to improve the detection performance of tiny faces .,0
5037,"To this end , many recent works propose some specific strategies , redesign the architecture and introduce new loss functions for tiny object detection .",0
5038,"In this report , we start from the popular one - stage RetinaNet [ 20 ] approach and apply some recent tricks to obtain a high performance face detector namely AInnoFace .",0
5039,"Specifically , we apply the Intersection over Union ( IoU ) loss function [ 45 ] for regression , employ the two - step classification and regression [ 4 ] for detection , revisit the data augmentation based on data - anchor - sampling [ 32 ] for training , utilize the max - out operation [ 54 ] for classification and use the multi -scale testing strategy [ 54 ] for inference .",0
5040,"As a consequence , the proposed face detection method achieves state - of - the - art performance on the most popular and challenging face detection benchmark WIDER FACE [ 43 ] dataset .",0
5041,Introduction,0
5042,"Face detection is a tremendously important field in computer vision needed for face recognition , sentiment analysis , video surveillance , and many other fields .",0
5043,"Given an arbitrary image , the goal of face detection is to determine whether there are any faces in the image , and if present , return the image location and extent of each face .",0
5044,The recent issue of face detection is how to improve the detection performance in unrestricted scenarios .,0
5045,"Because detecting faces in real - world images has many difficulties including occlusion , significant scale variation , different illumination conditions , various facial poses , rich facial expressions , etc .",0
5046,Many works are devoted to solving this issue and great progress has been achieved with the development of deep convolutional neural networks ( CNNs ) .,0
5047,"For example , the average precision ( AP ) performance on the challenging WIDER FACE dataset has been improved from 40 % to 90 % over recent years .",0
5048,"To improve the performance of face detection in unrestricted scenarios where exists plenty of tiny faces , some works combine traditional methods ( e.g. , cascade - based mechanism and part - based in DPM ) with deep learning methods ( e.g. , CNN ) to perform face detection .",0
5049,A number of works resort to the context information around the face region to find tiny faces based on the Faster R - CNN and SSD detectors .,0
5050,Several works redesign the architecture of modern object detection to better detect tiny faces .,0
5051,A series of works propose some special strategies for tiny faces into the generic object detection methods to improve face detection performance .,0
5052,There are many works present some new data augmentations for tiny faces to improve the performance .,0
5053,Some works introduce the attention mechanism on the feature maps to focus on face regions for better detection performance .,0
5054,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",1
5055,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,1
5056,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,1
5057,"Consequently , we achieve some new state - of - the - art AP results on the challenging face detection benchmark WIDER FACE dataset .",0
5058,Related Work,0
5059,Traditional Method,0
5060,Face detection has been extensively studied from its emergence in the 1990s to the present because of its wide practical applications .,0
5061,"The pioneering work of Viola and Jones uses the Haarlike feature and the AdaBoost strategy to train several cascaded face detectors , achieving a very good tradeoff between accuracy and efficiency in some simple and fixed scenarios .",0
5062,"Afterwards , subsequent works have made great progress by developing more advanced features and more powerful classifiers .",0
5063,"Apart from the boosted cascade methods , several studies introduce another famous framework of Deformable Part Model ( DPM ) to the filed of face detection task , which detect faces by modelling the relationship of deformable facial parts and achieve promising performance in some simple application scenarios .",0
5064,"However , these traditional face detectors are unreliable in complex scenarios because they depend on non-robust hand - crafted features and classifiers .",0
5065,Deep Learning Method,0
5066,Deep learning approaches significantly boost the recent progress in the face detection filed and the CNN - based face detectors have achieved the highest performance in the last few years .,0
5067,"The cascade CNN - based methods train a series of CNN models separately or jointly to perform face detection , and achieve promising accuracy and efficiency simultaneously .",0
5068,"After that , MTCNN and PCN add another extra branch to detect five facial landmarks and predict face angles via the multi-task learning in a coarse - to - fine manner under a cascade - style structure .",0
5069,Faceness obtains different scores according to the spatial structure and arrangement of facial parts to detect faces under severe occlusion and unconstrained pose variations .,0
5070,LDCF + utilizes the boosted decision tree classifier to detect faces .,0
5071,UnitBox introduces an Intersection - over - Union ( IoU ) loss to directly minimize the IoUs of the predictions and the ground - truths for more accurate location .,0
5072,Scale,0
5073,Face detects different scales of faces via applying a specialized set of CNNs with different structures .,0
5074,SAFD develops a scale proposal stage to automatically normalize face sizes prior to detection .,0
5075,Hu et al .,0
5076,explore the contextual information with some separate detectors for different scales to find tiny faces .,0
5077,S 2 AP finds face via paying attention to specific scales in image pyramid and valid locations in each scales layer .,0
5078,Zhu et al. use the Expected Max Overlapping ( EMO ) score to evaluate the quality of anchor setting .,0
5079,Bai et al.,0
5080,generate a clear super- resolution face from a blurry small one via to GAN to detect blurry small faces .,0
5081,"Besides , many state - of - the - are face detectors are evolved from generic object detection methods including the two - stage approach ( Faster R - CNN , R - FCN and FPN ) and the one - stage approach ( SSD , RefineDet and RetinaNet ) .",0
5082,"Based on Faster R - CNN and R - FCN , some face detection methods ( e.g. , Face R - CNN , Face R - FCN , CMS - RCNN and FDNet ) design several specific training and testing strategies with the consideration of the characteristics of face detection .",0
5083,"Similar to FPN , FANet aggregates higher - level features to augment lower - level features for face detection .",0
5084,"Based on SSD , DCFPN and FaceBoxes design a lightweight face detection network to achieve CPU real - time speed with promising result .",0
5085,"In contrast , high performance face detectors including S 3 FD , SFDet , SSH , PyramidBox and DSFD equip SSD with some specific strategies for better detection of small faces , such as architecture diagram , training strategy , contextual reasoning and multiple layers exploiting .",0
5086,"Besides , FAN and DFS use different types of attention mechanism on RetinaNet to handle hard faces .",0
5087,SRN combines the multi-step detection in RefineDet and the focal loss in RetinaNet to perform efficient and accurate face detection .,0
5088,"After that , VIM - FD and ISRN combine many previous techniques on SRN and achieve new state - of - the - art performance ..",0
5089,( a ) Backbone : a feedforward ResNet - 152 architecture extracts the multi-scale feature maps .,0
5090,( b ) Neck : a 6 - level Feature Pyramid Network ( FPN ) structure generates a richer multi-scale convolutional feature pyramid .,0
5091,"After that , two shared subnetworks are attached , one for classifying anchor boxes ( c ) and one for regressing from anchor boxes to ground - truth object boxes ( d ) .",0
5092,"At the end , we use the focal loss for the binary classification and the IoU loss for the regression .",0
5093,Method,0
5094,"Starting from the RetinaNet face detector baseline , we apply some recently proposed strategies to achieve state - of - the - art performance on the challenging WIDER FACE dataset .",0
5095,RetinaNet Baseline,0
5096,RetinaNet is one of the most popular one - stage object detection methods .,0
5097,"One - stage detectors that are applied over a regular , dense sampling of possible object locations have the potential to be faster and simpler , but have trailed the accuracy of two - stage detectors because of extreme class imbalance encountered during training .",0
5098,"To solve this issue , the focal loss is proposed in Retina Net as follow :",0
5099,and,0
5100,where y ?,0
5101,"{ 1 } specifies the ground - truth class , p ?",0
5102,"[ 0 , 1 ] is the model 's estimated probability for the class with label y = 1 , ?",0
5103,t is a balanced factor and ?,0
5104,a tunable focusing parameter .,0
5105,The focal loss is the reshaping of cross entropy loss such that it down - weights the loss assigned to well - classified examples .,0
5106,The novel focal loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training .,0
5107,"In this report , we take the modified RetinaNet shown in as our baseline , which is a single and unified network composed of a backbone network , a neck network and two task - specific subnetworks .",0
5108,The backbone and neck networks are responsible for computing a multi-scale convolutional feature maps over an entire input image .,0
5109,The first subnet performs classification on the backbones output and the second subnet performs convolution bounding box regression .,0
5110,"Specifically , we adopt the ResNet - 152 with 6 - level feature pyramid structure as our backbone network .",0
5111,We follow the way in FPN to generate the 6 - level feature maps ( P2 to P7 ) for detection .,0
5112,Io U Regression Loss,0
5113,The object detection task consists of the classification subtask and the regression subtask .,0
5114,"For regression , the smooth L1 loss is the common loss function used to reduce the difference between anchor boxes and ground - truth bounding boxes , and the Intersection over Union ( IoU ) is the most popular evaluation metric used in the object detection benchmarks .",0
5115,"However , as indicated in , there is a gap between optimizing the commonly used smooth L1 distance losses for regressing the parameters of a bounding box and maximizing this IoU metric value .",0
5116,"The optimal objective fora metric should be the metric itself , so we follow the UnitBox to minimize the difference between the predictions and the ground - truths via directly using IoU as the regression loss .",0
5117,The IoU regression loss function is defined as below :,0
5118,"where B p = ( x 1 , y 1 , x 2 , y 2 ) and B gt = ( x * 1 , y * 1 , x * 2 , y * 2 ) are the predicted bounding box and the ground - truth bounding box respectively , Intersection ( ) and U nion ( ) indicate the intersection and union area between B p and B gt .",0
5119,Selective Refinement Network,0
5120,"As described in Selective Refinement Network ( SRN ) , using RetinaNet to perform face detection still exists two problems : ( a ) low recall efficiency : the precision is not high enough at high recall rates , i.e. , the Precision - Recall curve extends far enough to the right but not steep enough ; ( b ) low location accuracy : the performance drops dramatically as the IoU threshold increases , i.e. , the accuracy of the bounding box location needs to be improved .",0
5121,"To solve the aforementioned two issues , Selective Two - step Classification ( STC ) and Selective Two - step Regression ( STR ) are proposed in SRN and we follow these designs to further improve the performance of our face detector .",0
5122,STC conducts two - step classification on three low level detection layers to filter out most simple negatives and reduce the search space for the subsequent classifier .,0
5123,It s loss function is :,0
5124,STR performs is two - step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor .,0
5125,It s loss function is :,0
5126,"where i is the anchor index , pi / q i and x i /t i are the prediction of classification and regression in the first / second step , l * i /g * i are the ground truth of class / location , N s 1 / N s 2 are the number of positive anchors in the first / second step , ?/?",0
5127,are the collection of classification / regression samples for the first step and ?,0
5128,"is the collection of samples for the second step , L FL is the sigmoid focal loss and [ l * i = 1 ] L r indicates that the IoU regression loss is computed only for positive anchors .",0
5129,Data Augmentation,0
5130,"Date augmentation is of importance for one - stage detectors to construct a robust model to adapt to variations of objects , especially for face detection where has plenty tiny faces .",0
5131,"Following most of the face detectors , we randomly expand and crop the original training images with additional random photometric distortion and flipping to generate the training samples .",0
5132,"Besides , with probability of 0.5 , we replace the above random cropping operation with the anchor - based sampling like data - anchor - sampling in PyramidBox to diversify the scale distribution of training samples .",0
5133,"The anchor - based sampling operation first randomly selects a face of size S face in a batch , then finds its nearest anchor scale S anchor .",0
5134,"After that , it chooses a random scale S random around the nearest anchor scale .",0
5135,"Finally , it resizes the image by S * = S random / S face and randomly crops a standard size of the training size containing the selected face to get the anchor-sampled training data .",0
5136,Max - out Label,0
5137,"To reduce the tiny false positives from background regions , the max - out operation for the background class is introduced in , and then the following work uses this max - out operation on both foreground and background classes .",0
5138,"In the proposed face detection method , the max - out operation is applied in the classification subnet to recall more faces and reduce false positives simultaneously .",0
5139,"To be more specific , the classification subnet first predicts c p +c n scores for each anchor , and then selects max { c p } and max {c n } as the final face and no - face confidence score to compute the classification loss .",0
5140,"Empirically , we set c p = 3 and c n = 3 to train our final model .",0
5141,Multi-scale Testing,0
5142,"Since there are plenty of tiny faces in the challenging WIDER FACE dataset , the multi-scale testing strategy is useful to improve the performance .",0
5143,We use the open source code 1 to conduct the multiscale testing during inference .,0
5144,It inputs the image to the trained model multiple times with different sizes and then merge these detection results with the bounding box voting operation .,0
5145,Experiment,0
5146,Experimental Dataset,0
5147,"We verify the proposed AInnoFace detector on the WIDER FACE dataset , which is a popular face detection benchmark dataset and whose images are selected from the publicly available WIDER dataset .",0
5148,"The WIDER FACE dataset contains 32 , 203 images and 393 , 703 annotated face bounding boxes with a high degree of variability in scale , pose , occlusion , expression , makeup and illumination as depicted in .",0
5149,"All the images are organized based on 61 event classes and are randomly selected from each event class by 40%/ 10 % / 50 % as training , validation and testing subsets .",0
5150,"Based on the detection rate of EdgeBox , the validation and testing subsets are divided into three difficulty levels :",0
5151,"Easy , Medium , Hard .",0
5152,The Average Precision ( AP ) is adopted as the evaluation metric .,0
5153,"Following MALF and Caltech datasets , this dataset does not release bounding box ground truth for the testing images and researchers are required to submit final prediction files to get the AP performance on the testing subset .",0
5154,The proposed method is trained on the training subset and evaluated on both validation and testing subsets .,0
5155,Anchor Detail,0
5156,Following we set two 2S and 2 ?,0
5157,2S anchor scales ( S is the downsampling factor of detection layer ) and one 1.25 aspect ratio .,0
5158,"Thus , there are A = 2 anchors at each location , covering the scale of 8 ? 362 pixels in the 1024 1024 input image .",0
5159,"During the training phase , anchors are assigned to ground - truth boxes using the ?",0
5160,"p IoU threshold and to background if their IoU is in [ 0 , ? n ) .",0
5161,"The rest anchors in [ ? n , ? p ) are ignored .",0
5162,"We set ? n = 0.3 and ? p = 0.7 for the first step , and ? n = 0.4 and ? p =",0
5163,0.5 for the second step .,0
5164,Optimization Detail,0
5165,The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet dataset .,1
5166,"We use the "" xavier "" method to randomly initialize the parameters in the newly added convolutional layers .",1
5167,"The stochastic gradient descent ( SGD ) algorithm is used to fine - tune the model with 0.9 momentum , 0.0001 weight decay and batch size 32 .",1
5168,The warmup strategy is applied to gradually ramp up the learning rate from 0.0003125 to 0.01 at the first 5 epochs .,1
5169,"After that , it switches to the regular learning rate schedule , i.e. , dividing by 10 at 100 and 120 epochs and ending at 130 epochs .",1
5170,The full training and testing codes are built on the PyTorch library .,1
5171,shows the comparison of the proposed AInnoFace detector with twenty - seven state - of - the - art methods on both the validation and testing subsets based on the precision - recall curve and AP .,0
5172,"As shown in , our face detector sets some new state - of - the - art results based on the AP score across the three subsets on both validation and testing subsets , i.e. , 97.0 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation subset , and 96.5 % ( Easy ) , 95.7 % ( Medium ) and 91.2 % ( Hard ) for testing subset .",1
5173,These results outperform all the compared state - of - the - art methods and demonstrate the superiority of our AInnoFace detector .,1
5174,Evaluation Result,0
5175,Conclusion,0
5176,"In this report , we present a high performance face detector by equipping the popular one - stage RetinaNet method with some recent tricks :",0
5177,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ;,0
5178,( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,0
5179,Experiments on the WIDER FACE dataset demonstrate that the proposed AInnoFace detector achieves the state - of - the - art detection performance .,0
5180,title,0
5181,Supervised Transformer Network for Efficient Face Detection,1
5182,abstract,0
5183,Large pose variations remain to be a challenge that confronts real - word face detection .,1
5184,"We propose a new cascaded Convolutional Neural Network , dubbed the name Supervised Transformer Network , to address this challenge .",0
5185,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously predicts candidate face regions along with associated facial landmarks .",0
5186,The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns .,0
5187,"The second stage , which is a RCNN , then verifies if the warped candidate regions are valid faces or not .",0
5188,"We conduct end - to - end learning of the cascaded network , including optimizing the canonical positions of the facial landmarks .",0
5189,This supervised learning of the transformations automatically selects the best scale to differentiate face / non - face patterns .,0
5190,"By combining feature maps from both stages of the network , we achieve state - of - the - art detection accuracies on several public benchmarks .",0
5191,"For real - time performance , we run the cascaded network only on regions of interests produced from a boosting cascade face detector .",0
5192,Our detector runs at 30 FPS on a single CPU core fora VGA - resolution image .,0
5193,Introduction,0
5194,"Among the various factors that confront real - world face detection , large pose variations remain to be a big challenge .",0
5195,"For example , the seminal Viola - Jones detector works well for near - frontal faces , but become much less effective for faces in poses that are far from frontal views , due to the weakness of the Haar features on non-frontal faces .",0
5196,There were abundant works attempted to tackle with large pose variations under the regime of the boosting cascade advocated by Viola and Jones .,0
5197,Most of them adopt a divide - and - conquer strategy to build a multi -view face detector .,0
5198,Some works proposed to train a detector cascade for each view and combine their results of all detectors at the test time .,0
5199,Some other works proposed to first estimate the face pose and then run the cascade of the corresponding face pose to verify the detection .,0
5200,"The complexity of the former approach increases with the number of pose categories , while the accuracy of the latter is prone to the mistakes of pose estimation .",0
5201,Part - based model offers an alternative solution .,0
5202,"These detectors are flexible and robust to both pose variation and partial occlusion , since they can reliably detect the faces based on some confident part detections .",0
5203,"However , these methods always require the target face to be large and clear , which is essential to reliably model the parts .",0
5204,"Other works approach to this issue by using more sophisticated invariant features other than Haar wavelets , e.g. , HOG , SIFT , multiple channel features , and high - level CNN features .",0
5205,"Besides these model - based methods ,",0
5206,"Shen et al. proposed to use an exemplar - based method to detect faces by image retrieval , which achieved state - of - the - art detection accuracy .",0
5207,It has been shown in recent years that a face detector trained end - to - end using DNN can significantly outperforms previous methods .,0
5208,"However , to effectively handle the different variations , especially pose variations , it often requires a DNN with lots of parameters , inducing high computational cost .",0
5209,"To address the conflicting challenge , Li et al. proposed a cascade DNN architecture at multiple resolutions .",0
5210,"It quickly rejects the background regions in the low resolution stages , and carefully evaluates the challenging candidates in the high resolution stage .",0
5211,"However , the set of DNNs in Li et al. are trained sequentially , instead of end - to - end , which may not be desirable .",0
5212,"In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .",1
5213,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .",1
5214,"Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .",1
5215,"Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .",1
5216,"The alignment step warps each candidate face region to a canonical pose , which maps the facial landmarks into a set of canonical positions .",0
5217,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .",1
5218,Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,1
5219,"In other words , those Non-top K regions are suppressed .",0
5220,This helps increase detection recall .,0
5221,"Inspired by previous work , which revealed that joint features from different spatial resolutions or scales will improve accuracy .",0
5222,"We concatenate the feature maps from the two cascaded networks together to form an architecture that is trained end - to - end , as shown in .",1
5223,"Note in the learning process , we treat the set of canonical positions also as parameters , which are learnt in the end - to - end learning process .",0
5224,Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,1
5225,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .",1
5226,We hence call our network a Supervised Transformer Network .,1
5227,"These two characteristics differentiate our model from the Spatial Transformer Network Transformer Network conducts regression on the transformation parameters directly , and b ) it is only supervised by the final recognition objective .",0
5228,The proposed Supervised Transformer Network can efficiently run on the G - PU .,0
5229,"However , in practice , the CPU is still the only choice inmost situations .",0
5230,"Therefore , we propose a region - of - interest ( ROI ) convolution scheme to make the run-time of the Supervised Transformer Network to be more efficient .",1
5231,It first uses a conventional boosting cascade to obtain a set of face candidate areas .,1
5232,"Then , we combine these regions into irregular binary ROI mask .",1
5233,"All DNN operations ( including convolution , ReLU , pooling , and concatenation ) are all processed inside the ROI mask , and hence significantly reduce the computation .",1
5234,Our contributions are :,0
5235,"1 ) we proposed a new cascaded network named Supervised Transformer Network trained end - to - end for efficient face detection ; 2 ) we introduced the supervised transformer layer , which enables to learn the optimal canonical pose to best differentiate face / non - face patterns ; 3 ) we introduced a Non-top K suppression scheme , which can achieve better recall without sacrificing precision ; 4 ) we introduced a ROI convolution scheme .",0
5236,It speeds up our detector 3x on CPU with little recall drop .,0
5237,"Our face detector outperformed the current best performing algorithms on several public benchmarks we evaluated , with real - time performance at 30 frames per second with VGA resolution .",0
5238,2 Network Architecture,0
5239,Overview,0
5240,"In this section , we will introduce the architecture of our proposed cascade network .",0
5241,"As illustrated in , the whole architecture consists of two stages .",0
5242,The first stage is a multi -task Region Proposal Network ( RPN ) .,0
5243,It produces a set of candidate face regions along with associated facial landmarks .,0
5244,We conduct Non-top K suppression to only keep the candidate face regions with responses ranked in the top K in a local neighborhood .,0
5245,"The second stage starts with a Supervised Transformer layer , and then a RCNN to further verify if a face region is a true face or not .",0
5246,"The transformer layer takes the facial landmarks and the candidate face regions , then warp the face regions into a canonical pose by mapping the detected facial landmarks into a set of canonical positions .",0
5247,This explicitly eliminates the effect of rotation and scale variation according to the facial points .,0
5248,"To make this clear , the geometric transformation are uniquely determined by the facial landmarks and the canonical positions .",0
5249,"In our cascade network , both the prediction of the facial landmarks and the canonical positions are learned in the end - to - end training process .",0
5250,"We call it a Supervised Transformer layer , as it receives supervision from two aspects .",0
5251,"On one hand , the learning of the prediction model of the facial landmarks are supervised by the annotated groundtruth facial landmarks .",0
5252,"On the other hand , the learning of both the canonical positions and the prediction model of the facial landmarks both are supervised by the final classification objective .",0
5253,"To make a final decision , we concatenate the fine - grained feature from the second - stage RCNN network and the global feature from the first - stage RPN network .",0
5254,The concatenated features are then put into a fully connected layer to make the final face / non - face arbitration .,0
5255,This concludes the whole architecture of our proposed cascade network .,0
5256,Multi- task RPN,0
5257,"The design of the multi-task RPN is inspired by the JDA detector , which validated that face alignment is helpful to distinguish faces / non - faces .",0
5258,Our method is very straightforward .,0
5259,We use a RPN to simultaneous detect faces and associated facial landmarks .,0
5260,"Our method is very similar to the work , except that our regression target is facial landmark locations , instead of bounding box parameters .",0
5261,The supervised transformer layer,0
5262,"In this section , we describe the detail of the supervised transformer layer .",0
5263,"As we know , similarity transformation was widely used in face detection and face recognition task to eliminate scale and rotation variation .",0
5264,"The common practice is to train a prediction model to detect the facial landmarks , and then warp the face image to a canonical pose by mapping the facial landmarks to a set of manually specified canonical locations .",0
5265,This process at least has two drawbacks :,0
5266,1 ) one needs to manually set the canonical locations .,0
5267,"Since the canonical locations determines the scale and offset of rectified face images , it often takes many try - and - errors to find a relatively good setting .",0
5268,"This is not only time - consuming , but also suboptimal .",0
5269,2 ) The learning of the prediction model for the facial landmark is supervised by the ground - truth facial landmark points .,0
5270,"However , labeling ground - truth facial landmarks is a highly subjective process and hence prone to introducing noise .",0
5271,We propose to learn both the canonical positions and the prediction of the facial landmarks end - to - end from the network with additional supervision information from the classification objective of the RCNN using end - to - end back propagation .,0
5272,"Specifically , we use the following formula to define a similarity transformation , i.e. ,",0
5273,"where x i , y i are the detected facial landmarks , x i ,?",0
5274,"i are the canonical positions , m * is the mean value of the corresponding variables , e.g. , m x = 1 N xi , N is the number of facial landmarks , a and bare parameters of similarity transforms .",0
5275,"We found that this two parameters model is equivalent to the traditional four parameters , but much simpler in derivation and avoid problems of numerical calculation .",0
5276,"After some straightforward mathematical derivation , we can obtain the least squares solution of the parameters , i.e. ,",0
5277,where,0
5278,"After obtaining the similarity transformation parameters , we can obtain the rectified image ?",0
5279,"given the original image I , using ? ( x , ? ) = I (x , y ) .",0
5280,"Each point ( x , ? ) in the rectified image can be mapped back to the original image space ( x , y) by",0
5281,"Since x and y may not be integers , bilinear interpolation is always used to obtain the value of I ( x , y ) .",0
5282,"Therefore , we can calculate the derivative by the chain rule",0
5283,"where L is the final classification loss and ? L ??( x , ? ) is the gradient signals back propagated from the RCNN network .",0
5284,The I x and I y are horizontal and vertical gradient of the original image,0
5285,"Here we use a bilinear interpolation , ? x = x??x? and ? y = y??y?. x l = ?x? , x r = x l + 1 , y t = ?y? , y b = y t + 1 are the left , right , top , bottom integer boundary of point ( x , y ) .",0
5286,"Similarly , we can obtain the derivative of other parameters .",0
5287,"Finally , we can obtain the gradient of the canonical positions of the facial landmarks , i.e. , ?L ?x i and ?L ??",0
5288,i .,0
5289,And the gradient with respect to the detected facial landmarks :,0
5290,?L ?x,0
5291,i and ?L ?y,0
5292,i .,0
5293,Please refer to the supplementary material for more detail .,0
5294,The proposed Supervised Transformer layer is put between of the RPN and RCNN networks .,0
5295,"In the end - to - end training , it automatically adjusts the canonical positions and guiding the detection of the facial landmarks such that the rectified image is more suitable for face / non - face classification .",0
5296,We will further illustrate this in the experiments .,0
5297,Non - top K suppression,0
5298,"In RCNN based object detection , after the region proposals , non-maximum suppression ( NMS ) is always adopted to reduce the region candidate number for efficiency .",0
5299,"However , the candidate with highest confidence score maybe rejected by the later stage RCNN .",0
5300,Decreasing the NMS overlap threshold will bring in lots of useless candidates .,0
5301,This will make subsequent RCNN slow .,0
5302,"Our idea is to keep K candidate regions with highest confidence for each potential face , since these samples are more promising for RCNN classifier .",0
5303,In the experiments part we will demonstrate that we can effectively improve the recall with the proposed Non-top K Suppression .,0
5304,Multi- granularity feature combination,0
5305,Some works have revealed that joint features from different spatial resolutions or scales will improve accuracy .,0
5306,The most straight - forward way maybe combining several RCNN networks with different input scales .,0
5307,"However , this approach will obviously increase the computation complexity significantly .",0
5308,"In our end - to - end network , the details of the RPN network structure is shown in .",0
5309,There are 3 convolution and 2 inception layers in our RPN network .,0
5310,"Therefore , we can calculate that its receptive field size is 85 .",0
5311,While the target face size is 36 ? 72 pixels .,0
5312,"Therefore , our RPN takes advantage of the surrounding contextual information around face regions .",0
5313,"On the other hand , the RCNN network focuses more on the rotation and scale variation fine grained detail in the inner face region .",0
5314,"So we concatenate these two features in an end - to - end training architecture , which makes the two parts more complementary .",0
5315,Experiments demonstrate that this kind of joint feature can significantly improve the face detection accuracy .,0
5316,"Besides , the proposed method is much more efficient .",0
5317,3,0
5318,The ROI convolution,0
5319,Motivation,0
5320,"As a practical face detection algorithm , real - time performance is very important .",0
5321,"However , the heavy computation incurred attest phase using DNN - based models often make them impractical in real - world systems .",0
5322,That is the reason why current DNN - based models heavily rely on a high - end GPU to increase the runtime performance .,0
5323,"However , high - end GPU is not often available in commodity computing system , so most often , we still need to run the DNN model with a CPU .",0
5324,"However , even using a high - end CPU with highly optimized code , it is still about 4 times slower than the runtime speed on a GPU .",0
5325,"More importantly , for portable devices , such as phones and tablets , mostly have low - end CPUs only , it is necessary to accelerate the test - phase performance of DNNs .",0
5326,"Ina typical DNN , the convolutional layers are the most computationally expensive and often take up about more than 90 % of the time in runtime .",0
5327,There were some works attempted to reduce the computational complexity of convolution layer .,0
5328,"For example , Jaderberg et al. applied a sparse decomposition to reconstruct the convolutional filters .",0
5329,"Some other works assume that the convolutional filters are approximately low - rank along certain dimensions , and can be approximately decomposed into a series of smaller filters .",0
5330,Our detector may also benefit from these model compression techniques .,0
5331,"Nevertheless , we propose a more practical approach to accelerate the runtime speed of our proposed Supervised Transformer Network for face detection .",0
5332,Our main idea is to use a conventional cascade based face detector to quickly reject non-face regions and obtain a binary ROI mask .,0
5333,The ROI mask has the same size as the input .,0
5334,The background area is represented by 0 and the face area is represented by 1 .,0
5335,"The DNN convolution is only computed within the region marked as 1 , ignoring all other regions .",0
5336,"Because most regions did not participate in the calculation , we can greatly reduce the amount of computation in the convolution layers .",0
5337,We want to emphasize that our method is different to those RCNN based algorithm which treated each candidate region independently .,0
5338,"In those models , features in the overlap subregions will be calculated repeatedly .",0
5339,"Instead , we use the ROI masks , so that different samples can share the feature in the overlapping area .",0
5340,It effectively reduces the computational cost by further avoiding repeated operations .,0
5341,"Meanwhile , in the following section , we will introduce the implementation details of our ROI convolution .",0
5342,"Similar to Caffe , we also take advantage of the matrix multiplication in the BLAS library to obtain almost a linear speedup .",0
5343,Implementation details,0
5344,Cascade pre-filter .,0
5345,"As shown in , we use a cascade detector as a prefilter .",0
5346,"It is basically a variant of the Volia - Jones 's detector , but it has more weak classifiers and is trained with more data .",0
5347,Our boosted classifier is consisted of 1000 weak classifiers .,0
5348,"Different form , we adopted a boosted fern as the weaker classifier , since a fern is more powerful than using a single Haar feature based decision stump , and more efficient than boosted tree on CPUs .",0
5349,"For completeness , we briefly describe our implementation .",0
5350,Each fern contains 8 binary nodes .,0
5351,"The splitting function is to compare the difference of two image pixel values in two different locations with a threshold , i.e. ,",0
5352,where p is the image patch .,0
5353,The patch size is fixed to 32 in our experiments .,0
5354,"The ( x 1 i , y 1 i , x 2 i , y 2 i , ? i ) are fern parameters learned from training data .",0
5355,Each fern splits the data space into 2 8 = 256 partitions .,0
5356,We use a Real - Boost algorithm for the cascade classification learning .,0
5357,"In each space partition , the classification score is computed as",0
5358,"where the enumerator and denominator are the sum of the weights of positive and negative samples in the space partition , respectively .",0
5359,The ROI mask .,0
5360,"After we obtain some candidate face regions , we will group them according to their sizes .",0
5361,The maximum size is twice larger than the minimum size in each group .,0
5362,"Since the smallest face size can be detected by the proposed DNN based face detector is 36 36 pixels , the first group contains the face size between 36 to 72 pixels .",0
5363,"While the second ground contains the face size between 72 to 144 , and soon ( as shown in ) .",0
5364,"It should be noted that , beginning from the second group , we need to downsample the image , such that the candidate face size in the image is always maintained between 36 to 72 pixels .",0
5365,"Besides , in order to retain some of the background information , we will double the side length of each candidate .",0
5366,But the side length will not exceed the receptive field size ( 85 ) of the following DNN face detector .,0
5367,"Finally , we set the ROI mask according to the sizes and positions of the candidate boxes in each group .",0
5368,We use this grouping strategy for two reasons .,0
5369,"First , when there is a face almost filling the whole image , we do not have to deal with the full original image size .",0
5370,"Instead , it will be down - sampled to a quite small resolution , so we can more effectively reduce the computation cost .",0
5371,"Secondly , since the following DNN detector only need to handle twice the scale variation , this is induces a great advantage when compared with the RPN in , which needs to handle all scale changes .",0
5372,This advantage allows us to use a relatively cheaper network for the DNN - based detection .,0
5373,"Besides , such a sparse pyramid structure will only increase about 33 % ( 1 2 2 + 1 4 2 + 1 8 2 ? 1 3 ) computation cost when compared with the computational cost at the base scale .",0
5374,Details of the ROI convolution .,0
5375,There are several ways to implement the convolutions efficiently .,0
5376,"Currently , the most popular method is to transform the convolutions into a matrix multiplication .",0
5377,"As described in and implemented in Caffe , this can be done by firstly reshaping the filter tensor into a matrix F with dimensions CK 2 N , where C and N are input and output channel numbers , and K is the filter width / height .",0
5378,"We can subsequently gather a data matrix by duplicating the original input data into a matrix D with dimensions W H CK 2 , W and H are output width and height .",0
5379,The computation can then be performed with a single matrix multiplication to form an output matrix O = DF with dimension W H N .,0
5380,This matrix multiplication can be efficiently calculated with optimized linear algebra libraries such as BLAS .,0
5381,"Our main idea in ROI convolution is to only calculate the area marked as 1 ( a.k.a , the ROI regions ) , while skipping other regions .",0
5382,"According to the ROI mask , we only duplicate the input patches whose centers are marked as 1 .",0
5383,So,0
5384,"Finally , we put each row of O ? to the corresponding channel of the output .",0
5385,The computation complexity of ROI convolution is M CK 2 N .,0
5386,"Therefore , we can linearly decrease the computation cost according to the mask sparsity .",0
5387,"As illustrated in , we only apply the ROI convolution in the test phase .",0
5388,We replace all convolution layers into ROI convolution layers .,0
5389,"After a max pooling , the size of the input will be halved .",0
5390,"So we also half sample the ROI mask , such that their size can be matched .",0
5391,The original DNN detector can run at 50 FPS on GPU and 10 FPS on CPU fora VGA image .,0
5392,"With ROI convolution , it can speedup to 30 FPS on CPU with little accuracy loss .",0
5393,Experiments,0
5394,"In this section , we will experimentally validate the proposed method .",0
5395,We collected about 400K face images from the web with various variations as positive training samples .,0
5396,"These images are exclusive from FDDB , AFW and PASCAL datasets .",0
5397,"We labeled all faces with 5 facial points ( two eyes center , nose tip , and two mouth corners ) .",0
5398,"For the negative training samples , we use the Coco database .",0
5399,"This dataset has pixel level annotations of various objects , including people .",0
5400,"Therefore , we covered all person areas with random color blocks , and ensure that no samples are drawn from those colored regions in these images .",0
5401,We use more than 120K images ( including 2014 training and validation data ) for the training .,0
5402,Some sample images are shown in .,0
5403,We use GoogleNet in both the RPN and RCNN networks .,0
5404,"The network structure is similar to that in FaceNet , but we cut all the convolution kernel number in half for efficiency .",0
5405,"Moreover , we only include two inception layers in RPN network ( as shown in ) and the input size of RCNN network is 64 ..",0
5406,Illustration of our negative training sample .,0
5407,We covered all person area with random color blocks in Coco dataset and ensured that no positive training samples are drawn from these regions in these images .,0
5408,"In order to avoid the initialization problem and improve the convergence speed , we first train the RPN network from random without the RCNN network .",0
5409,"After the predicted facial landmarks are largely correct , we add the RCNN network and perform end - to - end training together .",0
5410,"For evaluation , we use three challenging public datasets , i.e. , FDDB , AFW and PASCAL faces .",0
5411,All these three datasets are widely used as face detection benchmark .,0
5412,We employ the Intersection over Union ( IoU ) as the evaluation metric and fix the IoU threshold to 0.5 .,0
5413,Learning canonical position,0
5414,"In this part , we verify the effect of the Supervised Transformation in finding the best canonical position .",0
5415,"We intentionally initialize the Supervised Transformation with three inappropriate canonical positions according to three settings , respectively , i.e. , too large , too small , or with offset .",0
5416,"Then we perform the endto - end training and record the canonical points position after 10K , 100K , 500K iterations .",0
5417,"As shown in , each row shows the canonical positions movement for one kind of initializations .",0
5418,We also place the image warp result besides its corresponding canonical points .,0
5419,"We can observe that , for these three different kinds of initializations , they all eventually converge to a very close position setting after 500 K iterations .",0
5420,It demonstrated that the proposed Supervised Transformer module is robust to the initialization .,0
5421,It automatically adjusts the canonical positions such that the rectified image is more suitable for face / non - face classification .,0
5422,Ablative evaluation of various network components,0
5423,"As discussed in Sec. 2 , our end - to - end cascade network is consisted of four notable parts , i.e. , the multi-task RPN , the Supervised Transformer , the multigranularity feature combination , and non -top K suppression .",0
5424,"The former three will affect the network structure of training , while the last one only appear in the test phase .",0
5425,"In order to separately study the effect of each part , we conduct an ablative study by removing one or more parts from our network structure and evaluate the new network with the same training and testing data .",0
5426,"When removing the multi-task RPN , it means that we directly regress the face rectangle similar to , instead of facial points .",0
5427,"Without the Supervised Transformer layer , we simply replace it with a standard similarity transformation without training with back propagation .",0
5428,Without the feature combination component means that we directly use the output of the RCNN features to make the finial decision .,0
5429,"In the case that we removed multi-task RPN , there will be no facial points for Supervised Transformation or conventional similarity transformation .",0
5430,"In this situation , we directly resize the face patch into 64 64 and fed it into a RCNN network .",0
5431,There are 6 different ablative settings in total .,0
5432,"We perform end - to - end training with the same training samples for all settings , and evaluate the recall rate on the FDDB dataset when the false alarm number is 10 .",0
5433,We manually review the face detection results and add 67 unlabeled faces in the FDDB dataset to make sure all the false alarms are true .,0
5434,"As shown in , multi-task RPN , Supervised Transformer , and feature combination will bring about 1 % , 1 % , and 2 % recall improvement respectively .",1
5435,"Besides , these three parts are complementary , remove anyone part will cause a recall drop .",1
5436,"In the training phase , in order to increase the variation of training samples , we randomly select K positive / negative samples from each image for the RCNN network .",0
5437,"However , in the test phase , we need to balance the recall rate with efficiency .",0
5438,"Next , we will compare the proposed non -top K suppression with NMS in the testing phase ,",0
5439,"We present a sample visual result of RPN , NMS and non-top K suppression in .",0
5440,We keep the same number of candidates for both NMS and Non-top K suppression ( K = 3 in the visual result ) .,0
5441,We found that NMS tend to include too much noisy low confidence candidates .,1
5442,"We also compare the PR curves of using all candidates , NMS , and non-top K suppression .",0
5443,"Our non - top K suppression is very close to using all candidates , and achieved consistently better results than NMS under the same number of candidates .",1
5444,The effect of ROI convolution,0
5445,"In this section , we will validate the acceleration performance of the proposed ROI convolution algorithm .",0
5446,We train the Cascade pre-filter with the same training data .,0
5447,"By adjusting the classification threshold of the Cascade re-filter , we can obtain the ROI masks in different areas .",0
5448,"Therefore , we can strike for the right balance between speed and accuracy .",0
5449,We conduct the experiments on the FDDB database .,0
5450,"We resized all images to 1.5 times of the original size , the resulting average photos resolution is approximately 640 480 .",0
5451,"We evaluate the ROI mask sparsity , run - time speed 1 of each part , and the recall rate when the false alarm number is 10 under different pre-filter threshold .",0
5452,We also compare with the standard network without ROI convolution .,0
5453,Non- top K ( K = 3 ) suppression is adopted in all settings to make RCNN network more efficiency .,0
5454,"shows the average ROI mask sparsity , testing speed of each part , and recall rate of each setting .",0
5455,"Comparing the second row with the fourth row , it proves that we can linearly decrease the computation cost according to the mask sparsity .",0
5456,The last two rows show the recall rate and average test time of different settings .,0
5457,The original DNN detector can run at 10 FPS on CPU fora VGA image .,0
5458,"With ROI convolution , it can speedup to 30 FPS on CPU .",0
5459,We can achieve about 3 times speedup with only 0.6 % recall rate drop ..,0
5460,"Comparison with state - of - the - arts on the FDDB , AFW and PASCAL faces datasets .",0
5461,"BC D. Qualitative face detection results on ( a ) FDDB , ( b ) AFW , ( c ) PASCAL faces datasets .",0
5462,Comparing with state - of - the - art,0
5463,We conduct face detection experiments on three benchmark datasets .,0
5464,"On the FDDB dataset , we compare with all public methods .",1
5465,We regress the annotation ellipses with 5 facial points and ignore 67 unlabeled faces to make sure all false alarms are true .,0
5466,"On the AFW and PASCAL faces datasets , we compare with ( 1 ) deformable part based methods , e.g. structure model and Tree Parts Model ( TSM ) ; ( 2 ) cascade - based methods , e.g .",1
5467,"Headhunter ; ( 3 ) commercial system , e.g. face.com , Face ++ and Picasa .",1
5468,"We learn a global regression from 5 facial points to face rectangles to match the annotation for each dataset , and use toolbox from for the evaluation .",0
5469,shows that our method outperforms all previous methods by a considerable margin .,0
5470,Conclusion and future work,0
5471,"In this paper , we proposed a new Supervised Transformer Network for face detection .",0
5472,The superior performance on three challenge datasets shows its ability to learn the optimal canonical positions to best distinguish face / non - face patterns .,0
5473,"We also introduced a ROI convolution , which speeds up our detector 3x on CPU with little recall drop .",0
5474,Our future work will explore how to enhance the ROI convolution so that it does not incur additional drops in recall .,0
5475,title,0
5476,Face Detection Using Improved Faster RCNN,1
5477,abstract,0
5478,Faster RCNN has achieved great success for generic object detection including PASCAL object detection and MS COCO object detection .,0
5479,"In this report , we propose a detailed designed Faster RCNN method named FDNet1.0 for face detection .",0
5480,"Several techniques were employed including multi-scale training , multi-scale testing , light - designed RCNN , some tricks for inference and a vote - based ensemble method .",0
5481,"Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .",0
5482,narrative,0
5483,"object detectors including one stage methods ( e.g. , YOLO , SSD ) and two stage methods ( e.g. , Faster RCNN , RFCN ) .",0
5484,One stage methods refer broadly to architectures that use a single feed - forward full convolutional neural network to directly predict each proposal 's class and corresponding bounding box without requiring a second stage per-proposal classification operation and box refinement .,0
5485,"Two stage methods , especially Faster RCNN achieves better performance than one stage methods over many object detection benchmarks .",0
5486,"In the Faster R - CNN setting , object detection happens over two pipes .",0
5487,"In the first pipe , input image is directly processed by a feature extractor ( e.g. , Vgg16 , Inception , ResNet101 ) without any hand engineering , and features at the selected intermediate layer ( e.g. , "" conv 5_3 "" , "" re s 4 f "" ) will be fed to a convolutional layer , which simultaneously predict objectiveness scores and region bounds at each location on a regular grid according to predefined stride .",0
5488,The first pipe is also called region proposal network ( RPN ) .,0
5489,"In the second pipe , these proposals with higher scores in the RPN are used to crop features from the same intermediate feature map which are subsequently fed to the remainder of the feature extractor ( e.g. , two full connected layer , 5th block ) in order to predict a class and class - specific box refinement for each proposal .",0
5490,Face detection has achieved great success thanks to the appearance of one stage method and two stage methods .,0
5491,"However , there are still some issues with these methods that can be improved with elaborate design of the details .",0
5492,"In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .",1
5493,"A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .",1
5494,"At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .",1
5495,It is also beneficial for hard set to keep the small proposals ( < 16 pixels width / height ) at training and testing stage as there are many tinny faces of WIDER FACE dataset .,0
5496,"Furthermore , the multi-scale training and testing strategy are also applied in our work .",1
5497,Our key contributions are summarized as follows : ( 1 ) A light head based two - stage framework named FDNet1.0 is developed for face detection .,0
5498,"( 2 ) Some useful tricks are found to improve final face detection performance including multi-scale training , multi-scale testing , keep the small proposals at training and testing stage , directly select top - ranked proposals ( e.g. , 6000 ) without NMS in the RPN stage for R - CNN , a vote - based NMS ensemble strategy .",0
5499,"( 3 ) Our framework achieves two 1st places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy , medium , hard ) , one illustrative example of our results in the crowd case can be found in .",0
5500,"Face detection is one of the most fundamental and challenging problems in computer vision , and has been extensively studied for decades .",0
5501,"Compared against these hand - engineered features , a lot of progress for face detection has been made in recent years due to utilizing of modern object detectors , including Faster R - CNN , R - FCN , SSD , YOLO and their extensions .",0
5502,Hand - engineered approaches :,0
5503,A cascaded AdaBoost face detector is proposed to detect face by using Haar - like features .,0
5504,"Based on this groundbreaking work , more advanced hand - engineered features and more powerful machine learning algorithms are developed to improve face detection performance .",0
5505,"Additionally , deformable part models ( DPM ) is also employed for face detection by several research groups , which achieve remarkable performance .",0
5506,Single - stage approaches :,0
5507,CascadeCNN proposes a strategy to detect face coarse to fine .,0
5508,A mutli-task learning method named MTCNN is present to predict face and landmark location simultaneously .,0
5509,Dense - Box employs a fully deep convolutional neural network to directly predict face confidence and corresponding bounding box .,0
5510,"UnitBox introduces a novel intersection - over - union ( IoU ) loss to predict bounding box , which regresses the four bounds of a predicted box as a whole unit .",0
5511,SAFD and RSA unit devote to handle scale explicitly using CNN or RNN .,0
5512,S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .,0
5513,"Very recently , FAN presents an effective face detector based on feature pyramid network , which obtains state - of - the - art results .",0
5514,Two - stage approaches :,0
5515,Face R - CNN employs anew multi-task loss function based on Faster R - CNN framework to enhance performance .,0
5516,CMS - RCNN is proposed to enhance face detect performance by exploiting contextual information .,0
5517,Convnet introduces an end - to - end multi-task discriminative learning framework to increase occlusion robustness .,0
5518,"Based on R - FCN , Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .",0
5519,Proposed Approach,0
5520,"Faster RCNN , with two fully connected layers or all the convolution layers in ResNet 5 - th stage to predict RoI classification and regression , consumes a large memory and computing resource .",0
5521,"RFCN is fully convolutional with almost all computation shared on the entire image , but it has poor performance compared to Faster RCNN .",0
5522,"Inspired by , we develop a light - head Faster RCNN for face detection with good performance and inference speed .",0
5523,"In this section , we will present our method in detail .",0
5524,Light - Head Faster RCNN,0
5525,"Based on Faster RCNN , we make several effective modifications for improving detection performance .",0
5526,The architecture of our framework is depicted in .,0
5527,"Res Net architecture plays the role of feature extractor , the "" thin "" feature maps is built by a deformable layer before Region - of - Interest ( RoI ) warping , which will exploit image context and be robust to variations .",0
5528,And a single fully - connected layer is used in the R - CNN subnet .,0
5529,We use ResNet - v1-101 as backbone network to extract high - level feature .,0
5530,"The stride of ResNet - v1-101 is fixed to 16 pixels , which is not good enough for detecting different scale faces .",0
5531,"Therefore a large kernel - based deformable layer is attached on backbone network to exploit image context and be robust to variations , where "" D "" stand for deformable .",0
5532,"The output channel size of the deformable layer is 512 , and each ROI will be resized to 51277 , which will be fed to the following fully connected layer with 2048 channels .",0
5533,Figure 2 .,0
5534,Overview of our approach .,0
5535,"A deformable layer with 512 channels was employed to build "" thin "" feature maps with exploiting image context simultaneously before RoI warping .",0
5536,"Hence , RCNN can be designed with light - head to improve inference speed .",0
5537,"Additionally , anchors are carefully designed to obtain better location samples .",0
5538,"The aspect ratio is set to 1 , 1.5 , 2 and the scale is set to 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 based on statistical analysis on the training dataset .",0
5539,These smaller anchors are very helpful for sufficiently capturing tiny face .,0
5540,"As WIDER FACE dataset contain many extremely tinny faces ( < 16 pixels width / height ) , we keep these small proposals ( < 16 pixels width / height ) valid in the training and testing time .",0
5541,The experiments show that our method can achieve better performance .,0
5542,Multi - Scale Training and Testing,0
5543,The trained model can also be robust on different scale faces when both multi-scale training and testing strategy are used .,0
5544,"In our method , the shorter side is resized to 600,800,1000,1200,1400 pixels according to the statistical analysis on the training dataset .",0
5545,"Unlike , we only use horizontal image flipping augmentation , and no other hard example mining method is used in the training stage .",0
5546,"In the testing phase , the shorter side of each image is also resized to 600,800,1000,1200,1400 pixels and tested independently .",0
5547,Then all of the output results are merged .,0
5548,"Next , a voted - based NMS strategy is adapted .",0
5549,"We firstly delete the output bounding box whose IOU is lower than 0.3 with any other bounding boxes to suppress false positive samples , and then NMS is used to get the best bounding boxes .",0
5550,Experiments,0
5551,"We perform evaluation on WIDER FACE dataset which contains more challenges , including small scale , illumination , occlusion , background clutter and extreme poses when compared with other benchmarks .",0
5552,"A total of 393,703 labeled faces in 32,203 images from 61 different scenes are collected , of which 40 % are chosen as train set , 10 % as validation set and other 50 % as test set .",0
5553,"The validation set and the testing set are also divided into easy set , medium set and hard set according to the detection difficulties .",0
5554,It is noticed that our method is trained only on the train set and evaluate on both validation set and test set .,0
5555,Better performance might be achieved by merging the train set and the validation set for training .,0
5556,More detailed results of WIDER FACE are shown in .,0
5557,Implementation Details,0
5558,Single NVIDIA Tesla K80 is used for training and testing .,1
5559,Mini batch size is set to 1 considering memory consumption .,1
5560,"Specifically , ResNet_v1_101 trained on ImageNet - 128w is used for Faster RCNN feature extraction .",1
5561,It is helpful to freeze the first two blocks in the training stage as data size of WIDER FACE is not so large .,0
5562,"A deformable layer is used to output a "" thin "" feature map with exploiting image context .",0
5563,"Aspect ratios ( 1 , 1.5 , 2 ) and scales ( 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 ) are carefully designed to capture better locations of faces in the RPN stage , and the number of filters for the RPN layer is set as 512 .",1
5564,The anchors with highest IoU score or IoU score with the ground truth above 0.7 are defined as positive .,0
5565,"The anchors whose IoU score with the ground truth that is lower than 0.3 are defined as negative , while whose IoU score above 0.3 but lower than 0.7 will be ignored .",0
5566,The similar settings of anchors are used in the R - CNN stage .,0
5567,"The anchors with IoU score with the ground truth above 0.5 are assigned as positive , IoU score that is lower than 0.3 is defined as negative , IoU score above 0.3 but lower than 0.5 will be ignored .",0
5568,"By the way , the batch size of RPN and R - CNN is respectively assigned as 256 and 128 .",1
5569,"The initial learning rate is set to 1e - 3 , and decrease to 1e - 4 after 20w iterations .",1
5570,Weight decay is and momentum is set to 1e - 4 and 0.9 respectively .,1
5571,"In testing stage , multi-scale testing strategy is adapted to be robust to different scale faces .",0
5572,"Specifically , the shorter side of each image is also resized to 600 , 800 , 1000 , 1200 , 1400 pixels and tested independently .",0
5573,And a voted - based NMS strategy is used to get the final result .,0
5574,"We also find top - ranked 6000 proposals are directly selected without NMS during testing can boost 0.1 % , 0.3 % and 0.6 % on easy set , medium set and hard set respectively .",0
5575,Comparison on Benchmarks,0
5576,Our model is trained on the train set and evaluated on WIDER FACE validation set .,0
5577,"Compared with the recently published top approaches , FDNet1.0 wins two 1st places ( easy set = 95.9 % , medium set = 94.5 % ) and one 2nd place ( hard set = 87.9 % ) on the validation set , as illustrated in .",1
5578,We believe that more kinds of data augmentation and hard example mining would further boost detection performance .,0
5579,Conclusion,0
5580,"In this paper , we propose a novel framework named FDNet1.0 for face detection .",0
5581,FDNet 1.0 improves Faster RCNN by integrating several efficient techniques for better performance .,0
5582,Experimental results on challenging WIDER FACE dataset validate the effectiveness of our proposed algorithm .,0
5583,"In the future , we will try more kinds of data augmentation and hard example mining which may further boost detection performance .",0
5584,"We will also consider some ideas for faster inference speed , e.g. designing light backbone .",0
5585,title,0
5586,Detecting Faces Using Region - based Fully Convolutional Networks,1
5587,abstract,0
5588,Face detection has achieved great success using the region - based methods .,1
5589,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .",0
5590,"Based on Region - based Fully Convolutional Networks ( R - FCN ) , our face detector is more accurate and computationally efficient compared with the previous R - CNN based face detectors .",0
5591,"In our approach , we adopt the fully convolutional Residual Network ( ResNet ) as the backbone network .",0
5592,"Particularly , we exploit several new techniques including position - sensitive average pooling , multi-scale training and testing and on - line hard example mining strategy to improve the detection accuracy .",0
5593,"Over two most popular and challenging face detection benchmarks , FDDB and WIDER FACE , Face R - FCN achieves superior performance over state - of - the - arts .",0
5594,Introduction,0
5595,Face detection plays an important role in the modern face - relevant applications .,0
5596,"Despite the great progress made in recent years , the technical challenging of face detection still exists out of the complex variations of real - world face images .",0
5597,"As shown in , the visual faces vary a lot as the result of the affecting factors including occlusion on the facial part , different scales , illumination conditions , various poses of person , rich expressions , etc .",0
5598,"Recently , remarkable advances of objection detection have been driven by the success of region - based methods .",0
5599,"Among recent novel algorithms , Fast / Faster R - CNN are representative R - CNN based methods that perform region - wise detections on the regions of interest ( Ro Is ) .",0
5600,"However , directly applying the strategy of region - specific operation to fully convolutional networks , such as Residual Nets ( ResNets ) , results in inferior detection performance owing to the overwhelming classification accuracy .",0
5601,"In contrast , R - FCN is proposed to address the problem in the fully convolutional manner .",0
5602,"The ConvNet of R - FCN is built with the computations shared on the entire image , which leads to the improvement of training and testing efficiency .",0
5603,"Comparing with R - CNN based methods , R - FCN proposes much fewer region - wise layers to balance the learning of classification and detection for naturally combining fully convolutional network with region - based module .",0
5604,"As a specific area of generic object detection , face detection has achieved superior performance thanks to the appearance of region - based methods .",0
5605,Previous works primarily focus on the R - CNN based methods and achieve promising results .,0
5606,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .",1
5607,"According to the size of the general face , we carefully design size of anchors and RoIs .",1
5608,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",1
5609,"Furthermore , we also apply the multi-scale training and testing strategy in this work .",1
5610,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,1
5611,Our key contributions are summarized below :,0
5612,( 1 ) We develop a face detection framework that takes the special properties of face into account by integrating several innovative and effective techniques .,0
5613,"The proposed approach is based on R - FCN and is well suited for face detection , thus we call it Face R - FCN .",0
5614,( 2 ) We introduce a novel position - sensitive average pooling to re-weight embedding responses on score maps and eliminate the effect of non-uniformed contribution in each facial part .,0
5615,"( 3 ) By far , the proposed algorithm is benchmarked on WIDER FACE dataset and FDDB dataset .",0
5616,Our Face R - FCN has reached the first - rate performance over the state - of - the - arts on both datasets .,0
5617,Related Work,0
5618,"In the past decades , face detection has been extensively studied .",0
5619,The pioneering work of Viola and Jones invents a cascaded AdaBoost face detector using Haar - like features .,0
5620,"After that , numerous of works have focused on developing more advanced features and more powerful classifiers .",0
5621,"Besides the boosted cascade methods , several studies apply deformable part models ( DPM ) for face detection .",0
5622,The DPM methods detect faces by modeling the relationship of deformable facial parts .,0
5623,Recent progress in face detection mainly benefits from the powerful deep learning approaches .,0
5624,The CNN - based detectors have achieved the highest performance .,0
5625,construct cascaded CNNs to learn face detectors with a coarse - to - fine strategy .,0
5626,MTCNN develops a multi- task training framework to jointly learn the face detection and alignment .,0
5627,"UnitBox propose the intersectionover - union ( IoU ) loss function , to directly minimize the IoUs of the predictions and the groundtruths .",0
5628,"Recently , several methods use the Faster R - CNN framework to improve the face detection performance .",0
5629,"explores the contextual information for face detection and proposes a framework achieving high performance , especially improving the accuracy of tiny faces .",0
5630,"Most recently , propose to use single stage framework for face detection , with carefully designed strategies and achieve the state - of - the - art performance .",0
5631,"Similar to face detection , general object detection is advancing rapidly thanks to the deep learning approaches .",0
5632,"Typical work including R - CNN , Fast R - CNN , Faster R - CNN and their extensions .",0
5633,"Among these studies , R - FCN makes the detection in a nearly fully convolutional manner , which greatly enhances the efficiency of training and testing .",0
5634,The methods of hard example mining further help deep learning based object detection to improve the performance .,0
5635,"In , the authors proposed an on - line hard example mining ( OHEM ) algorithm to improve the object detection performance .",0
5636,also use hard example mining algorithms to boost the performance of face detection .,0
5637,Proposed Approach,0
5638,"In this section , the proposed Face R - FCN ( See is described in detail .",0
5639,"Since our framework is based on the R - FCN , we refer the reader to for more technical details .",0
5640,We improve the R - FCN framework for targeting face detection in three aspects .,0
5641,"First , we introduce additional smaller anchors and modify the position sensitive RoI pooling to a smaller size for suiting the detection of the tiny faces .",0
5642,"Second , we propose to use position - sensitive average pooling instead of normal average pooling for the last feature voting in R - FCN , which leads to an improved embedding .",0
5643,"Third , multi-scale training strategy and on - line Hard Example Mining ( OHEM ) strategy are adopted for training .",0
5644,"In the testing phase , we also ensemble the multi-scale detection results to improve the performance .",0
5645,The details of the proposed approach are described as follows .,0
5646,R - FCN,0
5647,Based Architecture,0
5648,R- FCN,0
5649,[ 6 ] is a region - based fully convolutional network initially proposed for object detection .,0
5650,"Unlike other region - based detectors such as Faster RCNN , R - FCN constructs a deeper fully convolutional network without increasing the speed overhead by shared computation on the entire image .",0
5651,"R - FCN builds upon 101 - layer ResNet , consists of a region proposal network ( RPN ) and a R - FCN module in contrast to R - CNN module that presented in Faster R - CNN .",0
5652,ResNet architecture in R - FCN plays the role of feature extractor .,0
5653,It is common knowledge that ResNet construct a very deep network which is able to extract highly representative image features .,0
5654,These features hold much larger receptive field where tiny face detection can be benefited from the context information .,0
5655,"From the feature maps that output by the fundamental ResNet , RPN generates a batch of the region of interests ( RoIs ) according to the anchors .",0
5656,These RoIs further are fed into two sibling position sensitive RoI pooling layer in R - FCN module to produce class score maps and bounding box prediction maps .,0
5657,"In the end of R - FCN , two global average pooling are applied on both class score maps and bounding box prediction maps respectively for aggregating the class scores and bounding box predictions .",0
5658,There are two major advantages that we adopt R - FCN over R - CNN .,0
5659,"Firstly , the position sensitive RoI pooling ingeniously encodes position information into each RoI by pooling group of feature maps to a certain location of the output score maps ; Secondly , without unnaturally injecting fully connected layers into ResNet architecture , the feature maps of R - FCN are trained more expressive and easier for the network to learn the class score and bounding box of faces .",0
5660,"Based on R - FCN ,",0
5661,We make several effective modifications for improving detection performance .,0
5662,"For better describing tiny faces , we introduce more anchors with smaller scales ( say , from 1 to 64 ) .",0
5663,These smaller anchors are very helpful for sufficiently capturing the extremely tiny faces .,0
5664,"Besides , we set smaller pooling size for position sensitive RoI pooling to reduce redundant information , and refine the following voting scheme ( average pooling ) to be position sensitive average pooling , which will be described in the following section .",0
5665,"Finally , we apply atrous convolution in the last stage of ResNet to keep the scale of feature maps without losing the contextual information in larger receptive field .",0
5666,Position - Sensitive Average Pooling,0
5667,"In the original R - FCN work , global average pooling is adopted to aggregate the features after position - sensitive RoI pooling into a single dimension .",0
5668,This operation leads to the uniform contribution of each position of the face .,0
5669,"However , the contribution of each part of the face maybe non-uniformed for detection .",0
5670,"For example , in terms of face recognition , eyes usually are paid more attentions than mouth which has been verified by experiments in .",0
5671,"Intuitively ,",0
5672,We believe such assumption that distinct regions on the face have different importance should also hold in face detection .,0
5673,"Hence , we propose to perform weighted average for each area of the output of position sensitive RoI pooling in order to re-weight the region , which is called position - sensitive average pooling .",0
5674,"Formally , let X = { X i | i = 1 , 2 , ... , M } denote the output M feature maps of a position - sensitive RoI pooling layer , and X i = {x i , j |j = 1 , 2 , ... , N 2 } denote the i th feature map , where N denotes the size of the pooled feature map .",0
5675,"Position - sensitive average pooling calculates the weighted average value of the feature responses to get the pooling feature Y = {y i | i = 1 , 2 , ... , M } from X , where y i is denoted as :",0
5676,where w j denotes the weight for the j - th position .,0
5677,Note that position - sensitive average pooling can bethought as performing feature embedding on every location of responses followed by average pooling .,0
5678,"Hence , it is very convenient to implement position - sensitive average pooling on most of the popular deep neural network frameworks .",0
5679,Multi - Scale Training and Testing,0
5680,"Inspired by , we perform multi-scale training and testing strategy to improve performance .",0
5681,"In the training phase , we resize the shortest side of the input to 1024 or 1200 pixels .",0
5682,"This training strategy keeps our model being robust on detecting the target at the different scale , especially on tiny faces .",0
5683,On - line Hard Example Mining ( OHEM ) is a simple yet effective technique for bootstrapping .,0
5684,"During training , we also apply OHEM on negative samples and set the positive and negative samples ratio to 1:3 in each mini-batch .",0
5685,"In the testing phase , we build an image pyramid for each test image .",0
5686,Each scale in the pyramid is independently tested .,0
5687,The results from various scales are eventually merged together as the final result of the image .,0
5688,"contains 5,171 labeled faces in 2,845 images .",0
5689,Example images of WIDER FACE and FDDB are shown in .,0
5690,Implementation Details,0
5691,Our training hyper - parameters are similar to Face R - CNN .,1
5692,"Different from Face R - CNN , we initialize our network with the pre-trained weights of 101 - layer ResNet trained on Image Net .",1
5693,"Specifically , we freeze the general kernels ( weights of few layers at the beginning ) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet .",1
5694,"In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .",1
5695,We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors .,1
5696,These anchors then map to the original image to calculate the IoU scores with the ground truth for further picking up with following rules :,0
5697,"First , the anchors with highest IoU score are strictly kept as positive ; Second , the anchors with IoU score above 0.7 are assigned as positive ; Third , If the anchors have IoU score that is lower than 0.3 , they are marked as negative .",0
5698,The R - FCN is then trained on the processed anchors ( proposals ) where the positive samples and negative samples are defined as IoU greater than 0.5 and between 0.1 and 0.5 respectively .,0
5699,The RPN and R - FCN are both learned jointly with the softmax loss and the smooth L1 loss .,1
5700,Non- maximum suppression ( NMS ) is adopted for regularizing the anchors with certain IoU scores .,1
5701,The proposals are processed by OHEM to train with hard examples .,0
5702,We set the 256 for the size of RPN mini-batch and 128 for R - FCN respectively .,1
5703,Approximate joint training strategy is applied for training in the end - to - end fashion .,0
5704,"We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales ( say , 1024 or 1200 ) .",1
5705,"In the testing stage , multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces .",1
5706,Comparison on Benchmarks,0
5707,WIDER FACE,0
5708,We train our model on the training set of WIDER FACE and perform evaluation on the validation set and test set following the Scenario - Int criterion .,0
5709,"As illustrated in , our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results .",1
5710,"In particular , on WIDER FACE hard subset , our approach is superior to the prior best - performing one by a clear margin , which demonstrates the robustness of our algorithm .",1
5711,FDDB,1
5712,There are two evaluation protocols for evaluating the FDDB dataset : one is 10 - fold cross- validation and the other is unrestricted training ( using the data outside FDDB for training ) .,0
5713,Our experiments strictly follow the protocol for unrestricted training .,0
5714,We use the training set of the WIDER FACE dataset to train our model ( denoted as Model - A in ) and compare against the recently published top approaches on FDDB .,0
5715,All of these approaches use the protocol for unrestricted training defined in .,0
5716,The discrete ROC curves and continuous ROC curves of these approaches are plotted in .,0
5717,"From , it is clearly that Face R - FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve .",1
5718,Our discrete ROC curve is superior to the prior best - performing method .,1
5719,We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives ( 98.49%/99.07 % ) .,1
5720,"For the reason that we do not optimize our method to regress the elliptical ground truth in FDDB dataset , our continuous ROC curve is lower than the first place and slightly lower than .",0
5721,"Additionally , one of the factors that may affect the performance of Face R - FCN demonstrated in the last row of 5 ( b ) : the false positive bounding boxes in images exactly contain faces from human perspective where these faces have not been annotated as ground truth .",0
5722,This factor partly leads to the lower performance comparing with .,0
5723,But the competitive result we achieved is still noticeable .,0
5724,All of these methods use the same Scenario - Int criterion .,0
5725,"Face R - FCN shows the superior performance over the prior methods across the three subsets ( easy , medium and hard ) in both validation and test sets .",1
5726,Best viewed in color .,0
5727,"Furthermore ,",0
5728,We expand the training dataset by augmenting with a privately collected dataset and use the enlarged dataset to train a more discriminative face detector ( denoted as Model - B ) .,0
5729,The discrete and continuous ROC curves of Model - B are also plotted in .,0
5730,"As expected , the performance of Face R - FCN is further improved .",0
5731,"Finally , we obtain the true positive rate 98. 99 % of the discrete ROC curve at 1000 false positives and 99. 42 % at 2000 false positives , which are new state - of - the - art among all the published methods on FDDB .",1
5732,Conclusion,0
5733,Face detection is a fundamental problem in vision task .,0
5734,"In this technical report , we propose a powerful face detection approach named Face R - FCN by integrating R - FCN and several sophisticated techniques for better detecting faces and boosting overall performance .",0
5735,"By reasoning the drawbacks of R - CNN and R - FCN , we explore the details and invent new designs to improve the popular detection framework specifically for face detection .",0
5736,The proposed approach is evaluated on the challenging WIDER FACE dataset and FDDB dataset .,0
5737,Our experimental results demonstrate the superiority of our approach over the state - of - the - arts .,0
5738,These innovations are inspired from past experience and we expect our innovations will be easy to generally applied to the future face detection architectures as past experience .,0
5739,The green frames in the image represent the face detection results while the red frames or ellipses represent the ground - truth annotations .,0
5740,"Note that in the last row of ( b ) , some of human faces detected by Face R - FCN have not been annotated as ground truth .",0
5741,title,0
5742,DSFD : Dual Shot Face Detector,0
5743,abstract,0
5744,"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively .",1
5745,"First , we propose a Feature Enhance Module ( FEM ) for enhancing the original feature maps to extend the single shot detector to dual shot detector .",0
5746,"Second , we adopt Progressive Anchor Loss ( PAL ) computed by two different sets of anchors to effectively facilitate the features .",0
5747,"Third , we use an Improved Anchor Matching ( IAM ) by integrating novel anchor assign strategy into data aug -",0
5748,Introduction,0
5749,"Face detection is a fundamental step for various facial applications , like face alignment , parsing , recognition , and verification .",0
5750,"As the pioneering work for face detection , Viola - Jones adopts AdaBoost algorithm with hand - crafted features , which are now replaced by deeply learned features from the convolutional neural network ( CNN ) that achieves great progress .",0
5751,"Although the CNN based face detectors have being extensively studied , detecting faces with high degree of variability in scale , pose , occlusion , expression , appearance and illumination in real - world scenarios remains a challenge .",0
5752,Previous state - of - the - art face detectors can be roughly divided into two categories .,0
5753,The first one is mainly based on the Region Proposal Network ( RPN ) adopted in Faster RCNN and employs two stage detection schemes .,0
5754,RPN is trained end - to - end and generates highquality region proposals which are further refined by Fast R - CNN detector .,0
5755,"The other one is Single Shot Detector ( SSD ) based one - stage methods , which get rid of RPN , and directly predict the bounding boxes and confidence .",0
5756,"Recently , one - stage face detection framework has attracted more attention due to its higher inference efficiency and straightforward system deployment .",0
5757,"Despite the progress achieved by the above methods , there are still some problems existed in three aspects :",0
5758,Feature learning Feature extraction part is essential fora face detector .,0
5759,"Currently , Feature Pyramid Network ( FPN ) is widely used in state - of - the - art face detectors for rich features .",0
5760,"However , FPN just aggregates hierarchical feature maps between high and low - level output layers , which does not consider the current layer 's information , and the context relationship between anchors is ignored .",0
5761,Loss design,0
5762,The conventional loss functions used in object detection include a regression loss for the face region and a classification loss for identifying if a face is detected or not .,0
5763,"To further address the class imbalance problem , Lin et al.",0
5764,propose Focal Loss to focus training on a sparse set of hard examples .,0
5765,"To use all original and enhanced features , propose Hierarchical Loss to effectively learn the network .",0
5766,"However , the above loss functions do not consider progressive learning ability of feature maps in both of different levels and shots .",0
5767,"Anchor matching Basically , pre-set anchors for each feature map are generated by regularly tiling a collection of boxes with different scales and aspect ratios on the image .",0
5768,Some works analyze a series of reasonable anchor scales and anchor compensation strategy to increase positive anchors .,0
5769,"However , such strategy ignores random sampling in data augmentation , which still causes imbalance between positive and negative anchors .",0
5770,"In this paper , we propose three novel techniques to address the above three issues , respectively .",1
5771,"First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .",1
5772,"Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .",1
5773,"Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",1
5774,"Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .",1
5775,The three aspects are complementary so that these techniques can work together to further improve the performance .,0
5776,"Besides , since these techniques are all related to two - stream design , we name the proposed network as Dual Shot Face Detector ( DSFD ) .",1
5777,"shows the effectiveness of DSFD on various variations , especially on extreme small faces or heavily occluded faces .",0
5778,"In summary , the main contributions of this paper include :",0
5779,A novel Feature Enhance Module to utilize different level information and thus obtain more discriminability and robustness features .,0
5780,Auxiliary supervisions introduced in early layers via a set of smaller anchors to effectively facilitate the features .,0
5781,An improved anchor matching strategy to match anchors and ground truth faces as far as possible to provide better initialization for the regressor .,0
5782,Comprehensive experiments conducted on popular benchmarks FDDB and WIDER FACE to demonstrate the superiority of our proposed DSFD network compared with the state - of - the - art methods .,0
5783,Related work,0
5784,We review the prior works from three perspectives .,0
5785,"Feature Learning Early works on face detection mainly rely on hand - crafted features , such as Harr - like features , control point set , edge orientation histograms .",0
5786,"However , hand - crafted features design is lack of guidance .",0
5787,"With the great progress of deep learning , handcrafted features have been replaced by Convolutional Neural Networks ( CNN ) .",0
5788,"For example , Overfeat , Cascade - CNN , MTCNN adopt CNN as a sliding window detector on image pyramid to build feature pyramid .",0
5789,"However , using an image pyramid is slow and memory inefficient .",0
5790,"As the result , most two stage detectors extract features on single scale .",0
5791,"R - CNN obtains region proposals by selective search , and then forwards each normalized image region through a CNN to classify .",0
5792,"Faster R - CNN , R - FCN employ Region Proposal Network ( RPN ) to generate initial region proposals .",0
5793,"Besides , ROIpooling and position - sensitive RoI pooling are applied to extract features from each region .",0
5794,"More recently , some research indicates that multi-scale features perform better for tiny objects .",0
5795,"Specifically , SSD , MS - CNN , SSH , S3 FD predict boxes on multiple layers of feature hierarchy .",0
5796,"FCN , Hypercolumns , Parsenet fuse multiple layer features in segmentation .",0
5797,"FPN , a top - down architecture , integrate high - level semantic information to all scales .",0
5798,"FPN - based methods , such as FAN , Pyramid Box achieve significant improvement on detection .",0
5799,"However , these methods do not consider the current layers information .",0
5800,"Different from the above methods that ignore the context relationship between anchors , we propose a feature enhance module that incorporates multi-level dilated convolutional layers to enhance the semantic of the features .",0
5801,Loss Design,0
5802,"Generally , the objective loss in detection is a weighted sum of classification loss ( e.g. softmax loss ) and box regression loss ( e.g. L 2 loss ) .",0
5803,Girshick et al. propose smooth L 1 loss to prevent exploding gradients .,0
5804,Lin et al.,0
5805,"discover that the class imbalance is one obstacle for better performance in one stage detector , hence they propose focal loss , a dynamically scaled cross entropy loss .",0
5806,"Besides , Wang et al.",0
5807,"design RepLoss for pedestrian detection , which improves performance in occlusion scenarios .",0
5808,FANet create a hierarchical feature pyramid and presents hierarchical loss for their architecture .,0
5809,"However , the anchors used in FANet are kept the same size in different stages .",0
5810,"In this work , we adaptively choose different anchor sizes in different stages to facilitate the features .",0
5811,Anchor Matching,0
5812,"To make the model more robust , most detection methods do data augmentation , such as color distortion , horizontal flipping , random crop and multiscale training .",0
5813,Zhang et al. propose an anchor compensation strategy to make tiny faces to match enough anchors during training .,0
5814,Wang et al.,0
5815,propose random crop to generate large number of occluded faces for training .,0
5816,"However , these methods ignore random sampling in data augmentation , while ours combines anchor assign to provide better data initialization for anchor matching .",0
5817,Dual Shot Face Detector,0
5818,"We firstly introduce the pipeline of our proposed framework DSFD , and then detailly describe our feature enhance module in Sec. 3.2 , progressive anchor loss in Sec. 3.3 and improved anchor matching in Sec. 3.4 , respectively .",0
5819,Pipeline of DSFD,0
5820,The framework of DSFD is illustrated in .,0
5821,"Our architecture uses the same extended VGG16 backbone as PyramidBox and S3FD , which is truncated before the classification layers and added with some auxiliary structures .",0
5822,"We select conv3 3 , conv4 3 , conv5 3 , conv fc7 , conv6 2 and conv7 2 as the first shot detection layers to generate six original feature maps named of 1 , of 2 , of 3 , of 4 , of 5 , of 6 .",0
5823,"Then , our proposed FEM transfers these original feature maps into six enhanced feature maps named ef 1 , ef 2 , ef 3 , ef 4 , ef 5 , ef 6 , which have the same sizes as the original ones and are fed into SSD - style head to construct the second shot detection layers .",0
5824,"Note that the input size of the training image is 640 , which means the feature map size of the lowest - level layer to highest - level layer is from 160 to 5 .",0
5825,"Different from S3FD and Pyramid - Box , after we utilize the receptive field enlargement in FEM and the new anchor design strategy , its unnecessary for the three sizes of stride , anchor and receptive field to satisfy equal - proportion interval principle .",0
5826,"Therefore , our DSFD is more flexible and robustness .",0
5827,"Besides , the original and enhanced shots have two different losses , respectively named First Shot progressive anchor Loss ( FSL ) and Second Shot progressive anchor Loss ( SSL ) .",0
5828,Feature Enhance Module,0
5829,Feature Enhance,0
5830,"Module is able to enhance original features to make them more discriminable and robust , which is called FEM for short .",0
5831,"For enhancing original neuron cell oc ( i , j , l ) , FEM utilizes different dimension information including upper layer original neuron cell oc ( i , j , l ) and current layer non-local neuron cells : nc ( i?? , j?? , l ) , nc ( i?? , j , l ) , ... , nc ( i , j+? , l ) , nc ( i +? , j+? , l ) .",0
5832,"Specially , the enhanced neuron cell ec ( i , j , l ) can be mathematically defined as follow :",0
5833,"where c i , j,l is a cell located in ( i , j ) coordinate of the feature maps in the l - th layer , f denotes a set of basic dilation convolution , elem - wise production , up - sampling or concatenation operations .",0
5834,"illustrates the idea of FEM , which is inspired by FPN and RFB .",0
5835,"Here , we first use 11 convolutional kernel to normalize the feature maps .",0
5836,"Then , we up - sample upper feature maps to do element - wise product with the current ones .",0
5837,"Finally , we split the feature maps to three parts , followed by three sub-networks containing different numbers of dilation convolutional layers .",0
5838,Progressive Anchor Loss,0
5839,"Different from the traditional detection loss , we design progressive anchor sizes for not only different levels , but also different shots in our framework .",0
5840,"Motivated by the statement in that low - level features are more suitable for small faces , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",0
5841,"First , our Second Shot anchor- based multi-task Loss function is defined as :",0
5842,"where N conf and N loc indicate the number of positive and negative anchors , and the number of positive anchors respectively , L conf is the softmax loss over two classes ( face vs. background ) , and L loc is the smooth L 1 loss between the parameterizations of the predicted box ti and ground - truth box g i using the anchor a i .",0
5843,"When p * i = 1 ( p * i = { 0 , 1 } ) , the anchor a i is positive and the localization loss is activated .",0
5844,?,0
5845,is a weight to balance the effects of the two terms .,0
5846,"Compared to the enhanced feature maps in the same level , the original feature maps have less semantic information for classification but more high resolution location information for detection .",0
5847,"Therefore , we believe that the original feature maps can detect and classify smaller faces .",0
5848,"As the result , we propose the First Shot multi-task Loss with a set of smaller anchors as follows :",0
5849,"where sa indicates the smaller anchors in the first shot layers , and the two shots losses can be weighted summed into a whole Progressive Anchor Loss as follows :",0
5850,"Note that anchor size in the first shot is half of ones in the second shot , and ?",0
5851,is weight factor .,0
5852,Detailed assignment on the anchor size is described in Sec. 3.4 .,0
5853,"In prediction process , we only use the output of the second shot , which means no additional computational cost is introduced .",0
5854,Improved Anchor Matching,0
5855,Current anchor matching method is bidirectional between the anchor and ground - truth face .,0
5856,"Therefore , anchor design and face sampling during augmentation are collaborative to match the anchors and faces as far as possible for better initialization of the regressor .",0
5857,"Our IAM targets on addressing the contradiction between the discrete anchor scales and continuous face scales , in which the faces are augmented by S input * S face / S anchor ( S indicates the spatial size ) with the probability of 40 % so as to increase the positive anchors , stabilize the training and thus improve the results .",0
5858,shows details of our anchor design on how each feature map cell is associated to the fixed shape anchor .,0
5859,We set anchor ratio 1.5:1 based on face scale statistics .,0
5860,Anchor size for the original feature is one half of the enhanced feature .,0
5861,"Additionally , with probability of 2 / 5 , we utilize anchor - based sampling like data - anchor - sampling in PyramidBox , which randomly selects a face in an image , crops sub - image containing the face , and sets the size ratio between sub-image and selected face to 640 / rand .",0
5862,"For the remaining 3 / 5 probability , we adopt data augmentation similar to SSD .",0
5863,"In order to improve the recall rate of faces and ensure anchor classification ability simultaneously , we set Intersection - over - Union ( IoU ) threshold 0.4 to assign anchor to its ground - truth faces .",0
5864,Experiments,0
5865,Implementation Details,0
5866,"First , we present the details in implementing our network .",0
5867,The backbone networks are initialized by the pretrained VGG / ResNet on Image Net .,1
5868,All newly added convolution layers ' parameters are initialized by the ' xavier ' method .,1
5869,"We use SGD with 0.9 momentum , 0.0005 weight decay to fine - tune our DSFD model .",1
5870,The batch size is set to 16 .,1
5871,"The learning rate is set to 10 ?3 for the first 40 k steps , and we decay it to 10 ? 4 and 10 ? 5 for two 10 k steps .",1
5872,"During inference , the first shot 's outputs are ignored and the second shot predicts top 5 k high confident detections .",1
5873,Non-maximum suppression is applied with jaccard overlap of 0.3 to produce top 750 high confident bounding boxes per image .,1
5874,"For 4 bounding box coordinates , we round down top left coordinates and roundup width and height to expand the detection bounding box .",1
5875,The official code has been released at : https://github.com/TencentYoutuResearch/FaceDetection-DSFD .,1
5876,"That means even with a higher threshold ( i.e. , 0.4 ) , using our IAM , we can still achieve more matched anchors .",0
5877,"Here , we choose a slightly higher threshold in IAM so that to better balance the number and quality of the matched faces .",0
5878,Analysis on DSFD,0
5879,"In this subsection , we conduct extensive experiments and ablation studies on the WIDER FACE dataset to evaluate the effectiveness of several contributions of our proposed framework , including feature enhance module , progressive anchor loss , and improved anchor matching .",1
5880,"For fair comparisons , we use the same parameter settings for all the experiments , except for the specified changes to the components .",0
5881,All models are trained on the WIDER FACE training set and evaluated on validation set .,0
5882,"To better understand DSFD , we select different baselines to ablate each component on how this part affects the final performance .",0
5883,"Feature Enhance Module First ,",1
5884,"We adopt anchor designed in S3FD , PyramidBox and six original feature maps generated by VGG16 to perform classification and regression , which is named Face SSD ( FSSD ) as the baseline .",0
5885,We then use VGG16 - based FSSD as the baseline to add feature enchance module for comparison .,0
5886,"shows that our feature enhance module can improve VGG16 - based FSSD from 92.6 % , 90.2 % , 79.1 % to 93.0 % , 91.4 % , 84.6 % .",1
5887,"Progressive Anchor Loss Second , we use Res50 - based FSSD as the baseline to add progressive anchor loss for comparison .",1
5888,"We use four residual blocks ' ouputs in ResNet to replace the outputs of conv3 3 , conv4 3 , conv5 3 , conv fc7 in VGG .",0
5889,"Except for VGG16 , we do not perform layer normalization .",0
5890,"shows our progressive anchor loss can improve Res50 - based FSSD using FEM from 95.0 % , 94.1 % , 88.0 % to 95.3 % , 94.4 % , 88.6 % .",1
5891,Improved Anchor Matching,1
5892,"To evaluate our improved anchor matching strategy , we use Res101 - based FSSD without anchor compensation as the baseline .",0
5893,"shows that our improved anchor matching can improve Res101 based FSSD using FEM from 95.8 % , 95.1 % , 89.7 % to 96.1 % , 95.2 % , 90.0 % .",1
5894,"Finally , we can improve our DSFD to 96.6 % , 95.7 % , 90.4 % with ResNet 152 as the backbone . Besides , shows that our improved anchor matching strategy greatly increases the number of ground truth faces that are closed to the anchor , which can reduce the contradiction between the discrete anchor scales and continuous face scales .",0
5895,"Moreover , shows the number distribution of matched anchor number for ground truth faces , which indicates our improved anchor matching can significantly increase the matched anchor number , and the averaged number of matched anchor for different scales of faces can be improved from 6.4 to about 6.9 .",0
5896,Comparison with RFB,0
5897,Our FEM differs from RFB in two aspects .,0
5898,"First , our FEM is based on FPN to make full use of feature information from different spatial levels , while RFB ignores .",0
5899,"Second , our FEM adopts stacked dilation convolutions in a multi-branch structure , which efficiently leads to larger Receptive Fields ( RF ) than RFB that only uses one dilation layer in each branch , e.g. , R 3 in FEM compared to R in RFB where indicates the RF of one dilation convolution .",0
5900,"Tab. 6 clearly demonstrates the superiority of our FEM over RFB , even when RFB is equipped with FPN .",0
5901,"From the above analysis and results , some promising conclusions can be drawn :",0
5902,1 ) Feature enhance is crucial .,0
5903,"We use a more robust and discriminative feature enhance module to improve the feature presentation ability , especially for hard face .",0
5904,"2 ) Auxiliary loss based on progressive anchor is used to train all 12 different scale detection feature maps , and it improves the performance on easy , medium and hard faces simultaneously .",0
5905,"3 ) Our improved anchor matching provides better initial anchors and ground - truth faces to regress anchor from faces , which achieves the improvements of 0.3 % , 0.1 % , 0.3 % on three settings , respectively .",0
5906,"Additionally , when we enlarge the training batch size ( i.e. , Large BS ) , the result in hard setting can get 91.2 % AP .",0
5907,Effects of Different Backbones,0
5908,"To better understand our DSFD , we further conducted experiments to examine how different backbones affect classification and detection performance .",0
5909,"Specifically , we use the same setting except for the feature extraction network , we implement SE - ResNet101 , DPN ? 98 , SE- ResNeXt101 324d following the ResNet 101 setting in our DSFD .",0
5910,"From , DSFD with SE - ResNeXt101 324d got 95.7 % , 94.8 % , 88.9 % , on easy , medium and hard settings respectively , which indicates that more complexity model and higher Top - 1 I ma - geNet classification accuracy may not benefit face detection AP .",0
5911,"Therefore , in our DSFD framework , better performance on classification are not necessary for better performance on detection , which is consistent to the conclusion claimed in .",0
5912,Our DSFD enjoys high inference speed benefited from simply using the second shot detection results .,0
5913,"For VGA resolution inputs to Res50 - based DSFD , it runs 22 FPS on NVIDA GPU P40 during inference .",0
5914,Comparisons with State - of - the - Art Methods,0
5915,"We evaluate the proposed DSFD on two popular face detection benchmarks , including WIDER FACE and Face Detection Data Set and Benchmark ( FDDB ) .",0
5916,"Our model is trained only using the training set of WIDER FACE , and then evaluated on both benchmarks without any further fine - tuning .",0
5917,We also follow the similar way used in to build the image pyramid for multi-scale testing and use more powerful backbone similar as .,0
5918,WIDER FACE Dataset,1
5919,"It contains 393 , 703 annotated faces with large variations in scale , pose and occlusion in total 32 , 203 images .",0
5920,"For each of the 60 event classes , 40 % , 10 % , 50 % images of the database are randomly selected as training , validation and testing sets .",0
5921,"Besides , each subset is further defined into three levels of difficulty : ' Easy ' , ' Medium ' , ' Hard ' based on the detection rate of a baseline detector .",0
5922,"As shown in , our DSFD achieves the best performance among all of the state - of - the - art face detectors based on the average precision ( AP ) across the three subsets , i.e. , 96.6 % ( Easy ) , 95.7 % ( Medium ) and 90.4 % ( Hard ) on validation set , and 96.0 % ( Easy ) , 95.3 % ( Medium ) and 90.0 % ( Hard ) on test set .",1
5923,"shows more examples to demonstrate the effects of DSFD on handling faces with various variations , in which the blue bounding boxes indicate the detector confidence is above 0.8 .",0
5924,FDDB Dataset,1
5925,"It contains 5 , 171 faces in 2 , 845 images taken from the faces in the wild data set .",0
5926,"Since WIDER FACE has bounding box annotation while faces in FDDB are represented by ellipses , we learn a post - hoc ellipses regressor to transform the final prediction results .",0
5927,"As shown in , our DSFD achieves state - of - the - art performance on both discontinuous and continuous ROC curves , i.e. 99.1 % and 86.2 % when the number of false positives equals to 1 , 000 .",1
5928,"After adding additional annotations to those unlabeled faces , the false positives of our model can be further reduced and outperform all other methods .",1
5929,Conclusions,0
5930,This paper introduces a novel face detector named Dual Shot Face Detector ( DSFD ) .,0
5931,"In this work , we propose a novel Feature Enhance Module that utilizes different level information and thus obtains more discriminability and robustness features .",0
5932,Auxiliary supervisions introduced in early layers by using smaller anchors are adopted to effectively facilitate the features .,0
5933,"Moreover , an improved anchor matching method is introduced to match anchors and ground truth faces as far as possible to provide better initialization for the regressor .",0
5934,"Comprehensive experiments are conducted on popular face detection benchmarks , FDDB and WIDER FACE , to demonstrate the superiority of our proposed DSFD compared with the state - of - the - art face detectors , e.g. , SRN and PyramidBox .",0
5935,title,0
5936,RetinaFace : Single - stage Dense Face Localisation in the Wild,1
5937,abstract,0
5938,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",1
5939,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .",0
5940,"Specifically ,",0
5941,We make contributions in the following five aspects :,0
5942,( 1 ) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal .,0
5943,( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .,0
5944,"( 3 ) On the WIDER FACE hard test set , RetinaFace outperforms the state of the art average precision ( AP ) by 1.1 % ( achieving AP equal to 91.4 % ) .",0
5945,"( 4 ) On the IJB - C test set , RetinaFace enables state of the art methods ( ArcFace ) to improve their results in face verification ( TAR = 89.59 % for FAR = 1 e - 6 ) .",0
5946,"( 5 ) By employing light - weight backbone networks , Retina Face can run real - time on a single CPU core fora VGA - resolution image .",0
5947,Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace.,1
5948,Introduction,0
5949,Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .,0
5950,"A narrow definition of face localisation may refer to traditional face detection , which aims at estimating the face bounding boxes without any scale and position prior .",0
5951,"Nevertheless , in this paper .",0
5952,The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .,0
5953,"Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .",0
5954,"we refer to a broader definition of face localisation which includes face detection , face alignment , pixelwise face parsing and 3D dense correspondence regression .",0
5955,That kind of dense face localisation provides accurate facial position information for all different scales .,0
5956,"Inspired by generic object detection methods , which embraced all the recent advances in deep learning , face detection has recently achieved remarkable progress .",0
5957,"Different from generic object detection , face detection features smaller ratio variations ( from 1:1 to 1:1.5 ) but much larger scale variations ( from several pixels to thousand pixels ) .",0
5958,"The most recent state - of - the - art methods focus on singlestage design which densely samples face locations and scales on feature pyramids , demonstrating promising performance and yielding faster speed compared to twostage methods .",0
5959,"Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .",0
5960,Our idea is examplified in .,0
5961,"Typically , face detection training process contains both classification and box regression losses .",0
5962,Chen et al. proposed to combine face detection and alignment in a joint cascade framework based on the observation that aligned face shapes provide better features for face classification .,0
5963,"Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .",0
5964,"Due to training data limitation , JDA , MTCNN and STN have not verified whether tiny face detection can benefit from the extra supervision of five facial landmarks .",0
5965,One of the questions we aim at answering in this paper is whether we can push forward the current best performance ( 90.3 % ) on the WIDER FACE hard test set by using extra supervision signal built of five facial landmarks .,0
5966,"In Mask R - CNN , the detection performance is significantly improved by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition and regression .",0
5967,That confirms that dense pixel - wise annotations are also beneficial to improve detection .,0
5968,"Unfortunately , for the challenging faces of WIDER FACE it is not possible to conduct dense face annotation ( either in the form of more landmarks or semantic segments ) .",0
5969,"Since supervised signals can not be easily obtained , the question is whether we can apply unsupervised methods to further improve face detection .",0
5970,"In FAN , an anchor - level attention map is proposed to improve the occluded face detection .",0
5971,"Nevertheless , the proposed attention map is quite coarse and does not contain semantic information .",0
5972,"Recently , self - supervised 3D morphable models have achieved promising 3 D face modelling in - the - wild .",0
5973,"Especially , Mesh Decoder achieves over real - time speed by exploiting graph convolutions on joint shape and texture .",0
5974,"However , the main challenges of applying mesh decoder into the single - stage detector are : ( 1 ) camera parameters are hard to estimate accurately , and ( 2 ) the joint latent shape and texture representation is predicted from a single feature vector ( 1 1 Conv on feature pyramid ) instead of the RoI pooled feature , which indicates the risk of feature shift .",0
5975,"In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .",0
5976,"To summarise , our key contributions are :",0
5977,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .",1
5978,"On the WIDER FACE hard subset , RetinaFace outperforms the AP of the state of the art two - stage method ( ISRN ) by 1.1 % ( AP equal to 91.4 % ) .",0
5979,"On the IJB - C dataset , RetinaFace helps to improve Ar - c Face 's verification accuracy ( with TAR equal to 89.59 % when FAR = 1 e - 6 ) .",0
5980,This indicates that better face localisation can significantly improve face recognition .,0
5981,"By employing light - weight backbone networks , Reti - na",0
5982,Face can run real - time on a single CPU core fora VGA - resolution image . Extra annotations and code have been released to facilitate future research .,0
5983,Related Work,0
5984,Image pyramid,0
5985,v.s. feature pyramid :,0
5986,"The slidingwindow paradigm , in which a classifier is applied on a dense image grid , can be traced back to past decades .",0
5987,"The milestone work of Viola - Jones explored cascade chain to reject false face regions from an image pyramid with real - time efficiency , leading to the widespread adoption of such scale - invariant face detection framework .",0
5988,"Even though the sliding - window on image pyramid was the leading detection paradigm , with the emergence of feature pyramid , sliding - anchor on multi-scale feature maps , quickly dominated face detection .",0
5989,Two - stage v.s. single - stage : Current face detection methods have inherited some achievements from generic object detection approaches and can be divided into two categories : two - stage methods ( e.g. Faster R - CNN ) and single - stage methods ( e.g. SSD and Reti - naNet ) .,0
5990,"Two - stage methods employed a "" proposal and refinement "" mechanism featuring high localisation accuracy .",0
5991,"By contrast , single - stage methods densely sampled face locations and scales , which resulted in extremely unbalanced positive and negative samples during training .",0
5992,"To handle this imbalance , sampling and re-weighting methods were widely adopted .",0
5993,"Compared to two - stage methods , single - stage methods are more efficient and have higher recall rate but at the risk of achieving a higher false positive rate and compromising the localisation accuracy .",0
5994,Context Modelling :,0
5995,"To enhance the model 's contextual reasoning power for capturing tiny faces , SSH and PyramidBox applied context modules on feature pyramids to enlarge the receptive field from Euclidean grids .",0
5996,"To enhance the non-rigid transformation modelling capacity of CNNs , deformable convolution network ( DCN ) employed a novel deformable layer to model geometric transformations .",0
5997,The champion solution of the WIDER Face Challenge 2018 indicates that rigid ( expansion ) and non-rigid ( deformation ) context modelling are complementary and orthogonal to improve the performance of face detection .,0
5998,Multi- task Learning :,0
5999,Joint face detection and alignment is widely used as aligned face shapes provide better features for face classification .,0
6000,"In Mask R - CNN , the detection performance was significantly improved by adding a branch for predicting an object mask in parallel with the existing branches .",0
6001,Densepose adopted the architecture of Mask - RCNN to obtain dense part labels and coordinates within each of the selected regions .,0
6002,"Neverthe - less , the dense regression branch in was trained by supervised learning .",0
6003,"In addition , the dense branch was a small FCN applied to each RoI to predict a pixel - to - pixel dense mapping .",0
6004,RetinaFace,0
6005,Multi- task Loss,0
6006,"For any training anchor i , we minimise the following multi -task loss :",0
6007,"( 1 ) Face classification loss L cls ( p i , p * i ) , where pi is the predicted probability of anchor i being a face and p * i is 1 for the positive anchor and 0 for the negative anchor .",0
6008,The classification loss L cls is the softmax loss for binary classes ( face / not face ) .,0
6009,represent the coordinates of the predicted box and ground - truth box associated with the positive anchor .,0
6010,"We follow to normalise the box regression targets ( i.e. centre location , width and height ) and use L box",0
6011,where R is the robust loss function ( smooth - L 1 ) defined in .,0
6012,". . , l * x 5 , l * y5 } i represent the predicted five facial landmarks and groundtruth associated with the positive anchor .",0
6013,"Similar to the box centre regression , the five facial landmark regression also employs the target normalisation based on the anchor centre .",0
6014,( 4 ) Dense regression loss L pixel ( refer to Eq. 3 ) .,0
6015,The loss - balancing parameters ?,0
6016,1 -?,0
6017,"3 are set to 0.25 , 0.1 and 0.01 , which means that we increase the significance of better box and landmark locations from supervision signals .",0
6018,Dense Regression Branch,0
6019,Mesh Decoder .,0
6020,"We directly employ the mesh decoder ( mesh convolution and mesh up - sampling ) from , which is a graph convolution method based on fast localised spectral filtering .",0
6021,"In order to achieve further acceleration , we also use a joint shape and texture decoder similarly to the method in , contrary to which only decoded shape .",0
6022,Below we will briefly explain the concept of graph convolutions and outline why they can be used for fast decoding .,0
6023,"As illustrated in , a 2D convolutional operation is a "" kernel - weighted neighbour sum "" within the Euclidean grid receptive field .",0
6024,"Similarly , graph convolution also employs the same concept as shown in .",0
6025,"However , the neighbour distance is calculated on the graph by counting the minimum number of edges connecting two vertices .",0
6026,"We follow to define a coloured face mesh G = ( V , E ) , where V ?",0
6027,"R n6 is a set of face vertices containing the joint shape and texture information , and E ?",0
6028,"{ 0 , 1 } nn is a sparse adjacency matrix encoding the connection status between vertices .",0
6029,The graph Laplacian is defined as,0
6030,"Following , the graph convolution with kernel g ?",0
6031,"can be formulated as a recursive Chebyshev polynomial truncated at order K ,",0
6032,where ? ?,0
6033,R K is a vector of Chebyshev coefficients and T k ( L ) ?,0
6034,R nn is the Chebyshev polynomial of order k evaluated at the scaled Laplacian L .,0
6035,The whole filtering operation is extremely efficient including K sparse matrix - vector multiplications and one dense matrix - vector multiplication,0
6036,Differentiable Renderer .,0
6037,After we predict the shape and texture parameters PST ?,0
6038,"R 128 , we employ an efficient differentiable 3D mesh renderer to project a colouredmesh DP ST onto a 2D image plane with camera parame -",0
6039,"camera location , camera pose and focal length ) and illumination parameters",0
6040,"location of point light source , colour values and colour of ambient lighting ) .",0
6041,Dense Regression Loss .,0
6042,"Once we get the rendered 2 D face R ( D PST , P cam , P ill ) , we compare the pixel - wise difference of the rendered and the original 2 D face using the following function :",0
6043,Experiments,0
6044,Dataset,0
6045,"The WIDER FACE dataset consists of 32 , 203 images and 393 , 703 face bounding boxes with a high degree of variability in scale , pose , expression , occlusion and illumination .",0
6046,"The WIDER FACE dataset is split into training ( 40 % ) , validation ( 10 % ) and testing ( 50 % ) subsets by randomly sampling from 61 scene categories .",0
6047,"Based on the detection rate of EdgeBox , three levels of difficulty ( i.e. Easy , Medium and Hard ) are defined by incrementally incorporating hard samples .",0
6048,Extra Annotations .,0
6049,"As illustrated in and Tab. 1 , we define five levels of face image quality ( according to how .",0
6050,An overview of the proposed single - stage dense face localisation approach .,0
6051,RetinaFace is designed based on the feature pyramids with independent context modules .,0
6052,"Following the context modules , we calculate a multi - task loss for each anchor .",0
6053,"difficult it is to annotate landmarks on the face ) and annotate five facial landmarks ( i.e. eye centres , nose tip and mouth corners ) on faces that can be annotated from the WIDER FACE training and validation subsets .",0
6054,"In total , we have annotated 84.6 k faces on the training set and 18.5 k faces on the validation set ..",0
6055,"We add extra annotations of five facial landmarks on faces that can be annotated ( we call them "" annotatable "" ) from the WIDER FACE training and validation sets .",0
6056,Implementation details,0
6057,Feature Pyramid .,0
6058,Retina,0
6059,"Face employs feature pyramid levels from P 2 to P 6 , where P 2 to P 5 are computed from the output of the corresponding ResNet residual stage ( C 2 through C 5 ) using top - down and lateral connections as in .",0
6060,P 6 is calculated through a 33 convolution with stride = 2 on C 5 .,0
6061,"C 1 to C 5 is a pre-trained ResNet - 152 classification network on the ImageNet - 11 k dataset while P 6 are randomly initialised with the "" Xavier "" method .",0
6062,Context Module .,0
6063,"Inspired by SSH and Pyramid - Box , we also apply independent context modules on five feature pyramid levels to increase the receptive field and enhance the rigid context modelling power .",0
6064,"Drawing lessons from the champion of the WIDER Face Challenge 2018 , we also replace all 3 3 convolution layers within the lateral connections and context modules by the deformable convolution network ( DCN ) , which further strengthens the non-rigid context modelling capacity .",0
6065,Loss Head .,0
6066,"For negative anchors , only classification loss is applied .",0
6067,"For positive anchors , the proposed multi-task loss is calculated .",0
6068,"We employ a shared loss head ( 1 1 conv ) across different feature maps H n W n 256 , n ? { 2 , . . . , 6 }.",0
6069,"For the mesh decoder , we apply the pre-trained model , which is a small computational overhead that allows for efficient inference .",0
6070,Anchor Settings .,0
6071,As illustrated in Tab .,0
6072,"2 , we employ scalespecific anchors on the feature pyramid levels from P 2 to P 6 like .",0
6073,"Here , P 2 is designed to capture tiny faces by tiling small anchors at the cost of more computational time and at the risk of more false positives .",0
6074,"We set the scale step at 2 1 / 3 and the aspect ratio at 1 : During training , anchors are matched to a ground - truth box when IoU is larger than 0.5 , and to the background when IoU is less than 0.3 .",0
6075,Unmatched anchors are ignored during training .,0
6076,"Since most of the anchors ( > 99 % ) are negative after the matching step , we employ standard OHEM to alleviate significant imbalance between the positive and negative training examples .",0
6077,"More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .",0
6078,Data Augmentation .,0
6079,"Since there are around 20 % tiny faces in the WIDER FACE training set , we follow and randomly crop square patches from the original images and resize these patches into 640 640 to generate larger training faces .",0
6080,"More specifically , square patches are cropped from the original image with a random size between [ 0.3 , 1 ] of the short edge of the original image .",0
6081,"For the faces on the crop boundary , we keep the overlapped part of the face box if its centre is within the crop patch .",0
6082,"Besides random crop , we also augment training data by random horizontal flip with the probability of 0.5 and photo-metric colour distortion .",0
6083,Training Details .,1
6084,"We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .",1
6085,"The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .",1
6086,The training process terminates at 80 epochs .,1
6087,Testing Details .,1
6088,"For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .",1
6089,Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .,1
6090,Ablation Study,0
6091,"To achieve a better understanding of the proposed Reti - naFace , we conduct extensive ablation experiments to examine how the annotated five facial landmarks and the pro-posed dense regression branch quantitatively affect the performance of face detection .",0
6092,"Besides the standard evaluation metric of average precision ( AP ) when IoU=0.5 on the Easy , Medium and Hard subsets , we also make use of the development server ( Hard validation subset ) of the WIDER Face Challenge 2018 , which employs a more strict evaluation metric of mean AP ( m AP ) for IoU=0.5:0.05:0.95 , rewarding more accurate face detectors .",0
6093,"As illustrated in Tab. 3 , we evaluate the performance of several different settings on the WIDER FACE validation set and focus on the observations of AP and mAP on the Hard subset .",0
6094,"By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .",0
6095,"Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .",1
6096,"By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .",1
6097,"Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .",1
6098,"This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .",0
6099,Method,0
6100,Easy . Ablation experiments of the proposed methods on the WIDER FACE validation subset .,0
6101,Face box Accuracy,1
6102,"Following the stander evaluation protocol of the WIDER FACE dataset , we only train the model on the training set and test on both the validation and test sets .",0
6103,"To obtain the evaluation results on the test set , we submit the detection results to the organisers for evaluation .",0
6104,"As shown in , we compare the proposed RetinaFace with other 24 state - of - the - art face detection algorithms ( i.e. Multiscale Cascade CNN , Two - stage CNN , ACF - WIDER , Faceness - WIDER , Multitask Cascade CNN , CMS - RCNN , LDCF + , HR , Face R - CNN , ScaleFace , SSH , SFD , Face R - FCN , MSCNN , FAN , Zhu et al. , Pyramid - Box , FDNet , SRN , FANet , DSFD , DFS , VIM - FD , ISRN ) .",0
6105,Our approach outper - forms these state - of - the - art methods in terms of AP .,0
6106,"More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .",1
6107,"Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .",1
6108,"In , we illustrate qualitative results on a selfie with dense faces .",0
6109,"RetinaFace successfully finds about 900 faces ( threshold at 0.5 ) out of the reported 1 , 151 faces .",1
6110,"Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .",0
6111,"Even though there are some failure cases of dense face localisation under heavy occlusion , the dense regression results on some clear and large faces are good and even show expression variations .",0
6112,Five Facial Landmark Accuracy,1
6113,"To evaluate the accuracy of five facial landmark localisation , we compare RetinaFace with MTCNN on the AFLW dataset ( 24,386 faces ) as well as the WIDER FACE validation set ( 18.5 k faces ) .",0
6114,"Here , we employ the face box size ( ?",0
6115,W H ) as the normalisation distance .,0
6116,"As shown in , we give the mean error of each facial landmark on the AFLW dataset .",0
6117,RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .,1
6118,"In , we show the cumulative error distribution ( CED ) curves on the WIDER FACE validation set .",0
6119,"Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .",1
6120,Dense Facial Landmark Accuracy,1
6121,"Besides box and five facial landmarks , RetinaFace also outputs dense face correspondence , but the dense regression branch is trained by self - supervised learning only .",0
6122,"Following , we evaluate the accuracy of dense facial landmark localisation on the AFLW2000 - 3D dataset considering ( 1 ) 68 landmarks with the 2D projection coordinates and ( 2 ) all landmarks with 3D coordinates .",0
6123,"Here , the mean error is still normalised by the bounding box size .",0
6124,"In , we give the CED curves of state - of - the - art methods as well as RetinaFace .",0
6125,"Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .",1
6126,"More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .",1
6127,( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .,1
6128,"As illustrated in , RetinaFace can easily handle faces with pose variations but has difficulty under complex scenarios .",0
6129,This indicates that mis-aligned and over-compacted feature representation ( 1 1 256 in RetinaFace ) impedes the single - stage framework achieving high accurate dense regression outputs .,0
6130,"Nevertheless , the projected face regions in the dense regression branch still have the effect of attention which can help to improve face detection as confirmed in the section of ablation study .",0
6131,Face Recognition Accuracy,1
6132,Face detection plays a crucial role in robust face recognition but its effect is rarely explicitly measured .,0
6133,"In this paper , we demonstrate how our face detection method can boost the performance of a state - of - the - art publicly available face recognition method , i.e. ArcFace .",0
6134,"ArcFace studied how different aspects in the training process of a deep convolutional neural network ( i.e. , choice of the training set , the network and the loss function ) affect large scale face recognition performance .",0
6135,"However , ArcFace paper did not study the effect of face detection by applying only the MTCNN for detection and alignment .",0
6136,"In this paper , we replace MTCNN by RetinaFace to detect and align all of the training data ( i.e. MS1M ) and test data ( i.e. LFW , CFP - FP , AgeDB - 30 and IJBC ) , and keep the embedding network ( i.e. ResNet100 ) and the loss function ( i.e. additive angular margin ) exactly the same as Arc -Face .",0
6137,"In Tab. 4 , we show the influence of face detection and alignment on deep face recognition ( i.e. ArcFace ) by comparing the widely used MTCNN and the proposed Reti-naFace .",0
6138,"The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .",1
6139,This result shows that the performance of frontal - profile face verification is now approaching that of frontal - frontal face verification ( e.g. 99.86 % on LFW ) .,0
6140,Methods,0
6141,"LFW CFP - FP AgeDB - 30 MTCNN + ArcFace In , we show the ROC curves on the IJB - C dataset as well as the TAR for FAR =",0
6142,1 e ? 6 at the end of each legend .,0
6143,We employ two tricks ( i.e. flip test and face detection score to weigh samples within templates ) to progressively improve the face verification accuracy .,0
6144,"Under fair comparison , TAR ( at FAR = 1 e ? 6 ) significantly improves from 88 . 29 % to 89.59 % simply by replacing MTCNN with RetinaFace .",0
6145,"This indicates that ( 1 ) face detection and align - In ( c ) , we compare the dense regression results from RetinaFace and Mesh Decoder .",0
6146,Retina,0
6147,Face can easily handle faces with pose variations but has difficulty to predict accurate dense correspondence under complex scenarios .,0
6148,ment significantly affect face recognition performance and ( 2 ) RetinaFace is a much stronger baseline that MTCNN for face recognition applications .,0
6149,Inference Efficiency,0
6150,"During testing , RetinaFace performs face localisation in a single stage , which is flexible and efficient .",0
6151,"Besides the above - explored heavy - weight model ( Res Net - 152 , size of 262 MB , and AP 91.8 % on the WIDER FACE hard set ) , we also resort to a light - weight model ( MobileNet - 0.25 , size of 1 MB , and AP 78.2 % on the WIDER FACE hard set ) to accelerate the inference .",0
6152,"For the light - weight model , we can quickly reduce the data size by using a 7 7 convolution with stride = 4 on the input image , tile dense anchors on P 3 , P 4 and P 5 as in , and remove deformable layers .",0
6153,"In addition , the first two convolutional layers initialised by the ImageNet pre-trained .",0
6154,ROC curves of 1:1 verification protocol on the IJB - C dataset .,0
6155,""" + F "" refers to flip test during feature embedding and "" + S "" denotes face detection score used to weigh samples within templates .",0
6156,We also give TAR for FAR = 1 e ? 6 at the end of the each legend .,0
6157,model are fixed to achieve higher accuracy .,0
6158,Tab. 5 gives the inference time of two models with respect to different input sizes .,0
6159,"We omit the time cost on the dense regression branch , thus the time statistics are irrelevant to the face density of the input image .",0
6160,"We take advantage of TVM to accelerate the model inference and timing is performed on the NVIDIA Tesla P40 GPU , Intel i7-6700 K CPU and ARM - RK3399 , respectively .",0
6161,"RetinaFace - ResNet - 152 is designed for highly accurate face localisation , running at 13 FPS for VGA images ( 640 480 ) .",0
6162,"By contrast , RetinaFace - MobileNet - 0.25 is designed for highly efficient face localisation which demonstrates considerable real - time speed of 40 FPS at GPU for 4 K images ( 4096 2160 ) , 20 FPS at multi-thread CPU for HD images ( 1920 1080 ) , and 60 FPS at single - thread CPU for VGA images ( 640 480 ) .",0
6163,"Even more impressively , 16 FPS at ARM for VGA images ( 640 480 ) allows fora fast system on mobile devices ..",0
6164,"Inference time ( m s ) of RetinaFace with different backbones ( Res Net - 152 and MobileNet - 0.25 ) on different input sizes ( VGA@640x480 , HD@1920x1080 and 4K@4096x2160 ) . "" CPU - 1 "" and "" CPU -m "" denote single - thread and multi-thread test on the Intel i7-6700 K CPU , respectively .",0
6165,""" GPU "" refers to the NVIDIA Tesla P40 GPU and "" ARM "" platform is RK3399 ( A72x 2 ) .",0
6166,Conclusions,0
6167,"We studied the challenging problem of simultaneous dense localisation and alignment of faces of arbitrary scales in images and we proposed the first , to the best of our knowledge , one - stage solution ( RetinaFace ) .",0
6168,Our solution outperforms state of the art methods in the current most challenging benchmarks for face detection .,0
6169,"Furthermore , when RetinaFace is combined with state - of - the - art practices for face recognition it obviously improves the accuracy .",0
6170,The data and models have been provided publicly available to facilitate further research on the topic .,0
6171,Acknowledgements,0
6172,Jiankang,0
6173,Deng acknowledges financial support from the Imperial President 's PhD Scholarship and GPU donations from NVIDIA .,0
6174,"Stefanos Zafeiriou acknowledges support from EPSRC Fellowship DEFORM ( EP / S010203/1 ) , FACER2VM ( EP / N007743/1 ) and a Google Faculty Fellowship .",0
6175,title,0
6176,Pyramid Box : A Context - assisted Single Shot Face Detector,0
6177,abstract,0
6178,"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment .",1
6179,"This paper proposes a novel contextassisted single shot face detector , named PyramidBox to handle the hard face detection problem .",0
6180,"Observing the importance of the context , we improve the utilization of contextual information in the following three aspects .",0
6181,"First , we design a novel context anchor to supervise high - level contextual feature learning by a semi-supervised method , which we call it PyramidAnchors .",0
6182,"Second , we propose the Low - level Feature Pyramid Network to combine adequate high - level context semantic feature and Low - level facial feature together , which also allows the PyramidBox to predict faces of all scales in a single shot .",0
6183,"Third , we introduce a contextsensitive structure to increase the capacity of prediction network to improve the final accuracy of output .",0
6184,"In addition , we use the method of Data - anchor - sampling to augment the training samples across different scales , which increases the diversity of training data for smaller faces .",0
6185,"By exploiting the value of context , PyramidBox achieves superior performance among the state - of - the - art over the two common face detection benchmarks , FDDB and WIDER FACE .",0
6186,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,1
6187,fluid/face_detection .,0
6188,Introduction,0
6189,Face detection is a fundamental and essential task in various face applications .,0
6190,The breakthrough work by Viola - Jones utilizes AdaBoost algorithm with Haar - Like features to train a cascade of face vs. non-face classifiers .,0
6191,"Since that , numerous of subsequent works are proposed for improving the cascade detectors .",0
6192,"Then , introduce deformable part models ( DPM ) into face detection tasks by modeling the relationship of deformable facial parts .",0
6193,These methods are mainly based on designed features which are less representable and trained by separated steps .,0
6194,"With the great breakthrough of convolutional neural networks ( CNN ) , a lot of progress for face detection has been made in recent years due to utilizing modern CNN - based object detectors , including R - CNN , SSD , YOLO , FocalLoss and their extensions .",0
6195,"Benefiting from the powerful deep learning approach and end - to - end optimization , the CNN - based face detectors have achieved much better performance and provided anew baseline for later methods .",0
6196,Recent anchor - based detection frameworks aim at detecting hard faces in uncontrolled environment such as WIDER FACE .,0
6197,SSH and S 3 FD develop scale - invariant networks to detect faces with different scales from different layers in a single network .,0
6198,Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .,0
6199,FAN proposes an anchor - level attention by highlighting the features from the face region to detect the occluded faces .,0
6200,"Though these works give an effective way to design anchors and related networks to detect faces with different scales , how to use the contextual information in face detection has not been paid enough attention , which should play a significant role in detection of hard faces .",0
6201,"Actually , as shown in , it is clear that faces never occur isolated in the real world , usually with shoulders or bodies , providing a rich source of contextual associations to be exploited especially when the facial texture is not distinguishable for the sake of low - resolution , blur and occlusion .",0
6202,We address this issue by introducing a novel framework of context assisted network to make full use of contextual signals as the following steps .,0
6203,"Firstly , the network should be able to learn features for not only faces , but also contextual parts such as heads and bodies .",0
6204,"To achieve this goal , extra labels are needed and the anchors matched to these parts should be designed .",0
6205,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .",1
6206,"Secondly , high - level contextual features should be adequately combined with the low - level ones .",0
6207,"The appearances of hard and easy faces can be quite differ -ent , which implies that not all high - level semantic features are really helpful to smaller targets .",0
6208,We investigate the performance of Feature Pyramid Networks ( FPN ) and modify it into a Low - level Feature Pyramid Network ( LFPN ) to join mutually helpful features together .,1
6209,"Thirdly , the predict branch network should make full use of the joint feature .",0
6210,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,1
6211,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",1
6212,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .",1
6213,"In order to learn more representable features , the diversity of hard - set samples is important and can be gained by data augmentation across samples .",0
6214,"For clarity , the main contributions of this work can be summarized as fivefold :",0
6215,1 .,0
6216,"We propose an anchor-based context assisted method , called PyramidAnchors , to introduce supervised information on learning contextual features for small , blurred and partially occluded faces .",0
6217,2 . We design the Low - level Feature Pyramid Networks ( LFPN ) to merge contextual features and facial features better .,0
6218,"Meanwhile , the proposed method can handle faces with different scales well in a single shot .",0
6219,"3 . We introduce a context - sensitive prediction module , consisting of a mixed network structure and max - in - out layer to learn accurate location and classification from the merged features .",0
6220,4 . We propose the scale aware Data - anchor - sampling strategy to change the distribution of training samples to put emphasis on smaller faces .,0
6221,5 . We achieve superior performance over state - of - the - art on the common face detection benchmarks FDDB and WIDER FACE .,0
6222,The rest of the paper is organized as follows .,0
6223,Section 2 provides an overview of the related works .,0
6224,Section 3 introduces the proposed method .,0
6225,Section 4 presents the experiments and Section 5 concludes the paper .,0
6226,Related Work,0
6227,Anchor - based Face Detectors .,0
6228,"Anchor was first proposed by Faster R - CNN , and then it was widely used in both two - stage and one single shot object detectors .",0
6229,Then anchor - based object detectors have achieved remarkable progress in recent years .,0
6230,"Similar to FPN , Lin uses translation - invariant anchor boxes , and Zhang designs scales of anchors to ensure that the detector can handle various scales of faces well .",0
6231,FaceBoxes introduces anchor densification to ensure different types of anchors have the same density on the image .,0
6232,S 3 FD proposed anchor matching strategy to improve the recall rate of tiny faces .,0
6233,Scale - invariant,0
6234,Face Detectors .,0
6235,"To improve the performance of face detector to handle faces of different scales , many state - of - the - art works construct different structures in the same framework to detect faces with variant size , where the high - level features are designed to detect large faces while low - level features for small faces .",0
6236,"In order to integrate high - level semantic feature into low - level layers with higher resolution , FPN proposed a top - down architecture to use high - level semantic feature maps at all scales .",0
6237,"Recently , FPN - style framework achieves great performance on both objection detection and face detection .",0
6238,Context - associated,0
6239,Face Detectors .,0
6240,"Recently , some works show the importance of contextual information for face detection , especially for finding small , blurred and occluded faces .",0
6241,CMS - RCNN used Faster R - CNN in face detection with body contextual information .,0
6242,Hu et al. trained separate detectors for different scales .,0
6243,SSH modeled the context information by large filters on each prediction module .,0
6244,"FAN proposed an anchor - level attention , by highlighting the features from the face region , to detect the occluded faces .",0
6245,PyramidBox,0
6246,"This section introduces the context - assisted single shot face detector , Pyramid - Box .",0
6247,We first briefly introduce the network architecture in Sec. 3.1 .,0
6248,"Then we present a context - sensitive prediction module in Sec. 3.2 , and propose a novel anchor method , named PyramidAnchors , in Sec. 3.3 .",0
6249,"Finally , Sec. 3.4 presents the associated training methodology including data - anchor - sampling and maxin - out .",0
6250,Network Architecture,0
6251,Anchor - based object detection frameworks with sophisticated design of anchors have been proved effective to handle faces of variable scales when predictions are made at different levels of feature map .,0
6252,"Meanwhile , FPN structures showed strength on merging high - level features with the lower ones .",0
6253,"The architecture of PyramidBox uses the same extended VGG16 backbone and anchor scale design as S 3 FD , which can generate feature maps at different levels and anchors with equal - proportion interval .",0
6254,Low - level FPN is added on this backbone and a Context - sensitive Predict Module is used as a branch network from each pyramid detection layer to get the final output .,0
6255,The key is that we design a novel pyramid anchor method which generates a series of anchors for each face at different levels .,0
6256,The details of each component in the architecture are as follows :,0
6257,Scale - equitable,0
6258,Backbone Layers .,0
6259,"We use the base convolution layers and extra convolutional layers in S 3 FD as our backbone layers , which keep layers of VGG16 from conv 1 1 to pool 5 , then convert fc 6 and fc 7 of VGG16 to conv fc layers , and then add more convolutional layers to make it deeper .",0
6260,Low - level Feature Pyramid Layers .,0
6261,"To improve the performance of face detector to handle faces of different scales , the low - level feature with highresolution plays a key role .",0
6262,"Hence , many state - of - the - art works construct different structures in the same framework to detect faces with variant size , where the high - level features are designed to detect large faces while lowlevel features for small faces .",0
6263,"In order to integrate high - level semantic feature into low - level layers with higher resolution , FPN proposed a top - down architecture to use high - level semantic feature maps at all scales .",0
6264,"Recently , FPN - style framework achieves great performance on both objection detection and face detection .",0
6265,"As we know , all of these works build FPN start from the top layer , which should be argued that not all high - level features are undoubtedly helpful to small faces .",0
6266,"First , faces that are small , blurred and occluded have different texture feature from the large , clear and complete ones .",0
6267,So it is rude to directly use all high - level features to enhance the performance on small faces .,0
6268,"Second , high - level features are extracted from regions with little face texture and may introduce noise information .",0
6269,"For example , in the backbone layers of our PyramidBox , the receptive field of the top two layers conv 7 2 and conv 6 2 are 724 and 468 , respectively .",0
6270,"Notice that the input size of training image is 640 , which means that the top two layers contain too much noisy context features , so they may not contribute to detecting medium and small faces .",0
6271,"Alternatively , we build the Low - level Feature Pyramid Network ( LFPN ) starting a top - down structure from a middle layer , whose receptive field should be close to the half of the input size , instead of the top layer .",0
6272,"Also , the structure of each block of LFPN , as same as FPN , one can see ( a ) for details .",0
6273,Pyramid Detection Layers .,0
6274,"We select lfpn 2 , lfpn 1 , lfpn 0 , conv fc 7 , conv 6 2 and conv 7 2 as detection layers with anchor size of 16 , 32 , 64 , 128 , 256 and 512 , respectively .",0
6275,"Here lfpn 2 , lfpn 1 and lfpn 0 are output layer of LFPN based on conv 3 3 , conv 4 3 and conv 5 3 , respectively .",0
6276,"Moreover , similar to other SSD - style methods , we use L2 normalization to rescale the norm of LFPN layers .",0
6277,Predict Layers .,0
6278,"Each detection layer is followed by a Context - sensitive Predict Module ( CPM ) , see Sec 3.2 .",0
6279,"Notice that the outputs of CPM are used for supervising pyramid anchors , see Sec. 3.3 , which approximately cover face , head and body region in our experiments .",0
6280,"The output size of the l -th CPM is w l h l cl , where w l = h l = 640 / 2 2 +l is the corresponding feature size and the channel size cl equals to 20 for l = 0 , 1 , . . . , 5 . Here the features of each channels are used for classification and regression of faces , heads and bodies , respectively , in which the classification of face need 4 ( = cpl + cn l ) channels , where cpl and cn l are max - in - out of foreground and background label respectively , satisfying",0
6281,"Moreover , the classification of both head and body need two channels , while each of face , head and body have four channels to localize .",0
6282,Pyramid Box loss layers .,0
6283,"For each target face , see in Sec. 3.3 , we have a series of pyramid anchors to supervise the task of classification and regression simultaneously .",0
6284,We design a PyramidBox Loss .,0
6285,"see Sec. 3.4 , in which we use softmax loss for classification and smooth L1 loss for regression .",0
6286,Context - sensitive Predict Module,0
6287,Predict Module .,0
6288,"In original anchor - based detectors , such as SSD and YOLO , the objective functions are applied to the selected feature maps directly .",0
6289,"As proposed in MS - CNN , enlarging the sub-network of each task can improve accuracy .",0
6290,"Recently , SSH increases the receptive field by placing a wider convolutional prediction module on top of layers with different strides , and DSSD adds residual blocks for each prediction module .",0
6291,"Indeed , both SSH and DSSD make the prediction module deeper and wider separately , so that the prediction module get the better feature to classify and localize .",0
6292,"Inspired by the Inception - ResNet , it is quite clear that we can jointly enjoy the gain of wider and deeper network .",0
6293,"We design the Context - sensitive Predict Module ( CPM ) , see ( b ) , in which we replace the convolution layers of context module in SSH by the residual - free prediction module of DSSD .",0
6294,This would allow our CPM to reap all the benefits of the DSSD module approach while remaining rich contextual information from SSH context module .,0
6295,Max - in - out .,0
6296,The conception of Maxout was first proposed by Goodfellow et al ..,0
6297,"Recently , S 3 FD [ 20 ] applied max - out background label to reduce the false positive rate of small negatives .",0
6298,"In this work , we use this strategy on both positive and negative samples .",0
6299,"Denote it as max - in - out , see ( c ) .",0
6300,"We first predict c p + c n scores for each prediction module , and then select max c p as the positive score .",0
6301,"Similarly , we choose the max score of c n to be the negative score .",0
6302,"In our experiment , we set c p = 1 and c n = 3 for the first prediction module since that small anchors have more complicated background , while c p = 3 and c n = 1 for other prediction modules to recall more faces .",0
6303,PyramidAnchors,0
6304,Recently anchor - based object detectors and face detectors have achieved remarkable progress .,0
6305,It has been proved that balanced anchors for each scale are necessary to detect small faces .,0
6306,But it still ignored the context feature at each scale because the anchors are all designed for face regions .,0
6307,"To address this problem , we propose a novel alternatively anchor method , named PyramidAnchors .",0
6308,"For each target face , PyramidAnchors generate a series of anchors corresponding to larger regions related to a face that contains more contextual information , such as head , shoulder and body .",0
6309,"We choose the layers to set such anchors by matching the region size to the anchor size , which will supervise higher - level layers to learn more representable features for lower - level scale faces .",0
6310,"Given extra labels of head , shoulder or body , we can accurately match the anchors to ground truth to generate the loss .",0
6311,"As it 's unfair to add additional labels , we implement it in a semi-supervised way under the assumption that regions with the same ratio and offset to different faces own similar contextual feature .",0
6312,"Namely , we can use a set of uniform boxes to approximate the actual regions of head , shoulder and body , as long as features from these boxes are similar among different faces .",0
6313,"For a target face localized at region target at original image , considering the anchor i , j , which means the j - th anchor at the i - th feature layer with stride size s i , we define the label of k-th pyramid - anchor by",0
6314,"fork = 0 , 1 , . . . , K , respectively , where spa is the stride of pyramid anchors .",0
6315,"anchor i , j s i denotes the corresponding region in the original image of anchor i , j , and anchor i , j s i /s pa k represents the corresponding down - sampled region by stride spa k .",0
6316,The threshold is the same as other anchor - based detectors .,0
6317,"Besides , a Pyramid Box Loss will be demonstrated in Sec. 3.4 .",0
6318,"In our experiments , we set the hyper parameter spa = 2 since the stride of adjacent prediction modules is 2 .",0
6319,"Furthermore , let threshold = 0.35 and K =",0
6320,"2 . Then label 0 , label 1 and label 2 are labels of face , head and body respectively .",0
6321,"One can see that a face would generate 3 targets in three continuous prediction modules , which represent for the face itself , the head and body corresponding to the face .",0
6322,shows an example .,0
6323,"Benefited from the PyramidBox , our face detector can handle small , blurred and partially occluded faces better .",0
6324,Notice that the pyramid anchors are generated automatically without any extra label and this semi-supervised learning help PyramidAnchors extract approximate contextual features .,0
6325,"In prediction process , we only use output of the face branch , so no additional computational cost is incurred at runtime , compared to standard anchor - based face detectors .",0
6326,Training,0
6327,"In this section , we introduce the training dataset , data augmentation , loss function and other implementation details .",0
6328,Train dataset .,0
6329,"We trained PyramidBox on 12 , 880 images of the WIDER FACE training set with color distort , random crop and horizontal flip .",0
6330,Data - anchor - sampling .,0
6331,"Data sampling is a classical subject in statistics , machine learning and pattern recognition , it achieves great development in recent years .",0
6332,"For the task of objection detection , Focus Loss address the class imbalance by reshaping the standard cross entropy loss .",0
6333,Here we utilize a data augment sample method named Data - anchor - sampling .,0
6334,"In short , data - anchor- sampling resizes train images by reshaping a random face in this image to a random smaller anchor size .",0
6335,"More specifically , we first randomly select a face of size sf ace in a sample .",0
6336,"As previously mentioned that the scales of anchors in our PyramidBox , as shown in Sec By resizing the original image with the scale s * and cropping a standard size of 640 640 containing the selected face randomly , we get the anchor -sampled train data .",0
6337,"For example , we first select a face randomly , suppose it s size is 140 , then its nearest anchor-size is 128 , then we need to choose a target size from 16 , 32 , 64 , 128 and 256 .",0
6338,"In general , assume that we select 32 , then we resize the original image by scale of 32/140 = 0.2285 .",0
6339,"Finally , by cropping a 640 640 sub - image from the last resized image containing the originally selected face , we get the sampled train data .",0
6340,"As shown in , data - anchor - sampling changes the distribution of the train data as follows :",0
6341,1 ) the proportion of small faces is larger than the large ones .,0
6342,2 ) generate smaller face samples through larger ones to increase the diversity of face samples of smaller scales .,0
6343,Pyramid Box loss .,0
6344,"As a generalization of the multi-box loss in , we employ the Pyramid Box Loss function for an image is defined as",0
6345,where the k - th pyramid - anchor loss is given by,0
6346,"( 3 ) Here k is the index of pyramid - anchors ( k = 0 , 1 , and 2 represents for face , head and body , respectively , in our experiments ) , and i is the index of an anchor and p k , i is the predicted probability of anchor i being the k - th object ( face , head or body ) .",0
6347,The ground - truth label defined by,0
6348,"For example , when k = 0 , the ground - truth label is equal to the label in Fast R - CNN , otherwise , when k ?",0
6349,"1 , one can determine the corresponding label by matching between the down - sampled anchors and ground - truth faces .",0
6350,"Moreover , t k,i is a vector representing the 4 parameterized coordinates of the predicted bounding box , and t * k , i is that of ground - truth box associated with a positive anchor , we can define it by",0
6351,where ?,0
6352,"x , k and ?",0
6353,"y ,k denote offset of shifts , s w , k and s h,k are scale factors respect to width and height respectively .",0
6354,"In our experiments , we set ? x , k = ?",0
6355,"y ,k = 0 , s w , k = s h ,k = 1 fork < 2 and ? x , 2 = 0 , ? y ,2 = t * h , s w ,2 = 7 8 , s h,2 = 1 fork =",0
6356,"2 . The classification loss L k , cls is log loss over two classes ( face vs. not face ) and the regression loss L k, reg is the smooth L 1 loss defined in .",0
6357,"The term p * k , i L k, reg means the regression loss is activated only for positive anchors and disabled otherwise .",0
6358,"The two terms are normalized with N k , cls , N k , reg , and balancing weights ? and ?",0
6359,"k fork = 0 , 1 , 2 .",0
6360,Optimization .,0
6361,"As for the parameter initialization , our PyramidBox use the pre-trained parameters from VGG16 .",0
6362,"The parameters of conv fc 67 and conv fc 7 are initialized by sub - sampling parameters from fc 6 and fc 7 of VGG16 and the other additional layers are randomly initialized with "" xavier "" in .",0
6363,"We use a learning rate of 10 ? 3 for 80 k iterations , and 10 ? 4 for the next 20 k iterations , and 10 ? 5 for the last 20 k iterations on the WIDER FACE training set with batch size 16 .",0
6364,We also use a momentum of 0.9 and a weight decay of 0.0005 .,0
6365,Experiments,0
6366,"In this section , we firstly analyze the effectiveness of our Pyramid Box through a set of experiments , and then evaluate the final model on WIDER FACE and FDDB face detection benchmarks .",0
6367,Model Analysis,0
6368,We analyze our model on the WIDER FACE validation set by contrast experiments .,0
6369,Baseline .,0
6370,Our Pyramid,0
6371,"Box shares the same architecture of S 3 FD , so we directly use it as a baseline .",0
6372,Contrast Study .,0
6373,"To better understand PyramidBox , we conduct contrast experiments to evaluate the contributions of each proposed component , from which we can get the following conclusions .",0
6374,Low - level feature pyramid network ( LFPN ) is crucial for detecting hard faces .,0
6375,"The results listed in prove that LFPN started from a middle layer , using conv fc7 in our Pyramid Box , is more powerful , which implies that features with large gap in scale may not help each other .",0
6376,The comparison between the first and forth column of indicates that LFPN increases the m AP by 1.9 % on hard subset .,0
6377,This significant improvement demonstrates the effectiveness of joining high - level semantic features with the low - level ones .,0
6378,Data - anchor - sampling makes detector easier to train .,0
6379,We employ Data - anchor - sampling based on LFPN network and the result shows that our data - anchor - sampling effectively improves the performance .,0
6380,"The mAP is increased by 0.4 % , 0.4 % and 0.6 % on easy , medium and hard subset , respectively .",0
6381,"One can see that Data - anchor - sampling works well not only for small hard faces , but also for easy and medium faces . PyramidAnchor and Pyramid",0
6382,Box loss is promising .,0
6383,"By comparing the first and last column in , one can see that PyamidAnchor effectively improves the performance , i.e. , 0.7 % , 0.6 % and 0.9 % on easy , medium and hard , respectively .",0
6384,"This dramatical improvement shows that learning contextual information is helpful to the task of detection , especially for hard faces .",0
6385,Wider and deeper context prediction module is better .,0
6386,shows that the performance of CPM is better than both DSSD module and SSH context module .,0
6387,"Notice that the combination of SSH and DSSD gains very little compared to SSH alone , which indicates that large receptive field is more important to predict the accurate location and classification .",0
6388,"In addition , by comparing the last two column of , one can find that the method of Max - in - out improves the mAP on WIDER FACE validation set about + 0.2 % ( Easy ) , + 0.3 % ( Medium ) and + 0.1 % ( Hard ) , respectively .",0
6389,"To conclude this section , we summarize our results in , from which one can see that m AP increase 2.1 % , 2.3 % and 4.7 % on easy , medium and hard subset , respectively .",0
6390,"This sharp increase demonstrates the effectiveness of proposed PyramidBox , especially for hard faces .",0
6391,Evaluation on Benchmark,0
6392,"We evaluate our PyramidBox on the most popular face detection benchmarks , including Face Detection Data Set and Benchmark ( FDDB ) and WIDER FACE .",0
6393,FDDB Dataset .,1
6394,"It has 5 , 171 faces in 2 , 845 images collected from the Yahoo !",0
6395,news website .,0
6396,We evaluate our face detector on FDDB against the other state - of - art methods .,0
6397,The PyramidBox achieves state - ofart performance and the result is shown in and .,1
6398,WIDER FACE Dataset .,1
6399,"It contains 32 , 203 images and 393 , 703 annotated faces with a high degree of variability in scale , pose and occlusion .",0
6400,"The database is split into training ( 40 % ) , validation ( 10 % ) and testing ( 50 % ) set , where both validation and test set are divided into "" easy "" , "" medium "" and "" hard "" subsets , regarding the difficulties of the detection .",0
6401,"Our PyramidBox is trained only on the training set and evaluated on both validation set and testing set comparing with the state - of - the - art face detectors , such as. presents the precision - recall curves and mAP values .",0
6402,"Our PyramidBox outperforms others across all three subsets , i.e. 0.961 ( easy ) , 0.950 ( medium ) , 0.889 ( hard ) for validation set , and 0.956 ( easy ) , 0.946 ( medium ) , 0.887 ( hard ) for testing set .",1
6403,Conclusion,0
6404,"This paper proposed a novel context - assisted single shot face detector , denoted as PyramidBox , to handle the unconstrained face detection problem .",0
6405,"We designed a novel context anchor , named PyramidAnchor , to supervise face detector to learn features from contextual parts around faces .",0
6406,"Besides , we modified feature pyramid network into a low - level feature pyramid network to combine features from high - level and high - resolution , which are effective for finding small faces .",0
6407,We also proposed a wider and deeper prediction module to make full use of joint feature .,0
6408,"In addition , we introduced Data - anchor - sampling to augment the train data to increase the diversity of train data for small faces .",0
6409,"The experiments demonstrate that our contributions lead PyramidBox to the state - of - the - art performance on the common face detection benchmarks , especially for hard faces .",0
6410,"In this section , we show the robustness of our PyramidBox algorithm by testing it on the face images having large variance in scale , blur , pose and occlusion .",0
6411,"Even in the images filled with small , blurred or partially occluded faces and big face with exaggerate expression , our PyramidBox can recall most of these faces , see .",0
6412,"Besides , the robustness of scale , occlusion , blur , and pose is respectively described in the and .",0
6413,"On the right of image , detector confidence is present to you directly by colorbar .",0
6414,Please zoom in for more details .,0
6415,title,0
6416,Recurrent Scale Approximation for Object Detection in CNN,1
6417,abstract,0
6418,"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice .",1
6419,"To address this , we devise a recurrent scale approximation ( RSA ) to compute feature map once only , and only through this map can we approximate the rest maps on other levels .",0
6420,"At the core of RSA is the recursive rolling out mechanism : given an initial map at a particular scale , it generates the prediction at a smaller scale that is half the size of input .",0
6421,"To further increase efficiency and accuracy , we ( a ) : design a scale - forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid .",0
6422,( b ) : propose a landmark retracing network ( LRN ) to trace back locations of the regressed landmarks and generate a confidence score for each landmark ; LRN can effectively alleviate false positives caused by the accumulated error in RSA .,0
6423,The whole system can be trained end - to - end in a unified CNN framework .,0
6424,Experiments demonstrate that our proposed algorithm is superior against state - of - the - art methods on face detection benchmarks and achieves comparable results for generic proposal generation .,0
6425,The source code of our system is available .,0
6426,1 .,0
6427,Introduction,0
6428,Object detection is one of the most important tasks in computer vision .,0
6429,The convolutional neural network ( CNN ) based approaches have been widely applied in object detection and recognition with promising performance .,0
6430,"To localize objects at arbitrary scales and locations in an image , we need to han -",0
6431,"Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection dle the variations caused by appearance , location and scale .",1
6432,"Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .",0
6433,"The location variations can be naturally solved via sliding windows , which can be efficiently incorporated into CNN in a fully convolutional manner .",0
6434,"However , CNN itself does not have an inherent mechanism to handle the scale variations .",0
6435,"The scale problem is often addressed via two ways , namely , multi-shot by single - scale detector and single - shot by multi-scale detector .",0
6436,"The first way , as shown in , handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection .",0
6437,"Models in such a philosophy probably have the highest recall as long as the sampling of scales is dense enough , but they suffer from high computation cost and more false positives .",0
6438,"The second way , as depicted in , forwards the image only once and then directly regresses objects at multiple scales .",0
6439,Such a scheme takes the scale variation as a black box .,0
6440,"Although more parameters and complex structures would improve the performance , the spirit of direct regression still has limitations in real - time applications , for example in face detection , the size of faces can vary from 20 20 to 1920 1080 .",0
6441,"To handle the scale variation in a CNN - based detection system in terms of both efficiency and accuracy , we are inspired by the fast feature pyramid work proposed by Dollr et al. , where a detection system using hand - crafted features is designed for pedestrian detection .",0
6442,It is found that image gradients across scales can be predicted based on natural image statistics .,0
6443,They showed that dense feature pyramids can be efficiently constructed on top of coarsely sampled feature pyramids .,0
6444,"In this paper , we extend the spirit of fast feature pyramid to CNN and go a few steps further .",0
6445,"Our solution to the feature pyramid in CNN descends from the observations of modern CNN - based detectors , including Faster - RCNN , R - FCN , SSD , YOLO and STN , where feature maps are first computed and the detection results are decoded from the maps afterwards .",0
6446,"However , the computation cost of generating feature maps becomes a bottleneck for methods using multi-scale testing and it seems not to be a neat solution to the scale variation problem .",0
6447,"To this end , our philosophy of designing an elegant detection system is that we calculate the feature pyramid once only , and only through that pyramid can we approximate the rest feature pyramids at other scales .",0
6448,The intuition is illustrated in .,0
6449,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",1
6450,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,1
6451,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,1
6452,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,1
6453,We propose two more schemes to further save the computational budget and improve the detection performance under the RSA framework .,0
6454,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,1
6455,"There are only a few scales of objects appearing in the image and hence most of the feature pyramids correspond to the background , indicating a redundancy if maps on all levels are computed .",0
6456,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,1
6457,The final score of identifying a face within an anchor is thereby revised by the LRN network .,1
6458,Such a design alleviates false positives caused by the accumulated error in the RSA unit .,0
6459,The pipeline of our proposed algorithm is shown in .,0
6460,The three components can be incorporated into a unified CNN framework and trained end - to - end .,1
6461,Experiments show that our approach is superior to other state - of the - art methods in face detection and achieves reasonable results for object detection .,0
6462,"To sum up , our contributions in this work are as follows :",0
6463,"1 ) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit ( RSA ) , which fully leverages efficiency and accuracy .",0
6464,"2 ) We propose a scale - forecast network to predict valid scales of the input , which further accelerates the detection pipeline .",0
6465,3 ) We devise a landmark retracing network to enhance the accuracy in face detection by utilizing landmark information .,0
6466,Related work,0
6467,Multi - shot by single - scale detector .,0
6468,A single - scale detector detects the target at atypical scale and can not handle features at other scales .,0
6469,An image pyramid is thus formulated and each level in the pyramid is fed into the detector .,0
6470,"Such a framework appeared in pre-deep - learning era and usually involves hand - crafted features , e.g. , HOG or SIFT , and some classifier like Adaboost , to verify whether the context at each scale contains a target object .",0
6471,"Recently , some CNN - based methods also employ such a spirit to predict the objectness and class within a sliding window at each scale .",0
6472,"In this way , the detector only handles features in a certain range of scales and the variance is taken over by the image pyramid , which could reduce the fitting difficulty for detector but potentially increase the computational cost .",0
6473,Single - shot by multi-scale detector .,0
6474,A multi-scale detector takes one shot for the image and generates detection results aross all scales .,0
6475,"RPN and YOLO have fixed size of the input scale , and proposals for all scales are generated in the final layer by using multiple classifiers .",0
6476,"However , it is not easy to detect objects in various scales based on the final feature map .",0
6477,Liu et al .,0
6478,resolved the problem via a multi - level combination of predictions from feature maps on different scales .,0
6479,And yet it still needs a large model for large receptive field for detection .,0
6480,Other works proposed to merge deep and shallow features in a conv / deconv structure and to merge boxes for objects from different scales .,0
6481,"These methods are usually faster than the single - scale detector since it only takes one shot for image , but the large - scale invariance has to be learned by an expensive feature classifier , which is unstable and heavy .",0
6482,Face detection .,0
6483,"Recent years have witnessed a performance boost in face detection , which takes advantage of the development in fully convolutional network .",0
6484,Multi - task RPN is applied to generate face confidence and landmarks together .,0
6485,Both single - scale and multi-scale strategies are introduced in these methods .,0
6486,"For example , Chen et . al propose a supervised spatial transform layer to utilize landmark information and thus enhance the quality of detector by a large margin .",0
6487,Our Algorithm,0
6488,"In this section , we depict each component of our pipeline ) in detail .",0
6489,"We first devise a scale - forecast network to predict potential scales of the input ; the RSA unit is proposed to learn and predict features at smaller scales based on the output of the scale - forecast network ; the image is fed into the landmark retracing network to detect faces of various sizes , using the scale prediction in Section 3.1 and approximation in Section 3.2 .",0
6490,The landmark retracing network stated in Section 3.3 can trace back features of regressed landmarks and generate individual confidence of each landmark to revise the final score of detecting a face .,0
6491,"At last , we discuss the superiority of our algorithm 's design over other alternatives in Section 3.4 .",0
6492,Scale - forecast Network,0
6493,We propose a scale - forecast network ( see ( a ) ) to predict the possible scales of faces given an input image of fixed size .,0
6494,The network is a half - channel version of ResNet - 18 with a global pooling at the end .,0
6495,"The output of this network is a probability vector of B dimensions , where B = 60 is the predefined number of scales .",0
6496,"Let B = { 0 , 1 , , B} denote the scale set , we define the mapping from a face size x , in the context of an image being resized to a higher dimension 2048 , to the index bin B as :",0
6497,"For example , if the face has size of 64 , it s corresponding bin index b = 10 2 .",0
6498,"Prior to being fed into the network , an image is first resized with the higher dimension equal to 224 .",0
6499,"During training , the loss of our scale - forecast network is a binary multi-class cross entropy loss :",0
6500,"where p b , p bare the ground truth label and prediction of the b - th scale , respectively .",0
6501,Note that the ground truth label for the neighbouring scales bi of an occurring scale b * ( p b * = 1 ) is not zero and is defined as the Gaussian sampling score :,0
6502,"where , ? are hyperparameters in the Gaussian distribution and N ( ) denotes the neighbour set .",0
6503,"Here we use 2 as the neighbour size and set , ? to b * , 1 / ?",0
6504,"2 ? , respectively .",0
6505,Such a practice could alleviate the difficulty of feature learning in the discrete distribution between occurring scales ( 1 ) and non-occurring scales ( 0 ) .,0
6506,"For inference , we use the Gaussian mixture model to determine the local maximum and hence the potential occurring scales .",0
6507,"Given observations x , the distribution , parameterized by ? , can be decomposed into K mixture components :",0
6508,where the i - th component is characterized by Gaussian distributions with weights ?,0
6509,"i , means i and covariance matrices ?",0
6510,i .,0
6511,"Here K = { 1 , ... , 6 } denotes selected scale numbers of six main scales from 2 5 to 2 11 and the scale selection is determined by the threshold ?",0
6512,i of each component .,0
6513,Finally the best fitting model with a specific K is used .,0
6514,Recurrent Scale Approximation ( RSA ) Unit,0
6515,The recurrent scale approximation ( RSA ) unit is devised to predict feature maps at smaller scales given a map at the largest scale .,0
6516,depicts the RSA unit .,0
6517,"The network architecture follows a build - up similar to the residual network , where we reduce the number of channels in each convolutional layer to half of the original version for time efficiency .",0
6518,The structure details are shown in Section 4.1 .,0
6519,"Given an input image I , I m denotes the downsampled result of the image with a ratio of 1 / 2 m , where m ?",0
6520,"{ 0 , , M } is the downsample level and M =",0
6521,5 . Note that I 0 is the original image .,0
6522,"Therefore , there are six scales in total , corresponding to the six main scale ranges defined in the scale - forecast network ( see Section 3.1 ) .",0
6523,"Given an input image I m , we define the output feature map of layer res2b as :",0
6524,where f ( ) stands for a set of convolutions with a total stride of 8 from the input image to the output map .,0
6525,The set of feature maps G mat different scales serves as the ground truth supervision of the recurrent unit .,0
6526,"The RSA module RSA ( ) takes as input the feature map of the largest scale G 0 at first , and repeatedly outputs a map with half the size of the input map :",0
6527,where,0
6528,Fm is the resultant map after being rolled out m times and w represents the weights in the RSA unit .,0
6529,"The RSA module has four convolutions with a total stride of 2 ( 1 , 2 , 1 , 1 ) and their kernal sizes are ( 1 ,3 , 3 , 1 ) .",0
6530,The loss is therefore the l 2 norm between prediction Fm and supervision G m across all scales :,0
6531,The gradients in the RSA unit are computed as :,0
6532,where x and y are spatial indeces in the feature map .,0
6533,The essence behind our RSA unit is to derive a mapping RSA ( ) ? f ( ) to constantly predict smaller - scale features based on the current map instead of feeding the network with inputs of different scales for multiple times .,0
6534,"In an informal mathematical expression , we have :",0
6535,to indicate the functionality of RSA : an approximation to f ( ) from the input at the largest scale 0 to its desired level m .,0
6536,"The computation cost of generating feature map F m using RSA is much lower than that of resizing the image and feeding into the network ( i.e. , f ( I m ) through conv1 to res2b ; see quantitative results in Section 4.4 ) .",0
6537,"During inference , we first obtain the possible scales of the input from the scale - forecast network .",0
6538,"The image is then resized accordingly so that the smallest scale ( corresponding to the largest feature map ) is resized to the range of [ 64 , 128 ] .",0
6539,The feature maps at other scales are thereby predicted by the output of RSA unit via Eqn ( 6 ) .,0
6540,depicts a rolled - out version of RSA to predict feature maps of smaller scales compared with the ground truth .,0
6541,We can observe from both the error rate and predicted feature maps in each level that RSA is capable of approximating the feature maps at smaller scales .,0
6542,Landmark Retracing Network,0
6543,"In the task of face detection , as illustrated in , the landmark retracing network ( LRN ) is designed to adjust the confidence of identifying a face and to dispose of false positives by learning individual confidence of each regressed landmark .",0
6544,"Instead of directly using the ground truth location of landmarks , we formulate such a feature learning of landmarks based on the regression output of landmarks in the final RPN layer .",0
6545,"Specifically , given the feature map F at a specific scale from RSA ( m is dropped for brevity ) , we first feed it into the res3a layer .",0
6546,There are two branches at the output : one is the landmark feature set P to predict the individual score of each landmark in a spatial context .,0
6547,The number of channels in the set equals to the number of landmarks .,0
6548,Another branch is continuing the standard RPN pipeline ( res3 b - 3 c ) which generates a set of anchors in the final RPN layer .,0
6549,"Let pi = [ p i 0 , p i 1 , , p ik , ] denote the classification probability in the final RPN layer , where k is the class index and i is the spatial location index on the map ; t ij denotes the regression target ( offset defined in ) of the j - th landmark in the i - th anchor , where j = { 1 , , 5 } is the landmark index .",0
6550,"Note that in face detection task , we only have one anchor so that pi contains one element .",0
6551,"In the traditional detection - to - landmark formulation , the following loss , which consists of two heads ( i.e. , classification and regression ) , is optimized :",0
6552,"where ? ( ) is the indicator function ; k * denotes the correct label of anchor i and we have only two classes here ( 0 for background , 1 for positive ) ; t * i is the ground truth regression target and S ( ) is the smoothing l 1 loss defined in .",0
6553,"However , as illustrated in , using the confidence of anchor p ik * alone results in false positives in some cases , which inspires us to take advantage of the landmark features based on the regression output .",0
6554,"The revised classification output , p trace ik * ( t ij ) , now considers both the feature in the final RPN layer as well as those in the landmark feature set :",0
6555,where p land ij is the classification output of point j from the landmark feature set P and it is determined by the regression output :,0
6556,where r ( ) stands for a mapping from the regression target to the spatial location on map P .,0
6557,"To this end , we have the revised loss for our landmark retracing network :",0
6558,"( 11 ) Apart from the detection - to - landmark design as previous work did , our retracing network also fully leverages the feature set of landmarks to help rectify the confidence of identifying a face .",0
6559,This is achieved by utilizing the regression output t ij to find the individual score of each landmark on the preceding feature map P. Such a scheme is in a landmark - to - detection spirit .,0
6560,Note that the landmark retracing network is trained endto - end with the RSA unit stated previously .,0
6561,The anchor associated with each location i is a square box of fixed size 64 ?,0
6562,2 . The landmark retracing operation is performed only when the anchor is a positive sample .,0
6563,The base landmark location with respect to the anchor is determined by the average location of all faces in the training set .,0
6564,"During test , LRN is fed with feature maps at various scales and it treats each scale individually .",0
6565,The final detection result is generated after performing NMS among results from multi-scales .,0
6566,Discussion,0
6567,Comparison to RPN .,0
6568,The region proposal network takes a set of predefined anchors of different sizes as input and conducts a similar detection pipeline .,0
6569,Anchors in RPN vary in size to meet the multi-scale training constraint .,0
6570,"During one iteration of update , it has to feed the whole image of different sizes ( scales ) from the start to the very end of the network .",0
6571,"In our framework , we resize the image once to make sure at least one face falls into the size of [ 64 , 128 ] , thus enforcing the network to be trained within a certain range of scales .",0
6572,"In this way , we are able to use only one anchor of fixed size .",0
6573,"The multi-scale spirit is embedded in an RSA unit , which directly predicts the feature maps at smaller scales .",0
6574,Such a scheme saves parameters significantly and could be considered as a ' semi ' multi-scale training and ' fully ' multi-scale test .,0
6575,Prediction - supervised or GT - supervised in landmark feature sets .,0
6576,Another comment on our framework is the supervision knowledge used in training the landmark features P .,0
6577,The features are learned using the prediction output of regression targets t ij instead of the ground truth targets t * ij .,0
6578,"In our preliminary experiments , we find that if p land i ? t * i , the activation in the landmark features would be heavily suppressed due to the misleading regression output by t ij ; however , if we relax the learning restriction and accept activations within a certain range of misleading locations , i.e. , p land i ? ti , the performance can be boosted further .",0
6579,Using the prediction of regression as supervision in the landmark feature learning makes sense since : ( a ) we care about the activation ( classification probability ) rather than the accurate location of each landmark ; ( b ) ti and p land i share similar learning workflow and thus the location oft i could better match the activation p land i in P.,0
6580,Experiments,0
6581,In this section we first conduct the ablation study to verify the effectiveness of each component in our method and compare exhaustively with the baseline RPN ; then we compare our algorithm with state - of - the - art methods in face detection and object detection on four popular benchmarks .,0
6582,Setup and Implementation Details,0
6583,Annotated Faces in the Wild ( AFW ) contains 205 images for evaluating face detectors ' performance .,0
6584,"However , some faces are missing in the annotations and could trigger the issue of false positives , we relabel those missing faces and report the performance difference in both cases .",0
6585,Face Detection,0
6586,"Data Set and Benchmark ( FDDB ) has 5,171 annotated faces in 2,845 images .",0
6587,It is larger and more challenging than AFW .,0
6588,"Multi - Attribute Labelled Faces ( MALF ) includes 5,250 images with 11,931 annotated faces collected from the Internet .",0
6589,The annotation set is cleaner than that of AFW and it is the largest benchmark for face detection .,0
6590,"Our training set has 184K images , including 171K images collected from the Internet and 12.9 K images from the training split of Wider Face Dataset .",0
6591,All faces are labelled with bounding boxes and five landmarks .,0
6592,"The structure of our model is a shallow version of the ResNet where the first seven ResNet blocks are used , i.e. , from conv1 to res3c .",1
6593,We use this model in scale - forecast network and LRN .,1
6594,"All numbers of channels are set to half of the original ResNet model , for the consideration of time efficiency .",1
6595,We first train the scale - forecast network and then use the output of predicted scales to launch the RSA unit and LRN .,1
6596,Note that the whole system ( RSA + LRN ) is trained end - to - end and the model is trained from scratch without resorting to a pretrained model since the number of channels is halved .,0
6597,The ratio of the positive and the negative is 1 : 1 in all experiments .,1
6598,"The batch size is 4 ; base learning rate is set to 0.001 with a decrease of 6 % every 10,000 iterations .",1
6599,"The maximum training iteration is 1,000,000 .",1
6600,We use stochastic gradient descent as the optimizer .,1
6601,The scale - forecast network is of vital importance to the computational cost and accuracy in the networks afterwards .,1
6602,reports the overall recall with different numbers of predicted scales on three benchmarks .,0
6603,"Since the number of faces and the number of potential scales in the image vary across datasets , we use the number of predicted scales per face ( x , total predicted scales over total number of faces ) and a global recall ( y , correct predicted scales overall ground truth scales ) as the evaluation metric .",0
6604,"We can observe from the results that our trained scale network recalls almost 99 % at x = 1 , indicating that on average we only need to generate less than two predictions per image and that we can retrieve all face scales .",1
6605,Based on this prior final feature .,0
6606,Investigation on the source layer to branch out the RSA unit .,0
6607,"For each case , we report the error rate v.s. the level of down - sampling ratio in the unit .",0
6608,"We can conclude that the deeper the RSA is branched out , the worse the feature approximation at smaller scales will be .",1
6609,"knowledge , during inference , we set the threshold for predicting potential scales of the input so that it has approximately two predictions .",0
6610,investigates the effect of appending the RSA unit to different layers .,0
6611,"For each case , the error rate between the ground truth and corresponding prediction is computed .",0
6612,We define the error rate ( ER ) on level m as :,0
6613,Performance of Scale - forecast Network,0
6614,Ablative Evaluation on RSA Unit,0
6615,where '. /' implies an element - wise division between maps ; N is the total number of samples .,0
6616,We use a separate validation set to conduct this experiment .,0
6617,"The image is first resized to higher dimension being 2048 and the RSA unit predicts six scales defined in Section 3.1 ( 1024 , 512 , 256 , 128 and 64 ) .",0
6618,Ground truth maps are generated accordingly as we iteratively resize the image ( see ) .,0
6619,"There are two remarks regarding the result : First , feature depth matters .",0
6620,Theoretically RSA can handle all scales of features in a deep CNN model and therefore can be branched out at any depth of the network .,1
6621,"However , results from the figure indicate that as we plug RSA at deeper layers , its performance decades .",0
6622,"Since features at deeper layers are more sparse and abstract , they barely contain information for RSA to approximate the features at smaller scales .",0
6623,"For example , in case final feature which means RSA is plugged at the final convolution layer after res3c , the error rate is almost 100 % , indicating RSA 's incapability of handling the insufficient information in this layer .",0
6624,The error rate decreases in shallower cases .,0
6625,"However , the computation cost of RSA at shallow layers is much higher than that at deeper layers , since the stride is .",0
6626,The proposed algorithm is more computationally efficient and accurate by design than baseline RPN .,0
6627,"Theoretical operations of each component are provided , denoted as ' Opts. ( VGA input ) ' below .",0
6628,The minimum operation in each component means only the scaleforecast network is used where no face appears in the image ; and the maximum operation indicates the amount when faces appear at all scales .,1
6629,The actual runtime comparison between ours and baseline RPN is reported in smaller and the input map of RSA is thus larger .,0
6630,The path during one - time forward from image to the input map right before RSA is shorter ; and the rolling out time increases accordingly .,0
6631,"Therefore , the trade - off is that we want to plug RSA at shallow layers to ensure a low error rate and at the same time , to save the computational cost .",0
6632,In practice we choose case res2b to be the location where RSA is branched out .,0
6633,Most of the computation happens before layer res2 b and it has an acceptable error rate of 3.44 % .,1
6634,We use this setting throughout the following experiments .,0
6635,"Second , butterfly effect exists .",0
6636,"For a particular case , as the times of the recurrent operation increase , the error rate goes up due to the cumulative effect of rolling out the predictions .",1
6637,"For example , in case res2b , the error rate is 3.44 % at level m = 1 and drops to 5.9 % after rolling out five times .",0
6638,Such an increase is within the tolerance of the system and still suffices the task of face detection .,0
6639,Our Algorithm vs. Baseline RPN,0
6640,"We compare our model ( denoted as RSA + LRN ) , a combination of the RSA unit and a landmark retracing network , with the region proposal network ( RPN ) .",0
6641,"In the first setting , we use the original RPN with multiple anchors ( denoted as RPN m ) to detect faces of various scales .",0
6642,"In the second setting , we modify the number of anchors to one ( denoted as RPN s ) ; the anchor can only detect faces in the range from 64 to 128 pixels .",0
6643,"To capture all faces , it needs to take multiple shots in an image pyramid spirit .",0
6644,The network structurse of both baseline RPN and our LRN descend from ResNet - 18 .,0
6645,"Anchor sizes in the first setting RPN mare 32 ? 2 , 64 ? 2 , , 1024 ?",0
6646,"2 and they are responsible for detecting faces in the range of , [ 64 , 128 ) , , [ 1024 , 2048 ] , respectively .",0
6647,"In the second setting RPN s , we first resize the image length to 64 , 256 , , 2048 , then test each scale individually and merge all results through NMS .",0
6648,shows the theoretical computation cost and test performance of our algorithm compared with baseline RPN .,0
6649,"We can observe that RPN s needs six shots for the same image during inference and thus the computation cost is much larger than ours or RPN m ; Moreover , RPN m performs worse than the rest two for two reasons :",0
6650,"First , the receptive field is less than 500 and therefore it can not seethe context of faces larger than 500 pixels ; second , it is hard for the network ( it s model capacity much less than the original ResNet ) to learn the features of faces in a wide scale range from 32 to 2048 .",0
6651,depicts the runtime comparison during test .,0
6652,The third column LRN means without using the RSA unit .,0
6653,Our method runs fast enough compared with its counterparts for two reasons .,0
6654,"First , there are often one or two valid scales in the image , and the scale - forecast network can automatically select some particular scales , and ignore all the other invalid ones in the multi -scale test stage ; second , the input of LRN descends from the output of RSA to predict feature maps at smaller scales ; it is not necessary to compute feature maps of multiple scales in a multi- shot manner as RPN m does .",0
6655,shows the comparison against other approaches on three benchmarks .,0
6656,"On AFW , our algorithm achieves an AP of 99.17 % using the original annotation and an AP of 99 . 96 % using the revised annotation 7 ( c ) .",0
6657,"On FDDB , RSA + LRN recalls 93.0 % faces with 50 false positives 7 ( a ) .",0
6658,"On MALF , our method recalls 82.4 % faces with zero false positive 7 ( d ) .",0
6659,It should be noticed that the shape and scale definition of bounding box on each benchmark varies .,0
6660,"For instance , the annotation on FDDB is ellipse while others are rectangle .",0
6661,"To address this , we learn a transformer to fit each annotation from the landmarks .",0
6662,This strategy significantly enhances performance in the continuous setting on FDDB ..,0
6663,"Our proposed model can detect faces at various scales , including the green annotations provided in AFW as well as faces marked in red that are of small sizes and not labeled in the dataset ..",0
6664,Comparison to state - of - the - art approaches on face detection benchmarks .,0
6665,"The proposed algorithm ( Scale - forecast network with RSA + LRN , tagged by LRN + RSA ) outperforms other methods by a large margin .",0
6666,"' revised gt. ' and ' original gt. ' in AFW stand for fully annotated faces by us and partially labeled annotations provided by the dataset , respectively .",0
6667,Face Detection,0
6668,RSA on Generic Object Proposal,0
6669,We now verify that the scale approximation learning by RSA unit also generalizes comparably well on the generic region proposal task .,0
6670,Region proposal detection is a basic stage for generic object detection task and is more difficult than face detection .,0
6671,ILSVRC DET is a challenging dataset for generic object detection .,0
6672,It contains more than 300K images for training and 20K images for validation .,0
6673,"We use a subset ( around 170 k images ) of the original training set for training , where each category has at most 1000 samples ; for test we use the val2 split with 9917 images .",0
6674,We choose the single anchor RPN with ResNet - 101 as the baseline .,0
6675,RSA unit is set after res3b3 .,0
6676,"The anchors are of size 128 ? 2 squared , 128256 and 256128 .",0
6677,"During training , we randomly select an object and resize the image so that the object is rescaled to .",0
6678,Scale - forecast network is also employed to predict the higher dimension of objects in the image .,0
6679,Recalls with different number of proposals are shown in .,0
6680,The original RPN setting has 18 anchors with 3 as - pect ratios and 6 scales .,0
6681,"Without loss of recall , RPN + RSA reduces around 61.05 % computation cost compared with the single - scale RPN , when the number of boxes is over 100 .",0
6682,RPN + RSA is also more efficient and recalls more objects than original RPN .,0
6683,Our model and the single - anchor RPN both perform better than the original RPN .,0
6684,This observation is in accordance with the conclusion in face detection .,0
6685,"Overall , our scheme of using RSA plus LRN competes comparably with the standard RPN method in terms of computation efficiency and accuracy .",0
6686,Conclusion,0
6687,"In this paper , we prove that deep CNN features of an image can be approximated from a large scale to smaller scales by the proposed RSA unit , which significantly accelerates face detection while achieving comparable results in object detection .",0
6688,"In order to make the detector faster and more accurate , we devise a scale - forecast network to predict the potential object scales .",0
6689,We further design a landmark retracing network to fuse global and local scale information to enhance the predictor .,0
6690,Experimental results show that our algorithm significantly outperforms state - of - the - art methods .,0
6691,Future work includes exploring RSA on generic object detection task .,0
6692,Representation approximation between video frames is also an interesting research avenue .,0
6693,title,0
6694,Selective Refinement Network for High Performance Face Detection,1
6695,abstract,0
6696,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",1
6697,"This paper presents a novel single - shot face detector , named Selective Refinement Network ( SRN ) , which introduces novel twostep classification and regression operations selectively into an anchor- based face detector to reduce false positives and improve location accuracy simultaneously .",0
6698,"In particular , the SRN consists of two modules : the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",0
6699,"The STC aims to filter out most simple negative anchors from low level detection layers to reduce the search space for the subsequent classifier , while the STR is designed to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the subsequent regressor .",0
6700,"Moreover , we design a Receptive Field Enhancement ( RFE ) block to provide more diverse receptive field , which helps to better capture faces in some extreme poses .",0
6701,"As a consequence , the proposed SRN detector achieves state - of - the - art performance on all the widely used face detection benchmarks , including AFW , PASCAL face , FDDB , and WIDER FACE datasets .",0
6702,Codes will be released to facilitate further studies on the face detection problem .,0
6703,Introduction,0
6704,"Face detection is a long - standing problem in computer vision with extensive applications including face alignment , face analysis , face recognition , etc .",0
6705,"Starting from the pioneering work of Viola - Jones , face detection has made great progress .",0
6706,"The performances on several well - known datasets have been improved consistently , even tend to be saturated .",0
6707,To further improve the performance of face detection has become a challenging issue .,0
6708,"In our opinion , there remains room for improvement in two aspects : ( a ) recall efficiency : number of false positives needs to be reduced at the high recall rates ; ( b ) location accuracy : accuracy of the bounding box location needs to be improved .",0
6709,These two problems are elaborated as follows .,0
6710,"On the one hand , the average precision ( AP ) of current face detection algorithms is already very high , but the precision is not high enough at high recall rates , e.g. , as shown *",0
6711,"These authors contributed equally to this work . The STR provides better initialization for the subsequent regressor , ( d ) which produces more accurate locations , i.e. , as the IoU threshold increases , the AP gap gradually increases .",0
6712,"in ( b ) of RetinaNet , the precision is only about 50 % ( half of detections are false positives ) when the recall rate is equal to 90 % , which we define as the low recall efficiency .",0
6713,"Reflected on the shape of the Precision - Recall curve , it has extended far enough to the right , but not steep enough .",0
6714,The reason is that existing algorithms pay more attention to pursuing high recall rate but ignore the problem of excessive false positives .,0
6715,"Analyzing with anchor-based face detectors , they detect faces by classifying and regressing a series of preset anchors , which are generated by regularly tiling a collection of boxes with different scales and aspect ratios .",0
6716,"To detect the tiny faces , e.g. , less than 16 16 pixels , it is necessary to tile plenty of small anchors over the image .",0
6717,"This can improve the recall rate yet cause the the extreme class imbalance problem , which is the culprit leading to excessive false positives .",0
6718,"To address this issue , researchers propose several solutions .",0
6719,R - CNN - like detectors ) address the class imbalance by a two - stage cascade and sampling heuristics .,0
6720,"As for single - shot detectors , RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down - weight the loss assigned to well - classified examples .",0
6721,Refine,0
6722,Det addresses this issue using a preset threshold to filter out negative anchors .,0
6723,"However , Retina",0
6724,"Net takes all the samples into account , which also leads to quite a few false positives .",0
6725,"Although Refine Det filters out a large number of simple negative samples , it uses hard negative mining in both two steps , and does not make full use of negative samples .",0
6726,"Thus , the recall efficiency of them both can be improved .",0
6727,"On the other hand , the location accuracy in the face detection task is gradually attracting the attention of researchers .",0
6728,"Although current evaluation criteria of most face detection datasets ( Jain and Learned - Miller 2010 ; do not focus on the location accuracy , the WIDER Face Challenge 1 adopts MS COCO ) evaluation criterion , which puts more emphasis on bounding box location accuracy .",0
6729,"To visualize this issue , we use different IoU thresholds to evaluate our trained face detector based on RetinaNet on the WIDER FACE dataset .",0
6730,"As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .",0
6731,"To this end , propose iterative regression during inference to improve the accuracy .",0
6732,Cascade R - CNN addresses this issue by cascading R - CNN with different IoU thresholds .,0
6733,RefineDet ) applies two - step regression to single - shot detector .,0
6734,"However , blindly adding multi-step regression to the specific task ( i.e. , face detection ) is often counterproductive .",0
6735,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .",1
6736,"The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",1
6737,"Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .",1
6738,"As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .",0
6739,"On the other hand , the design of STR draws on the cascade idea to coarsely adjust the locations and sizes of anchors ( illustrated in ( c ) ) from high levels of detection layers to provide better initialization for the subsequent regressor .",0
6740,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .",1
6741,"Extensive experiments have been conducted on AFW , PASCAL face , FDDB , and WIDER FACE benchmarks and we set a new state - of - the - art performance .",0
6742,"In summarization , we have made the following main contributions to the face detection studies :",0
6743,1 http://wider-challenge.org,0
6744,We present a STC module to filter out most simple negative samples from low level layers to reduce the classification search space .,0
6745,We design a STR module to coarsely adjust the locations and sizes of anchors from high level layers to provide better initialization for the subsequent regressor .,0
6746,We introduce a RFE module to provide more diverse receptive fields for detecting extreme - pose faces .,0
6747,"We achieve state - of - the - art results on AFW , PASCAL face , FDDB , and WIDER FACE datasets .",0
6748,Related Work,0
6749,Face detection has been a challenging research field since its emergence in the 1990s .,0
6750,"Viola and Jones pioneer to use Haar features and AdaBoost to train a face detector with promising accuracy and efficiency , which inspires several different approaches afterwards .",0
6751,"Apart from those , another important job is the introduction of Deformable Part Model ( DPM ) .",0
6752,"Recently , face detection has been dominated by the CNNbased methods .",0
6753,improves detection accuracy by training a serious of interleaved CNN models and following work proposes to jointly train the cascaded CNNs to realize end - to - end optimization .,0
6754,MTCNN ) proposes a joint face detection and alignment method using multi -task cascaded CNNs .,0
6755,Faceness formulates face detection as scoring facial parts responses to detect faces under severe occlusion .,0
6756,UnitBox introduces an IoU loss for bounding box prediction .,0
6757,EMO proposes an Expected Max Overlapping score to evaluate the quality of anchor matching .,0
6758,SAFD ) develops a scale proposal stage which automatically normalizes face sizes prior to detection .,0
6759,S 2 AP pays attention to specific scales in image pyramid and valid locations in each scales layer .,0
6760,PCN ) proposes a cascade - style structure to rotate faces in a coarseto - fine manner .,0
6761,Recent work ( Bai et al . 2018 ) designs a novel network to directly generate a clear super- resolution face from a blurry small one .,0
6762,"Additionally , face detection has inherited some achievements from generic object detectors , such as It consists of STC , STR , and RFB .",0
6763,STC uses the first - step classifier to filter out most simple negative anchors from low level detection layers to reduce the search space for the second - step classifier .,0
6764,STR applies the first - step regressor to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the second - step regressor .,0
6765,RFE provides more diverse receptive fields to better capture extreme - pose faces .,0
6766,ture .,0
6767,FAN proposes an anchorlevel attention into RetinaNet to detect the occluded faces .,0
6768,"In this paper , inspired by the multi-step classification and regression in RefineDet ) and the focal loss in RetinaNet , we develop a state - of - the - art face detector .",0
6769,Selective Refinement Network,0
6770,Network Structure,0
6771,"The overall framework of SRN is shown in , we describe each component as follows .",0
6772,Backbone .,0
6773,We adopt ResNet - 50 ) with 6 - level feature pyramid structure as the backbone network for SRN .,0
6774,"The feature maps extracted from those four residual blocks are denoted as C2 , C3 , C4 , and C5 , respectively .",0
6775,C6 and C7 are just extracted by two simple down - sample 3 3 convolution layers after C5 .,0
6776,The lateral structure between the bottom - up and the top - down pathways is the same as ) .,0
6777,"P2 , P3 , P4 , and P5 are the feature maps extracted from lateral connections , corresponding to C2 , C3 , C4 , and C5 that are respectively of the same spatial sizes , while P6 and P7 are just down - sampled by two 3 3 convolution layers after P5 .",0
6778,Dedicated Modules .,0
6779,"The STC module selects C2 , C3 , C4 , P2 , P3 , and P4 to perform two - step classification , while the STR module selects C5 , C6 , C7 , P5 , P6 , and P7 to conduct two - step regression .",0
6780,The RFE module is responsible for enriching the receptive field of features that are used to predict the classification and location of objects .,0
6781,Anchor Design .,0
6782,"At each pyramid level , we use two specific scales of anchors ( i.e. , 2S and 2 ?",0
6783,"2 S , where S represents the total stride size of each pyramid level ) and one aspect ratios ( i.e. , 1.25 ) .",0
6784,"In total , there are A = 2 anchors per level and they cover the scale range 8 ?",0
6785,362 pixels across levels with respect to the network 's input image .,0
6786,Loss Function .,0
6787,"We append a hybrid loss at the end of the deep architecture , which leverage the merits of the focal loss and the smooth L 1 loss to drive the model to focus on more hard training examples and learn better regression results .",0
6788,Selective Two - Step Classification,0
6789,"Introduced in RefineDet , the two - step classification is a kind of cascade classification implemented through a two - step network architecture , in which the first step filters out most simple negative anchors using a preset negative threshold ? =",0
6790,0.99 to reduce the search space for the subsequent step .,0
6791,"For anchor - based face detectors , it is necessary to tile plenty of small anchors over the image to detect small faces , which causes the extreme class imbalance between the positive and negative samples .",0
6792,"For example , in the SRN structure with the 1024 1024 input resolution , if we tile 2 anchors at each anchor point , the total number of samples will reach 300 k .",0
6793,"Among them , the number of positive samples is only a few dozen or less .",0
6794,"To reduce search space of classifier , it is essential to do two - step classification to reduce the false positives .",0
6795,"However , it is unnecessary to perform two - step classification in all pyramid levels .",0
6796,"Since the anchors tiled on the three higher levels ( i.e. , P5 , P6 , and P7 ) only account for 11.1 % and the associated features are much more adequate .",0
6797,"Therefore , the classification task is relatively easy in these three higher pyramid levels .",0
6798,"It is thus dispensable to apply two - step classification on the three higher pyramid levels , and if applied , it will lead to an increase in computation cost .",0
6799,"In contrast , the three lower pyramid levels ( i.e. , P2 , P3 , and P4 ) have the vast majority of samples ( 88.9 % ) and lack of adequate features .",0
6800,It is urgently needed for these low pyramid levels to do two - step classification in order to alleviate the class imbalance problem and reduce the search space for the subsequent classifier .,0
6801,"Therefore , our STC module selects C2 , C3 , C4 , P2 , P3 , and P4 to perform two - step classification .",0
6802,"As the statistical result shown in , the STC increases the positive / negative sample ratio by approximately 38 times , from around 1:15441 to 1:404 .",0
6803,"In addition , we use the focal loss in both two steps to make full use of samples .",0
6804,Unlike Re-fine,0
6805,"Det , the SRN shares the same classification module in the two steps , since they have the same task to distinguish the face from the background .",0
6806,The experimental results of applying the two - step classification on each pyramid level are shown in .,0
6807,"Consistent with our analysis , the two - step classification on the three lower pyramid levels helps to improve performance , while on the three higher pyramid levels is ineffective .",0
6808,"The loss function for STC consists of two parts , i.e. , the loss in the first step and the second step .",0
6809,"For the first step , we calculate the focal loss for those samples selected to perform two - step classification .",0
6810,"And for the second step , we just focus on those samples that remain after the first step filtering .",0
6811,"With these definitions , we define the loss function as :",0
6812,"where i is the index of anchor in a mini-batch , pi and q i are the predicted confidence of the anchor i being a face in the first and second steps , l * i is the ground truth class label of anchor i , N s 1 and N s 2 are the numbers of positive anchors in the first and second steps , ?",0
6813,"represents a collection of samples selected for two - step classification , and ?",0
6814,represents a sample set that remains after the first step filtering .,0
6815,The binary classification loss L FL is the sigmoid focal loss over two classes ( face vs. background ) .,0
6816,Selective Two - Step Regression,0
6817,"In the detection task , to make the location of bounding boxes more accurate has always been a challenging problem .",0
6818,"Current one - stage methods rely on one - step regression based on various feature layers , which is inaccurate in some challenging scenarios , e.g. , MS COCO - style evaluation standard .",0
6819,"In recent years , using cascade structure to conduct multi-step regression is an effective method to improve the accuracy of the detection bounding boxes .",0
6820,"However , blindly adding multi-step regression to the specific task ( i.e. , face detection ) is often counterproductive .",0
6821,Experimental results ( see ) indicate that applying two - step regression in the three lower pyramid levels impairs the performance .,0
6822,The reasons behind this phenomenon are twofold :,0
6823,1 ) the three lower pyramid levels are associated with plenty of small anchors to detect small faces .,0
6824,"These small faces are characterized by very coarse feature representations , so it is difficult for these small anchors to perform two - step regression ; 2 ) in the training phase , if we let the network pay too much attention to the difficult regression task on the low pyramid levels , it will cause larger regression loss and hinder the more important classification task .",0
6825,"Based on the above analyses , we selectively perform twostep regression on the three higher pyramid levels .",0
6826,The motivation behind this design is to sufficiently utilize the detailed features of large faces on the three higher pyramid levels to regress more accurate locations of bounding boxes and to make three lower pyramid levels pay more attention to the classification task .,0
6827,This divide - and - conquer strategy makes the whole framework more efficient .,0
6828,"The loss function of STR also consists of two parts , which is shown as below :",0
6829,"where g * i is the ground truth location and size of anchor i , x i is the refined coordinates of the anchor i in the first step , ti is the coordinates of the bounding box in the second step , ?",0
6830,"represents a collection of samples selected for two - step regression , l * i and ?",0
6831,are the same as defined in STC .,0
6832,"Similar to Faster R - CNN ( Ren et al. 2017 ) , we use the smooth L 1 loss as the regression loss Lr . The Iverson bracket indicator function [ l * i = 1 ] outputs 1 when the condition is true , i.e. , l * i = 1 ( the anchor is not the negative ) , and 0 otherwise .",0
6833,Hence [l * i = 1 ] L r indicates that the regression loss is ignored for negative anchors .,0
6834,Receptive Field Enhancement,0
6835,"At present , most detection networks utilize ResNet and VG - GNet as the basic feature extraction module , while both of them possess square receptive fields .",0
6836,The singleness of the receptive field affects the detection of objects with different aspect ratios .,0
6837,"This issue seems unimportant in face detection task , because the aspect ratio of face annotations is about 1:1 in many datasets .",0
6838,"Nevertheless , statistics shows that the WIDER FACE training set has a considerable part of faces that have an aspect ratio of more than 2 or less than 0.5 .",0
6839,"Consequently , there is mismatch between the receptive field of network and the aspect ratio of faces .",0
6840,"To address this issue , we propose a module named Receptive Field Enhancement ( RFE ) to diversify the receptive field of features before predicting classes and locations .",0
6841,"In particular , RFE module replaces the middle two convolution layers in the class subnet and the box subnet of RetinaNet .",0
6842,The structure of RFE is shown in .,0
6843,"Our RFE module adopts a four - branch structure , which is inspired by the Inception block .",0
6844,"To be specific , first , we use a 11 convolution layer to decrease the channel number to one quarter of the previous layer .",0
6845,"Second , we use 1 k and k 1 ( k = 3 and 5 ) convolution layer to provide rectangular receptive field .",0
6846,"Through another 1 1 convolution layer , the feature maps from four branches are concatenated together .",0
6847,"Additionally , we apply a shortcut path to retain the original receptive field from previous layer .",0
6848,Training and Inference,0
6849,Training Dataset .,0
6850,All the models are trained on the training set of the WIDER FACE dataset .,0
6851,"It consists of 393 , 703 annotated face bounding boxes in 32 , 203 images with variations in pose , scale , facial expression , occlusion , and lighting condition .",0
6852,"The dataset is split into the training ( 40 % ) , validation ( 10 % ) and testing ( 50 % ) sets , and defines three levels of difficulty : Easy , Medium , Hard , based on the detection rate of EdgeBox ( Zitnick and Dollr 2014 ) .",0
6853,Data Augmentation .,0
6854,"To prevent over-fitting and construct a robust model , several data augmentation strategies are used to adapt to face variations , described as follows .",0
6855,1 ) Applying some photometric distortions introduced in previous work to the training images .,0
6856,"2 ) Expanding the images with a random factor in the interval [ 1 , 2 ] by the zero - padding operation .",0
6857,3 ) Cropping two square patches and randomly selecting one for training .,0
6858,"One patch is with the size of the image 's shorter side and the other one is with the size determined by multiplying a random number in the interval [ 0.5 , 1.0 ] by the image 's shorter side .",0
6859,4 ) Flipping the selected patch randomly and resizing it to 1024 1024 to get the final training sample .,0
6860,Anchor Matching .,0
6861,"During the training phase , anchors need to be divided into positive and negative samples .",0
6862,"Specifically , anchors are assigned to ground - truth face boxes using an intersection - over - union ( IoU ) threshold of ? p ; and to background if their IoU is in [ 0 , ? n ) .",0
6863,"If an anchor is unassigned , which may happen with overlap in [ ? n , ? p ) , it is ignored during training .",0
6864,"Empirically , we set ? n = 0.3 and ? p = 0.7 for the first step , and ? n = 0.4 and ? p =",0
6865,0.5 for the second step .,0
6866,Optimization .,0
6867,"The loss function for SRN is just the sum of the STC loss and the STR loss , i.e. , L = L STC + L STR .",1
6868,"The backbone network is initialized by the pretrained ResNet - 50 model and all the parameters in the newly added convolution layers are initialized by the "" xavier "" method .",1
6869,"We fine - tune the SRN model using SGD with 0.9 momentum , 0.0001 weight decay , and batch size 32 .",1
6870,"We set the learning rate to 10 ?2 for the first 100 epochs , and decay it to 10 ? 3 and 10 ? 4 for another 20 and 10 epochs , respectively .",1
6871,We implement SRN using the Py - Torch library .,1
6872,Inference .,0
6873,"In the inference phase , the STC first filters the regularly tiled anchors on the selected pyramid levels with the negative confidence scores larger than the threshold ? = 0.99 , and then STR adjusts the locations and sizes of selected anchors .",0
6874,"After that , the second step takes over these refined anchors , and outputs top 2000 high confident detections .",0
6875,"Finally , we apply the non-maximum suppression ( NMS ) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results .",0
6876,Experiments,0
6877,We first analyze the proposed method in detail to verify the effectiveness of our contributions .,0
6878,"Then we evaluate the final model on the common face detection benchmark datasets , including AFW ( Zhu and Ramanan 2012 ) , PAS - CAL Face , FDDB ( Jain and Learned - Miller 2010 ) , and WIDER FACE ) .",0
6879,Model Analysis,0
6880,We conduct a set of ablation experiments on the WIDER FACE dataset to analyze our model in detail .,0
6881,"For a fair comparison , we use the same parameter settings for all the experiments , except for specified changes to the components .",0
6882,All models are trained on the WIDER FACE training set and evaluated on the validation set .,0
6883,Ablation Setting .,0
6884,"To better understand SRN , we ablate each component one after another to examine how each proposed component affects the final performance .",0
6885,"Firstly , we use the ordinary prediction head in ) instead of the proposed RFE .",0
6886,"Secondly , we ablate the STR or STC module to verity their effectiveness .",0
6887,The results of ablation experiments are listed in and some promising conclusions can be drawn as follows .,0
6888,Selective Two - step Classification .,0
6889,"Experimental results of applying two - step classification to each pyramid level are shown in , indicating that applying two - step classification to the low pyramid levels improves the performance , especially on tiny faces .",0
6890,"Therefore , the STC module selectively applies the two - step classification on the low pyramid levels ( i.e. , P2 , P3 , and P4 ) , since these levels are associated with lots of small anchors , which are the main source of false positives .",0
6891,"As shown in , we find that after using the STC module , the AP scores of the detector are improved from 95.1 % , 93.9 % and 88.0 % to 95.3 % , 94.4 % and In order to verify whether the improvements benefit from reducing the false positives , we count the number of false positives under different recall rates .",0
6892,"As listed in , our STC effectively reduces the false positives across different recall rates , demonstrating the effectiveness of the STC module . Selective Two - step Regression .",0
6893,We only add the STR module to our baseline detector to verify its effectiveness .,0
6894,"As shown in , it produces much better results than the baseline , with 0.8 % , 0.9 % and 0.8 % AP improvements on the Easy , Medium , and Hard subsets .",0
6895,Experimental results of applying two - step regression to each pyramid level ( see ) confirm our previous analysis .,0
6896,"Inspired by the detection evaluation metric of MS COCO , we use 4 IoU thresh - olds { 0.5 , 0.6 , 0.7 , 0.8 } to compute the AP , so as to prove that the STR module can produce more accurate localization .",0
6897,"As shown in , the STR module produces consistently accurate detection results than the baseline method .",0
6898,"The gap between the AP across all three subsets increases as the IoU threshold increases , which indicate that the STR module is important to produce more accurate detections .",0
6899,"In addition , coupled with the STC module , the performance is further improved to 96.1 % , 95.0 % and 90.1 % on the Easy , Medium and Hard subsets , respectively .",0
6900,Receptive Field Enhancement .,0
6901,The RFE is used to diversify the receptive fields of detection layers in order to capture faces with extreme poses .,0
6902,"Comparing the detection results between fourth and fifth columns in , we notice that RFE consistently improves the AP scores in different subsets , i.e. , 0.3 % , 0.3 % , and 0.1 % APs on the Easy , Medium , and Hard categories .",0
6903,"These improvements can be mainly attributed to the diverse receptive fields , which is useful to capture various pose faces for better detection accuracy .",0
6904,Evaluation on Benchmark,0
6905,AFW Dataset .,1
6906,It consists of 205 images with 473 labeled faces .,0
6907,The images in the dataset contains cluttered backgrounds with large variations in both face viewpoint and appearance .,0
6908,"We compare SRN against seven state - of the - art methods and three commercial face detectors ( i.e. , Face.com , Face + + and Picasa ) .",0
6909,"As shown in , SRN outperforms these state - of - the - art methods with the top AP score ( 99.87 % ) .",1
6910,PASCAL Face Dataset .,1
6911,"It has 1 , 335 labeled faces in 851 images with large face appearance and pose variations .",0
6912,"We present the precision - recall curves of the proposed SRN method and six state - of - the - art methods and three commercial face detectors ( i.e. , SkyBiometry , Face + + and Picasa ) in .",0
6913,SRN achieves the state - of - the - art results by improving 4.99 % AP score compared to the second best method STN .,1
6914,FDDB Dataset .,1
6915,"It contains 5 , 171 faces annotated in 2 , 845 images with a wide range of difficulties , such as occlusions , difficult poses , and low image resolutions .",0
6916,We evaluate the proposed SRN detector on the FDDB dataset and compare it with several state - of - the - art methods .,0
6917,"As shown in ( c ) , our SRN sets a new state - of - the - art performance , i.e. , 98.8 % true positive rate when the number of false positives is equal to 1000 .",1
6918,"These results indicate that SRN is robust to varying scales , large appearance changes , heavy occlusions , and severe blur degradations that are prevalent in detecting face in unconstrained real - life scenarios .",0
6919,WIDER FACE Dataset .,1
6920,We compare SRN with eighteen state - of - the - art face detection methods on both the validation and testing sets .,0
6921,"To obtain the evaluation results on the testing set , we submit the detection results of SRN to the authors for evaluation .",0
6922,"As shown in , we find that SRN performs favourably against the state - of - the - art based on the average precision ( AP ) across the three subsets , especially on the Hard subset which contains a large amount of small faces .",1
6923,"Specifically , it produces the best AP scores in all subsets of both validation and testing sets , i.e. , 96.4 % ( Easy ) , 95.3 % ( Medium ) and 90.2 % ( Hard ) for validation set , and 95.9 % ( Easy ) , 94.9 % ( Medium ) and 89.7 % ( Hard ) for testing set , surpassing all approaches , which demonstrates the superiority of the proposed detector .",0
6924,Conclusion,0
6925,"In this paper , we have presented SRN , a novel single shot face detector , which consists of two key modules , i.e. , the STC and the STR .",0
6926,"The STC uses the first - step classifier to filter out most simple negative anchors from low level detection layers to reduce the search space for the second - step classifier , so as to reduce false positives .",0
6927,"And the STR applies the first - step regressor to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the second - step regressor , in order to improve the location accuracy of bounding boxes .",0
6928,"Moreover , the RFE is introduced to provide diverse receptive fields to better capture faces in some extreme poses .",0
6929,"Extensive experiments on the AFW , PASCAL face , FDDB and WIDER FACE datasets demonstrate that SRN achieves the state - of - the - art detection performance .",0
6930,title,0
6931,FaceBoxes : A CPU Real - time Face Detector with High Accuracy,0
6932,abstract,0
6933,"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive .",1
6934,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .",0
6935,"Specifically , our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .",0
6936,The RDCL is designed to enable Face - Boxes to achieve real - time speed on the CPU .,0
6937,The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales .,0
6938,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the image , which significantly improves the recall rate of small faces .",0
6939,"As a consequence , the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA - resolution images .",0
6940,"Moreover , the speed of FaceBoxes is invariant to the number of faces .",0
6941,"We comprehensively evaluate this method and present stateof - the - art detection performance on several face detection benchmark datasets , including the AFW , PASCAL face , and FDDB .",0
6942,Code is available at https://github.com/sfzhang15/FaceBoxes .,1
6943,Introduction,0
6944,Face detection is one of the fundamental problems in computer vision and pattern recognition .,0
6945,"It plays an important role in many subsequent face - related applications , such as face alignment , face recognition and face tracking .",0
6946,"With the great progress over the past few decades , especially the breakthrough of convolutional neural network , face detection has been successfully applied in our daily life under various scenarios .",0
6947,"However , there are still some tough challenges in uncontrolled face detection problem , especially for the CPU devices .",0
6948,The challenges mainly come from two requirements for face detectors :,0
6949,1 ) The large visual variation of faces in the cluttered backgrounds requires face detectors to accurately address a complicated face and non-face classification problem ; 2 ) The large search space of possible face positions and face sizes further imposes a time efficiency requirement .,0
6950,"These two requirements are conflicting , since high - accuracy face detectors tend to be computationally expensive .",0
6951,"Therefore , it is one of the remaining open issues for practical face detectors on the CPU devices to achieve real - time speed as well as maintain high performance .",0
6952,"In order to meet these two conflicting requirements , face detection has been intensely studied mainly in two ways .",0
6953,The early way is based on hand - craft features .,0
6954,"Following the pioneering work of Viola - Jones face detector , most of the early works focus on designing robust features and training effective classifiers .",0
6955,"Besides the cascade structure , the deformable part model ( DPM ) is introduced into face detection tasks and achieves remarkable performance .",0
6956,"However , these methods highly depend on nonrobust hand - craft features and optimize each component separately , making the face detection pipeline sub-optimal .",0
6957,"In brief , they are efficient on the CPU but not accurate enough against the large visual variation of faces .",0
6958,"The other way is based on the convolutional neural network ( CNN ) which has achieved remarkable successes in recent years , ranging from image classification to object detection .",0
6959,"Recently , CNN has been successfully introduced into the face detection task as feature extractor in the traditional face detection framewrok .",0
6960,"Moreover , some face detectors have inherited valid techniques from the generic object detection methods , such as Faster R - CNN .",0
6961,These CNN based face detection methods are robust to the large variation of facial appearances and demonstrate state - of - the - art performance .,0
6962,"But they are too time - consuming to achieve real - time speed , especially on the CPU devices .",0
6963,These two ways have their own advantages .,0
6964,The former has fast speed while the latter owns high accuracy .,0
6965,"To perform well on both speed and accuracy , one natural idea is to combine the advantages of these two types of methods .",0
6966,"Therefore , cascaded CNN based methods are proposed to put features learned by CNN into cascade framework in order to boost the performance and keep efficient .",0
6967,"However , there are three problems in cascaded CNN based methods :",0
6968,1 ) Their speed is negatively related to the number of faces on the image .,0
6969,"The speed would dramatically degrade as the number of faces increases ; 2 ) The cascade based detectors optimize each component separately , making the training process extremely complicated and the final model sub-optimal ; 3 ) For the VGA - resolution images , their runtime efficiency on the CPU is about 14 FPS , which is not fast enough to reach the real - time speed .",0
6970,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .",1
6971,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .",1
6972,The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,1
6973,"The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .",1
6974,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .",1
6975,"Consequently , for VGA - resolution images , our face detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU .",0
6976,"More importantly , the speed of FaceBoxes is invariant to the number of faces on the image .",0
6977,"We comprehensively evaluate this method and demonstrate state - of - the - art detection performance on several face detection benchmark datasets , including the AFW , PASCAL face , and FDDB .",0
6978,"For clarity , the main contributions of this work can be summarized as four - fold :",0
6979,We design the Rapidly Digested Convolutional Layers ( RDCL ) to enable face detection to achieve real - time speed on the CPU ; We introduce the Multiple Scale Convolutional Layers ( MSCL ) to handle various scales of face via enriching receptive fields and discretizing anchors over layers .,0
6980,"We present a new anchor densification strategy to improve the recall rate of small faces ; We further improve the state - of - the - art performance on the AFW , PASCAL face , and FDDB datasets .",0
6981,The rest of the paper is organized as follows .,0
6982,Section 2 reviews the related work .,0
6983,Analysis of the FaceBoxes is presented in section,0
6984,3 . Section 4 shows the experimental results and section 5 concludes the paper .,0
6985,Related work,0
6986,Modern face detection approaches can be roughly divided into two different categories .,0
6987,"One is based on handcraft features , and the other one is built on CNN .",0
6988,This section briefly reviews these two kinds of methods .,0
6989,Hand - craft based methods,0
6990,Previous face detection systems are mostly based on hand - craft features .,0
6991,"Since the seminal Viola - Jones face detector that proposes to combine Haar feature , Adaboost learning and cascade inference for face detection , many subsequent works are proposed for real - time face detection , such as new local features , new boosting algorithms and new cascade structures .",0
6992,"Besides the cascade framework , methods based on structural models progressively achieve better performance and become more and more efficient .",0
6993,Some researches introduce the deformable part model ( DPM ) into face detection tasks .,0
6994,"These works use supervised parts , more pose partition , better training or more efficient inference to achieve remarkable detection performance .",0
6995,CNN based methods,0
6996,The first use of CNN for face detection can be traced back to 1994 .,0
6997,Vaillant et al.,0
6998,use a trained CNN in a sliding windows manner to detect faces .,0
6999,Rowley et al .,0
7000,"introduce a retinally connected neural network for upright frontal face detection , and a "" router "" network designed to estimate the orientation for rotation invariant face detection .",0
7001,Garcia et al.,0
7002,develop a neural network to detect semi-frontal faces .,0
7003,Osadchy et al. train a CNN for simultaneous face detection and pose estimation .,0
7004,These earlier methods can get relatively good performance only on easy dataset .,0
7005,Recent years have witnessed the advance of CNN based face detectors .,0
7006,CCF uses boosting on top of CNN features for face detection .,0
7007,Farfade et al.,0
7008,fine - tune CNN model trained on 1 k Image Net classification task for face and non-face classification task .,0
7009,Faceness trains a series of CNNs for facial attribute recognition to detect partially occluded faces .,0
7010,CascadeCNN develops a cascade architecture built on CNNs with powerful discriminative capability and high performance .,0
7011,Qin et al. propose to jointly train CascadeCNN to realize end - to - end optimization .,0
7012,"Similar to , MTCNN proposes a multi- task cascaded CNNs based framework for joint face detection and alignment .",0
7013,UnitBox introduces a new intersectionover - union loss function .,0
7014,CMS - RCNN uses Faster R - CNN in face detection with body contextual information .,0
7015,Convnet integrates CNN with 3 D face model in an endto - end multi - task learning framework .,0
7016,STN proposes a new supervised transformer network and a ROI convolution for face detection .,0
7017,FaceBoxes,0
7018,"This section presents our three contributions that make the FaceBoxes accurate and efficient on the CPU devices : the Rapidly Digested Convolutional Layers ( RDCL ) , the Multiple Scale Convolutional Layers ( MSCL ) and the anchor densification strategy .",0
7019,"Finally , we introduce the associated training methodology .",0
7020,Rapidly Digested Convolutional Layers,0
7021,"Most of the CNN based face detection methods are usually limited by the heavy cost of time , especially on the CPU devices .",0
7022,"More precisely , the convolution operation for CPU is extremely time - consuming when the size of input , kernel and output are large .",0
7023,"Our RDCL is designed to fast shrink the input spatial size by suitable kernel size with reducing the number of output channels , enabling the FaceBoxes to reach real - time speed on the CPU devices as follows :",0
7024,Shrinking the spatial size of input :,0
7025,"To rapidly shrink the spatial size of input , our RDCL sets a series of large stride sizes for its convolution and pooling layers .",0
7026,"As illustrated in , the stride size of Conv1 , Pool1 , Conv2 and Pool2 are 4 , 2 , 2 and 2 , respectively .",0
7027,"The total stride size of RDCL is 32 , which means the input spatial size is reduced by 32 times quickly .",0
7028,Choosing suitable kernel size :,0
7029,"The kernel size of the first few layers in one network should be small so as to speedup , while it is also supposed to be large enough to alleviate the information loss brought by the spatial size reducing .",0
7030,"As shown in , to keep efficient as well as effective , we choose 77 , 55 and 33 kernel size for Conv1 , Conv2 and all Pool layers , respectively .",0
7031,Reducing the number of output channels :,0
7032,We utilize the C. ReLU activation function ( illustrated in ) to reduce the number of output channels .,0
7033,"C.ReLU is motivated from the observation in CNN that the filters in the lower layers form pairs ( i.e. , filters with opposite phase ) .",0
7034,"From this observation , C. ReLU can double the number of output channels by simply concatenating negated outputs before applying ReLU .",0
7035,Using C.ReLU significantly increases speed with negligible decline inaccuracy .,0
7036,Multiple Scale Convolutional Layers,0
7037,The proposed method is based on RPN which is developed as a class - agnostic proposer in the scenario of multicategory object detection .,0
7038,"For the single - category detection task ( e.g. , face detection ) , RPN is naturally a detector for the only category concerned .",0
7039,"However , as a stand - alone face detector , RPN is notable to obtain competitive performances .",0
7040,We argue that such unsatisfactory performance comes from two aspects .,0
7041,"Firstly , the anchors in the RPN are only associated with the last convolutional layer whose feature and resolution are too weak to handle faces of various sizes .",0
7042,"Secondly , an anchor-associated layer is responsible for detecting faces within a corresponding range of scales , but it only has a single receptive field that can not match different scales of faces .",0
7043,"To solve the above two problems , our MSCL is designed along the following two dimensions :",0
7044,Multi - scale design along the dimension of network depth .,0
7045,"As shown in , our designed MSCL consists of several layers .",0
7046,These layers decrease in size progressively and form the multi-scale feature maps .,0
7047,"Similar to , our default anchors are associated with multi-scale feature maps ( i.e. , Inception3 , Conv3 2 and Conv4 2 ) .",0
7048,"These layers , as a multi-scale design along the dimension of network depth , discretize anchors over multiple layers with different resolutions to naturally handle faces of various sizes .",0
7049,Multi-scale design along the dimension of network width .,0
7050,"To learn visual patterns for different scales of faces , output features of the anchor-associated layers should correspond to various sizes of receptive fields , which can be easily fulfilled via Inception modules .",0
7051,The Inception module consists of multiple convolution branches with different kernels .,0
7052,"These branches , as a multi-scale design along the dimension of network width , is able to enrich the receptive fields .",0
7053,"As shown in , the first three layers in MSCL are based on the Inception module .",0
7054,"illustrates our Inception implementation , which is a cost-effective module to capture different scales of faces .",0
7055,Anchor densification strategy,0
7056,"As illustrated in , we impose 1:1 aspect ratio for the default anchors ( i.e. , square anchor ) , because the face box is approximately square .",0
7057,"The scale of anchor for the Incep-tion3 layer is 32 , 64 and 128 pixels , for the Conv3 2 layer and Conv4 2 layer are 256 and 512 pixels , respectively .",0
7058,The tiling interval of anchor on the image is equal to the stride size of the corresponding anchor-associated layer .,0
7059,"For example , the stride size of Conv3 2 is 64 pixels and its anchor is 256 256 , indicating that there is a 256 256 anchor for every 64 pixels on the input image .",0
7060,"We define the tiling density of anchor ( i.e. , A density ) as follows :",0
7061,"Here , A scale is the scale of anchor and A interval is the tiling interval of anchor .",0
7062,"The tiling intervals for our default anchors are 32 , 32 , 32 , 64 and 128 , respectively .",0
7063,"According to Equ. ( 1 ) , the corresponding densities are 1 , 2 , 4 , 4 and 4 , where it is obviously that there is a tiling density imbalance problem between anchors of different scales .",0
7064,"Comparing with large anchors ( i.e. , 128128 , 256256 and 512512 ) , small anchors ( i.e. , 32 32 and 64 64 ) are too sparse , which results in low recall rate of small faces .",0
7065,"To eliminate this imbalance , we propose a new anchor densification strategy .",0
7066,"Specifically , to densify one type of anchors n times , we uniformly tile A number = n 2 anchors around the center of one receptive field instead of only tiling one at the center of this receptive field to predict .",0
7067,Some examples are shown in .,0
7068,"In our paper , to improve the tiling density of the small anchor , our strategy is used to densify the 32 32 anchor 4 times and the 64 64 anchor 2 times , which guarantees that different scales of anchor have the same density ( i.e. , 4 ) on the image , so that various scales of faces can match almost the same number of anchors .",0
7069,Training,0
7070,"This subsection introduces the training dataset , data augmentation , matching strategy , loss function , hard negative mining , and other implementation details .",0
7071,Training dataset .,0
7072,"Our model is trained on 12 , 880 images of the WIDER FACE training subset .",0
7073,Data augmentation .,0
7074,Each training image is sequentially processed by the following data augmentation strategies :,0
7075,Color distortion :,0
7076,Applying some photo-metric distortions similar to . Random cropping :,0
7077,"We randomly crop five square patches from the original image : one is the biggest square patch , and the size of the others range between [ 0.3 , 1 ] of the short size of the original image .",0
7078,Then we arbitrarily select one patch for subsequent operations .,0
7079,Scale transformation :,0
7080,"After random cropping , the selected square patch is resized to 1024 1024 . Horizontal flipping :",0
7081,The resized image is horizontally flipped with probability of 0.5 .,0
7082,Face - box filter :,0
7083,"We keep the overlapped part of the face box if its center is in the above processed image , then filter out these face boxes whose height or width is less than 20 pixels .",0
7084,Matching strategy .,0
7085,"During training , we need to determine which anchors correspond to a face bounding box .",0
7086,"We first match each face to the anchor with the best jaccard overlap , and then match anchors to any face with jaccard overlap higher than a threshold ( i.e. , 0.35 ) .",0
7087,Loss function .,0
7088,Our loss function is the same as RPN in Faster R - CNN .,0
7089,We adopt a 2 - class softmax loss for classification and the smooth L1 loss for regression .,0
7090,Hard negative mining .,0
7091,"After the anchor matching step , most of the anchors are found to be negative , which introduces a significant imbalance between the positive and negative examples .",0
7092,"For faster optimization and stable training , we sort them by the loss values and pick the top ones so that the ratio between the negatives and positives is at most 3:1 .",0
7093,Other implementation details .,0
7094,"All the parameters are randomly initialized with the "" xavier "" method .",0
7095,"We finetune the resulting model using SGD with 0.9 momentum , 0.0005 weight decay and batch size 32 .",0
7096,"The maximum number of iterations is 120 k and we use 10 ? 3 learning rate for the first 80 k iterations , then continue training for 20 k iterations with 10 ? 4 and 10 ? 5 , respectively .",0
7097,Our method is implemented in the Caffe library .,0
7098,Experiments,0
7099,"In this section , we firstly introduce the runtime efficiency of FaceBoxes , then analyze our model in an ablative way , finally evaluate it on the common face detection benchmarks .",0
7100,Runtime efficiency,0
7101,CNN based methods have always been accused of its runtime efficiency .,0
7102,"Although the existing CNN face detectors can be accelerated via high - end GPUs , they are not fast enough inmost practical applications , especially CPU based applications .",0
7103,"As described below , our FaceBoxes is efficient enough to meet practical requirements .",0
7104,"During inference , our method outputs a large number of boxes ( e.g. , 8 , 525 boxes fora VGA-resolution image ) .",0
7105,"We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS , then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes .",1
7106,We measure the speed using Titan X ( Pascal ) and cuDNN v 5.1 with Intel Xeon E5-2660v3@2.60 GHz .,1
7107,"As listed in Tab. 1 , comparing with recent CNN - based methods , our FaceBoxes can run at 20 FPS on the CPU with state - of - the - art accuracy .",0
7108,"Besides , our method can run at 125 FPS using a single GPU and has only 4.1 MB in size. , it s m AP is the true positive rate at 179 false positives and with ROI convolution , its FPS can be accelerated to 30 with 0.6 % recall rate drop .",0
7109,Approach,0
7110,Model analysis,0
7111,We carried out extensive ablation experiments on the FDDB dataset to analyze our model .,0
7112,"Comparing with AFW and PASCAL face , FDDB is much more difficult so that analyzing our model on FDDB is convincing .",0
7113,"For all the experiments , we use the same settings , except for specified changes to the components .",0
7114,Ablative Setting .,0
7115,"To better understand FaceBoxes , we ablate each component one after another to examine how each proposed component affects the final performance .",0
7116,"1 ) Firstly , we ablate the anchor densification strategy .",0
7117,"2 ) Then , we replace MSCL with three convolutional layers , which all have 3 3 kernel size and whose output number is the same as the first three Inception modules of MSCL .",0
7118,"Meantime , we only associate the anchors with the last convolutional layer .",0
7119,"3 ) Finally , we take the place of C. ReLU with ReLU in RDCL .",0
7120,The ablative results are listed in Tab .,0
7121,2 and some promising conclusions can be summed up as follows :,0
7122,Contribution,0
7123,FaceBoxes Anchor densification strategy is crucial .,1
7124,"Our anchor densification strategy is used to increase the density of small anchors ( i.e. , 32 32 and 64 64 ) in order to improve the recall rate of small faces .",1
7125,From the results listed in Tab .,0
7126,"2 , we can see that the m AP on FDDB is reduced from 96.0 % to 94.9 % after ablating the anchor densification strategy .",1
7127,"The sharp decline ( i.e. , 1.1 % ) demonstrates the effectiveness of the proposed anchor densification strategy .",0
7128,MSCL is better .,0
7129,The comparison between the second and third columns in Tab .,0
7130,"2 indicates that MSCL effectively increases the m AP by 1.0 % , owning to the diverse receptive fields and the multi -scale anchor tiling mechanism .",0
7131,RDCL is efficient and accuracy - preserving .,1
7132,The design of RDCL enables our FaceBoxes to achieve real - time speed on the CPU .,0
7133,As reported in Tab .,0
7134,"2 , RDCL leads to a negligible decline on accuracy but a significant improvement on speed .",0
7135,"Specifically , the FDDB mAP decreases by 0.1 % in return for the about 19.3 ms speed improvement .",0
7136,Evaluation on benchmark,0
7137,"We evaluate the FaceBoxes on the common face detection benchmark datasets , including Annotated Faces in the Wild ( AFW ) , PASCAL Face , and Face Detection Data Set and Benchmark ( FDDB ) .",0
7138,AFW dataset .,1
7139,It has 205 images with 473 faces .,0
7140,"We evaluate FaceBoxes against the well - known works and commercial face detectors ( e.g. , Face.com , Face + + and Picasa ) .",0
7141,"As illustrated in , our FaceBoxes outperforms all others by a large margin .",1
7142,shows some qualitative results on the AFW dataset .,0
7143,PASCAL face dataset .,1
7144,"It is collected from the test set of PASCAL person layout dataset , consisting of 1335 faces with large face appearance and pose variations from 851 images .",0
7145,shows the precision - recall curves on this dataset .,0
7146,"Our method significantly outperforms all other methods and commercial face detectors ( e.g. , SkyBiometry , Face + + and Picasa ) .",1
7147,shows some qualitative results on the PASCAL face dataset .,0
7148,FDDB dataset .,1
7149,"It has 5 , 171 faces in 2 , 845 images taken from news articles on Yahoo websites .",0
7150,"FDDB adopts the bounding ellipse , while our FaceBoxes outputs rectangle bounding box .",0
7151,This inconsistency has a great impact to the continuous score .,0
7152,"For a more fair comparison under the continuous score evaluation , we train an elliptical regressor to transform our predicted bounding boxes to bounding ellipses .",0
7153,We evaluate our face detector on FDDB against the other methods .,0
7154,The results are shown in and .,0
7155,Our FaceBoxes achieves the state - of - the - art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves .,1
7156,These results indicate that our FaceBoxes can robustly detect unconstrained faces .,0
7157,shows some qualitative results on the FDDB .,0
7158,Conclusion,0
7159,"Since effective models for the face detection task tend to be computationally prohibitive , it is challenging for the CPU devices to achieve real - time speed as well as maintain high performance .",0
7160,"In this work , we present a novel face detector with superior performance on both speed and accuracy .",0
7161,"The proposed method has a lightweight yet powerful network structure , which consists of RDCL and MSCL .",0
7162,"The former enables FaceBoxes to achieve real - time speed , and the latter aims at enriching receptive fields and discretizing anchors over different layers to handle faces of various scales .",0
7163,"Besides , a new anchor densification strategy is proposed to improve the recall rate of small faces .",0
7164,The experiments demonstrate that our contributions lead Face - Boxes to the state - of - the - art performance on the common face detection benchmarks .,0
7165,"The proposed detector is very fast , achieving 20 FPS for VGA - resolution images on CPU and can be accelerated to 125 FPS on GPU .",0
7166,title,0
7167,WIDER FACE : A Face Detection Benchmark,1
7168,abstract,0
7169,Face detection is one of the most studied topics in the computer vision community .,0
7170,Much of the progresses have been made by the availability of face detection benchmark datasets .,0
7171,We show that there is a gap between current face detection performance and the real world requirements .,0
7172,"To facilitate future face detection research , we introduce the WIDER FACE dataset , which is 10 times larger than existing datasets .",0
7173,"The dataset contains rich annotations , including occlusions , poses , event categories , and face bounding boxes .",0
7174,"Faces in the proposed dataset are extremely challenging due to large variations in scale , pose and occlusion , as shown in Fig .",0
7175,"1 . Furthermore , we show that WIDER FACE dataset is an effective training source for face detection .",0
7176,"We benchmark several representative detection systems , providing an overview of state - of - the - art performance and propose a solution to deal with large scale variation .",0
7177,"Finally , we discuss common failure cases that worth to be further investigated .",0
7178,Dataset can be downloaded at : mmlab.ie.cuhk.edu.hk/projects/WIDERFace,0
7179,Introduction,0
7180,"Face detection is a critical step to all facial analysis algorithms , including face alignment , face recognition , face verification , and face parsing .",0
7181,"Given an arbitrary image , the goal of face detection is to determine whether or not there are any faces in the image and , if present , return the image location and extent of each face .",0
7182,"While this appears as an effortless task for human , it is a very difficult task for computers .",0
7183,"The challenges associated with face detection can be attributed to variations in pose , scale , facial expression , occlusion , and lighting condition , as shown in .",0
7184,Face detection has made significant progress after the seminal work by Viola and Jones .,0
7185,"Modern face detectors can easily detect near frontal faces and are widely used in real world applications , such as digital camera and electronic photo album .",0
7186,"Recent research in this area focuses on the unconstrained scenario , where a number of intricate factors .",0
7187,"We propose a WIDER FACE dataset for face detection , which has a high degree of variability in scale , pose , occlusion , expression , appearance and illumination .",0
7188,We show example images ( cropped ) and annotations .,0
7189,The annotated face bounding box is denoted in green color .,0
7190,"The WIDER FACE dataset consists of 393 , 703 labeled face bounding boxes in 32 , 203 images ( Best view in color ) .",0
7191,"such as extreme pose , exaggerated expressions , and large portion of occlusion can lead to large visual variations in face appearance .",0
7192,"Publicly available benchmarks such as FDDB , AFW , PASCAL FACE , have contributed to spurring interest and progress in face detection research .",0
7193,"However , as algorithm performance improves , more chal - lenging datasets are needed to trigger progress and to inspire novel ideas .",0
7194,"Current face detection datasets typically contain a few thousand faces , with limited variations in pose , scale , facial expression , occlusion , and background clutters , making it difficult to assess for real world performance .",0
7195,"As we will demonstrate , the limitations of datasets have partially contributed to the failure of some algorithms in coping with heavy occlusion , small scale , and atypical pose .",0
7196,"In this work , we make three contributions .",0
7197,We introduce a large - scale face detection dataset called WIDER FACE .,1
7198,"It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .",1
7199,"The faces vary largely in appearance , pose , and scale , as shown in .",1
7200,"In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .",1
7201,"We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .",0
7202,"Within this framework , a set of convolutional networks with various size of input are trained to deal with faces with a specific range of scale .",0
7203,"We benchmark four representative algorithms , either obtained directly from the original authors or reimplemented using open - source codes .",0
7204,We evaluate these algorithms on different settings and analyze conditions in which existing methods fail .,0
7205,Related Work,0
7206,Brief review of recent face detection methods :,0
7207,Face detection has been studied for decades in the computer vision literature .,0
7208,"Modern face detection algorithms can be categorized into four categories : cascade based methods , part based methods , channel feature based methods , and neural network based methods .",0
7209,Here we highlight a few notable studies .,0
7210,A detailed survey can be found in .,0
7211,The seminal work by Viola and Jones introduces integral image to compute Haar - like features inconstant time .,0
7212,These features are then used to learn AdaBoost classifier with cascade structure for face detection .,0
7213,Various later studies follow a similar pipeline .,0
7214,"Among those variants , SURF cascade achieves competitive performance .",0
7215,Chen et al. learns face detection and alignment jointly in the same cascade framework and obtains promising detection performance .,0
7216,One of the well - known part based methods is deformable part models ( DPM ) .,0
7217,Deformable part models define face as a collection of parts and model the connections of parts through Latent Support Vector Machine .,0
7218,The part based methods are more robust to occlusion compared with cascade - based methods .,0
7219,"A recent study demonstrates state - of - the art performance with just a vanilla DPM ,",0
7220,achieving better results than more sophisticated DPM variants .,0
7221,Aggregated channel feature ( ACF ) is first proposed by Dollar et al. to solve pedestrian detection .,0
7222,"Later on , Yang et al . applied this idea on face detection .",0
7223,"In particular , features such as gradient histogram , integral histogram , and color channels are combined and used to learn boosting classifier with cascade structure .",0
7224,"Recent studies show that face detection can be further improved by using deep learning , leveraging the high capacity of deep convolutional networks .",0
7225,We anticipate that the new WIDER FACE data can benefit deep convolutional network that typically requires large amount of data for training .,0
7226,Existing datasets :,0
7227,We summarize some of the well - known face detection datasets in .,0
7228,"AFW , FDDB , and PASCAL FACE",0
7229,WIDER FACE,0
7230,Dataset,0
7231,Overview,0
7232,"To our knowledge , WIDER FACE dataset is currently the largest face detection dataset , of which images are selected from the publicly available WIDER dataset .",0
7233,"We choose 32 , 203 images and label 393 , 703 faces with a high degree of variability in scale , pose and occlusion as depicted in .",0
7234,WIDER FACE dataset is organized based on 60 event classes .,0
7235,"For each event class , we randomly select 40% / 10 % / 50 % data as training , validation and testing sets .",0
7236,"Here , we specify two training / testing scenarios :",0
7237,"Scenario - Ext : A face detector is trained using any external data , and tested on the WIDER FACE test partition .",0
7238,"Scenario - Int : A face detector is trained using WIDER FACE training / validation partitions , and tested on WIDER FACE test partition .",0
7239,We adopt the same evaluation metric employed in the PAS - CAL VOC dataset .,0
7240,"Similar to MALF and Caltech datasets , we do not release bounding box ground truth for the test images .",0
7241,"Users are required to submit final prediction files , which we shall proceed to evaluate .",0
7242,Data Collection,0
7243,Collection methodology .,0
7244,WIDER FACE dataset is a subset of the WIDER dataset .,0
7245,The images in WIDER were collected in the following three steps :,0
7246,"1 ) Event categories were defined and chosen following the Large Scale Ontology for Multimedia ( LSCOM ) , which provides around 1 , 000 concepts relevant to video event analysis .",0
7247,2 ) Images are retrieved using search engines like Google and Bing .,0
7248,"For each category , 1 , 000 - 3 , 000 images were collected .",0
7249,3 ) The data were cleaned by manually examining all the images and filtering out images without human face .,0
7250,"Then , similar images in each event category were removed to ensure large diversity in face appearance .",0
7251,"A total of 32 , 203 images are eventually included in the WIDER FACE dataset .",0
7252,Annotation policy .,0
7253,We label the bounding boxes for all the recognizable faces in the WIDER FACE dataset .,0
7254,"The bounding box is required to tightly contain the forehead , chin , and cheek , as shown in .",0
7255,"If a face is occluded , we still label it with a bounding box but with an estimation on the scale of occlusion .",0
7256,"Similar to the PASCAL VOC dataset , we assign an ' Ignore ' flag to the face which is very difficult to be recognized due to low resolution and small scale ( 10 pixels or less ) .",0
7257,"After annotating the face bounding boxes , we further annotate the following attributes : pose ( typical , atypical ) and occlusion level ( partial , heavy ) .",0
7258,Each annotation is labeled by one annotator and cross - checked by two different people . The proposals are generated by using Edgebox .,0
7259,Y-axis denotes for detection rate .,0
7260,X-axis denotes for average number of proposals per image .,0
7261,Lower detection rate implies higher difficulty .,0
7262,We show histograms of detection rate over the number of proposal for different settings ( a ) Different face detection datasets .,0
7263,Properties of WIDER FACE,0
7264,"WIDER FACE dataset is challenging due to large variations in scale , occlusion , pose , and background clutters .",0
7265,These factors are essential to establishing the requirements fora real world system .,0
7266,"To quantify these properties , we use generic object proposal approaches , which are specially designed to discover potential objects in an image ( face can be treated as an object ) .",0
7267,"Through measuring the number of proposals vs. their detection rate of faces , we can have a preliminary assessment on the difficulty of a dataset and potential detection performance .",0
7268,"In the following assessments , we adopt EdgeBox as object proposal , which has good performance in both accuracy and efficiency as evaluated in .",0
7269,Overall . ( a ) shows that WIDER FACE has much lower detection rate compared with other face detection datasets .,0
7270,The results suggest that WIDER FACE is a more challenging face detection benchmark compared to exist -.,0
7271,Histogram of detection rate for different event categories .,0
7272,"Event categories are ranked in an ascending order based on the detection rate when the number of proposal is fixed at 10 , 000 . Top 1 ? 20 , 21 ? 40 , 41 ? 60 event categories are denoted in blue , red , and green , respectively .",0
7273,Example images for specific event classes are shown .,0
7274,Y-axis denotes for detection rate .,0
7275,X-axis denotes for event class name .,0
7276,ing datasets .,0
7277,"Following the principles in KITTI and MALF datasets , we define three levels of difficulty : ' Easy ' , ' Medium ' , ' Hard ' based on the detection rate of EdgeBox , as shown in the .",0
7278,"The average recall rates for these three levels are 92 % , 76 % , and 34 % , respectively , with 8 , 000 proposal per image .",0
7279,Scale .,0
7280,"We group the faces by their image size ( height in pixels ) into three scales : small ( between 10 - 50 pixels ) , medium ( between 50 - 300 pixels ) , large ( over 300 pixels ) .",0
7281,We make this division by considering the detection rate of generic object proposal and human performance .,0
7282,"As can be observed from , the large and medium scales achieve high detection rate ( more than 90 % ) with 8 , 000 proposals per image .",0
7283,"For the small scale , the detection rates consistently stay below 30 % even we increase the proposal number to 10 , 000 .",0
7284,Occlusion .,0
7285,Occlusion is an important factor for evaluating the face detection performance .,0
7286,"Similar to a recent study , we treat occlusion as an attribute and assign faces into three categories : no occlusion , partial occlusion , and heavy occlusion .",0
7287,"Specifically , we ask annotator to measure the fraction of occlusion region for each face .",0
7288,A face is defined as ' partially occluded ' if 1 % - 30 % of the total face area is occluded .,0
7289,A face with occluded area over 30 % is labeled as ' heavily occluded ' .,0
7290,shows some examples of partial / heavy occlusions . ( c ) shows that the detection rate decreases as occlusion level increases .,0
7291,"The detection rates of faces with partial or heavy occlusions are below 50 % with 8 , 000 proposals .",0
7292,"Pose . Similar to occlusion , we define two pose deformation levels , namely typical and atypical .",0
7293,shows some faces of typical and atypical pose .,0
7294,Face is annotated as atypical under two conditions : either the roll or pitch degree is larger than 30 - degree ; or the yaw is larger than 90 - degree . ( d ) suggests that faces with atypical poses are much harder to be detected .,0
7295,Event . Different events are typically associated with different scenes .,0
7296,"WIDER FACE contains 60 event categories covering a large number of scenes in the real world , as shown in and .",0
7297,"To evaluate the influence of event to face detection , we characterize each event with three factors : scale , occlusion , and pose .",0
7298,For each factor we compute the detection rate for the specific event class and then rank the detection rate in an ascending order .,0
7299,"Based on the rank , events are divided into three partitions : easy ( 41 - 60 classes ) , medium ( 21 - 40 classes ) and hard ( 1 - 20 classes ) .",0
7300,We show the partitions based on scale in .,0
7301,Partitions based on occlusion and pose are included in the appendix .,0
7302,Effective training source .,0
7303,As shown in the,0
7304,Multi-scale Detection Cascade,0
7305,"Multi - scale detection cascade CNN consists of a set of face detectors , with each of them only deals with faces in a relatively small range of scale .",0
7306,Each face detector consists of two stages .,0
7307,The first stage generates multi-scale proposal from a fully - convolutional network .,0
7308,The second stage gives face and non-face prediction of the candidate windows generate from first stage .,0
7309,"If the candidate window is classified as face , we further refine the location of the candidate window .",0
7310,Experimental Results,0
7311,Benchmarks,0
7312,"As we discussed in Sec. 2 , face detection algorithms can be broadly grouped into four representative categories .",0
7313,"For each class , we pick one algorithm as a baseline method .",0
7314,"We select VJ , ACF , DPM , and Faceness as baselines .",1
7315,"The VJ , DPM , and Faceness detectors are either obtained from the authors or from open source library ( OpenCV ) .",0
7316,The ACF detector is reimplemented using the open source code .,0
7317,"We adopt the Scenario - Ext here ( see Sec. 3.1 ) , that is , these detectors were trained by using external datasets and are used ' as is ' without re-training them on WIDER FACE .",0
7318,We employ PASCAL VOC evaluation metric for the evaluation .,0
7319,"Following previous work , we conduct linear transformation for each method to fit the annotation of WIDER FACE .",0
7320,Overall .,1
7321,"In this experiment , we employ the evaluation setting mentioned in Sec. 3.3 .",0
7322,The results are shown in ( a.1 ) - ( a.3 ) .,0
7323,"Faceness outperforms other methods on three subsets , with DPM and ACF as marginal second and third .",1
7324,"For the easy set , the average precision ( AP ) of most methods are over 60 % , but none of them surpasses 75 % .",1
7325,The performance drops 10 % for all methods on the medium set .,1
7326,The hard set is even more challenging .,1
7327,"The performance quickly decreases , with a AP below 30 % for all methods .",0
7328,"To trace the reasons of failure , we examine performance on varying subsets of the data .",0
7329,Scale .,1
7330,"As described in Sec. 3.3 , we group faces according to the image height : small ( 10 - 50 pixels ) , medium ( 50 - 300 pixels ) , and large ( 300 or more pixels ) scales .",0
7331,The results of small scale are abysmal : none of the algorithms is able to achieve more than 12 % AP .,1
7332,This shows that current face detectors are incapable to deal with faces of small scale .,0
7333,Occlusion .,1
7334,Occlusion handling is a key performance metric for any face detectors .,0
7335,"In , we show the impact of occlusion on detecting faces with a height of at least 30 pixels .",0
7336,"As mentioned in Sec. 3.3 , we classify faces into three categories : un - occluded , partially occluded ( 1 % - 30 % area occluded ) and heavily occluded ( over 30 % area occluded ) .",0
7337,"With partial occlusion , the performance drops significantly .",1
7338,The maximum AP is only 26.5 % achieved by Faceness .,1
7339,The performance further decreases in the heavy occlusion setting .,0
7340,The best performance of baseline methods drops to 14.4 % .,0
7341,"It is worth noting that Faceness and DPM , which are part based models , already perform relatively better than other methods on occlusion handling .",0
7342,Pose .,1
7343,"As discussed in Sec. 3.3 , we assign a face pose as atypical if either the roll or pitch degree is larger than 30 degree ; or the yaw is larger than 90 - degree .",0
7344,Otherwise a face pose is classified as typical .,0
7345,We show results in .,0
7346,Faces which are un - occluded and with a scale larger than 30 pixels are used in this experiment .,0
7347,The performance clearly degrades for atypical pose .,0
7348,"The best performance is achieved by Faceness , with a recall below 20 % .",1
7349,The results suggest that current face detectors are only capable of dealing with faces with out - of - plane rotation and a small range of in - plane rotation .,0
7350,Summary .,0
7351,"Among the four baseline methods , Faceness tends to outperform the other methods .",0
7352,VJ performs poorly on all settings .,0
7353,DPM gains good performance on medium / large scale and occlusion .,0
7354,"ACF outperforms DPM on small scale , no occlusion and typical pose settings .",0
7355,"However , the overall performance is poor on WIDER FACE , suggesting a large room of improvement .",0
7356,WIDER FACE as an Effective Training Source,0
7357,"In this experiment , we demonstrate the effectiveness of WIDER FACE dataset as a training source .",0
7358,We adopt Scenario - Int here ( see Sec. 3.1 ) .,0
7359,We train ACF and Faceness on WIDER FACE to conduct this experiment .,0
7360,These two algorithms have shown relatively good performance on WIDER FACE previous benchmarks see ( Sec. 5.1 ) .,0
7361,Faces with a scale larger than 30 pixels in the training set are used to retrain both methods .,0
7362,We train the ACF detector using the same training parameters as the baseline ACF .,0
7363,The negative samples are generated from the training images .,0
7364,"For the Faceness detector , we first employ models shared by the authors to generate face proposals from the WIDER FACE training set .",0
7365,"After that , we train the classifier with the same procedure described in .",0
7366,We test these models ( denoted as ACF - WIDER and Faceness - WIDER ) on WIDER FACE testing set and FDDB dataset .,0
7367,WIDER FACE .,0
7368,"As shown in , the retrained models perform consistently better than the baseline models .",0
7369,The average AP improvement of retrained ACF detector is 5.4 % in comparison to baseline ACF detector .,0
7370,"For the Faceness , the retrained Faceness model obtain 4.2 % improvement on WIDER hard test set .",0
7371,FDDB .,0
7372,We further evaluate the retrained models on FDDB dataset .,0
7373,"Similar to WIDER FACE dataset , the retrained models achieve improvement in comparison to the baseline methods .",0
7374,"The retrained ACF detector achieves a recall rate of 87.48 % , outperforms the baseline ACF by a considerable margin of 1.4 % .",0
7375,The retrained Faceness detector obtains a. Comparison of per class AP .,0
7376,"To save space , we only show abbreviations of category names here .",0
7377,The event category is organized based on the rank sequence in ( from hard to easy events based on scale measure ) .,0
7378,We compare the accuracy of Faceness and ACF models retrained on WIDER FACE training set with the baseline Faceness and ACF .,0
7379,"With the help of WIDER FACE dataset , accuracies on 56 out of 60 categories have been improved .",0
7380,"The re-trained Faceness model wins 30 out of 60 classes , followed by the ACF model with 26 classes .",0
7381,Faceness wins 1 medium class and 3 easy classes .,0
7382,high recall rate of 91.78 % .,0
7383,The recall rate improvement of the retrained Faceness detector is 0.8 % in comparison to the baseline Faceness detector .,0
7384,It worth noting that the retrained Faceness detector performs much better than the baseline Faceness detector when the number of false positive is less than 300 .,0
7385,Event .,0
7386,We evaluate the baseline methods on each event class individually and report the results in .,0
7387,Faces with a height larger than 30 pixels are used in this experiment .,0
7388,We compare the accuracy of Faceness and ACF models retrained on WIDER FACE training set with the baseline Faceness and ACF .,0
7389,"With the help of WIDER FACE dataset , accuracies on 56 out of 60 event categories have been improved .",0
7390,It is interesting to observe that the accuracy obtained highly correlates with the difficulty levels specified in Sec. 3.3 ( also refer to ) .,0
7391,"For example , the best performance on "" Festival "" which is assigned as a hard class is no more than 46 % AP .",0
7392,Evaluation of Multi-scale Detection Cascade,0
7393,In this experiment we evaluate the effectiveness of the proposed multi-scale cascade algorithm .,0
7394,"Apart from the ACF - WIDER and Faceness - WIDER models ( Sec. 5.2 ) , we establish a baseline based on a "" Two - stage CNN "" .",0
7395,This model differs to our multi-scale cascade model in the way it handles multiple face scales .,0
7396,"Instead of having multiple networks targeted for different scales , the two - stage CNN adopts a more typical approach .",0
7397,"Specifically , its first stage consists only a single network to perform face classification .",0
7398,"During testing , an image pyramid that encompasses different scales of a test image is fed to the first stage to generate multi -scale face proposals .",0
7399,The second stage is similar to our multi-scale cascade model - it performs further refinement on proposals by simultaneous face classification and bounding box regression .,0
7400,We evaluate the multi-scale cascade CNN and baseline methods on WIDER Easy / Medium / Hard subsets .,0
7401,"As shown in , the multi-scale cascade CNN obtains 8.5 % AP improvement on the WIDER Hard subset compared to the retrained Faceness , suggesting its superior capability in handling faces with different scales .",0
7402,"In particular , having multiple networks specialized on different scale range is shown effective in comparison to using a single network to handle multiple scales .",0
7403,"In other words , it is difficult fora single network to handle large appearance variations caused by scale .",0
7404,"For the WIDER Medium subset , the multi-scale cascade CNN outperforms other baseline methods with a considerable margin .",0
7405,All models perform comparably on the WIDER Easy subset .,0
7406,Conclusion,0
7407,"We have proposed a large , richly annotated WIDER FACE dataset for training and evaluating face detection algorithms .",0
7408,We benchmark four representative face detection methods .,0
7409,"Even considering an easy subset ( typically with faces of over 50 pixels height ) , existing state - of - the - art algorithms reach only around 70 % AP , as shown in .",0
7410,"With this new dataset , we wish to encourage the community to focusing on some inherent challenges of face detection - small scale , occlusion , and extreme poses .",0
7411,These factors are ubiquitous in many real world applications .,0
7412,"For instance , faces captured by surveillance cameras in public spaces or events are typically small , occluded , and atypical poses .",0
7413,These faces are arguably the most interesting yet crucial to detect for further investigation .,0
7414,Appendix,0
7415,Training Multi-scale Proposal Network,0
7416,We provide details of the training process for multi-scale proposal networks .,0
7417,"In this step , we train four fully convolutional networks for face classification and scale classification .",0
7418,"The network structures are summarized in , we initialize the layers from Conv 1 to Conv 5 using the Imagenet 1 , 000 categories pre-trained Clarifai net .",0
7419,"For the Proposal Network 4 , we initialize the layers from Conv 2 to Conv 5 using pre-trained Clarifai net .",0
7420,The remaining layers in each network are randomly initialized with weights drawn from a Gaussian distribution of = 0 and ? = 0.01 .,0
7421,"To account for the multi-label scenario , cross - entropy loss is adopted as shown below :",0
7422,"( y i log p ( y i | I i ) + ( 1 ? y i ) log ( 1 ? p ( y i | I i ) ) ) , and",0
7423,"Back propagation and SGD are also employed here for optimizing Eqn.. Similar to , we set the initial finetuning learning rate as one - tenth of the corresponding pretraining learning rate and drop it by a factor of 10 throughout training .",0
7424,"After training , we conduct hard negative mining on the training set and further tune the proposal networks using hard negative samples .",0
7425,Training Face Detector,0
7426,"In this section , we provide more details of the training process for face detection .",0
7427,"As mentioned at beginning , we train .",0
7428,Histogram of detection rate for different event categories .,0
7429,"Event categories are ranked based on detection rate when number of proposal is 10 , 000 in the ascending order .",0
7430,"Top 1 ? 20 , 21 ? 40 , 41 ? 60 event categories are denote in blue , red , green respectively .",0
7431,Example images for specific event class are shown .,0
7432,Y-axis denotes for detection rate .,0
7433,X-axis denotes for event class name ..,0
7434,Full name of abbreviation of event categories .,0
7435,title,0
7436,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,1
7437,abstract,0
7438,"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .",1
7439,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .",1
7440,"In this paper , we present a face detection approach named Contextual Multi - Scale Region - based Convolution Neural Network ( CMS - RCNN ) to robustly solve the problems mentioned above .",0
7441,"Similar to the region - based CNNs , our proposed network consists of the region proposal component and the region - of - interest ( RoI ) detection component .",0
7442,"However , far apart of that network , there are two main contributions in our proposed network that play a significant role to achieve the state - of - theart performance in face detection .",0
7443,"Firstly , the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions .",0
7444,"Secondly , our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system .",0
7445,"The proposed approach is benchmarked on two recent challenging face detection databases , i.e. the WIDER FACE Dataset which contains high degree of variability , as well as the Face Detection Dataset and Benchmark ( FDDB ) .",0
7446,"The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin , and consistently achieves competitive results on FDDB against the recent state - of - the - art face detection methods .",0
7447,INTRODUCTION,0
7448,"Detection and analysis on human subjects using facial feature based biometrics for access control , surveillance systems and other security applications have gained popularity over the past few years .",0
7449,Several such biometrics systems are deployed in security checkpoints across the globe with more being deployed everyday .,0
7450,"Particularly , face recognition has been one of the most popular biometrics modalities attractive to security departments .",0
7451,"Indeed , the uniqueness of facial features across individuals can be captured much more easily than other biometrics .",0
7452,"In order to take into account a face recognition algorithm , however , face detection usually needs to be done first .",0
7453,"The problem of face detection has been intensely studied for decades with the aim of ensuring the generalization of robust algorithms to unseen face images , , , , , , , , , , , .",0
7454,"Although the detection accuracy in recent face detection algorithms , , , , , has been highly improved due to the advancement of deep Convolutional Neural Networks ( CNN ) , they are still far from achieving the same detection capabilities as a human due to a number of challenges , are always the important factors that need to be considered .",0
7455,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",1
7456,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .",1
7457,"In other words , this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face , seeing the body will increase our confidence .",0
7458,Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation .,1
7459,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .",0
7460,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,1
7461,A confidence score and bounding box regression are computed for every candidate .,0
7462,"In the end , the face detection system is able to decide the quality of the detection results by thresholding these generated confidence scores in given face images .",0
7463,The architecture of our proposed CMS - RCNN network for unconstrained face detection is illustrated in .,0
7464,Our approach is evaluated on two challenging face detection databases and compared against numerous recent face detection methods .,0
7465,"Firstly , the proposed CMS - RCNN method is compared against four strong baselines , , on the WIDER FACE Dataset , a large scale face detection benchmark database .",0
7466,"This experiment shows its capability to detect face images in the wild , e.g. under occlusions , illumination , facial poses , low - resolution conditions , etc .",0
7467,"Our method outperforms the baselines by a huge margin in all easy , medium , and hard partitions .",0
7468,"It is also benchmarked on the Face Detection Data Set and Benchmark ( FDDB ) , a dataset of face regions designed for studying the problem of unconstrained face detection .",0
7469,The experimental results show that the proposed CMS - RCNN approach consistently achieves highly competitive results against the other state - of - the - art face detection methods .,0
7470,The rest of this paper is organized as follows .,0
7471,"In section 2 , we summarize prior work in face detection .",0
7472,"Section 3 reviews a general deep learning framework , the background as well as the limitations of the Faster R - CNN in the problem of face detection .",0
7473,"In Section 4 , we introduce our proposed CMS - RCNN approach for the problem of unconstrained face detection .",0
7474,"Section 5 presents the experimental face detection results and comparisons obtained using our proposed approach on two challenging face detection databases , i.e. the WIDER FACE Dataset and the FDDB database .",0
7475,"Finally , our conclusions in this work are presented in Section 6 .",0
7476,RELATED WORK,0
7477,Face detection has been a well studied area of computer vision .,0
7478,One of the first well performing approaches to the problem was the Viola - Jones face detector .,0
7479,It was capable of performing real time face detection using a cascade of boosted simple Haar classifiers .,0
7480,The concepts of boosting and using simple features has been the basis for many different approaches since the Viola - Jones face detector .,0
7481,These early detectors tended to work well on frontal face images but not very well on faces in different poses .,0
7482,"As time has passed , many of these methods have been able to deal with off - angle face detection by utilizing multiple models for the various poses of the face .",0
7483,This increases the model size but does afford more practical uses of the methods .,0
7484,Some approaches have moved away from the idea of simple features but continued to use the boosted learning framework .,0
7485,Li and Zhang used SURF cascades for general object detection but also showed good results on face detection .,0
7486,"More recent work on face detection has tended to focus on using different models such as a Deformable Parts Model ( DPM ) , .",0
7487,"Zhu and Ramanan 's work was an interesting approach to the problem of face detection where they combined the problems of face detection , pose estimation , and facial landmarking into one framework .",0
7488,"By utilizing all three aspects in one framework , they were able to outperform the state - of - the - art at the time on real world images .",0
7489,Yu et al . extended this work by incorporating group sparsity in learning which landmarks are the most salient for face detection as well as incorporating 3 D models of the landmarks in order to deal with pose .,0
7490,Chen et al .,0
7491,have combined ideas from both of these approaches by utilizing a cascade detection framework while simultaneously localizing features on the face for alignment of the detectors .,0
7492,"Similarly , Ghiasi and Fowlkes have been able to use heirarchical DPMs not only to achieve good face detection in the presence of occlusion but also landmark localization .",0
7493,"However , Mathias et al. were able to show that both DPM models and rigid template detectors similar to the Viola - Jones detector have a lot of potential that has not been adequately explored .",0
7494,"By retraining these models with appropriately controlled training data , they were able to create face detectors that perform similarly to other , more complex state - of - the - art face detectors .",0
7495,All of these approaches to face detection were based on selecting a feature extractor beforehand .,0
7496,"However , there has been work done in using a ConvNet to learn which features are used to detect faces .",0
7497,Neural Networks have been around fora longtime but have been experiencing a resurgence in popularity due to hardware improvements and new techniques resulting in the capability to train these networks on large amounts of training data .,0
7498,Li et al. utilized a cascade of CNNs to perform face detection .,0
7499,The cascading networks allowed them to process different scales of faces at different levels of the cascade while also allowing for false positives from previous networks to be removed at later layers in a similar approach to other cascade detectors .,0
7500,Yang et al .,0
7501,approached the problem from a different perspective more similar to a DPM approach .,0
7502,"In their method , the face is broken into several facial parts such as hair , eyes , nose , mouth , and beard .",0
7503,"By training a detector on each part and combining the score maps intelligently , they were able to achieve accurate face detection even under occlusions .",0
7504,Both of these methods require training several networks in order to achieve their high accuracy .,0
7505,"Our method , on the other hand , can be trained as a single network , end - to - end , allowing for less annotation of training data needed while maintaining highly accurate face detection .",0
7506,The ideas of using contextual information in object detection have been studied in several recent work with very high detection accuracy .,0
7507,Divvala et al .,0
7508,"reviewed the the role of context in a contemporary , challenging object detection in their empirical evaluation analysis .",0
7509,"In their conclusions , the context information not only reduces the overall detection errors , but also the remaining errors made by the detector are more reasonable .",0
7510,Bell et al. introduced an advanced object detector method named Inside - Outside Network ( ION ) to exploit information both inside and outside the region of interest .,0
7511,"In their approach , the contextual information outside the region of interest is incorporated using spatial recurrent neural networks .",0
7512,"Inside the network , skip pooling is used to extract information at multiple scales and levels of abstraction .",0
7513,"Recently , Zagoruyko et al. have presented the MultiPath network with three modifications to the standard Fast R - CNN object detector , i.e. skip connections that give the detector access to features at multiple network layers , a foveal structure to exploit object context at multiple object resolutions , and an integral loss function and corresponding network adjustment that improve localization .",0
7514,The information in their proposed network can flow along multiple paths .,0
7515,Their MultiPath network is combined with DeepMask object proposals to solve the object detection problem .,0
7516,"Unlike all the previous approaches that select a feature extractor beforehand and incorporate a linear classifier with the depth descriptor beside RGB channels , our method solves the problem under a deep learning framework where the global and the local context features , i.e. multi scaling , are synchronized to Faster Region - based Convolutional Neural Networks in order to robustly achieve semantic detection .",0
7517,BACKGROUND,0
7518,"The recent studies in deep ConvNets have achieved significant results in object detection , classification and modeling .",0
7519,"In this section , we review various well - known Deep ConvNets .",0
7520,"Then , we show the current limitations of the Faster R - CNN , one of the state - of - the - art deep ConvNet methods in object detection , in the defined context of the face detection .",0
7521,Region - based Convolution Neural Networks,0
7522,One of the most important approaches for the object detection task is the family of Region - based Convolution Neural Networks ( R - CNN ) .,0
7523,"R - CNN , the first generation of this family , applies the high - capacity deep ConvNet to classify given bottomup region proposals .",0
7524,"Due to the lack of labeled training data , it adopts a strategy of supervised pre-training for an auxiliary task followed by domain - specific fine - tuning .",0
7525,Then the ConvNet is used as a feature extractor and the system is further trained for object detection with Support Vector Machines ( SVM ) .,0
7526,"Finally , it performs boundingbox regression .",0
7527,The method achieves high accuracy but is very time - consuming .,0
7528,"The system takes along time to generate region proposals , extract features from each image , and store these features in a hard disk , which also takes up a large amount of space .",0
7529,"At testing time , the detection process takes 47s per image using VGG - 16 network implemented in GPU due to the slowness of feature extraction .",0
7530,"In other words , R - CNN is slow because it processes each object proposal independently without sharing computation .",0
7531,Fast R - CNN solves this problem by sharing the features between proposals .,0
7532,"The network is designed to only compute a feature map once per image in a fully convolutional style , and to use ROI - pooling to dynamically sample features from the feature map for each object proposal .",0
7533,"The network also adopts a multi - task loss , i.e. classification loss and bounding - box regression loss .",0
7534,"Based on the two improvements , the framework is trained endto - end .",0
7535,The processing time for each image significantly reduced to 0.3s .,0
7536,Fast R - CNN accelerates the detection network using the ROI - pooling layer .,0
7537,"However the region proposal step is designed out of the network hence still remains a bottleneck , which results in sub-optimal solution and dependence on the external region proposal methods .",0
7538,Faster R - CNN addresses the problem with fast R - CNN by introducing the Region Proposal Network ( RPN ) .,0
7539,An RPN is implemented in a fully convolutional style to predict the object bounding boxes and the objectness scores .,0
7540,"In addition , the anchors are defined with different scales and ratios to achieve the translation invariance .",0
7541,The RPN shares the full - image convolution features with the detection network .,0
7542,Therefore the whole system is able to complete both proposal generation and detection computation within 0.2 s using very deep VGG - 16 model .,0
7543,"With a smaller ZF model , it can reach the level of real - time processing .",0
7544,Limitations of Faster R - CNN,0
7545,"The Region - based CNN family , e.g. Faster R - CNN and its variants , achieves the state - of - the - art performance results in object detection on the PASCAL VOC dataset .",0
7546,"These methods can detect objects such as vehicles , animals , people , chairs , and etc. with very high accuracy .",0
7547,"In general , the defined objects often occupy the majority of a given image .",0
7548,"However , when these methods are tested on the challenging Microsoft COCO dataset , the performance drops a lot , since images contain more small , occluded and incomplete objects .",0
7549,Similar situations happen in the problem of face detection .,0
7550,"We focus on detecting only facial regions that are sometimes small , heavily occluded and of low resolution ( as shown in ) .",0
7551,The detection network in designed Faster R - CNN is unable to robustly detect such tiny faces .,0
7552,"The intuition point is that the Regions of Interest pooling layer , i.e. ROIpooling layer , builds features only from the last single high level feature map .",0
7553,"For example , the global stride of the ' conv5 ' layer in the VGG - 16 model is 16 .",0
7554,"Therefore , given a facial region with the sizes less than 16 16 pixels in an image , the projected ROI - pooling region for that location will be less than 1 pixel in the ' conv5 ' layer , even if the proposed region is correct .",0
7555,"Thus , the detector will have much difficulty to predict the object class and the bounding box location based on information from only one pixel .",0
7556,Other Face Detection Method Limitations,0
7557,Other challenges in object detection in the wild include occlusion and low - resolution .,0
7558,"For face detection , it is very common for people to wear stuffs like sunglasses , scarf and hats , which occlude the face .",0
7559,"In such cases , the methods that only extract features from faces do notwork well .",0
7560,"For example , Faceness consider finding faces through scoring facial parts responses by their spatial structure and arrangement , which works well on clear faces .",0
7561,"But when facial parts are missing due to occlusion or when face itself is too small , facial parts become more hard to detect .",0
7562,"Therefore , the body context information plays its role .",0
7563,"As an example of context - dependent objects , faces often come together with human body .",0
7564,"Even though the faces are occluded , we can still locate it only by seeing the whole human body .",0
7565,"Similar advantages for faces at low - resolution , i.e. tiny faces .",0
7566,The deep features can not tell much about tiny faces since their receptive field is too small to be informative .,0
7567,Introducing context information can extend the area to extract features and make them meaningful .,0
7568,"On the other hand , the context information also helped with reducing false detection as discussed previously , since context information tells the difference between real faces with bodies and face - like patterns without bodies .",0
7569,CONTEXTUAL MULTI - SCALE R - CNN,0
7570,"Our goal is to detect human faces captured under various challenging conditions such as strong illumination , heavily occlusion , extreme off - angles , and low resolution .",0
7571,"Under these conditions , the current CNN - based detection systems suffer from two major problems , i.e. 1 ) tiny faces are hard to identify ; 2 ) only face region is taken into consideration for classification .",0
7572,"In this section , we show why these problems hinder the ability of a face detection system .",0
7573,"Then , our proposed network is presented to address these problems by using the Multi - Scale Region Proposal Network ( MS - RPN ) and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) , as illustrated in .",0
7574,"Similar to Faster R - CNN , the MS - RPN outputs several region candidates and the CMS - CNN computes the confidence score and bounding box for each candidate .",0
7575,Identifying Tiny Faces,0
7576,Why tiny faces are hard to be robustly detected by the previous region - based CNNs ?,0
7577,The reason is that in these networks both the proposed region and the classification score are produced from one single high - level convolution feature map .,0
7578,"This representation does n't have enough information for the multiple tasks , i.e. region proposal and RoI detection .",0
7579,"For example , Faster R - CNN generates region candidates and does RoI-pooling from the ' conv5 ' layer of the VGG - 16 model , which has a overall stride of 16 .",0
7580,One issue is that the reception field in this layer is quite large .,0
7581,"When the face size is less than 16 - by - 16 pixels , the corresponding output in ' conv5 ' layer is less than 1 pixel , which is insufficient to encode informative features .",0
7582,"The other issue is that as the convolution layers go deeper , each pixel in the feature map gather more and more information outside the original input region so that it contains lower proportion of information for the region of interest .",0
7583,These two issues together make the last convolution layer less representative for tiny faces .,0
7584,Multiple Scale Faster- RCNN,0
7585,"Our solution for this problem is a combination of both global and local features , i.e. multiple scales .",0
7586,"In this architecture , the feature maps are incorporated from lower level convolution layers with the last convolution layer for both MS - RPN and CMS - CNN .",0
7587,"Features from lower convolution layer help get more information for the tiny faces , because stride in lower convolution layer will not be too small .",0
7588,"Another benefit is that both low - level feature with localization capability and high - level feature with semantic information are fused together , since face detection needs to localize the face as well as to identify the face .",0
7589,"In the MS - RPN , the whole lower level feature maps are down - sampled to the size of high level feature map and then concatenated with it to form a unified feature map .",0
7590,Then we reduce the dimension of the unified feature map and use it to generate region candidates .,0
7591,"In the CMS - CNN , the region proposal is projected into feature maps from multiple convolution layers .",0
7592,"And RoI-pooling is performed in each layer , resulting in a fixed - size feature tensor .",0
7593,"All feature tensors are normalized , concatenated and dimension - reduced to a single feature blob , which is forwarded to two fully connected layers to compute a representation of the region candidate .",0
7594,L2 Normalization,0
7595,"In both MS - RPN and CMS - CNN , concatenation of feature maps is done with L2 normalization layer , shown in , since the feature maps from different layer have generally different properties in terms of numbers of channels , scale of value and norm of feature map pixels .",0
7596,"Generally , comparing with values in shallower layers , the values in deeper layers are usually too small , which leads to the dominance of shallower layers .",0
7597,"In practice , it is impossible for the system to readjust and tune value from each layer for best performance .",0
7598,"Therefore , L2 normalization layers before concatenation are crucial for the robustness of the system because it keeps the value from each layer in roughly the same scale .",0
7599,"The normalization is performed within each pixel , and all feature map is treated independently :",0
7600,where the x andx stand for the original pixel vector and the normalized pixel vector respectively .,0
7601,d stands for the number of channels in each feature map tensor .,0
7602,"During training , scaling factors ?",0
7603,i will be updated to readjust the scale of the normalized features .,0
7604,"For each channel i , the scaling factor follows :",0
7605,where y i stand for the re-scaled feature value .,0
7606,"Following the back - propagation and chain rule , the update for scaling factor ?",0
7607,is :,0
7608,New Layer in Deep Learning Caffe Framework,0
7609,"The system integrate information from lower layer feature maps , i.e. third and fourth convolution layers , to extract determinant features for tiny faces .",0
7610,"For both parts of our system , i.e. MS - RPN and CMS - CNN , the L2 normalization layers are inserted before concatenation of feature maps from the three layers .",0
7611,The features were re-scaled to proper values and concatenated to a single feature map .,0
7612,"We set the initial scaling factor in a special way , following two rules .",0
7613,"First , the average scale for each feature map is roughly identical ; second , after the following 1 1 convolution , the resulting tensor should have the same average scale as the conv5 layer in the work of Faster R - CNN .",0
7614,"As implied , after the following 1 1 convolution , the tensor should be the same as the original architecture in Faster R - CNN , in terms of its size , scale of values and function for the downstream process .",0
7615,Integrating Body Context,0
7616,"When humans are searching for faces , they try to look for not only the facial patterns , e.g. eyes , nose , mouth , but also the human bodies .",0
7617,Sometimes a human body makes us more convinced about the existence of a face .,0
7618,"In addition , sometimes human body helps to reject false positives .",0
7619,"If we only look at face regions , we may make mistakes identifying them .",0
7620,"For example , shows two cases where body region plays a significant role for correct detection .",0
7621,This intuition is not only true for human but also valid in computer vision .,0
7622,"Previous research has shown that contextual reasoning is a critical piece of the object recognition puzzle , and that context not only reduces the overall detection errors , but , more importantly , the remaining errors made by the detector are more reasonable .",0
7623,"Based on this intuition , our network is designed to make explicit reference to the human body context information in the RoI detection .",0
7624,"In our proposed network , the contextual body reasoning is implemented by explicitly grouping body information from convolution feature maps shown as the red blocks in .",0
7625,"Specifically , additional RoI-pooling operations are performed for each region proposal in convolution feature maps to represent the body context features .",0
7626,"Then same as the face feature tensors , these body feature tensors are normalized , concatenated and dimension - reduced to a single feature blob .",0
7627,After two fully connected layers the final body representation is concatenated with the face representation .,0
7628,They together contribute to the computation of confidence score and bounding box regression .,0
7629,"With projected region proposal as the face region , the additional RoI-pooling region represents the body region and satisfies a pre-defined spatial relation with the face region .",0
7630,"In order to model this spatial relation , we make a simple hypothesis that if there is a face , there must exist a body , and the spatial relation between each face and body is fixed .",0
7631,This assumption may not be true all the time but should cover most of the scenarios since most people we see in the real world are either standing or sitting .,0
7632,"Therefore , the spatial relation is roughly fixed between the face and the vertical body .",0
7633,"Mathematically , this spatial relation can be represented by four parameters presented in Equation .",0
7634,"where x ( * ) , y ( * ) , w ( * ) , and h ( * ) denote the two coordinates of the box center , width , and height respectively .",0
7635,And band f stand for body and face respectively .,0
7636,"t x , t y , t w , and t hare the parameters .",0
7637,"Through out this paper , we fix the for parameters such that the two projected RoI regions of face and body satisfies a certain spatial ratio illustrated in the famous drawing in .",0
7638,Information Fusion,0
7639,It 's worth noticing that in our deep network architecture we have multiple face feature maps and body context feature maps for each proposed region .,0
7640,"A critical issue is how we effectively fuse these information , i.e. what computation to apply and in which stage .",0
7641,"In our network , features extracted from different convolution layers need to be fused together to get a uniform representation .",0
7642,"They can not be naively concatenated due to the overall differences of the numbers of channels , scales of values and norms of feature map pixels among these layers .",0
7643,The detailed research shows that the deeper layers often contain smaller values than the shallower layers .,0
7644,"Therefore , the larger values will dominate the smaller ones , making the system rely too much on shallower features rather than a combination of multiple scale features causing the system to no longer be robust .",0
7645,We adopt the normalization layer from to address this problem .,0
7646,The system takes the multiple scale features and apply L2 normalization along the channel axis of each feature map .,0
7647,"Then , since the channel size is different among layers , the normalized feature map from each layer needed to be re-weighted , so that their values are at the same scale .",0
7648,"After that , the feature maps are concatenated to one single feature map tensor .",0
7649,This modification helps to stabilize the system and increase the accuracy .,0
7650,"Finally , the channel size of the concatenated feature map is shrunk to fit right in the original architecture for the downstream fully - connected layers .",0
7651,Another crucial question is whether to fuse the face information and the body information at a early stage or at the very end of the network .,0
7652,Here we choose the late fusion strategy in which face features and body context features are extracted in two parallel pipelines .,0
7653,At the very end of the network two representations for face and body context are concatenated together to form along feature vector .,0
7654,Then this feature vector is forwarded to compute confidence score and bounding box regression .,0
7655,"The other strategy is the early fusion , in which face feature maps and body context feature maps get concatenated right after RoI pooling and normalization .",0
7656,"These two strategies both combine the information from face and body context , but we prefer the late fusion .",0
7657,The reason is that we want the network to make decisions in a more semantic space .,0
7658,We care more about the existence of the face and the body .,0
7659,The localization information is already encoded in the predefined spatial relation mentioned in Section 4.2 .,0
7660,Moreover empirical experiments also show that late fusion strategy works better .,0
7661,Implementation Details,0
7662,Our CMS - RCNN is implemented in the Caffe deep learning framework .,1
7663,"The first 5 sets of convolution layers have the same architecture as the deep VGG - 16 model , and during training their parameters are initialized from the pre-trained VGG - 16 .",1
7664,"For simplicity we refer to the last convolution layers in set 3 , 4 and 5 as ' conv3 ' , ' conv4 ' , and ' conv5 ' respectively .",0
7665,All the following layers are connected exclusively to these three layers .,0
7666,"In the MS - RPN , we want ' conv3 ' , ' conv4 ' , and ' conv5 ' to be synchronized to the same size so that concatenation can be applied .",1
7667,So ' conv3 ' is followed by pooling layer to perform down - sampling .,1
7668,"Then ' conv3 ' , ' conv4 ' , and ' conv5 ' are normalized along the channel axis to a learnable re-weighting scale and concatenated together .",1
7669,"To ensure training convergence , the initial re-weighting scale needs to be carefully set .",1
7670,"Here we set the initial scale of ' conv3 ' , ' conv4 ' , and ' conv5 ' to be 66.84 , 94.52 , and 94.52 respectively .",1
7671,"In the CMS - CNN , the RoI pooling layer already ensure that the pooled feature maps have the same size .",1
7672,Again we normalize the pooled features to make sure the downstream values are at reasonable scales when training is initialized .,0
7673,"Specifically , features pooled from ' conv3 ' , ' conv4 ' , and ' conv5 ' are initialized with scale to be 57.75 , 81.67 , and 81.67 respectively , for both face and body pipelines .",1
7674,"The MS - RPN and the CMS - CNN share the same parameters for all convolution layers so that computation can be done once , resulting in higher efficiency .",1
7675,"Additionally , in order to shrink the channel size of the concatenated feature map , a 11 convolution layer is then employed .",1
7676,Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R - CNN .,0
7677,EXPERIMENTS,0
7678,This section presents the face detection bechmarking using our proposed CMS - RCNN approach on the WIDER FACE dataset and the Face Detection Data Set and Benchmark ( FDDB ) database .,0
7679,The WIDER FACE dataset is experimented with high degree of variability .,0
7680,"Using this database , our proposed approach robustly outperforms strong baseline methods , including Two - stage CNN , Multi-scale Cascade CNN , Faceness and Aggregate Channel Features ( ACF ) , by a large margin .",0
7681,We also show that our model trained on WIDER FACE dataset generalizes well enough to the FDDB database .,0
7682,"The trained model consistently achieves competitive results against the recent state - of - the - art face detection methods on this database , including HyperFace , DP2 MFD , CCF , Faceness , NPDFace , MultiresHPM , DDFD , CascadeCNN , ACF - multiscale , Pico , HeadHunter , Joint Cascade , Boosted Exemplar , and PEP - Adapt .",0
7683,Experiments on WIDER FACE Dataset,1
7684,Data description,0
7685,WIDER FACE is a public face detection benchmark dataset .,0
7686,"It contains 393,703 labeled human faces from 32,203 images collected based on 61 event classes from internet .",0
7687,"The database has many human faces with a high degree of pose variation , large occlusions , low - resolutions and strong lighting conditions .",0
7688,"The images in this database are organized and split into three subsets , i.e. training , validation and testing .",0
7689,"Each contains 40 % , 10 % and 50 % respectively of the original databases .",0
7690,The images and the ground - truth labels of the training and the validation sets are available online for experiments .,0
7691,"However , in the testing set , only the testing images ( not the ground - truth labels ) are available online .",0
7692,All detection results are sent to the database server for evaluating and receiving the Precision - Recall curves .,0
7693,"In our experiments , the proposed CMS - RCNN is trained on the training set of the WIDER FACE dataset containing 159,424 annotated faces collected in 12,880 images .",0
7694,The trained model on this database are used in testing of all databases .,0
7695,Testing and Comparison,0
7696,"During the testing phase , the face images in the testing set are divided into three parts based on their detection rates on EdgeBox .",0
7697,"In other words , face images are divided into three levels according to the difficulties of the detection , i.e. Easy , Medium and Hard .",0
7698,"The proposed CMS - RCNN model is compared against recent strong face detection methods , i.e. Two - stage CNN , Multiscale Cascade CNN , Faceness , and Aggregate Channel Features ( ACF ) .",0
7699,All these methods are trained on the same training set and tested on the same testing set .,0
7700,The Precision - Recall curves and AP values are shown in .,0
7701,Our method outperforms those strong baselines by a large margin .,0
7702,"It achieves the best average precision in all level faces , i.e. AP = 0.902 ( Easy ) , 0.874 ( Medium ) and 0.643 ( Hard ) , and outperforms the second best baseline by 26.0 % ( Easy ) , 37.4 % ( Medium ) and 60.8 % ( Hard ) .",1
7703,"These results suggest that as the difficulty level goes up , CMS - RCNN can detect challenging faces better .",0
7704,So it has the ability to handle difficult conditions hence is more closed to human detection level .,0
7705,shows some examples of face detection results using the proposed CMS - RCNN on this database .,0
7706,With Context v.s. Without Context,0
7707,"As we show in Section 4.2 that human vision can benefit from additional context information for better detection and recognition , we show in this section how does explicit contextual reasoning in the network help improve the model performance .",0
7708,"To prove this , we test our models with and without body context information on the validation set of WIDER FACE dataset .",0
7709,The model without body context is implemented by removing the context pipeline and only use the representation from face pipeline to compute the confidence score and the bounding box regression .,0
7710,We compare their performances as illustrated in .,0
7711,The Faster R - CNN method is setup as a baseline .,0
7712,"Starting from 0 in recall , two curves of our models are overlapped at first , which means that two models perform as well as each other on some easy faces .",0
7713,Then the curve of model without context starts to drop quicker than the 0 0 ..,0
7714,"Precision - Recall curves obtained by our proposed CMS - RCNN ( red ) and the other baselines , i.e. Twostage CNN , Multi-scale Cascade CNN , Faceness , and Aggregate Channel Features ( ACF ) .",0
7715,All methods trained and tested on the same training and testing set of the WIDER FACE dataset .,0
7716,"model with context , suggesting the model with context can handle the challenging conditions better when faces become more and more difficult .",0
7717,Thus eventually the model with context achieves a higher recall value .,0
7718,"Additionally , the context model produces a longer PR curve , which means that contextual reasoning can help finding more faces .",0
7719,Visualization of False Positives,0
7720,"As it is well known that precision - recall curves get dropped due to the false positives , we are interested in the false positives produced by our CMS - RCNN model .",0
7721,We are curious about what object can fool our model to treat it as a face .,0
7722,"Is it due to over-fitting , data bias , or miss labeling ?",0
7723,"In order to visualize the false positives , we test the CMS - RCNN model on the WIDER FACE validation set and pick all the false positives according to the ground truth .",0
7724,Then those positives are sorted by the confidence score in a descending order .,0
7725,"We choose the top 20 false positives as illustrated in Because their confidence scores are high , they are the objects most likely to cause our model making mistakes .",0
7726,"It turns out that most of the false positives are actually human faces caused by miss labeling , which is a problem of the dataset itself .",0
7727,"For other false positives , we find the errors made by our model are rather reasonable .",0
7728,They all have the pattern of human face as well as the shape of human body .,0
7729,Experiments on FDDB Face Database,1
7730,"To show that our method generalizes well to other database , the proposed CMS - RCNN is also benchmarked on the FDDB database .",0
7731,It is a standard database for testing and evaluation of face detection algorithms .,0
7732,"It contains annotations for 5,171 faces in a set of 2,845 images taken from the Faces in the Wild dataset .",0
7733,Most of the images in the FDDB database contain less than 3 faces that are clear or slightly occluded .,0
7734,The faces generally have large sizes and high resolutions compared to WIDER FACE .,0
7735,We use the same model trained on WIDER FACE training set presented in Section 5.1 to perform the evaluation on the FDDB database .,0
7736,"The evaluation is performed based on the discrete criterion following the same rules in PASCAL VOC Challenge , i.e. if the ratio of the intersection of a detected region with an annotated face region is greater than 0.5 , it is considered as a true positive detection .",0
7737,The evaluation is proceeded following the FDDB evaluation protocol and .,0
7738,Our method achieves the best recall rate on this database .,1
7739,Numbers in the legend show the average precision scores .,0
7740,"compared against the published methods provided in the protocol , i.e. HyperFace , DP2 MFD , CCF , Faceness , NPDFace , MultiresHPM , DDFD , CascadeCNN , ACF - multiscale , Pico , HeadHunter , Joint Cascade , Boosted Exemplar , and PEP - Adapt .",0
7741,The proposed CMS - RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods ( as shown ) .,1
7742,This is concrete evidence to demonstrate that CMS - RCNN robustly detects unconstrained faces .,0
7743,shows some examples of the face detection results using the proposed CMS - RCNN on the FDDB dataset .,0
7744,CONCLUSION AND FUTURE WORK,0
7745,"This paper has presented our proposed CMS - RCNN approach to robustly detect human facial regions from images collected under various challenging conditions , e.g. highly occlusions , low resolutions , facial expressions , illumination variations , etc .",0
7746,"The approach is benchmarked on two challenging face detection databases , i.e. the WIDER FACE Dataset and the FDDB , and compared against recent other face detection methods .",0
7747,The experimental results show that our proposed approach outperforms strong baselines on the WIDER FACE and consistently achieves very competitive results against state - of - the - art methods on the FDDB .,0
7748,"In our implementation , the proposed CMS - RCNN consists of the MS - RPN and the CMS - CNN .",0
7749,"During training , they are merged together in an approximate joint training style for each SGD iteration , in which the derivatives w.r.t. the proposal boxes ' coordinates are ignored .",0
7750,In the future we want to go to the fully joint training so that the network can be trained in end - to - end fashion ..,0
7751,Examples of the top 20 false positives from our CMS - RCNN model tested on the WIDER FACE validation set .,0
7752,"In fact these false positives include many human faces not in the dataset due to mislabeling , which means that our method is robust to the noise in the data ..",0
7753,More results of unconstrained face detection under challenging conditions using our proposed CMS - RCNN .,0
7754,title,0
7755,abstract,0
7756,narrative,0
7757,?,0
7758,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",1
7759,Recent studies show that deep learning approaches can achieve impressive performance on these two tasks .,0
7760,"In this paper , we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance .",0
7761,"In particular , our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse - to - fine manner .",0
7762,"In addition , in the learning process , we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection .",0
7763,"Our method achieves superior accuracy over the state - of - the - art techniques on the challenging FDDB and WIDER FACE benchmark for face detection , and AFLW benchmark for face alignment , while keeps real time performance .",0
7764,"Index Terms - Face detection , face alignment , cascaded convolutional neural network",0
7765,I. INTRODUCTION,0
7766,"ACE detection and alignment are essential to many face applications , such as face recognition and facial expression analysis .",0
7767,"However , the large visual variations of faces , such as occlusions , large pose variations and extreme lightings , impose great challenges for these tasks in real world applications .",0
7768,"The cascade face detector proposed by Viola and Jones utilizes Haar - Like features and AdaBoost to train cascaded classifiers , which achieve good performance with real - time efficiency .",0
7769,"However , quite a few works indicate that this detector may degrade significantly in real - world applications with larger visual variations of human faces even with more advanced features and classifiers .",0
7770,"Besides the cascade structure , introduce deformable part models ( DPM ) for face detection and achieve remarkable performance .",0
7771,"However , they need high computational expense and may usually require expensive annotation in the training stage .",0
7772,"Recently , convolutional neural networks ( CNNs ) achieve remarkable progresses in a variety of computer vision tasks , such as image classification and face recognition .",0
7773,"Inspired by the good per - K. - P. formance of CNNs in computer vision tasks , some of the CNNs based face detection approaches have been proposed in recent years .",0
7774,Yang et al.,0
7775,train deep convolution neural networks for facial attribute recognition to obtain high response in face regions which further yield candidate windows of faces .,0
7776,"However , due to its complex CNN structure , this approach is time costly in practice .",0
7777,Li et al.,0
7778,"use cascaded CNNs for face detection , but it requires bounding box calibration from face detection with extra computational expense and ignores the inherent correlation between facial landmarks localization and bounding box regression .",0
7779,Face alignment also attracts extensive interests .,0
7780,Regression - based methods and template fitting approaches are two popular categories .,0
7781,"Recently ,",0
7782,Zhang et al. proposed to use facial attribute recognition as an auxiliary task to enhance face alignment performance using deep convolutional neural network .,0
7783,"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .",1
7784,"Though there exist several works attempt to jointly solve them , there are still limitations in these works .",0
7785,"For example ,",0
7786,Chen et al. jointly conduct alignment and detection with random forest using features of pixel value difference .,0
7787,"But , the handcraft features used limits its performance .",0
7788,Zhang et al.,0
7789,"use multi - task CNN to improve the accuracy of multi-view face detection , but the detection accuracy is limited by the initial detection windows produced by a weak face detector .",0
7790,"On the other hand , in the training process , mining hard samples in training is critical to strengthen the power of detector .",0
7791,"However , traditional hard sample mining usually performs an offline manner , which significantly increases the manual operations .",0
7792,"It is desirable to design an online hard sample mining method for face detection and alignment , which is adaptive to the current training process automatically .",0
7793,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .",1
7794,The proposed CNNs consist of three stages .,1
7795,"In the first stage , it produces candidate windows quickly through a shallow CNN .",1
7796,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .",1
7797,"Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .",1
7798,"Thanks to this multi-task learning framework , the performance of the algorithm can be notably improved .",0
7799,The major contributions of this paper are summarized as follows :,0
7800,II .,0
7801,APPROACH,0
7802,"In this section , we will describe our approach towards joint face detection and alignment .",0
7803,A.,0
7804,Overall Framework,0
7805,The overall pipeline of our approach is shown in .,0
7806,"Given an image , we initially resize it to different scales to build an image pyramid , which is the input of the following three - stage cascaded framework :",0
7807,"Stage 1 : We exploit a fully convolutional network [?] , called Proposal Network ( P - Net ) , to obtain the candidate windows and their bounding box regression vectors in a similar manner as .",0
7808,Then we use the estimated bounding box regression vectors to calibrate the candidates .,0
7809,"After that , we employ non-maximum suppression ( NMS ) to merge highly overlapped candidates .",0
7810,"Stage 2 : all candidates are fed to another CNN , called Refine Network ( R - Net ) , which further rejects a large number of false candidates , performs calibration with bounding box regression , and NMS candidate merge .",0
7811,Stage 3 :,0
7812,"This stage is similar to the second stage , but in this stage we aim to describe the face in more details .",0
7813,"In particular , the network will output five facial landmarks ' positions .",0
7814,B. CNN Architectures,0
7815,"In , multiple CNNs have been designed for face detection .",0
7816,"However , we noticed its performance might be limited by the following facts :",0
7817,( 1 ) Some filters lack diversity of weights that may limit them to produce discriminative description .,0
7818,"Compared to other multi-class objection detection and classification tasks , face detection is a challenge binary classification task , so it may needless numbers of filters but more discrimination of them .",0
7819,"To this end , we reduce the number of filters and change the 55 filter to a 33 filter to reduce the computing while increase the depth to get better performance .",0
7820,"With these improvements , compared to the previous architecture in , we can get better performance with less runtime ( the result is shown in .",0
7821,"For fair comparison , we use the same data for both methods ) .",0
7822,Our CNN architectures are showed in .,0
7823,C. Training,0
7824,"We leverage three tasks to train our CNN detectors : face / non - face classification , bounding box regression , and facial landmark localization .",0
7825,1 ) Face classification :,0
7826,The learning objective is formulated as a two - class classification problem .,0
7827,"For each sample , we use the cross - entropy loss : where is the probability produced by the network that indicates a sample being a face .",0
7828,The notation denotes the ground - truth label .,0
7829,2 ) Bounding box regression :,0
7830,"For each candidate window , we predict the offset between it and the nearest ground truth ( i.e. , the bounding boxes ' left top , height , and width ) .",0
7831,"The learning objective is formulated as a regression problem , and we employ the Euclidean loss for each sample :",0
7832,where ?,0
7833,regression target obtained from the network and is the ground - truth coordinate .,0
7834,"There are four coordinates , including left top , height and width , and thus .",0
7835,3 ) Facial landmark localization :,0
7836,"Similar to the bounding box regression task , facial landmark detection is formulated as a regression problem and we minimize the Euclidean loss :",0
7837,where ?,0
7838,is the facial landmark 's coordinate obtained from the network and is the ground - truth coordinate .,0
7839,"There are five facial landmarks , including left eye , right eye , nose , left mouth corner , and right mouth corner , and thus .",0
7840,4 ) Multi-source training :,0
7841,"Since we employ different tasks in each CNNs , there are different types of training images in the learning process , such as face , non-face and partially aligned face .",0
7842,"In this case , some of the loss functions ( i.e. , Eq. ( 1 ) - ( 3 ) ) are not used .",0
7843,"For example , for the sample of background region , we only compute , and the other two losses are set as 0 .",0
7844,This can be implemented directly with a sample type indicator .,0
7845,Then the overall learning target can be formulated as :,0
7846,where is the number of training samples .,0
7847,denotes on the task importance .,0
7848,"We use in P - Net and R - Net , while in O - Net for more accurate facial landmarks localization .",0
7849,is the sample type indicator .,0
7850,"In this case , it is natural to employ stochastic gradient descent to train the CNNs .",0
7851,5 ) Online Hard sample mining :,0
7852,"Different from conducting traditional hard sample mining after original classifier had been trained , we do online hard sample mining in face classification task to be adaptive to the training process .",0
7853,"In particular , in each mini-batch , we sort the loss computed in the forward propagation phase from all samples and select the top 70 % of them as hard samples .",0
7854,Then we only compute the gradient from the hard samples in the backward propagation phase .,0
7855,That means we ignore the easy samples that are less helpful to strengthen the detector while training .,0
7856,Experiments show that this strategy yields better performance without manual sample selection .,0
7857,Its effectiveness is demonstrated in the Section III .,0
7858,III .,0
7859,EXPERIMENTS,0
7860,"In this section , we first evaluate the effectiveness of the proposed hard sample mining strategy .",0
7861,"Then we compare our face detector and alignment against the state - of - the - art methods in Face Detection Data Set and Benchmark ( FDDB ) , WIDER FACE , and Annotated Facial Landmarks in the Wild ( AFLW ) benchmark .",1
7862,"FDDB dataset contains the annotations for 5,171 faces in a set of 2,845 images .",0
7863,"WIDER FACE dataset consists of 393,703 labeled face bounding boxes in 32,203 images where 50 % of them for testing into three subsets according to the difficulty of images , 40 % for training and the remaining for validation .",0
7864,"AFLW contains the facial landmarks annotations for 24,386 faces and we use the same test subset as .",0
7865,"Finally , we evaluate the computational efficiency of our face detector .",0
7866,A. Training Data,0
7867,"Since we jointly perform face detection and alignment , here we use four different kinds of data annotation in our training process : ( i ) Negatives : Regions that the Intersection - over - Union ( IoU ) ratio less than 0.3 to any ground - truth faces ; ( ii ) Positives : IoU above 0.65 to aground truth face ; ( iii ) Part faces : IoU between 0.4 and 0.65 to aground truth face ; and ( iv ) Landmark faces : faces labeled 5 landmarks ' positions .",0
7868,"Negatives and positives are used for face classification tasks , positives and part faces are used for bounding box regression , and landmark faces are used for facial landmark localization .",0
7869,The training data for each network is described as follows :,0
7870,"1 ) P- Net : We randomly crop several patches from WIDER FACE to collect positives , negatives and part face .",0
7871,"Then , we crop faces from CelebA as landmark faces",0
7872,"2 ) R - Net : We use first stage of our framework to detect faces from WIDER FACE to collect positives , negatives and part face while landmark faces are detected from CelebA .",0
7873,3 ) O - Net : Similar to R - Net to collect data but we use first two stages of our framework to detect faces .,0
7874,B .,0
7875,The effectiveness of online hard sample mining,1
7876,"To evaluate the contribution of the proposed online hard sample mining strategy , we train two O - Nets ( with and without online hard sample mining ) and compare their loss curves .",0
7877,"To make the comparison more directly , we only train the O - Nets for the face classification task .",0
7878,All training parameters including the network initialization are the same in these two O - Nets .,0
7879,"To compare them easier , we use fix learning rate .",0
7880,shows the loss curves from two different training ways .,0
7881,It is very clear that the hard sample mining is beneficial to performance improvement .,1
7882,C.,0
7883,The effectiveness of joint detection and alignment,1
7884,"To evaluate the contribution of joint detection and alignment , we evaluate the performances of two different O - Nets ( joint facial landmarks regression task and do not joint it ) on FDDB ( with the same P - Net and R - Net for fair comparison ) .",0
7885,We also compare the performance of bounding box regression in these two O - Nets. suggests that joint landmarks localization task learning is beneficial for both face classification and bounding box regression tasks .,1
7886,D. Evaluation on face detection,1
7887,"To evaluate the performance of our face detection method , we compare our method against the state - of - the - art methods in FDDB , and the state - of - the - art methods in WIDER FACE .",0
7888,( a ) - ( d ) shows that our method consistently outperforms all the previous approaches by a large margin in both the benchmarks .,1
7889,We also evaluate our approach on some challenge photos 1 .,0
7890,E .,0
7891,Evaluation on face alignment,1
7892,"In this part , we compare the face alignment performance of our method against the following methods : RCPR , TSPM , Luxand face SDK , ESR , CDM , SDM , and TCDCN .",0
7893,"In the testing phase , there are 13 images that our method fails to detect face .",0
7894,So we crop the central region of these 13 images and treat them as the input for O - Net .,0
7895,"The mean error is measured by the distances between the estimated Examples are showed in http://kpzhang93.github.io/SPL/index.html landmarks and the ground truths , and normalized with respect to the inter-ocular distance .",0
7896,( e ) shows that our method outperforms all the state - of - the - art methods with a margin .,1
7897,F. Runtime efficiency,0
7898,"Given the cascade structure , our method can achieve very fast speed in joint face detection and alignment .",0
7899,It takes 16 fps on a 2.60 GHz CPU and 99 fps on GPU ( Nvidia Titan Black ) .,0
7900,Our implementation is currently based on un - optimized MATLAB code .,0
7901,IV .,0
7902,CONCLUSION,0
7903,"In this paper , we have proposed a multi-task cascaded CNNs based framework for joint face detection and alignment .",0
7904,"Experimental results demonstrate that our methods consistently outperform the state - of - the - art methods across several challenging benchmarks ( including FDDB and WIDER FACE benchmarks for face detection , and AFLW benchmark for face alignment ) while keeping real time performance .",0
7905,"In the future , we will exploit the inherent correlation between face detection and other face analysis tasks , to further improve the performance .",0
7906,title,0
7907,A Fast and Accurate Unconstrained Face Detector,0
7908,abstract,0
7909,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .",1
7910,"First , a new image feature called Normalized Pixel Difference ( NPD ) is proposed .",0
7911,"NPD feature is computed as the difference to sum ratio between two pixel values , inspired by the Weber Fraction in experimental psychology .",0
7912,"The new feature is scale invariant , bounded , and is able to reconstruct the original image .",0
7913,"Second , we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations , so that complex face manifolds can be partitioned by the learned rules .",0
7914,"This way , only a single soft - cascade classifier is needed to handle unconstrained face detection .",0
7915,"Furthermore , we show that the NPD features can be efficiently obtained from a lookup table , and the detection template can be easily scaled , making the proposed face detector very fast .",0
7916,"Experimental results on three public face datasets ( FDDB , GENKI , and CMU - MIT ) show that the proposed method achieves state - of - the - art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes .",0
7917,INTRODUCTION,0
7918,The objective of face detection is to find and locate faces in an image .,0
7919,It is the first step in automatic face recognition applications .,1
7920,Face detection has been well studied for frontal and near frontal faces .,0
7921,"The Viola and Jones ' face detector is the most well known face detection algorithm , which is based on Haar - like features and cascade AdaBoost classifier .",0
7922,"However , in unconstrained scenes such as faces in a crowd , state - of - the - art face detectors fail to perform well due to large pose variations , illumination variations , occlusions , expression variations , out - of - focus blur , and low image resolution .",0
7923,"For example , the Viola - Jones face detector fails to detect most of the face images in the Face Detection Data set and Benchmark ( FDDB ) database ( examples shown in ) due to the difficulties mentioned above .",0
7924,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .",1
7925,We are interested in face detection in unconstrained scenarios such as video surveillance or images captured by hand - held devices .,0
7926,"Numerous face detection methods have been developed following Viola and Jones ' work , mainly focusing on extracting different types of features and developing different cascade structures .",0
7927,"A variety of complex features , , , , , , , , .",0
7928,"Face images annotated ( red ellipses ) in the FDDB database. , have been proposed to replace the Haarlike features used in .",0
7929,"While these methods can improve the face detection performance to some extent , they generate a very large number ( hundreds of thousands ) of features and the resulting systems take too much time to train .",0
7930,"Another development in face detection has been to learn different cascade structures for multiview face detection , such as parallel cascade , pyramid architecture , and Width - First - Search ( WFS ) tree .",0
7931,All these methods need to learn one cascade classifier for each specific facial view ( or view range ) .,0
7932,"In unconstrained scenarios , however , it is not easy to define all possible views of a face , and the computational cost increases with an increasing number of classifiers in complex cascade structures .",0
7933,"Moreover , these approaches require manual labeling of face pose in each training image .",0
7934,"While some of the available methods , , can handle multiview faces , they are notable ar Xiv:1408.1656v3 [ cs.CV ] 7 Sep 2015 to simultaneously consider other challenges such as occlusion .",0
7935,"In fact , since these methods require partitioning multiview data into known poses , occlusion is not easy to handle in this way .",0
7936,"On the other hand , while several studies addressed face detection under occlusion , , , , , they constrained themselves to detect only frontal faces under occlusion .",0
7937,"As discussed in , a robust face detection algorithm should be effective under arbitrary variations in pose and occlusion , which remains an unresolved challenging problem .",0
7938,"In this paper , we are interested in developing effective features and robust classifiers for unconstrained face detection with arbitrary facial variation .",0
7939,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .",1
7940,"An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .",1
7941,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .",1
7942,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .",1
7943,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .",1
7944,"While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .",0
7945,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .",1
7946,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",1
7947,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .",1
7948,The novelty of this work is summarized as follows :,0
7949,"A new type of feature , called NPD is proposed , which is efficient to compute and has several desirable properties , including scale invariance , boundedness , and enabling reconstruction of the original image .",0
7950,A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability .,0
7951,"In this way , only a single soft - cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints , without pose labeling or clustering in the training stage .",0
7952,The advantages of the proposed approach include :,0
7953,"The NPD feature evaluation is extremely fast , requiring a single memory access using a lookup table .",0
7954,Multiscale face detection can be easily achieved by applying pre-scaled detection templates .,0
7955,The unconstrained face detector does not depend on pose specific cascade structure design ; pose labeling or clustering in the training stage is also not required .,0
7956,"The face detector is able to handle illumination variations , pose variations , occlusions , outof - focus blur , and low resolution face images in unconstrained scenarios .",0
7957,The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / .,1
7958,The remainder of this paper is organized as follows .,0
7959,In Section 2 we review the related work .,0
7960,In Section 3 we introduce the NPD feature space .,0
7961,The proposed NPD based face detection method is presented in Section 4 .,0
7962,Experimental results are provided in Section 5 .,0
7963,"Finally , we summarize the contributions in Section 6 .",0
7964,RELATED WORK,0
7965,"As indicated in a survey of face detection methods , the most popular face detection methods are appearance based , which use local feature representation and classifier learning .",0
7966,Viola and Jones ' face detector was the first one to apply rectangular Haar - like features in a cascaded AdaBoost classifier for real - time face detection .,0
7967,Many approaches have been proposed around the Viola - Jones detector to advance the state of the art in face detection .,0
7968,"Lienhart and Maydt proposed an extended set of Haar - like features , where 45 rotated rectangular features were introduced .",0
7969,"Li et al. proposed another extension of Haar - like features , where the rectangles can be spatially set apart with a flexible distance .",0
7970,"A similar feature , called the diagonal filter was also proposed by Jones and Viola .",0
7971,"Various other local texture features have been introduced for face detection , such as the modified census transform , local binary pattern ( LBP ) , MB - LBP , LBP histogram , and the locally assembled binary feature .",0
7972,These features have been shown to be robust to illumination variations .,0
7973,Mita et al.,0
7974,proposed the joint Haarlike features to capture the co-occurrence of effective Haar - like features .,0
7975,Huang et al.,0
7976,"proposed a sparse feature set in a granular space , where granules were represented by rectangles , and each individual sparse feature was learned as a combination of granules .",0
7977,"A problem with the approaches in and is that the joint feature space is very large , making the optimal combination a difficult task .",0
7978,"While more sophisticated features may provide better discrimination power than Haar - like features for the face detection task , they generally increase the computational cost .",0
7979,"In contrast , ordinal relationships among image regions are simple yet effective image features , , , , , , .",0
7980,Sinha studied several robust ordinal relationships in face images and developed a face detection method accordingly .,0
7981,Liao et al .,0
7982,further showed that ordinal features can be effectively learned by AdaBoost classifier for face recognition .,0
7983,Sadr et al .,0
7984,"showed that pixelwise ordinal features ( POF ) , i.e. ordinal relationship ( x > y ) between any two pixels , can faithfully encode image structures .",0
7985,Lepetit and Fua applied POF features in random trees for keypoint recognition .,0
7986,Shotton applied POF features in random forests for image categorization and segmentation .,0
7987,"For facial analysis , Baluja et al.",0
7988,"showed that POF features are good enough for discriminating between five facial orientations , a relatively simpler task than face detection .",0
7989,Wang et al .,0
7990,applied the random forest classifier together with POF features for facial landmark localization .,0
7991,"Abramson and Steux proposed a pixel control point based feature for face detection , where each feature is associated with two sets of pixel locations ( control points ) .",0
7992,"Besides different feature representations , some researchers have also tried different AdaBoost algorithms and weak classifiers .",0
7993,"For weak classifiers utilized in boosting , Lienhart et al. and Brubaker et al.",0
7994,have shown that classification and regression trees ( CART ) work better than simple decision stumps .,0
7995,"In this paper , we show that the optimal ordinal / contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree .",0
7996,"In this way , unconstrained face variations can be automatically partitioned into different leaves of the learned quadratic tree classifier .",0
7997,"Given that the original Viola - Jones face detector has limitations for multiview face detection , various cascade structures have been proposed to tackle multiview face detection , , , .",0
7998,Jones and Viola extended their face detector by training one face detector for each specific pose .,0
7999,"To avoid evaluating all face detectors on each scanning subwindow , they developed a pose estimation step ( similar to Rowley et al. ) before face detection , and then only the face detector trained on that estimated pose was applied .",0
8000,"In this two - stage detection structure , if the pose estimation is not reliable , the face is not likely to be detected in the second stage .",0
8001,"Wu et al. proposed a parallel cascade structure for multiview face detection , where all face detectors tuned to different views have to be evaluated for each scanning window ; they did use the first few cascade layers of all face detectors to estimate the pose for speedup .",0
8002,"Li and Zhang proposed a coarse - to - fine pyramid architecture for multiview face detection , where the entire range of face poses was divided into increasingly smaller subranges , resulting in a more efficient detection structure .",0
8003,"Huang et al. proposed a WFS tree based multiview face detection approach , which also works in a coarse - to - fine manner .",0
8004,"They proposed the Vector Boost algorithm for multiclass learning , which is well suited for multiview pose estimation .",0
8005,"However , all these methods need to learn a cascade classifier for each specific view ( or view range ) of a face , requiring an input face image to go through different branches of the detection structure .",0
8006,"Hence , their computational cost generally increases with the number of classifiers in complex cascade structures .",0
8007,"Moreover , these approaches require manual labeling of the face pose in each training image .",0
8008,"Instead of designing a detection structure , Lin and Liu proposed to learn the multiview face detector as a single cascade classifier .",0
8009,"They derived a multiclass boosting algorithm , called MBHBoost by sharing features among different classes .",0
8010,This is a simpler approach to multiview face detection than designing complex cascade structures .,0
8011,"Nevertheless , it still requires manual labeling of poses .",0
8012,"In uncontrolled environments , however , it is not easy to define specific views of a face by discretizing the pose space , because a face could be in arbitrary pose simultaneously in yaw ( out - of - plane ) , roll ( in - plane ) , and pitch ( up - anddown ) angles .",0
8013,"To avoid manual labeling , Seemann et al. suggested learning viewpoint clusters automatically for object detection .",0
8014,"However , for human faces , Kim and Cipolla showed that clustering by traditional techniques like K - Means does not result in categorized poses .",0
8015,"They hence proposed a multiclassifier boosting ( MCBoost ) for human perceptual clustering of object images , which showed promise for clustering face poses .",0
8016,"However , the clusters are not always related to pose variations ; in addition to different pose clusters , they also obtained clusters with various illumination variations .",0
8017,"Face detection in presence of occlusion is also an important issue in unconstrained face detection , but it has received less attention compared to multiview face detection .",0
8018,"This is probably because , compared to pose variations , it is more difficult to categorize arbitrary occlusions into predefined classes .",0
8019,"Hotta proposed a local kernel based SVM method for face detection , which was better than global kernel based SVM in detecting occluded frontal faces .",0
8020,Lin et al .,0
8021,considered 8 kinds of manually defined facial occlusions by training 8 additional cascade classifiers besides the standard face detector .,0
8022,"Lin and Liu further proposed the MBHBoost algorithm to handle faces with one of 12 in - plane rotations or one of 8 types of occlusions , with each kind of rotation and occlusion treated as a different class .",0
8023,Chen et al.,0
8024,"proposed a modified Viola - Jones face detector , where the trained detector was divided into sub- classifiers related to several predefined local patches , and the outputs of sub -classifiers were fused .",0
8025,"Goldmann et al. proposed a component - based approach for face detection , where the two eyes , nose , and mouth were detected separately , and further connected in a topology graph .",0
8026,"However , none of the above methods considered face detection with both occlusions and pose variations simultaneously in unconstrained scenarios .",0
8027,"As discussed in , a robust face detector should be effective under arbitrary variations in pose and occlusion , which has not yet been solved .",0
8028,"Recently , unconstrained face detection has gained attention .",0
8029,Jain and Learned - Miller developed the FDDB database and benchmark for the development of unconstrained face detection algorithms .,0
8030,"This database contains images collected from the Internet , and presents challenging scenarios for face detection .",0
8031,"Subburaman and Marcel proposed a fast bounding box estimation technique for face detection , where the bounding box is predicted by small patch based local search .",0
8032,Jain and Learned - Miller proposed an online domain adaption approach to improve the performance of the Viola - Jones face detector on the FDDB database .,0
8033,"Li et al. proposed the use of SURF feature in an AdaBoost cascade , and area under the curve ( AUC ) criterion to speedup the face detector training .",0
8034,"Shen et al. proposed an exemplar - based face detection approach , which retrieves images from a large annotated face dataset ; facial landmark locations are inferred from the annotations .",0
8035,This method is further improved in by boosting .,0
8036,Li et al. proposed a probabilistic elastic part ( PEP ) model to adapt any pre-trained face detector to a specific image collection like FDDB by an additional post -processing classifier .,0
8037,"Zhu and Ramanan proposed to jointly detect a face , estimate its pose , and localize face landmarks in the wild by a Deformable Parts - based Model ( DPM ) , which was further improved in and .",0
8038,Chen et al. proposed to combine the face detection and landmark estimation tasks in a joint cascade framework to refine face detection by precise landmark detections .,0
8039,Yang et al .,0
8040,"investigated the use of channel features for face detection , which achieves promising performance .",0
8041,"Despite the availability of these methods for unconstrained face detection , the detection accuracy is still not satisfactory , especially when the detector is required to have low false alarms .",0
8042,NORMALIZED PIXEL DIFFERENCE FEA - TURE SPACE,0
8043,The Normalized Pixel Difference ( NPD ) feature between two pixels in an image is defined as,0
8044,"where x , y ?",0
8045,"0 are intensity values of the two pixels 1 , and f ( 0 , 0 ) is defined as 0 when x = y = 0 .",0
8046,1 .,0
8047,"For ease of representation , sometimes we also denote x and y as pixels instead of pixel values .",0
8048,We use subscripts to differentiate between pixel and pixel values only when pixel locations are under discussion .,0
8049,The NPD feature measures the relative difference between two pixel values .,0
8050,"The sign off ( x , y) indicates the ordinal relationship between the two pixels x and y , and the magnitude off ( x , y) measures the relative difference ( as a percentage of the joint intensity x + y ) between x and y .",0
8051,"Note that the definition f ( 0 , 0 ) 0 is reasonable because , in this case , there is no difference between the two pixels x and y .",0
8052,"Compared to the absolute difference | x ? y| , NPD is invariant to scale change of the pixel intensities .",0
8053,"Weber , a pioneer in experimental psychology , stated that the just - noticeable difference in the magnitude change of a stimulus is proportional to the magnitude of the stimulus , rather than its absolute value .",0
8054,This is known as the Weber 's Law .,0
8055,"In other words , the human perception of difference in stimulus is often measured as a fraction of the original stimulus , that is , in a form ? I/ I , which is called the Weber Fraction .",0
8056,Chen et al .,0
8057,"proposed a local image descriptor , called Weber 's Law Descriptor for face recognition , which was computed from Weber Fractions of pixels in a 3 3 window .",0
8058,"The proposed feature in Eq. ( 1 ) has also been used in other fields such as remote sensing , where the Normalized Difference Vegetation Index ( NDVI ) is defined as the difference to sum ratio between the visible red and the near infrared spectra to estimate the green vegetation coverage .",0
8059,The NPD feature has a number of desirable properties .,0
8060,"First , the NPD feature is antisymmetric , so either f ( x , y) or f ( y , x ) is adequate for feature representation , resulting in a reduced feature space .",0
8061,"Therefore , in an s s image patch ( vectorized asp 1 , where p = s s ) , NPD feature f ( x i , x j ) for pixel pairs 1 ? i < j ? p is computed , resulting ind = p ( p ? 1 ) / 2 features .",0
8062,"For example , in a 2020 face template , there are ( 20 20 ) ( 20 20 ? 1 ) / 2 = 79 , 800 NPD features in total .",0
8063,"We call the resulting feature space the NPD feature space , denoted as ? npd ( ?",0
8064,Rd ) .,0
8065,"Second , the sign off ( x , y ) is an indicator of the ordinal relationship between x and y .",0
8066,"Ordinal relationship has been shown to bean effective encoding for object detection and recognition , , because ordinal relationship encodes the intrinsic structure of an object image and it is invariant under various illumination changes .",0
8067,"However , simply using the sign to encode the ordinal relationship is likely to be sensitive to noise when x and y have similar values .",0
8068,In the next section we will show how to learn robust ordinal / contrastive relationships with NPD features .,0
8069,"Third , the NPD feature is scale invariant , which is expected to be robust against illumination changes .",0
8070,"This is important for image representation , since illumination change is always a troublesome issue for both object detection and recognition .",0
8071,"Fourth , as shown in Appendix A , the NPD feature f ( x , y ) is bounded in [ - 1 , 1 ] .",0
8072,The bounded property makes the NPD feature amenable to histogram binning or threshold learning in tree - based classifiers .,0
8073,"The proof of Theorem 1 is shown in Appendix B , which also gives a linear - time approach to reconstruct the original image up to a scale factor .",0
8074,Theorem 1 states that each point in the feature space ?,0
8075,npd corresponds to a group of intensity - scaled images in the original pixel intensity space .,0
8076,"In contrast , the scale invariance property says that all intensity - scaled images are "" compressed "" to a point in the bounded feature space ?",0
8077,npd .,0
8078,"Therefore , ?",0
8079,"npd is a feature space which is invariant to scale variations , but it carries all the necessary information from the original space .",0
8080,NPD FOR FACE DETECTION,0
8081,Deep Quadratic,0
8082,Tree,0
8083,The classic Viola - Jones face detector learns representative features by boosted stumps .,0
8084,A stump is a basic tree classifier with one threshold that splits anode in two leaves .,0
8085,There are two limitations with stumps .,0
8086,"First , this shallow structure can not capture interactions between different feature dimensions .",0
8087,"Second , the simple thresholding ignores higher - order information contained in a feature .",0
8088,"Therefore , in this paper , we consider a quadratic splitting strategy and a deeper tree structure .",0
8089,"Specifically , fora feature x , we consider the tree node splitting as",0
8090,"where a , b , care constants w.r.t. x , and t is the splitting threshold .",0
8091,"With appropriate coefficients , this corresponds to checking whether x is in a range [? 1 , ? 2 ] or not , where ? 1 and ?",0
8092,2 are two learned thresholds .,0
8093,"Compared to the original linear splitting x < t , Eq.",0
8094,"( 2 ) considers both the first - order and second - order information of x , enabling a better interpretation of the splitting rule .",0
8095,"Particularly , for the proposed NPD feature , three kinds of object structures can be learned :",0
8096,where ? 1 < 0 and ? 2 >,0
8097,"0 . Eq. ( 3 ) applies if the object pixel x is notably darker than pixel y ( e.g. f 1 in ) , while Eq. ( 4 ) covers the case when pixel x is notably brighter than pixel y ( e.g. f 2 in ) .",0
8098,These two kinds of structures can also be learned by a classic stump .,0
8099,"They are also known as ordinal relationships similar as in , except that a better threshold is learned instead of the default threshold 0 .",0
8100,"In contrast , if Eq. ( 5 ) does not hold , then there will be a notable edge or contrast between pixels x and y ( e.g. f 3 and f 4 in ) , but the polarity is uncertain .",0
8101,"For example , f 3 in represents a notable edge between the face and background , but the background pixel can be either darker or brighter than the face .",0
8102,This kind of contrastive structure can only be learned by a quadratic splitting .,0
8103,"In practice , instead of solving Eq. ( 2 ) for quadratic splitting , we propose to quantize the feature range into L discrete bins ( e.g. L= 256 in this paper ) , and do an exhaustive search to determine the two optimal thresholds , where the weighted mean square error is applied as the optimal splitting criterion .",0
8104,"Thanks to the bounded property of the proposed NPD feature , this quantization can be easily done .",0
8105,"Besides , we build an L-bin histogram of the sample weights , and apply a one-dimensional integral technique similar as in to speedup the splitting .",0
8106,"Furthermore , we apply the quadratic splitting to learn a deep tree ( e.g. depth of eight in this paper ) , instead of a stump or a shallow tree for face detection .",0
8107,"This way , several NPD features are optimally combined together to represent the intrinsic face structure .",0
8108,An example is shown in .,0
8109,"The proposed deep quadratic tree is well suited for face detection with arbitrary pose variations , since similar views can be clustered in the same leaf node of the tree .",0
8110,Face Detector,0
8111,"Given that the proposed NPD features contain redundant information , we also apply the AdaBoost algorithm to select the most discriminative features and construct strong classifiers .",0
8112,We adopt the Gentle AdaBoost algorithm to learn the NPD feature based deep quadratic trees .,0
8113,"As in , a cascade classifier is further learned for rapid face detection .",0
8114,We only learn one single cascade classifier for unconstrained face detection robust to occlusions and pose variations .,0
8115,This implementation has the advantage that there is no need to label the pose of each face image manually or cluster the poses before training the detector .,0
8116,"In the learning process , the algorithm automatically divides the whole face manifold into several sub-manifolds by the deep quadratic trees .",0
8117,"Besides , we adopt the soft cascade structure for efficient training and early rejection of negative samples .",0
8118,"Specifically , soft cascade can be regarded as a single AdaBoost classifier with one exit per weak classifier .",0
8119,"In each iteration , a deep quadratic tree is learned as the weak classifier , and a threshold of the current AdaBoost classifier is also learned for rejecting nonfaces .",0
8120,"Finally , the learned deep quadratic trees and thresholds are aggregated sequentially to represent an ensemble .",0
8121,Below is a summary of how the proposed method handles the unconstrained face detection problem .,0
8122,Occlusion .,0
8123,"In contrast to Haar - like features that are sensitive to occlusions because of large support , NPD features are computed by only two pixel values , making them robust to occlusion .",0
8124,Illumination .,0
8125,"Since NPD features are scale invariant , they are robust to illumination changes .",0
8126,Blur or low image resolution .,0
8127,"Because the NPD features involve only two pixel values , they do not require rich texture information on the face .",0
8128,This makes NPD features effective in handling blurred or low resolution face images .,0
8129,Implementation Details,0
8130,we used the Annotated Facial Landmarks in the Wild ( AFLW ) database for training our unconstrained face detector .,0
8131,"The AFLW database contains 25,993 face annotations in 21,997 real - world images collected from Flickr .",0
8132,"This is an unconstrained face database including large face variations in pose , illumination , expression , ethnicity , age , gender , etc .",0
8133,"We cropped 21,730 face images from AFLW .",0
8134,"Together with their mirrored images and perturbations in positions , we had 217,300 face images in total for training .",0
8135,Some examples are shown in .,0
8136,"For bootstrapping nonface images , we also used the AFLW images , but masked the facial regions with random images containing no faces , as shown in .",0
8137,We used a detection template of 24 24 pixels .,1
8138,"We set the maximum depth of the tree classifiers to be learned as eight , so that at most eight NPD features need to be evaluated for each tree classifier .",1
8139,"In the soft cascade training , we set the threshold of each exit as the minimal score of positive samples , i.e. we did not reject positive samples during training .",1
8140,"Our final detector contains 1,226 deep quadratic trees , and 46,401 NPD features .",1
8141,"Nevertheless , the average number of feature evaluations per detection window is only 114.5 considering stagewise nonface rejection , which is quite reasonable .",0
8142,"For an analysis , we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees ( CART ) with depth of four .",1
8143,"A subset of the training data 2 in was used , including 12,102 face images and 12,315 nonface images .",1
8144,The detection template is 20 20 pixels .,1
8145,"The detector cascade contains 15 stages , and for each stage , the target false accept rate was 0.5 , with a detection rate of 0.999 .",1
8146,Detector Speed Up,0
8147,"To further speedup the learned NPD detector for face detection , we develop the following two techniques .",0
8148,"First , for 8 - bit gray images , we build a 256 256 lookup table to store pre-computed NPD features .",0
8149,"This way , computing f ( x , y) in Eq. 1 only requires one memory access from the lookup table .",0
8150,"Second , the learned face detection template ( e.g. 20 20 used in this paper ) can be easily scaled to enable multiscale face detection .",0
8151,"So , we pre-compute multiscale detection templates and apply them to detect faces at various scales .",0
8152,"This way , iterative rescaling of images for multiscale detection is avoided .",0
8153,EXPERIMENTS,0
8154,"We evaluate the performance of the NPD face detector on three public - domain databases , FDDB , GENKI , and CMU - MIT .",0
8155,"We also provide an analysis of the proposed method , report the face detection speed , and report unconstrained face detection performances under illumination variations , pose variations , occlusion , and blur , respectively .",0
8156,"In the test stage , a scale factor of 1.2 was set for multiscale detection .",0
8157,"A postprocessing method similar to the OpenCV face detection module was implemented , which merges nearby detections by the disjoint set algorithm .",0
8158,"For each detected face , we summarized the scores of AdaBoost classifiers in all stages of the cascade to be the final score ; this score was used to generate the Receiver Operating Characteristic ( ROC ) curves .",0
8159,Evaluation on FDDB Database,1
8160,The FDDB dataset covers challenging scenarios for face detection .,0
8161,"Images in FDDB comes from the Faces in the Wild dataset , which is a large collection of Internet images collected from the Yahoo News .",0
8162,"It contains 2,845 images with a total of 5,171 faces , with a wide range of challenging scenarios including arbitrary pose , occlusions , different lightings , expressions , low resolutions , and out - of - focus faces .",0
8163,All faces in the database have been annotated with elliptical regions .,0
8164,shows some examples of the annotated faces from the FDDB database .,0
8165,"For benchmark evaluation , Jain and Learned - Miller provided an evaluation code fora comparison of different face detection algorithms .",0
8166,"There are two metrics for performance evaluation based on ROC : discrete score metric and continuous score metric , which correspond to coarse match ( similar to previous evaluations in the face detection literature ) and precise match , respectively , between the detection and the ground truth .",0
8167,"The database is divided into 10 subsets for performance evaluation , and the obtained detection results are accumulated to generate the ROC curve .",0
8168,We compared our method with state - of - the - art results reported on the FDDB website .,0
8169,"shows a comprehensive comparison of detection rates of various algorithms on the FDDB database at FP = 0 , 10 , and 100 , where methods marked with a 9 were trained on the same AFLW database as ours .",0
8170,"It can be observed that the proposed method outperforms most of the baseline methods except four methods , , , published recently .",1
8171,The proposed NPD face detector is the second best one at FP = 0 for the discrete metric and the third best one for the continuous metric .,1
8172,"Specifically , the NPD detector detects about 3 .",0
8173,"According to , ten ROC curves should be obtained and averaged for the final performance report , however , what is actually done in the FDDB results webpage ( Footnote 4 ) is that all detection results of the 10 subsets are first merged , and then a single ROC curve is evaluated .",0
8174,We followed the latter one for consistency to existing results .,0
8175,4 . http://vis-www.cs.umass.edu/fddb/results.html,0
8176,54 % of the annotated FDDB faces in coarse sense ( 50 % overlap with ground truth ) without any false alarms .,0
8177,The ROC curves of recent methods are depicted in for the discrete score metric and in for the continuous score metric .,0
8178,"In both Figs . 5 and 6 , the curve labels in the legend are sorted in descending order of the detection rates at zero false positives ( FP = 0 ) .",0
8179,"It can be observed that the proposed NPD detector is among the top performers for the discrete metric , though it is not as good as the four recent methods for the continuous metric .",1
8180,"However , note that the FDDB database uses ellipses for groundtruth of face annotations , and several methods ( e.g. Yan - DPM , and HeadHunter ) output similar elliptical detections to improve the performance especially with the continuous metric .",0
8181,"The proposed detector outputs square detections , followed by a 20 % horizontal expansion and 50 % vertical expansion as suggested in .",0
8182,"This processing is not as good as making elliptical detections , but is still better than the original square detections .",0
8183,"Compared to recent methods , the Joint Cascade algorithm is the most competitive one to us in terms of accuracy and speed ( see Sec. 5.6 ) .",1
8184,"However , the Joint Cascade method used a sophisticated postprocessing classifier to remove hard negatives and hence improved the results .",0
8185,Other methods are not efficient as compared in Sec. 5.6 .,0
8186,"Especially , the DPM based methods , , are known to be quite slow .",0
8187,"The method of Zhu-Ramanan has the advantage of learning from only hundreds of face images and it jointly outputs face bounding box , pose , and landmarks .",0
8188,But it requires manual landmark and pose annotations as face prior knowledge before train - with the discrete score metric ..,0
8189,ROC curves of recent methods on the FDDB database with the continuous score metric .,0
8190,ing .,0
8191,The performance of the Zhu-Ramanan model is quite impressive considering such a small training data .,0
8192,"However , the runtime cost of their model is very expensive .",0
8193,"As reported in , fora 1480 986 image , Zhu and Ramanan 's detector takes 231 seconds to run and allocates up to 2 GB memory .",0
8194,"In contrast , our model is more efficient , requiring only a few milliseconds per image and only 50 MB of memory as discussed in Sec. 5.6 .",0
8195,shows some examples of detected faces in the FDDB database by the proposed NPD method .,0
8196,"Many rotated , occluded , and out - of - focus faces can be successfully detected by the proposed method .",0
8197,"Some faces ( e.g. the 2nd image in row 1 , and the 4th image in row 3 in ) that are not annotated in the ground truth can still be detected by the proposed method .",0
8198,"However , there are a number of faces that can not be detected by the proposed method , especially in very crowded scenes ( see the 1st image and the 3rd image in row 1 , and the last two images in row 5 in .",0
8199,"Therefore , unconstrained face detection in crowded scenes is still very challenging and deserves more attention .",0
8200,Evaluation on GENKI Database,1
8201,"The GENKI database was collected by the Machine Perception Laboratory , University of California , San Diego .",0
8202,"We evaluated the current release of the GENKI database , GENKI - R2009a , on its SZSL subset , which contains 3,500 images collected from the Internet .",0
8203,"These images include a wide range of backgrounds , illumination conditions , geographical locations , personal identity , and ethnicity .",0
8204,"Some examples of face images from the GENKI database are shown in , with labeled detections by the proposed NPD method .",0
8205,Most images in the GENKI dataset contain only a single face .,0
8206,"In that sense , the GENKI dataset is not as challenging as the FDDB dataset .",0
8207,"Some of the images in the GENKI - SZSL dataset contain faces that are not labeled , therefore they are not suitable for the face detection evaluation task .",0
8208,"After removing such unlabeled images , we are left with 3,270 images for face detection evaluation .",0
8209,"We evaluated our unconstrained face detector , as well as the Viola - Jones face detector implemented in OpenCV 2.4 , and a commercial face detector , PittPatt .",0
8210,"We again used the benchmark evaluation code in for performance evaluation , but slightly modified the code for allowing ground truth annotations as rectangles .",0
8211,The ROC curves of the three methods are shown in for both the discrete and continuous score metrics .,0
8212,The results show that the proposed NPD face detector significantly outperforms both the Viola - Jones and PittPatt face detectors .,1
8213,Evaluation on CMU - MIT Database,1
8214,The CMU - MIT face dataset is one of the early benchmarks for face detection .,0
8215,"The CMU - MIT frontal face data set contains 130 gray - scale images with a total of 511 faces , most of which are not occluded .",0
8216,We applied our frontal NPD face detector described in Subsection 4.2.1 on this database .,0
8217,We also used the modified benchmark evaluation code from with the discrete score metric for performance evaluation .,0
8218,"shows the ROC curves for the proposed NPD face detector , the Soft cascade method , the SURF cascade method , and the Viola - Jones detector .",0
8219,"The results show that , compared to the Viola - Jones frontal face detector , the NPD detector performs better when the number of false positives , FP < 50 , while it is slightly worse than Viola - Jones at higher FPs .",1
8220,"Compared to the SURF cascade detector , the NPD .",0
8221,Detected faces in the FDDB database by the proposed NPD method .,0
8222,"Green boxes are detections by the NPD detector , while red ellipses are ground truth annotations .",0
8223,"detector is better when FP < 3 , but SURF cascade method outperforms NPD at higher FPs .",0
8224,"Note that the SURF cascade method uses a face template of size 40 40 pixels , which is four times larger than our face detection template ( 20 20 pixels ) .",0
8225,"Generally , a larger face template contains more features for face description , but is computationally more expensive and may have a limitation in detecting blurred faces .",0
8226,"In addition , the proposed NPD method is not as good as the Soft cascade , the state - of - the - art method on the CMU - MIT dataset .",0
8227,"Still , the proposed NPD method can detect about 80 % of the frontal faces without any false positives , which is promising .",0
8228,Some of the detected faces in the CMU - MIT dataset by the proposed NPD method are shown in .,0
8229,Analysis of the Proposed Face Detector,0
8230,"Since the proposed face detector is a combination of the NPD features and tree classifiers , it is instructive to determine the contribution of each of these two components .",0
8231,"In the following , we trained all compared face detectors on the same training set and cascade training settings described in Section 4.2.1 .",0
8232,"First , we fixed the classic regression tree ( CART ) based weak learner with depth of four , and compared the proposed NPD feature to three .",0
8233,Detected faces in the GENKI - SZSL dataset by the proposed NPD method ..,0
8234,Detected faces in the CMU - MIT dataset by the proposed NPD method ..,0
8235,ROC curves for face detection on the CMU - MIT dataset .,0
8236,"other local features , namely Haar - like features , LBP , and POF , , , .",0
8237,"Since LBP is a discrete label , we treated it as a categorical variable in the regression tree learning , that is , for branching at each tree node , the algorithm finds the optimal criterion that splits the discrete LBP codes into two groups .",0
8238,"Using the same training set , we trained the three detectors using Haar , LBP , and POF , respectively .",0
8239,The model complexity of these detectors is summarized in .,0
8240,"It can be observed that , the CART based NPD model is more efficient than the POF model , though it requires slightly more feature evaluations than the Haar and LBP models .",0
8241,"However , it should be noted that the computation of Haar - like features requires computing integral images , while for LBP , each feature needs to compare 8 pairs of pixels and convert the resulting binary string to the corresponding decimal number .",0
8242,"In contrast , using lookup tables as aforementioned , computing the NPD feature requires only one memory access .",0
8243,"The four detectors with different local features were tested on the FDDB database , resulting in ROC curves shown in for both the discrete and continuous score metrics .",0
8244,"The NPD detector performs better than the Haar , LBP , and POF detectors with the same CART based weak learners .",0
8245,"The performance improvements due to NPD features over Haar , LBP , and POF features are about 6 % , 19 % , and 15 % , respectively , for discrete metric , and about 4 % , 13 % , and 10 % , respectively , for continuous metric , at FP = 1 .",0
8246,"NPD is better than POF , because with NPD features the regression tree learns optimal thresholds to form more robust ordinal rules .",0
8247,"NPD performs better than Haar and LBP , especially at low false positives , indicating that combining optimal pixel - level features in regression trees provides better discrimination between faces and nonfaces .",0
8248,"We also tried a variation of NPD , defined as f ( x , y ) = x?y ? x 2 +y 2 and denoted as NPD2 .",0
8249,"The comparison on FDDB are illustrated in , showing that the performance of NPD is slightly better than that of NPD2 .",0
8250,"Therefore , given that NPD is simpler than NPD2 , we prefer the formulation of Eq. ( 1 ) .",0
8251,"Next , we fixed the NPD feature representation and the classic cascade architecture , and compared three different weak learners , namely the stump classifier , the classic regression tree CART , and the proposed deep quadratic tree ( DQT ) .",0
8252,Both CART and DQT were with depth of four .,0
8253,"As shown in , the stump based detector requires much more weak classifiers than CART , indicating that combining NPD features in a deeper regression tree is much more effective .",0
8254,"Furthermore , shows that using CART does not increase the average computation cost compared to stump w.r.t. average feature evaluations .",0
8255,"In addition , the proposed DQT based learner further reduces the number of weak classifiers and average feature evaluations required .",0
8256,"The three face detectors were tested on the FDDB database , resulting in ROC curves shown in for both the discrete score metric and continuous score metric .",0
8257,"As illustrated , using CART instead of stump classifier improves the face detection performance by about 0 % - 17 % for discrete metric and 0 % - 11 % for continuous metric .",0
8258,The improvement is larger at smaller false positives .,0
8259,This verifies that tree classifiers help to optimally combine NPD features for the complex unconstrained face detection task .,0
8260,"Besides , the DQT based detector further improves the performance , due to its quadratic splitting capability compared to linear splitting .",0
8261,"Finally , with NPD + DQT , we compared the soft cascade detector and the classic cascade detector , as shown in and .",0
8262,"Clearly , with comparable performance , soft cascade further reduces the model complexity .",0
8263,Evaluation Under Specific Challenges,0
8264,"In the following , we evaluate how the proposed NPD face detector performs under illumination variation , pose variation , occlusion , and blur ( or low resolution ) .",0
8265,Note that these four challenges are often encountered simultaneously in an image .,0
8266,"In our selection of the four subsets , one per specific challenge , we focused on the main source of variation in each image .",0
8267,"For each challenge , we selected 100 images from the FDDB database ( examples are shown in , and ran our unconstrained NPD face detector ( trained with AFLW ) on each subset separately .",0
8268,"shows that the NPD face detector performs the best on the pose and illumination subsets , thanks to the scale - invariant NPD features and the deep quadratic trees .",0
8269,"For the occlusion and blur subsets , the performance largely drops .",0
8270,"These results indicate that occlusion and blur are the two major challenges for unconstrained face detection , which have not been well addressed in the literature .",0
8271,"The NPD face detector is also compared with the Viola - Jones face detector implemented in OpenCV 2.4 , and the commercial face detector PittPatt on the four subsets of FDDB discussed above .",0
8272,The resulting ROC curves with the discrete score metric are shown in .,0
8273,These plots show that the proposed NPD face detector outperforms both the Viola - Jones and the PittPatt face detectors on all the four subset - . ROC curves for face detection on four subsets from the FDDB database with the discrete score metric .,0
8274,s .,0
8275,"The reasons for the superior performance of the proposed method under illumination variations , pose variations , occlusions , and blur , were discussed in Subsection 4.2 .",0
8276,Detection Speed,0
8277,"For handheld devices like mobile phones , the available resources for computation and memory are rather limited .",0
8278,"Therefore , face detector 's complexity and detection speed are very important for embedded systems .",0
8279,"In this subsection , we report the detection speed of the proposed NPD face detector , compared with the Viola - Jones 5 face detector in OpenCV 2.4 , which is known to be optimized for speed .",0
8280,"The proposed NPD face detector is implemented in C ++ , which requires about 50 MB of memory in runtime .",0
8281,"The model size of our frontal detector is 41 KB , while that of the unconstrained detector is 831 KB .",0
8282,"Two platforms were selected for this evaluation : ( i ) a normal desktop PC with the Intel Core i5-2400 @ 3.1 GHz CPU ( 4 cores , 4 threads ) , and ( ii ) a netbook with Intel Atom N450 @ 1.6 GHz processor ( 1 core , 2 threads ) , to simulate low - end devices .",0
8283,"For face detection evaluation , a video clip of the movie "" Jobs "" was used .",0
8284,"This video clip shows a busy campus , with each frame containing from one to tens of faces .",0
8285,"The length of the video clip is about 2 minutes , containing 3,950 frames in total .",0
8286,The original resolution is 1280 720 .,0
8287,"To test the detection speed at various resolutions , the original video clip was cropped and resized to 1920 1080 , 800600 , and 640480 .",0
8288,"In this evaluation , the minimal face size to detect was set to 40 40 pixels for frontal detector as in and 8080 for unconstrained detector as in , , , and the scaling factor was 1.2 .",0
8289,The multi threading technique was enabled in both NPD and OpenCV detectors for parallel computation .,0
8290,"Note that we only calculated the face detection time , regardless of the video decoding time .",0
8291,"The test results ( in terms of Frame Per Second , FPS ) of frontal detectors are shown in .",0
8292,"The detection parameters of the SURF cascade method are the same as our algorithm , except that authors in used an i 7@3.2 GHz CPU ( 4 cores , 8 threads ) for the desktop computer .",0
8293,It can be observed that the NPD detector is much faster than both the OpenCV and SURF cascade detectors .,0
8294,"On Atom N450 processor , the detection speed of the NPD detector is about 9 times faster than the detection speed of the OpenCV detector ; on i5 processor the speed of the NPD detector is about 7 times the speed of the OpenCV detector .",0
8295,"shows that our frontal face detector can run in real - time ( 29.6 FPS ) on i5 desktop PC for processing 1920 1080 high definition videos , and 177.6 FPS for VGA videos .",0
8296,"On the low - end Atom platform , the NPD detector can run in near real - time for VGA videos .",0
8297,The reasons for the high processing speed of NPD are two folds .,0
8298,"First , the NPD feature is simple , involving only two pixels .",0
8299,"Further with the lookup table technique , the evaluation of each NPD feature requires only one memory access .",0
8300,"Second , the NPD feature can be easily scaled to various sizes of detection templates .",0
8301,"Therefore , pre-calculating and storing multiscale templates can speedup detection because rescaling the input image is avoided .",0
8302,"Next , we also evaluate our unconstrained detector and compare it to recent methods , as shown in Table 4 .",0
8303,The proposed method is much faster than Yan - DPM and ACF with either a single thread or multi-threads .,0
8304,The NPD detector achieves similar speed as that of Joint Cascade method .,0
8305,"Using multi-thread i5 CPU , we are able to achieve 70 FPS for unconstrained face detection on VGA frames , which is slower than the frontal detector , but still quite efficient . 16.2 n / a @ 3.1 GHz",0
8306,"1280 720 63.3 8.9 n/ a ( 4 cores , 4 threads ) 1920 1080 29 . 3.6 n/ a * "" n / a "" means results are not reported in for the SURF detector .",0
8307,SUMMARY AND FUTURE WORK,0
8308,We have proposed a fast and accurate method for face detection in cluttered scenes .,0
8309,"First , a simple feature called NPD is proposed , which has properties of scale invariance , boundedness , and reconstruction ability .",0
8310,"Second , we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations .",0
8311,"As a result , a single soft - cascade AdaBoost classifier is able to achieve promising results for face detection with large pose variations and occlusions .",0
8312,"Evaluations on three public face databases show that the proposed method achieves state - of - the - art performance for unconstrained face detection , and an analysis show that occlusions and blur are two big challenges for face detection .",0
8313,"The proposed detector is also efficient , about 6 times faster than the Viola - Jones face detector implemented in OpenCV 2.4 .",0
8314,It is interesting to apply the proposed NPD feature and the classifier learning method for other tasks such as face attribute classification and pedestrian detection .,0
8315,title,0
8316,LFFD : A Light and Fast Face Detector for Edge Devices,0
8317,abstract,0
8318,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",1
8319,This paper introduces a Light and Fast Face Detector ( LFFD ) for edge devices .,0
8320,The proposed method is anchorfree and belongs to the one - stage category .,0
8321,"Specifically , we rethink the importance of receptive field ( RF ) and effective receptive field ( ERF ) in the background of face detection .",0
8322,"Essentially , the RFs of neurons in a certain layer are distributed regularly in the input image and theses RFs are natural "" anchors "" .",0
8323,"Combining RF "" anchors "" and appropriate RF strides , the proposed method can detect a large range of continuous face scales with 100 % coverage in theory .",0
8324,The insightful understanding of relations between ERF and face scales motivates an efficient backbone for onestage detection .,0
8325,"The backbone is characterized by eight detection branches and common layers , resulting in efficient computation .",0
8326,Comprehensive and extensive experiments on popular benchmarks : WIDER FACE and FDDB are conducted .,0
8327,A new evaluation schema is proposed for application - oriented scenarios .,0
8328,"Under the new schema , the proposed method can achieve superior accuracy ( WIDER FACE Val / Test - Easy : 0.910/0.896 , Medium : 0.881/0.865 , Hard : 0.780/0.770 ; FDDB - discontinuous : 0.973 , continuous : 0.724 ) .",0
8329,Multiple hardware platforms are introduced to evaluate the running efficiency .,0
8330,The proposed method can obtain fast inference speed ( NVIDIA TITAN Xp : 131.45 FPS at 640480 ; NVIDIA TX2 : 136.99 PFS at 160120 ; Raspberry Pi 3 Model B + : 8.44 FPS at 160120 ) with model size of 9 MB .,0
8331,Method m AP ( % ),0
8332,Subset Easy Medium Hard ISRN 0.967 0.958 0.909 VIM-FD 0.967 0.957 0.907 DSFD 0.966 0.957 0.904 SRN 0.964 0.952 0.901 Pyramid Box 0.961 0.950 0.889 .,0
8333,Accuracy of the top - 5 methods on validation set of WIDER FACE .,0
8334,Introduction,0
8335,Face detection is a long - standing problem in computer vision .,0
8336,"In practice , it is the prerequisite to some face - related applications , such as face alignment and face recognition .",0
8337,"Besides , face detectors are always deployed on edge devices , such as mobile phones , IP cameras and IoT ( Internet of Things ) sensors .",0
8338,These devices have limited memory storage and low computing power .,0
8339,"Under such condition , face detectors that have high accuracy and fast running speed are in demand .",0
8340,"Current state of the art face detectors have achieved fairly high accuracy on convictive benchmark WIDER FACE by leveraging pre-trained heavy backbones like VGG16 , Resnet50/152 and Densenet 121 .",0
8341,We investigate the top - 5 methods on WIDER FACE and present their accuracy in .,0
8342,It can be observed that these methods have similar accuracy with marginal gaps which are hardly perceived in practical applications .,0
8343,It is difficult and unpractical to further boost the accuracy by using more complex and heavier backbones .,0
8344,"In our view , to better balance accuracy and latency is crucial for applying face detection to more applicable areas .",0
8345,Face detection is a fast - growing branch of general object detection in the past decade .,0
8346,The early work of Viola - Jones face detector proposes a classic detection framework - cascade classifiers with hand - crafted features .,0
8347,One of its well - known followers is aggregate channel features ( ACF ) which can take advantages of channel features effectively .,0
8348,"Although the methods mentioned above can achieve fast running speed , they rely on hand - crafted features and are not trained end - to - end , resulting in not robust detection accuracy .",0
8349,"Recently , convolutional neural network ( CNN ) based face detectors show great progress partially owing to the success of WIDER FACE benchmark .",0
8350,These methods can be roughly divided into two categories : two - stage methods and one - stage methods .,0
8351,"Two - stage methods consist of proposal selection and localization regression , which are mainly originated from R - CNN series .",0
8352,"Whereas , one - stage methods coherently combine classification and bounding box ( bbox ) regression , always achieving anchor- based and multi-scale detection simultaneously .",0
8353,"For most one - stage methods , anchor design and matching strategy is one of the essential components .",0
8354,"In order to improve the accuracy , these methods propose more complex modules based on heavy backbones .",0
8355,"Although the above methods can achieve state of the art results , they may not properly balance accuracy and latency .",0
8356,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .",1
8357,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,1
8358,One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,1
8359,These boxes always have different sizes and aspect ratios to cover objects with different scales and shapes .,0
8360,"Therefore , anchors play an important role inmost one - stage detection methods .",0
8361,"For some face detectors , sophisticated anchor strategies are crucial parts of the contributions .",0
8362,"However , anchor based methods may face three challenges :",0
8363,1 ) anchor matching is unable to sufficiently coverall face scales .,0
8364,"Although this can be relieved , it remains a problem ; 2 ) matching anchors to groundtruth bboxes is determined by thresholding IOU ( Intersection over Union ) .",0
8365,"The threshold is set empirically and it is difficult to make a solid investigation of its impact ; 3 ) setting the number of anchors for different scales depends on experiences , which may induce sample imbalance and redundant computation .",0
8366,"In our point of view , RF of neurons in feature maps are inherent and natural "" anchors "" .",0
8367,RF can easily handle above challenges .,0
8368,"Firstly , continuous scales of faces can be predicted within a certain RF size , rather than discrete scales in anchor - based methods .",0
8369,"Secondly , matching strategy is clear , namely a RF is matched to a groundtruth bbox if and only if it s center falls in the groundtruth bbox .",0
8370,"Thirdly , the number of RFs is naturally fixed and they are regularly distributed in the input image .",0
8371,"What 's more , we make a qualitative analysis on pairing face scales and RF sizes by understanding the insights of ERF , resulting in an efficient backbone with eight detection branches .",0
8372,"The backbone only consists of common layers ( conv33 , conv11 , ReLU and residual connection ) , which is much lighter than VGG16 , Resnet50 and Densenet121 .",0
8373,"Consequently , the final model has only 2.1 M parameters ( versus VGG16-138.3 M and Resnet50-25.5M ) and achieves superior accuracy and running speed , which makes it appropriate for edge devices .",0
8374,"In summary , the main contributions of this paper include :",0
8375,"We study the relations of RF , ERF and face detection .",0
8376,The relevant understanding motivates the network design .,0
8377,"We introduce the RF to overcome the drawbacks of the previous anchor - based strategies , resulting in a anchorfree method .",0
8378,We proposed a new backbone with common layers for accurate and fast face detection .,0
8379,Extensive and comprehensive experiments on multiple hardware platforms are conducted on benchmarks WIDER FACE and FDDB to firmly demonstrate the superiority of the proposed method for edge devices .,0
8380,Related Work,0
8381,Face detection has attracted a lot of attention since a decade ago .,0
8382,Early works,0
8383,Early face detectors leverage hand - crafted features and cascade classifiers to detect faces in forms of sliding window .,0
8384,Viola - Jones face detector uses Adaboost with Haar - like features to train face classifiers discriminatively .,0
8385,"Subsequently , utilizing more effective handcrafted features and more powerful classifiers becomes the mainstream .",0
8386,"These methods are not trained end - to - end , treating feature learning and classifier training separately .",0
8387,"Although achieving fast running speed , they can not obtain satisfied accuracy .",0
8388,CNN - based methods Current CNN - based face detectors benefit from two - stage and one - stage general object detection .,0
8389,"Both and are based on faster R - CNN , adapting the original faster R - CNN to face detection .",0
8390,Zhang et al. proposes a cascaded CNN for coarse - to - fine face detection with inside cascaded structure .,0
8391,"Recently , one - stage face detectors are dominant .",0
8392,MTCNN performs face detection in a sliding window manner and relies on image pyramid .,0
8393,"HR is an advanced version of MTCNN to some extent , also requiring image pyramid .",0
8394,Image pyramid has some drawbacks like slow speed and high memory cost .,0
8395,S3 FD takes RF into consideration for detection branch design and proposes an anchor matching strategy to improve hit rate .,0
8396,"In , Zhu et al.",0
8397,focuses on detecting small faces by proposing a robust anchor generating and matching strategy .,0
8398,It can be concluded that anchor related strategies are crucial for face detection .,0
8399,"Following S3FD , Pyramid",0
8400,Box enhances the backbone with low - level feature pyramid layers ( LFPN ) for better multi-scale detection .,0
8401,SSH constructs three detection modules cooperating with context modules for scale - invariant face detection .,0
8402,"DSFD is characterized by feature enhance modules , early layer supervision and an improved anchor matching strategy for better initialization .",0
8403,"S3FD , Pyramid Box , SSH and DSFD use VGG16 as backbones , leading to big model size and inefficient computation .",0
8404,FaceBoxes aims to make the face detector run in real - time by rapidly reducing the size of input images .,0
8405,"In detail , it reaches a large stride size 32 after four layers : two convolution layers and two pooling layers .",0
8406,"Although the running speed of FaceBoxes is fast , it abandons the detection of small faces , resulting in relatively low accuracy on WIDER FACE .",0
8407,"Different from FaceBoxes , our method handles the detection of small faces delicately , achieving fast running speed and large scale coverage in the meantime .",0
8408,It can be observed that the networks used by recent state of the art methods tend to become more complex and heavier .,0
8409,"In our view , to gain marginal improvement inaccuracy at the cost of running speed is not appropriate for practical applications .",0
8410,Light and Fast Face Detector,0
8411,"In this section , we first revisit the concept of RF and its relation to face detection in Sec. 3.1 .",0
8412,"Then Sec. 3.2 describes the rationality and advantages of using RFs as natural "" anchors "" .",0
8413,"Subsequently , the details of the proposed network is depicted in Sec. 3.3 .",0
8414,"Finally , we present the specifications of network training in Sec. 3.4 .",0
8415,Revisit RF in the Background of Face Detection,0
8416,"In the beginning , we make a brief description of RF and its properties .",0
8417,"RF is a definite area of the input image , which affects the activation of the corresponding neuron .",0
8418,RF determines the range that a neuron can see in the original input .,0
8419,"Intuitively , the target object can be well detected with high probabilities if it is enclosed by a certain RF .",0
8420,"In general , the neurons in shallow layers have small RFs and those in deeper layers have large RFs .",0
8421,One of the important properties of RF is that each input pixel contributes differently for the neuron 's activation .,0
8422,"Specifically , the pixels locating around the center of RF have larger impact .",0
8423,And the impact decreases gradually when the pixels are faraway from the center .,0
8424,This phenomenon is named as effective receptive field ( ERF ) .,0
8425,ERFs inherently exist in neural networks and present a Gaussian - like distribution .,0
8426,"Thus , mak - ing the target object in the middle of the RF is also important .",0
8427,The proposed LFFD benefits from the above observations .,0
8428,Face detection is a well - known branch of general object detection and it has some characteristics .,0
8429,"First , big faces are approximately rigid due to their unmovable components , such as eyes , noses and mouths .",0
8430,"Although there are facial expression changes , hair occlusion and other unconstrained situations , big faces are still distinguishable .",0
8431,"Second , tiny or small faces have to be treated differently compared to big faces .",0
8432,Tiny faces always have unrecognizable appearances ( an example is shown in ) .,0
8433,"It is even difficult for humans to make a face / non - face decision by only seeing the facial area of a tiny face , and the same goes for CNN based classifiers .",0
8434,"With more context information including necks and shoulders , tiny faces become easier to recognize .",0
8435,Detailed discussion can be referred to ..,0
8436,Tiny faces detection .,0
8437,"The top - left image only contains a face , and the top - right image depicts a face with sufficient context information .",0
8438,It is easy to see that the face becomes more distinguishable with the context information gradually increasing .,0
8439,The lower part describes the relation between RF and ERF for detecting the tiny face .,0
8440,"Based on above understandings , faces with different sizes need various RF strategies :",0
8441,"for tiny / small faces , ERFs have to cover the faces as well as sufficient context information ;",0
8442,"for medium faces , ERFs only have to contain the faces with little context information ;",0
8443,"for large faces , only keeping them in RFs is enough .",0
8444,These strategies guide us to design an effective backbone .,0
8445,"RFs as Natural "" Anchor """,0
8446,One-stage detectors are mostly characterized by predefined bbox anchors .,0
8447,"In order to detect different objects , anchors are in multiple aspect ratios and sizes .",0
8448,These anchors are always redundantly defined .,0
8449,"In terms of face detection , it is rational to use 1:1 aspect ratio anchors since faces are approximately square , which is also mentioned in .",0
8450,The shapes of RFs are also square if the width and height of the kernel are equal .,0
8451,"The proposed method regards RFs as natural "" anchors "" .",0
8452,"For the neurons in the same layer , their RFs are regularly tiled in the input image .",0
8453,The number and size of RFs are inherently determined once the network is built .,0
8454,"As for matching strategy , the proposed method uses a straight and concise way - the RF is matched to a groundtruth bbox if and only if its center falls in the groundtruth bbox , other than thresholding IOU .",0
8455,"In the typical anchor - based method S3FD , Zhang et al .",0
8456,also analyses the influence of ERFs and designs anchor augmentation for tiny faces in particular .,0
8457,"In spite of improving the anchor hit rate , S3 FD induces the anchor imbalance problem ( too many anchors for tiny faces ) which has to be addressed by additional means .",0
8458,"However , the proposed method can achieve 100 % face coverage in theory by controlling the RF stride .",0
8459,"Besides , RF with our matching strategy can naturally handle continuous face scales .",0
8460,"For an instance , RFs of 100 pixels are able to predict faces between 20 pixels to 40 pixels .",0
8461,"In this way , anchor imbalance problem is greatly relieved and faces from each scale are equally treated .",0
8462,"Based on the above discussion , we do not create any anchors and the proposed method do not really match anchors to groundtruth bboxes .",0
8463,"Therefore , the proposed method is anchor-free .",0
8464,Network Architecture,0
8465,"According to above analyses , we can design a specialised backbone for face detection .",0
8466,There are two factors that determine the placement of loss branches - the size and stride of RFs .,0
8467,"The size of RFs guarantees that the learned features of faces are robust and distinguishable , whereas the stride ensures the 100 % coverage .",0
8468,The overall architecture of the proposed network is illustrated in .,0
8469,"The proposed method can detect faces that are lager than 10 pixels ( the size of a face is indicated by the longer side ) , since WIDER FACE benchmark dataset requires faces more than 10 pixels to be detected .",0
8470,It can be observed that the proposed backbone is one - stage with four parts .,0
8471,The concrete information about loss branches can be found in .,0
8472,The tiny part has 10 convolution layers .,0
8473,"The first two lay - ers downsample the input with stride 4 , stride 2 from each .",0
8474,"Therefore , RFs of other convolution layers in this part are in stride",0
8475,4 . One crucial principle is : downsample the input as quick as possible while keeping the 100 % face coverage .,0
8476,This part has two loss branches .,0
8477,The loss branch 1 stems from c 8 whose RF size is 55 for continuous face scale 10 - 15 .,0
8478,"Similarly , the loss branch 2 is from c 10 with RF size 71 for continuous face scale 15 - 20 .",0
8479,"Obviously , we can make sure that centers of at least two RFs can fall in the smallest face , thus achieving 100 % coverage .",0
8480,"There is a special case that one center may fall in more than two faces at the same time , in which the corresponding RF is ignored directly .",0
8481,"As we have discussed in Sec. 3.1 , tiny faces need more context information and ERFs are smaller than RFs .",0
8482,"To this end , we use much larger RFs than average face scales .",0
8483,"The ratios of RFs and average face scales are 4.4 and 4.0 for branch 1 and branch 2 , respectively .",0
8484,"In , such ratios are gradually decreased from 4.4 to 1.3 , because larger faces needless context information .",0
8485,"In the backbone , all convolution layers have the kernel size of 33 .",0
8486,"Nevertheless , the kernel size of convolution layers in branches is 11 which does not change the size of RFs .",0
8487,"In each branch , there are two sub - branches , one for face classification and the other one for bbox regression .",0
8488,The small part is in charge of two continuous face scales 20 - 40 and 40 - 70 .,0
8489,The first convolution layer c 11 in this part downsamples the feature maps by 2 .,0
8490,"For the subsequent parts , their first convolution layers accomplish the same function .",0
8491,"In small part , the RF increasing speed becomes 16 compared to that of tiny part",0
8492,8 . So it takes less convolution layers to reach the targeted RF sizes .,0
8493,"The medium part is similar to the small part , having only one branch .",0
8494,"At the end of the backbone , the large part has seven convolution layers .",0
8495,These layers easily enlarge the detection scale without too much computation gain due to small feature maps .,0
8496,Three branches are from this part .,0
8497,"Since big faces are much easier to detect , the ratios of RFs and average face scales are relatively small .",0
8498,The proposed method can detect a large range of faces from 10 pixels to 560 pixels within one inference .,0
8499,"The overall backbone only consists of conv 33 , conv 11 , ReLU and residual connection .",0
8500,"The main reason is that conv 33 and conv 11 are highly optimized by inference libraries , such as cuDNN * , ncnn , mace and paddle - mobile , since they are most widely used .",0
8501,"We do not adopt BN as components due to slow inference speed , although it has become the standard configuration of many networks .",0
8502,We compare the speed between the original backbone and the one with BN : the original one can achieve 7.6 ms and the * https://developer.nvidia.com/cudnn https://github.com/Tencent/ncnn https://github.com/XiaoMi/mace https://github.com/PaddlePaddle/paddle-mobile,0
8503,"one with BN only has 8.9 ms , resulting in 17 % slower ( resolution : 640480 , hardware : TITAN X ( Pascal ) ) .",0
8504,"In stead of using BN , we train much more iterations for better convergence .",0
8505,"As shown in , in each part , residual connections are placed side by side for easily training the deep backbone .",0
8506,The number of filters of all convolution layers in the first two parts is 64 .,0
8507,"We do not increase the filters , since the first two parts have relatively large feature maps which are computationally expensive .",0
8508,"However , the number of filters in the last two parts can be increased to 128 without too much additional computation .",0
8509,More details can be found in .,0
8510,Training Details,0
8511,"In this subsection , we describe the training related details in several aspects .",0
8512,Dataset and data augmentation .,0
8513,"The proposed method is trained on the training set of WIDER FACE benchmark , including 12,880 images with more than 150,000 valid faces .",0
8514,Faces less than 10 pixels are discarded directly .,0
8515,Data augmentation is important for improving the robustness .,0
8516,The detailed strategies are listed as follows :,0
8517,"Color distort , such as random lighting noise , random contrast , random brightness , et al .",0
8518,More information can refer to . Random sampling for each scale .,0
8519,"In the proposed network , there are eight loss branches , each in charge of a certain continuous scale .",0
8520,"Thus , we have to guarantee that : 1 ) the number of faces for each branch is approximately the same ; 2 ) each face can be sampled for each branch with the same probability .",0
8521,"To this end , we first randomly select an image , and then randomly select a face in the image .",0
8522,"Second , a continuous face scale is selected and the face is randomly resized within the scale as well as the entire image and other face bboxes .",0
8523,"Finally , we crop a sub-image of 640640 at the center of the selected face , filling the outer space with black pixels .",0
8524,Randomly horizontal flip .,0
8525,We flip the cropped image with probability of 0.5 .,0
8526,Loss function .,0
8527,"In each loss branch , there are two sub - branches for face classification and bbox regression .",0
8528,"For face classification , we use softmax with cross - entropy loss over two classes .",0
8529,The matched RF anchors are positive and the others are negative .,0
8530,and .,0
8531,Faces that fall in gray scales are also ignored by the corresponding branch .,0
8532,"For bbox regression , we adopt L2 loss directly .",0
8533,The regression groundtruth is defined as :,0
8534,where RF,0
8535,Hard negative mining .,0
8536,"For each branch , negative RF anchors are usually more than positive ones .",0
8537,"For stable and better training , only a fractional negative RF anchors are used for back - propagation : we sort the loss values of all negative anchors and only select the top ones for learning .",0
8538,The ratio between the positive and negative anchors is at most 1:10 .,0
8539,"Empirically , hard negative mining can bring faster and stable convergence .",0
8540,Training parameters .,0
8541,We initialize all parameters with xavier method and train the network from scratch .,1
8542,"The inputs first minus 127.5 , and then divided by 127.5 .",0
8543,"The optimization method is SGD with 0.9 momentum , zero weight decay and batch size 32 .",1
8544,The reason for zero weight decay is that the number of parameters in the proposed network is much less than that of VGG16 .,0
8545,"Thus , there is no need to punish .",0
8546,The initial learning rate is 0.1 .,1
8547,"We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000 , 1,000,000 , 1,200,000 and 1,400,000 .",1
8548,The training time is about 5 days with two NVIDIA GTX 1080 TI .,1
8549,Our method is implemented using MXNet and the source code is released .,1
8550,Experiments,0
8551,"In this section , comprehensive and extensive experiments are conducted .",0
8552,"Firstly , a new evaluation schema is proposed and the evaluation results on benchmarks are presented .",0
8553,"Secondly , we analyse the running efficiency on multiple platforms .",0
8554,"Thirdly , we further investigate the amount of computation and storage memory cost , introducing the computation efficiency rate .",0
8555,Evaluation on Benchmarks,0
8556,"In this subsection , a new evaluation schema is described at the beginning .",0
8557,The new schema is named as Single Inference on the Original ( SIO ) .,0
8558,SIO is proposed to reform the evaluation procedure for real - world applications .,0
8559,We notice https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices that the latency in some practical scenarios has the same importance as the accuracy .,0
8560,"The conventional evaluation procedure involves some tricky means , such as flips and image pyramids , for achieving higher accuracy .",0
8561,"However , the time consumption is not acceptable by doing that .",0
8562,"To this end , SIO can be easily operated in the following way :",0
8563,1 ) keep the image in its original size as the net input ; 2 ) the net does only one inference with the original image .,0
8564,The outputs of SIO are fed to the subsequent metrics .,0
8565,"In the experiments , we have to reproduce the results according to SIO schema .",0
8566,"Therefore , we collect the compared methods which have released codes and models .",0
8567,"Finally , the following methods are taken for comparison : DSFD ( Resnet152 backbone ) , Pyramid Box ( VGG16 backbone ) , S3 FD ( VGG16 backbone ) , SSH ( VGG16 backbone ) and FaceBoxes .",1
8568,DSFD and Pyramid Box are state of the art methods .,0
8569,The proposed method is named .,0
8570,Performance results on the validation set of WIDER FACE .,0
8571,The values in ( ) are results from the original papers .,0
8572,as LFFD .,0
8573,LFFD and FaceBoxes do not rely on existing pretrained backbones and are trained from scratch .,0
8574,We evaluate all methods on two benchmarks : FDDB and WDIER FACE .,0
8575,FDDB dataset .,1
8576,FDDB contains 2845 images with 5171 unconstrained faces .,0
8577,There are two types of scoring : discrete score and continuous score .,0
8578,The first scoring criterion is obtained by thresholding IOU .,0
8579,And the second criterion directly uses IOU ratios .,0
8580,We show final evaluation results of LFFD on FDDB against above five methods in .,0
8581,The overall performance on both scoring types shows the similar trends .,0
8582,"DSFD , Pyramid Box , S3FD and SSH can achieve high accuracy with marginal gaps .",1
8583,"The proposed LFFD gains slightly lower accuracy than the first four methods , but outperforms FaceBoxes evidently .",1
8584,The results indicate that LFFD is superior for detecting unconstrained faces .,1
8585,WIDER FACE dataset .,1
8586,"In WIDER FACE , there are 32,203 images and 393,703 labelled faces .",0
8587,"These faces are in a high degree of variability in scale , pose and occlusion .",0
8588,"Until now , WIDER FACE is the most widely used benchmark for face detection .",0
8589,"All images are randomly divided into three subsets : training set ( 40 % ) , validation set ( 10 % ) and testing set ( 50 % ) .",0
8590,"Furthermore , images in each subset are graded to three levels ( Easy , Medium and Hard ) according to the difficulties for detection .",0
8591,"Roughly speaking , a large number of tiny / small faces are in Medium and Hard parts .",0
8592,The groundtruth annotations are available only for training and validation sets .,0
8593,All the compared methods are trained on training set .,0
8594,"We report the results on the validation and testing sets in and 4 , respectively .",0
8595,Some observations can be made .,0
8596,"Firstly , performance drop is evident for DSFD , PyramidBox , S3FD and SSH compared to their original results .",1
8597,"On the one hand , achieving high accuracy through only one inference is relatively difficult .",0
8598,"On the other hand , the tricks can indeed improve the accuracy impressively .",0
8599,"Secondly , Pyramid Box obtains the best results on Hard parts , whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces .",1
8600,"Thirdly , FaceBoxes does not get desirable results on Medium and Hard parts .",1
8601,"Since Face - Boxes produces large stride 32 rapidly , which means that faces smaller than 32 pixels are hardly detected .",0
8602,"To make it clearer , we conduct additional experiments for FaceBoxes , named as FaceBoxes3. 2 , in which the both sides of input images are enlarged 3.2 .",0
8603,We can see that the results on Medium and Hard parts are improved remarkably .,0
8604,The performance drop on Easy parts is attributed to that some faces are resized too large to be detected .,0
8605,"To some extent , the results of FaceBoxes and FaceBoxes 3. 2 indicate that FaceBoxes can not cover faces with large range .",0
8606,"Fourthly , the proposed method LFFD consistently outperforms Face - Boxes , although having gaps with state of the art methods .",1
8607,"Additionally , LFFD is better than SSH that uses VGG16 as the backbone on Hard parts .",1
8608,Running Efficiency,0
8609,"In this subsection , we analyse the running speed of all methods on three different platforms .",0
8610,The information of each platform and related libraries are listed in .,0
8611,We use batchsize 1 and a few common resolutions for testing .,0
8612,"For fair comparison , FaceBoxes 3.2 is used here instead of FaceBoxes .",0
8613,The running speed is measured in ms and the corresponding FPS .,0
8614,"The final results are presented in , 7 and 8 .",0
8615,"In , we also add VGG16 and Resnet50 for sufficient comparison .",0
8616,"SSH and S3FD are based on VGG16 , having similar speed with VGG16 .",0
8617,"Whereas , Pyramid - Box is much slower due to additional complex modules , although based on VGG16 as well .",0
8618,"DSFD can achieve state of the art accuracy , but it has the slowest running speed .",0
8619,"The proposed LFFD runs the fastest at 38402160 , and FaceBoxes 3.2 obtains the highest speed at other three resolutions .",0
8620,Both LFFD and FaceBoxes 3. 2 can reach or even exceed the real - time running speed ( > 30 FPS ) at the first .,0
8621,Information of hardware platforms and related running libraries .,0
8622,three resolutions .,0
8623,The aforementioned trend that state of the art methods pursue higher accuracy at the cost of running speed is clearly verified .,0
8624,TX2 and Raspberry Pi 3 are edge devices with low computation power .,0
8625,"DSFD , Pyramid Box , S3FD and SSH are either too slow or failed to run on these two platforms .",0
8626,"Thus , we only evaluate the proposed LFFD and FaceBoxes 3.2 at lower resolutions in and 8 .",0
8627,The overall results show that LFFD is faster than FaceBoxes 3. 2 except for the case at 640480 on Raspberry Pi 3 .,0
8628,LFFD can better benefit from optimizations of ncnn than FaceBoxes 3. 2 at low resolutions 160120 and 320240 .,0
8629,"Parameter , Computation and Model Size",0
8630,"We investigate the compared methods from the perspective of parameter , computation and model size in this subsection .",0
8631,The edge devices always have constrained storage memories .,0
8632,It is necessary to consider the memory usage of face detectors .,0
8633,The number of parameters is highly related to the model size .,0
8634,"However , less parameters do not mean less computation .",0
8635,"Following , we use FLOPs to measure the computation at resolution 640480 .",0
8636,All the information is presented in .,0
8637,"For state of the art methods DSFD and PyramidBox , they have large amounts of parameters and FLOPs .",0
8638,The proposed LFFD and FaceBoxes 3.2 have light networks which are appropriate to deploy on edge devices .,0
8639,"To further demonstrate the efficiency of the proposed network , we define a new metric :",0
8640,where t indicates the running time .,0
8641,"E net reflects the computation efficiency of networks ( the larger , the more efficient ) and can be calculated at a certain resolution on a specific platform .",0
8642,We compute this metric for LFFD and FaceBoxes 3.2 at 640480 on three platforms ( LFFD vs. FaceBoxes 3.2 ) :,0
8643,1.22G / ms vs. 0.42G / ms on TITAN Xp ; 0.14G / ms vs. 0.04G / ms on TX2 ;,0
8644,0.0022G / ms vs. 0.00088G / ms on Raspberry Pi 3 ;,0
8645,"Evidently , the proposed network has much more efficient computation , which demonstrates the superiority of the concise network design .",0
8646,Conclusion,0
8647,This paper introduces alight and fast face detector that properly balances accuracy and latency .,0
8648,"By deeply rethinking the RF in the background of face detection , we propose an anchor-free method to overcome the drawbacks of anchor - based methods .",0
8649,"The proposed method regards the RFs as natural "" anchors "" which can cover continuous face scales and reach nearly 100 % hit rate .",0
8650,"After investigating the essential relations between ERFs and face scales , we delicately design an simple but efficient network with eight detecting branches .",0
8651,"The proposed network consists of common building blocks with less filters , resulting in fast inference speed .",0
8652,Comprehensive and extensive experiments are conducted to fully analyse the proposed method .,0
8653,"The final results demonstrate that our method can achieve superior accuracy with small model size and efficient computation , which makes it an excellent candidate for edge devices .",0
8654,title,0
8655,EXTD : Extremely Tiny Face Detector via Iterative Filter Reuse,0
8656,abstract,0
8657,"In this paper , we propose a new multi-scale face detector having an extremely tiny number of parameters ( EXTD ) , less than 0.1 million , as well as achieving comparable performance to deep heavy detectors .",0
8658,"While existing multiscale face detectors extract feature maps with different scales from a single backbone network , our method generates the feature maps by iteratively reusing a shared lightweight and shallow backbone network .",0
8659,"This iterative sharing of the backbone network significantly reduces the number of parameters , and also provides the abstract image semantics captured from the higher stage of the network layers to the lower - level feature map .",0
8660,The proposed idea is employed by various model architectures and evaluated by extensive experiments .,0
8661,"From the experiments from WIDER FACE dataset , we show that the proposed face detector can handle faces with various scale and conditions , and achieved comparable performance to the more massive face detectors that few hundreds and tens times heavier in model size and floating point operations .",0
8662,Introduction,0
8663,"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .",1
8664,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .",1
8665,The state - of - the - art ( SOTA ) face detectors for in - the - wild images employ the framework of the recent object detectors .,0
8666,"These methods can even handle a various scale of faces with difficult conditions such as distortion , rotation , and occlusion .",0
8667,"Among them , the face detectors using multiple feature - maps from different layer locations , which mainly stem from , are dominantly used since * Clova AI Research , NAVER Corp .",0
8668,We follow the alphabetical order except the first author .,0
8669,Our method ( star ) shows comparable mAP to S3FD with a significantly smaller model .,0
8670,Red stars denote the proposed models with various sizes .,0
8671,' S3FD + M ' denotes the S3FD variation using MobileFaceNet as a backbone network instead of VGG - 16 .,0
8672,Best viewed in wide and colored vision .,0
8673,these methods can handle the faces with various scale in a single forward path .,0
8674,"While these methods achieved impressive detection performance , they commonly share two problems .",0
8675,One is their large number of parameters .,0
8676,"Since they use a large classification network such as VGG - 16 , ResNet - 50 or 101 , and DenseNet - 169 , the number of total parameters exceed 20 million , over 80 Mb supposing 32 - bit floating point for each parameter .",0
8677,"Furthermroe , the amount of floating point operations ( FLOPs ) also exceeds 100G , and these make it nearly impossible to use the face detectors in CPU or mobile environment , where the most face applications run in .",0
8678,"The second problem , from the architecture perspective , is the limited capacity of the low - level feature map in capturing object semantics .",0
8679,The most single - shot detector ( SSD ) variant object and face detectors struggle the problem because the low - level feature map passes shallow convolutional layers .,0
8680,"To alleviate the problem , the variants of Feature pyramid network ( FPN ) architecture such as are used but requires additional parameters and memories for re-expanding the feature map .",0
8681,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .",1
8682,"The main discovery is that we can share the network in generating each feature - map , as shown in .",1
8683,"As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .",1
8684,"The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .",1
8685,"Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .",1
8686,"Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .",1
8687,"For SSD based architecture , we adopt the setting from .",0
8688,"For the FPN architectures , we refer an up - sampling strategy from .",0
8689,The backbone network is designed to have less than 0.1 million parameters with employing inverted residual blocks proposed in MobileNet - V2 .,0
8690,"We note that our model does not require any extra layer commonly defined as in , and is trained from scratch .",0
8691,"We evaluated the proposed detector and its variants on WIDER FACE dataset , the most widely used and similar to the in - the - wild situation .",0
8692,The main contributions of this work can be summarized as follows :,0
8693,"We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size , as well as provide abundant object semantic information to the lower stage feature maps .",0
8694,"( 2 ) We design a lightweight backbone network for the proposed iterative feature map generation with 0.1 M number of parameters , which less than 400 Kb , and achieved comparable mAP to the heavy face detection methods .",0
8695,"( 3 ) We employ the iterative network sharing idea to the widely used detection architectures , FPN and SSD , and show the effectiveness of the proposed scheme .",0
8696,Related Works,0
8697,Face detectors :,0
8698,Face detection has been an important research topic since an initial stage of computer vision researches .,0
8699,Viola et al .,0
8700,"proposed a face detection method using Haar features and Adaboost with decent performance , and several different approaches followed .",0
8701,"After deep learning has become dominant , many face detection methods applying the techniques have been published .",0
8702,"In the early stages , various attempts were tried to employ the deep architecture to face detection , such as cascade architecture , and occlusion handling .",0
8703,"Recent face detectors has been designed based on the architecture of generic object detectors including Faster - RCNN , R - FCN , SSD , FPN , and Reti - na Net .",0
8704,"Face RCNN and its variants apply Faster - RCNN , and use R - FCN for detecting faces with meaningful performance improvements .",0
8705,"Also , to cope with the various scale of faces with single forward path , object detectors such as SSD , RetinaNet , and FPN are dominantly adopted since they use features from multiple layer locations for detecting objects with various scale in a single forward path .",0
8706,S3FD achieved promising performance by applying SSD with introducing multiple strategies to handle the small size of faces .,0
8707,FAN uses Retina,0
8708,Net by applying anchor level attention to detect the occluded faces .,0
8709,"After S3FD , many improved versions are introduced and achieved performance gain from the previous methods .",0
8710,FPN based face detection methods achieved SOTA performance by enhancing the expression capacity of the lower - level feature map used for detecting small faces .,0
8711,"The mentioned SOTA methods commonly use classification network such as VGG - 16 , ResNet - 50 or 101 , and DenseNet - 169 as a backbone of the model .",0
8712,"These classification networks have a large number of parameters exceeding 20 million , and the model size is over 80 Mb supposing 32 - bit floating point for each parameter .",0
8713,"Some cascade methods such as report decent m AP with the smaller mount of model size , about 3.8 Mb .",0
8714,"However , the size is still burdensome to the devices like mobile , because users generally want their applications not to exceed few ten 's of Mb. Also , the face detector should mostly be much smaller than the total size of the application because a face detector is usually an end - level function of the application .",0
8715,"Here , we propose a new scheme of iteratively sharing the backbone network , which can be applicable to both SSD and FPN based architectures .",0
8716,"The method achieves comparable accuracy to the original models , and the overall model size is extremely smaller as well .",0
8717,Lightweight generic object detectors :,0
8718,"Recently , for detecting general objects in condition with a limited resource such as mobile devices , various single - stage , and two - stage lightweight detectors were proposed .",0
8719,"For the single - stage detectors , MobileNet - SSD , MobileNetV2 - SSDLite , Pelee and Tiny - DSOD were proposed .",0
8720,"For two - stage detectors , Light - Head R - CNN and Thunder Net were proposed .",0
8721,"The mentioned methods achieved meaningful accuracy and size trade - off , but we aim to develop a detector which has a much smaller number of parameters with introducing a new paradigm , iterative use of the backbone network .",0
8722,Recurrent convolutional network :,0
8723,The idea of recurrently using convolutional layers has been applied to various computer vision applications .,0
8724,Sharesnet and Iamnn applied recurrent residual network into classification task .,0
8725,Guo et al .,0
8726,reduce the parameters by sharing depthwise convolutional filters in learning multiple visual domain data .,0
8727,"The iterative sharing is also applied to dynamic routing , fast inference of video , feature transfer , super-resolution , and recently in segmentation .",0
8728,"In this paper , we introduce a method applying the concept of iterative convolutional layer sharing in the face detection task , which is the first to the best of our knowledge .",0
8729,EXTD,0
8730,"In this section , we introduce the main components of the proposed work including iterative feature map generation , the architectures of the proposed face detection models , backbone networks , and classification and regression head design .",0
8731,"Also , implementation details for designing and training the models will be introduced .",0
8732,"shows the overall framework of the proposed method with two variations , SSD - like , and FPN - like frameworks .",0
8733,"In the proposed method , we get multiple feature maps with different resolutions by recurrently passing the backbone network .",0
8734,Let assume that F ( ) and E ( ) each denotes the backbone network and the first Conv layer with stride two .,0
8735,"Then , the iterative process is defined as follows :",0
8736,Iterative Feature Map Generation,0
8737,"Here , the set {f 1 , .. , f N } denote the set of feature maps , and x is the image .",0
8738,"In FPN version , we upsample each feature map and connect the previous feature maps via skipconnection .",0
8739,"The upsampling step U i ( ) is conducted with bilinear upsampling followed by an upsampling block composed of separable convolution and point - wise convolution , inspired by .",0
8740,"The resultant set of the feature map G = {g 1 , ... , g N } is obtained as ,",0
8741,"For the SSD - like architecture , which is the first variant , we extract feature maps f i and connect the classification and regression head to the feature maps .",0
8742,"In FPN - like architecture , the feature maps g i from equation are used .",0
8743,"The classification and regression heads are designed by a 3x3 convolutional network and hence , both models are designed as a fully convolutional network .",0
8744,This enables the models to deal with various size of images .,0
8745,The detailed implementation of the heads is introduced in below sections .,0
8746,"For all the cases , we set the image x to have 640x640 resolution in training phase and use N = 6 number of feature maps .",0
8747,"Hence , we get 160x160 , 80x80 , 40x40 , 20x20 , 10x10 and 5 x 5 resolution feature maps .",0
8748,"In each location of the feature map , prior anchor candidates for the face is defined , following the same setting as S3FD .",0
8749,One notable property of this architecture is that this method provides more abundant semantic information in lower - level feature maps compared to the face detectors adopting SSD architecture .,0
8750,"While the existing methods commonly report the problem that the lower - level feature maps only contain limited semantic information due to their limited length of depth , our iterative architecture repeatedly shows intermediate level features and the various scale of faces to the network .",0
8751,"We conjecture that the different features have similar semantics because the target objects in our case are faces , and the faces share homogeneous shapes regardless of their scale dissimilar to general objects .",0
8752,"In Section 4 , we show that the proposed method clearly enhances the detection accuracy for small size faces , and this can be more improved by taking the FPN architecture .",0
8753,Model Component Description,0
8754,"In the proposed model , a lightweight backbone network reducing the feature map resolution by half is used .",0
8755,"The network is composed of inverted residual blocks followed by one 3x3 convolutional ( Conv ) filter with stride 2 , based on .",0
8756,"The inverted residual block is composed of a set of point - wise Conv , separable Conv , and point - wise Conv .",0
8757,"In each block , the channel width is expanded in the first point - wsie Conv and then , squeezed by the last point - wise Conv filter .",0
8758,"The default setting of the network depth is set to 6 or 8 , and the output channel width is set to 32,48 or 64 , which do not largely exceed overall 0.1 million parameters .",0
8759,"Different from MobileNet - V2 , PReLU ( or leaky - ReLU ) is applied and shown to be more successful than ReLU in training the proposed recurrent architecture .",0
8760,This phenomenon will be further discussed in Section 4 .,0
8761,"Other than the inverted residual block , the proposed architecture also includes feature extraction block , upsampling blocks , and classification and regression heads .",0
8762,The detailed description of the components is introduced in 3 .,0
8763,The figures in ( a ) and ( b ) each shows the inverted residual block architecture .,0
8764,"Residual skip - connection is applied when the input and output channel width are equivalent , and at the same time , the stride is set to one .",0
8765,The upsampling block in ( c ) consists of bilinear upsample layer followed by depth - wise and point - wise Conv blocks .,0
8766,Fea -ture extraction block ( d ) is defined by a 3 x3 Conv network followed by batch normalization and the activation function .,0
8767,The classification ( e ) and regression ( f ) heads are also defined by a 3 x3 Conv network .,0
8768,The implementation of the head is described in Section 3.3 .,0
8769,Classification and Regression Head Design,0
8770,"For detecting the faces using the generated feature maps , we use a classification head and a regression head for each feature map to classify whether each prior box contains a face , and to regress the prior box to the exact location .",0
8771,The classification and regression heads are both defined as single 3 x3 Conv filters as shown in .,0
8772,The classification head Ci has two - dimensional output channel c i except C 1 that having four - dimensional channels .,0
8773,"For C 1 , we apply Maxout approach to select two of the four channels for alleviating the false positive rate of the small faces , as introduced in S3FD .",0
8774,"The regression head R i is defined to have output feature r i to have four - dimensionional channel , and each denotes width , height ratio , and center locations , adopting the dominantly used setting in RPN .",0
8775,Training,0
8776,"The proposed backbone network and the classification and regression head are jointly trained by a multitask loss function from RPN composed of a classification loss l c and a regression loss l r as ,",0
8777,"Here , j is the index of the anchor boxes , and the label c * j ?",0
8778,"{ 0 , 1 } and r * j is the ground truth of the anchor box .",0
8779,The label c * j is set to 1 when Jaccard overlap between the anchor box and ground trurh box is higher than a threshold t.,0
8780,The denominator N cls denotes the total number of positive and negative samples .,0
8781,"The regression loss is computed only for the positive sample and hence , the number N reg is defined by N reg = j c * j .",0
8782,The parameter ?,0
8783,is defined to balance the two losses because N cls and N reg are different from each other .,0
8784,The vector r * j denotes the ground truth box location and size for the face .,0
8785,"The classification loss l c and the regression loss l rare defined as cross -entropy loss and smooth - 1 loss , respectively .",0
8786,"The primary obstacle for the classification in the face detection task is a class imbalance problem between the face and the background , especially regarding the small faces .",0
8787,"To alleviate the problem , we also adopt the strategies including online hard negative mining and scale compensation anchor matching introduced in S3FD .",0
8788,"Using the hard negative mining technique , we balance the ratio of positive and negative samples N neg / N pos to 3 and the balancing parameter ?",0
8789,is set to 4 .,0
8790,"Also , from the scale compensation anchor matching strategy , we first pick the positive samples where the Jaccard overlap is over 0.35 , and then further pick the remaining samples in sorted order from the samples that their Jaccard overlap is larger than 0.1 if the number of positive samples is insufficient .",0
8791,"For Data augmentation , we follow the conventional augmentation setting from S3FD .",0
8792,"The augmentation includes color distortions , random crop , horizontal flip , and vertical flip .",0
8793,The proposed method is implemented with PyTorch and NAVER Smart Machine Learning ( NSML ) system .,0
8794,Please refer Appendix A to seethe detailed training and optimization settings for training the proposed network .,0
8795,Code will be available at https ://github.com/clovaai.,1
8796,Experiments,0
8797,"In this section , we quantitatively and qualitatively analyze the proposed method with various ablations .",0
8798,"For the quantitative analysis , we compare the detection performance of the proposed method and the SOTA face detection algorithms .",0
8799,"Qualitatively , we show that our method can successfully detect faces in various conditions .",0
8800,Experimental Setting,0
8801,"Datasets : we tested the proposed method and ablations of the method with WIDER FACE dataset , which is most recent and is similar to in - the - wild face detection situation .",0
8802,"The images in the dataset are divided into Easy , Medium , and Hard cases which are roughly categorized by different scales : large , medium , and small , of faces .",0
8803,"The Hard case includes all the images of the dataset , and the Easy and Medium cases both are the subsets of the Hard case .",0
8804,"The dataset has total 32,203 images with 393,703 labeled faces and is split into training ( 40 % ) , validation ( 20 % ) and testing ( 40 % ) set .",0
8805,We trained the detectors with the training set and evaluated them with validation and test sets .,0
8806,Comparison :,0
8807,"Since our method followed the training and implementation details such as anchor design , data augmentation , and feature - map resolution design equivalent to S3FD , which has become one of the baseline methods in face detection field , we mostly evaluated the performance by comparing the S3FD model and its SOTA variations .",0
8808,"The other techniques based on the S3FD model such as Pyramid anchor , Feature enhancement module , Improved anchor matching , and Progressive anchor loss would be able to be adapted to the proposed model without revising the proposed structure .",0
8809,"Also , we used the MobileFaceNet , the face variant of the MobileNet - V2 , to the S3FD model instead of VGG - 16 to seethe effectiveness of the proposed method compared to the case of using the lightweight backbone network .",0
8810,Variations :,0
8811,We applied the proposed recurrent scheme mainly into the FPN - based structure .,0
8812,"For the model , we designed three variations which have a different number of parameters , lighter one having 0.063 M parameters with 32 channels for each feature maps , intermediate one having 0.1 M parameters with 48 channels , and the heavier one with 64 channels and 0.16M parameters when designed as FPN .",0
8813,See Appendix,0
8814,B for the detailed configuration of the backbone networks for each case .,0
8815,"Also , we tested different activation functions : ReLU , PReLU , and Leaky - ReLU for each model .",1
8816,"The negative slope of the Leaky - ReLU is set to 0.25 , which is identical to the initial negative slope of the PReLU .",1
8817,"In the following section , we will term each variation by a combination of abbreviations ; EXTD - model - channel - activation .",0
8818,"For example , the term EXTD - FPN - 32 - PReLU denotes the proposed model combined with FPN , with feature channel width 32 and with activation function PReLU .",0
8819,"As an ablation , we also applied the proposed recurrent backbone into SSD - like structure as well .",0
8820,The ablation was trained and tested with the same conditions to the FPNbased version and abbreviated as SSD .,0
8821,"Same as FPN case , for example , the term EXTD - SSD - 32 - PReLU denotes the proposed model combined with SSD , with feature channel width 32 and with activation function PReLU .",0
8822,Performance Analysis,0
8823,"In , we list the quantitative evaluation results of face detection in WIDER FACE dataset and the comparison to the SOTA face detectors .",0
8824,"The table shows the m AP of the models on Easy , Medium , Hard cases for both validation and test sets of the dataset .",0
8825,"Also , the table includes model information such as their backbone networks , number of parameters , and total number of adder arithmetics ( Madds ) .",0
8826,"In , the precision recall curve for the proposed and the other methods are presented .",0
8827,shows the examples of the face detection results from images with various conditions .,0
8828,"In , we evaluate the latency of the models in terms of the resolution of images , which measured via a machine with CPU i 7 core and NVIDIA TITAN - X .",0
8829,"For a fair comparison , all the inference processes of the models are implemented by PyTorch 1.0 .",0
8830,Comparison to the Existing Methods :,0
8831,"The results in 138 times lighter in model size and are 28.3 , 19.2 , and 11 times lighter in Madds .",1
8832,"When compared to SOTA face detectors such as Pyra - midBox and DSFD , our best model EXTD - FPN - 64 - PReLU achieved lower results .",1
8833,The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4 % .,1
8834,"Considering that PyramidBox inherits from S3FD and our model follows the equivalent training and detection setting to S3FD , our model would have a possibility to further increase the detection performance by adding the schemes proposed in Pyramid Box .",0
8835,"The m AP gap to DSFD , which is tremendously heavier , is about 5.0 % , but it would be safe to suggest that the proposed method offers more decent trade - off in that DSFD uses about 2860 times more parameters than the proposed method .",1
8836,This is also meaningful result in that our method did not use any kind of pre-training of the backbone network using the other dataset such as Image Net.,0
8837,shows the ROC curves of the proposed EXTD - FPN - 64 - PReLU and the other methods .,0
8838,"From the graphs , we can see that our method is included in the SOTA group of the detectors using heavyweight pre-trained backbone networks .",0
8839,"When it comes to our SSD - based variations , they got lower mAP results than FPN - based variants .",1
8840,"However , when compared with the S3FD version trained with Mo - bile FaceNet backbone network , the proposed SSD variants achieved comparable or better detection performance .",1
8841,"It is a meaningful result in that the proposed variations have smaller feature map width , S3FD - Mobile Face Net holds feature map size of , and use the smaller number of layer blocks ; inverted residual blocks same as MobileFaceNet , repeatedly .",0
8842,This shows that the proposed itertative scheme efficiently reduces the number of parameters without loss of accuracy .,0
8843,"Also , from the graph in , we showed that our EXTD achieved faster inference speed to the S3FD , which is considered as real - time face detector , in a wide range of an input image resolution .",0
8844,"This shows that the proposed face detector can safely alter S3FD without losing accuracy and with consuming much smaller capacity , as well as maintaining the inference speed .",0
8845,It is interesting to note that the inference was much slow when using MobileFaceNet instead of VGG - 16 .,0
8846,"It would mainly be due to that Mobile - FaceNet version should pass more filters ( 48 ) than VGG - 16 version , and the inference times of the filters including pooling , depth - wise , point - wise and ordinary convolutional filters are not that different in Pytorch implementation .",0
8847,Detection performance regarding the Face Scale :,0
8848,One notable characteristic of the proposed method captured from the evaluation is that our detector obtained better performance when dealing with a small size of faces .,0
8849,"From the table , we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases .",1
8850,"Since the Easy and Medium cases are subsets of the Hard dataset , this means that the proposed method is especially fitted to capture small sized faces .",0
8851,"This tendency is commonly observed for different variations , for the different model architecture , and for the different channel widths .",0
8852,"This supports the proposition suggested in Section 3.1 that the proposed recurrent structure strengthens the feature map , especially for the lower - level feature maps , and hence enhance the detection performance of the small faces .",0
8853,Variation Analysis,0
8854,The evaluation on the variations of the proposed EXTD is summarized in The value in the parentheses shows the margin between the best model in the block ( written in boldface ) .,0
8855,Effect of the Model Architecture :,0
8856,"From the table , we can find two common observations among the proposed variations .",0
8857,"First , for all the different channel width , FPN based architecture achieved better detection performance compared to SSD based architecture , especially for detecting small faces .",1
8858,"The idea of expanding the number of layers for reaching the largest sized feature - map , for detecting the smallest size of objects , is a common strategy for SSD variant methods .",0
8859,"This approach assumes that typical SSD structure passes too small number of layers and hence , the resultant feature - map could not import much information useful for the detection task .",0
8860,"In the face detection task , this assumption seems to be correct in that the FPN based models notably achieved superior detection performance on small faces compared to SSD based models for all the cases .",0
8861,"Second , for both SSD based and FPN based model , channel width was another key factor for performance enhancement .",0
8862,"As the channel width increased by 32 to 64 , we can see that the detection accuracy significantly enhanced for all the cases ; Easy , Medium , and Hard .",0
8863,"Considering that we used a smaller number of layers for 48 and 64 channel cases than the case with 32 channel , this shows that having enough size of channel width is critical for embedding sufficient information to the feature map for detecting faces .",0
8864,Effect of the Activation functions :,0
8865,"From the evaluation , we found that the choice of the activation function is another factor governing the detection performance of the proposed method .",0
8866,"In all the cases including FPN based and SSD based structures , PReLU was the most effective choice when it comes to mAP , but the gap between Leaky - ReLU was not that significant for the FPN variants .",0
8867,"When tested with SSD based architecture , PReLU outperformed Leaky - ReLU with larger margin than those using FPN structure .",1
8868,It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases .,0
8869,"When the channel width was set to 32 , m AP for all the three cases were lower than 10 % to 20 % compared to those using other activation functions .",0
8870,The decreases were alleviated as the channel width increased .,0
8871,"When the channel width was 48 , the gap was about 2.2 % , and in the channel width 64 case , the margin was about 1.2 % .",0
8872,"From the results , we conjecture that the nature of ReLU that set all the negative values to zero occurs information loss in the proposed iterative process since it makes the feature map too sparse , and this information loss would be much critical when the channel width is small .",0
8873,Conclusion,0
8874,"In this paper , we proposed a new face detector which significantly reduces the model sizes as well as maintaining the detection accuracy .",0
8875,"By re-using backbone network layers recurrently , we reduced the vast amount of the network parameters and also obtained comparable performance to recent deep face detection methods using heavy backbone networks .",0
8876,We showed that our methods achieved very close mAP to the baseline S3 FD only with hundreds time smaller parameters and with using tens time smaller Madd without using pre-training .,0
8877,We expect that our method can be further improved by applying recent techniques of the SOTA detectors which integrated to S3FD .,0
8878,Appendix A. Implementation detail,0
8879,"For training the proposed architecture , a stochastic gradient descent optimizer ( SGD ) with learning rate 1e ? 3 , with 0.9 momentum , 0.0005 weight decay , and batch size 16 is used .",0
8880,"The training is conducted from scratch , and the network weights were initialized with He-method .",0
8881,"The maximum iteration number is basically set to 240K , and we drop the learning rate to 1e ? 4 and 1e ? 5 at 120 K and 180K iterations .",0
8882,"Also , we test the architecture with twice larger iterations 480 K as well .",0
8883,"In this case , the learning rate is dropped at 240 K and 360K iterations .",0
8884,"Similar to the other networks using depth - wise separable networks , further performance improvements were observed when training the network with larger iteration .",0
8885,title,0
8886,Robust Face Detection via Learning Small Faces on Hard Images,1
8887,abstract,0
8888,"Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces .",0
8889,"A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images .",0
8890,"In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images .",0
8891,"Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .",0
8892,"We build an anchor-based deep face detector , which only output a single feature map with small anchors , to specifically learn small faces and train it by a novel hard image mining strategy .",0
8893,"Extensive experiments have been conducted on WIDER FACE , FDDB , Pascal Faces , and AFW datasets to show the effectiveness of our method .",0
8894,"Our method achieves APs of 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset respectively , which surpass the previous state - of - the - arts , especially on the hard subset .",0
8895,Code and model are available at https,0
8896,Introduction,0
8897,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",1
8898,"Stem from the recent successful development of deep neural networks , massive CNN - based face detection approaches have been proposed and achieved the state - of - the - art performance .",0
8899,"However , face detection remains a challenging task due to occlusion , illumination , makeup , as well as pose and scale variance , as shown in the benchmark dataset WIDER FACE .",0
8900,"Current state - of - the - art CNN - based face detectors at - tempt to address these challenges by employing more powerful backbone models , exploiting feature pyramid - style architectures to combine features from multiple detection feature maps , designing denser anchors and utilizing larger contextual information .",0
8901,"These methods and techniques have been shown to be successful to build a robust face detector , and improve the performance towards human - level for most images .",0
8902,"In spite of their success for most images , an evident performance gap still exists especially for those hard images which contain small , blurred and partially occluded faces .",0
8903,We realize that these hard images have become the main barriers for face detectors to achieve human - level detection performance .,0
8904,"In , we show that , even on the train set of WIDER FACE , the official pre-trained SSH 1 still fails on some of the images with extremely hard faces .",0
8905,We show two such hard training images in the upper right corner in .,0
8906,"On the other hand , most training images with easy faces can be almost perfectly detected ( see the illustration in the right lower corner of ) .",0
8907,"As shown in left part of , over two thirds of the training images already obtained perfect detection accuracy , which indicates that those easy images are less useful towards training a robust face detector .",0
8908,"To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images .",0
8909,This issue is most related to anchor - level hard example mining discussed in OHEM .,0
8910,"However , due to the sparsity of ground - truth faces and positive anchors , traditional anchor - level hard example mining mainly focuses on mining hard negative anchors , and mining hard anchors on well - detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images .",0
8911,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .",1
8912,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .",1
8913,This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,1
8914,"We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead .",0
8915,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .",1
8916,Small faces are typically hard and have attracted extensive research attention .,0
8917,"Existing methods aim at building a scale - invariant face detector to learn and infer on both small and big faces , with multiple levels of detection features and anchors of different sizes .",0
8918,"Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training .",1
8919,"More specifically , large faces are automatically ignored during training due to our anchor design , so that the model can fully focus on the small hard faces .",0
8920,"Additionally , experiments demonstrate that this design effectively achieves improvements on detecting all faces in spite of its simple and shallow architecture .",0
8921,"To conclude , in this paper , we propose a novel face detector with the following contributions :",0
8922,"We propose a hard image mining strategy , to improve the robustness of our detector to those extremely hard faces .",0
8923,"This is done without any extra modules , parameters or computation overhead added on the existing detector .",0
8924,"We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes .",0
8925,This allows our model to be simple and focus on difficult small faces without struggling with scale variance .,0
8926,"Our face detector establishes state - of - the - art performance on all popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces , and AFW .",0
8927,"We achieve 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset .",0
8928,"Our method also achieves APs of 99.00 and 99.60 on Pascal Faces and AFW respectively , as well as a TPR of 98.7 on FDDB .",0
8929,The remainder of this paper is organized as follows .,0
8930,"In Section 2 , we discuss some studies have been done which are related to our paper .",0
8931,"In Section 3 , we dive into details of our proposed method , and we discuss experiment results and ablation experiments in Section 4 .",0
8932,"Finally , conclusions are drawn in Section 5 .",0
8933,Related work,0
8934,Face detection has received extensive research attention .,0
8935,"With the emergence of modern CNN and object detector , there are many face detectors proposed to achieve promising performances , by adapting general object detection framework into face detection domain .",0
8936,"We briefly review hard example mining , face detection architecture , and anchor design & matching .",0
8937,Hard example mining,0
8938,"Hard example mining is an important strategy to improve model quality , and has been studied extensively in image classification and general object detection .",0
8939,"The main idea is to find some hard positive and hard negative examples at each step , and put more effort into training on those hard examples .",0
8940,"Recently , with modern detection frameworks proposed to boost the performance , OHEM and Focal loss have been proposed to select hard examples .",0
8941,OHEM computed the gradients of the networks by selecting the proposals with highest losses in every minibatch ; while Focal loss aimed at naturally putting more focus on hard and misclassified examples by adding a factor to the standard cross entropy criterion .,0
8942,"However , these algorithms mainly focused on anchor - level or proposal - level mining .",0
8943,It can not handle the imbalance of easy and hard images in the dataset .,0
8944,"In our paper , we propose to exploit hard example mining on image level , i.e. hard image mining , to improve the quality of face detector on extremely hard faces .",0
8945,"More specifically , we assign difficulty scores to training images while training with an SGD mechanism , and re-sample the training images to build a new training subset at the next epoch .",0
8946,Face Detection Architecture,0
8947,"Recent state - of - the - art face detectors are generally built based on Faster - RCNN , R - FCN or SSD .",0
8948,"SSH exploited the RPN ( Region Proposal Network ) from Faster - RCNN to detect faces , by building three detection feature maps and designing six anchors with different sizes attached to the detection feature maps .",0
8949,"S 3 FD and PyramidBox , on the other hand , adopted SSD as their detection architecture with six different detection feature :",0
8950,The framework of our face detector .,0
8951,"We take VGG16 as our backbone CNN , and we fuse two layers ( conv4 3 and conv 5 3 ) after dimension reduction and bilinear upsampling , to generate the final detection feature map .",0
8952,"Based on that , we add a detection head for classification and bounding - box regression .",0
8953,maps .,0
8954,"Different from S 3 FD , Pyramid Box exploited a feature pyramid - style structure to combine features from different detection feature maps .",0
8955,"Our proposed method , on the other hand , only builds single level detection feature map , based on VGG16 , for classification and bounding - box regression , which is both simple and effective .",0
8956,Anchor design and matching,0
8957,"Usually , anchors are designed to have different sizes to detect objects with different scales , in order to build a scaleinvariant detector .",0
8958,"SSD as well as its follow - up detectors S 3 FD and PyramidBox , had six sets of anchors with different sizes , ranging from ( 16 16 ) to ( 512 512 ) , and their network architectures had six levels of detection feature maps , with resolutions ranging from 1 4 to 1 128 , respectively .",0
8959,"Similarly , SSH had the same anchor setting , and those anchors were attached to three levels of detection feature maps with resolutions ranging from 1 8 to .",0
8960,"The difference between SSH and S 3 DF is that in SSH , anchors with two neighboring sizes shared the same detection feature map , while in S 3 DF , anchors with different sizes are attached to different detection feature maps .",0
8961,SNIP discussed an alternative approach to handle scales .,0
8962,"It showed that CNNs are not robust to changes in scale , so training and testing on the same scales of an image pyramid can be a more optimal strategy .",0
8963,"In our paper , we exploit this idea by limiting the anchor sizes to be ( 1616 ) , ( 32 32 ) and ( 64 64 ) .",0
8964,"Then those faces with either too small or too big sizes will not be matched to any of the anchors , thus will be ignored during the training and testing .",0
8965,"By removing those large anchors with sizes larger than ( 64 64 ) , our network focuses more on small faces which are potentially more difficult .",0
8966,"To deal with large faces , we use multiscale training and testing to resize them to match our anchors .",0
8967,"Experiments show this design performs well on both small and big faces , although it has fewer detection feature maps and anchor sizes .",0
8968,Proposed method,0
8969,"In this section , we introduce our proposed method for effective face detection .",0
8970,"We first discuss the architecture of our detector in Section 3.1 , then we elaborate our hard image mining strategy in Section 3.2 , as well as some other useful training techniques in Section 3.3 .",0
8971,Single - level small face detection framework,0
8972,The framework of our face detector is illustrated in Figure 2 .,0
8973,"We use VGG16 network as our backbone CNN , and combine conv4 3 and conv5 3 features , to build the detection feature map with both low - level and high - level semantic information .",0
8974,"Similar to SSH , we apply 11 convolution layers after conv4 3 and conv5 3 to reduce dimension , and then apply a 33 convolution layer on the concatenation of these two dimension reduced features .",0
8975,"The output feature of the 33 convolution layer is the final detection feature map , which will be fed into the detection head for classification and bounding - box regression .",0
8976,The detection feature map has a resolution of 1 8 of the original image ( of size H W ) .,0
8977,We attach three anchors at each point in the grid as default face detection boxes .,0
8978,Then we do classification and bounding - box regression on those 3 H 8 W 8 anchors .,0
8979,"Unlike many other face detectors which build multiple feature maps to detect face with a variant range of scales , inspired by SNIP , faces are trained and inferred with roughly the same scales .",0
8980,"We only have one detection feature map , with three sets of anchors attached to it .",0
8981,"The anchors have sizes of ( 16 16 ) , :",0
8982,The framework of our dilated detection head for classification and regression .,0
8983,"Based on the detection feature from the backbone CNN , we first perform a dimension reduction to reduce the number of channels from 512 to 128 .",0
8984,"Then we put three convolution layers with the shared weight , and different dilation rates , to generate final detection and classification features .",0
8985,"and ( 64 64 ) , and the aspect ratio is set to be 1 .",0
8986,"By making this configuration , our network only trains and infers on small and medium size of faces ; and we propose to handle large faces by shrinking the images in the test phase .",0
8987,"We argue that there is no speed or accuracy degradation for large faces , since inferring on a tiny image ( with short side containing 100 or 300 pixels ) is very fast , and the shrinked large face will still have enough information to be recognized .",0
8988,"To handle the difference of anchor sizes attached to the same detection feature map , we propose a detection head which uses different dilation rates for anchors with different sizes , as shown in .",0
8989,"The intuition is that in order to detect faces with different sizes , different effective receptive fields are required .",0
8990,This naturally requires the backbone feature map to be invariant to scales .,0
8991,"To this end , we adopt different dilation rates for anchors with different sizes .",0
8992,"For anchors with size ( 16 16 ) , ( 32 32 ) and ( 64 64 ) , we use a convolution with kernel size of 3 and dilation rate of 1 , 2 and 4 to gather context features at different scales .",0
8993,These three convolution layers share weights to reduce the model size .,0
8994,"With this design , the input of the 3 3 convolution , will be aligned to the same location of faces , regardless of the size of faces and anchors .",0
8995,Ablation experiments show the effectiveness of this multi-dilation design .,0
8996,Hard image mining,0
8997,"Different from OHEM discussed in Section 3.3 , which selects proposals or anchors with the highest losses , we propose a novel hard image mining strategy at image level .",0
8998,"The intuition is that most images in the dataset are very easy , and we can achieve a very high AP even on the hard subset of the WIDER FACE val dataset with our baseline model .",0
8999,"We believe not all training images should be treated equally , and well - recognized images will not help towards training a more robust face detector .",0
9000,"To put more attention on training hard images instead of easy ones , we use a subset D of all training images D , to contain hard ones for training .",0
9001,"At the beginning of each epoch , we build D based on the difficulty scores obtained in the previous epoch .",0
9002,We initially use all training images to train our model ( i.e. D = D ) .,0
9003,This is due to the fact that our initial Im - age Net pre-trained model will only give random guess towards face detection .,0
9004,"In this case , there is no easy image .",0
9005,"In other words , every image is considered as hard image and fed to the network for training at the first epoch .",0
9006,"During the training procedure , we dynamically assign different difficulty scores to training images , which is defined by the metric where A ( I ) + is the set of positive anchors for image I , with IoU over 0.5 against ground - truth boxes , l is the classification logit and l ( I ; ? ) a , 1 , l ( I ; ? ) a , 0 are the logits of anchor a for image Ito be foreground face and background .",0
9007,"All images are initially marked as hard , and any image with WPAS greater than a threshold th will be marked as easy image .",0
9008,"At the beginning of each epoch , we first randomly shuffle the training dataset to generate the complete training list D = [ I i 1 , I i2 , , I in ] for the following epoch of training .",0
9009,"Then given an image marked as easy , we remove it from D with a probability of p.",0
9010,"The remaining training list D = [ I ij i , I ij 2 , , I ij k ] , which focuses more on hard images , will be used for training at this epoch .",0
9011,"Note that for multi - GPU training , each GPU will maintain its training list D independently .",0
9012,"In our experiments , we set the probability p to be 0.7 , and the threshold th to be 0.85 .",0
9013,Training strategy,0
9014,Multi - scale training and anchor matching,0
9015,"Since we only have anchors covering a limited range of face scales , we train our model by varying the sizes of training images .",0
9016,"During the training phase , we resize the training images so that the short side of the image contains s pixels , where sis randomly selected from { 400 , 800 , 1200 } .",0
9017,We also set an upper bound of 2000 pixels to the long side of the image considering the GPU memory limitation .,0
9018,"For each anchor , we assign a label { + 1 , 0 , ? 1 } based on how well it matches with any ground - truth face bounding box .",0
9019,"If an anchor has an IoU ( Intersection over Union ) over 0.5 against a ground - truth face bounding box , we assign + 1 to that anchor .",0
9020,"On the other hand , if the IoU against any ground - truth face bounding box is lower than 0.3 , we assign 0 to that anchor .",0
9021,"All other anchors will be given ? 1 as the label , and thus will be ignored in the classification loss .",0
9022,"By doing so , we only train on faces with designated scales .",0
9023,"Those faces with no anchor matching will be simply ignored , since we do not assign the anchor with largest IoU to it ( thus assign the corresponding anchor label + 1 ) as Faster - RCNN does .",0
9024,"This anchor matching strategy will ignore the large faces , and our model can put more capacity on learning different face patterns on hard small faces instead of memorizing the change in scales .",0
9025,"For the regression loss , all anchors with IoU greater than 0.3 against ground - truth faces will betaken into account and contribute to the smooth 1 loss .",0
9026,"We use a smaller threshold ( i.e. 0.3 ) because ( 1 ) this will allow imperfectly matched anchors to be able to localize the face , which maybe useful during the testing and ( 2 ) the regression task has less supervision since unlike classification , there are no negative anchors for computing loss and the positive anchors are usually sparse .",0
9027,Anchor - level hard example mining,0
9028,OHEM has been proven to be useful for object detection and face detection in .,0
9029,"During our training , in parallel with our newly proposed hard image mining , we also exploit the traditional hard anchor mining method to focus more on the hard and misclassificed anchors .",0
9030,"Given a training image with size H W , there are 3 H 8 W 8 anchors at the detection head , and we only select 256 of them to be involved in computing the classification loss .",0
9031,"For all positive anchors with IoU greater than 0.5 against ground - truth boxes , we select the top 64 of them with lowest confidences to be recognized as face .",0
9032,"After selecting positive anchors , ( 256 ?",0
9033,#pos anchor ) negative anchors with highest face confidence are selected to compute the classification loss as the hard negative anchors .,0
9034,"Note that we only perform OHEM for classification loss , and we keep all anchors with IoU greater than 0.3 for computing regression loss , without selecting a subset based on either classification loss or bounding - box regression loss .",0
9035,Data augmentation,0
9036,"Data augmentation is extremely useful to make the model robust to light , scale changes and small shifts .",0
9037,"In our proposed method , we exploit cropping and photometric distortion as data augmentation .",0
9038,"Given a training image after resizing , we crop a patch of it with a probability of 0.5 .",0
9039,"The patch has a height of H and a width of W which are independently drawn from U ( 0.6H , H ) and U ( 0.6 W , W ) , where U is the uniform distribution and H , Ware the height and width of the resized training image .",0
9040,All ground - truth boxes whose centers are located inside the patch are kept .,0
9041,"After the random cropping , we apply photometric distortion following SSD by randomly modifying the brightness , contrast , saturation and hue of the cropped image randomly .",0
9042,Experiments,0
9043,"To verify the effectiveness of our model and proposed method , we conduct extensive experiments on popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces and AFW .",0
9044,"It is worth noting that the training is only performed on the train set of WIDER FACE , and we use the same model for evaluation on all these datasets without further fine - tuning .",0
9045,Experimental settings,0
9046,"We train our model on the train set of WIDER FACE , which has 12880 images with 159 k faces annotated .",0
9047,"We flip all images horizontally , to double the size of our training dataset to 25760 .",0
9048,"For each training image , we first randomly resize it , and then we use the cropping and photometric distortion data augmentation methods discussed in Section 3.3 to pre-process the resized image .",0
9049,"We use an ImageNet pretrained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization .",1
9050,"We train the model with the itersize to be 2 , for 46 k iterations , with a learning rate of 0.004 , and then for another 14 k iterations with a smaller learning rate of 0.0004 .",1
9051,"During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum .",1
9052,"The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate .",1
9053,"Since our model is designed and trained on only small faces , we use a multiscale image pyramid for testing to deal with faces larger than our anchors .",0
9054,"Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset .",1
9055,We also follow the testing strategies used in Pyra - midBox 2 such as horizontal flip and bounding - box voting .,1
9056,Experiment results,0
9057,"WIDER FACE dataset includes 3226 images and 39708 faces labelled in the val dataset , with three subsetseasy , medium and hard .",0
9058,"In , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets .",1
9059,"As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin .",1
9060,"Since the hard set is a super set of small and medium , which contains all faces taller than 10 pixels , the performance on hard set can represent the performance on the full testing dataset more accurately .",0
9061,"Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - thearts .",1
9062,There is also a WIDER FACE test dataset with no annotations provided publicly .,0
9063,"It contains 16097 images , and is evaluated by WIDER FACE author team .",0
9064,We report the performance of our method at for the hard subset .,0
9065,"FDDB dataset includes 5171 faces on a set of 2845 images , and we use our model trained on WIDER FACE train set to infer on the FDDB dataset .",0
9066,We use the raw boundingbox result without fitting it into ellipse to compute ROC .,0
9067,"We show the discontinuous ROC curve at compared with , and our method achieves the state - of - the - art performance of TPR = 98.7 % given 1000 false positives .",1
9068,Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset .,0
9069,"We show the PR curve at compared with , and our method achieves a new the state - of - the - art performance of AP = 99.0 .",1
9070,AFW dataset includes 473 faces labelled in a set of 205 images .,0
9071,"As shown in compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 .",1
9072,Ablation study and diagnosis Ablation experiments,0
9073,"In order to verify the performance of our single level face detector , as well as the effectiveness of our proposed hard image mining , the dilated - head classification and regression structure , we conduct various ablation experiments on the WIDER FACE val dataset .",0
9074,All results are summarized in .,0
9075,"From , we can see that our single level baseline model can achieve performance comparable to the current : Ablation experiments .",0
9076,Baseline - Three is a face detector similar to SSH with three detection feature maps .,0
9077,Baseline - Single is our proposed detector with single detection feature map shown in .,0
9078,HIM and DH represents hard image mining ( Subsection 3.2 ) and dilated head architecture ) .,0
9079,"state - of - the - art face detector , especially on the hard subset .",0
9080,"Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors .",1
9081,This confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces .,0
9082,We also separately verify our newly proposed hard image mining ( HIM ) and dilated head architecture ( DH ) described in Subsection 3.2 and respectively .,0
9083,HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead .,1
9084,"DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors .",1
9085,Combining HIM and DH together can improve further towards the state - of - the - art performance .,1
9086,Diagnosis of hard image mining,0
9087,We investigate the effects of our hard image mining mechanism .,0
9088,We show the ratio of | D | and | D ? D | ( i.e. the ratio of the number of selected training images to the number of ignored training images ) in for each epoch .,0
9089,"We can see that at the first epoch , all training images are used to train the model .",0
9090,"Meanwhile , as the training process continues , more and more training images will be ignored .",0
9091,"At the last epoch , over a half images will be ignored and thus will not be included in D .",0
9092,Diagnosis of data augmentation,0
9093,We investigate the effectiveness of the photometric distortion as well as the cropping mechanisms as discussed in Subsection 3.3 .,0
9094,The ablation results evaluated on WIDER FACE val dataset are shown in .,0
9095,Both photometric distortion and cropping can contribute to a more robust face detector .,1
9096,Diagnosis of multi-scale testing,0
9097,"Our face detector with one detection feature map is design for small face detection , and our anchors are only capable of capturing faces with sizes ranging from ( 64 64 ) .",0
9098,"As a result , it is critical to adopt multi-scale testing to deal with large faces .",0
9099,"Different from SSH , S 3 FD and PyramidBox , our testing pyramid includes some extreme small scales ( i.e. short side contains only 100 or 300 pixels ) .",0
9100,"In , we show the effectiveness of these extreme small scales to deal with easy and large images .",0
9101,"Our full evaluation resizes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid .",0
9102,We diagnose the impact of the extra small scales ( i.e. 100 and 300 ) by removing them from the image pyramid . :,0
9103,"Diagnosis of multi-scale testing. , the extra small scales are crucial to detect easy faces .",1
9104,"Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only 78.2 , which is even lower than the performance on medium and hard which contain much harder faces .",0
9105,"We will show in the next subsection that these extra small scales ( 100 and 300 ) lead to negligible computation overhead , due to the lower resolution .",0
9106,As shown in,0
9107,Diagnosis of accuracy / speed trade - off,0
9108,We evaluate the speed of our method as well as some other popular face detectors in .,0
9109,"For fair comparison , we run all methods on the same machine , with one Titan X ( Maxwell ) GPU , and Intel",0
9110,Core i7-4770K,0
9111,3.50 GHz .,0
9112,"All methods except for Pyramid Box are based on Caffe1 implementation , which is compiled with CUDA 9.0 and CUDNN 7 .",0
9113,"For PyramidBox , we follow the official fluid code and the default configurations 3 .",0
9114,We use the officially built Pad - dlePaddle with CUDA 9.0 and CUDNN 7 .,0
9115,"For SSH , S 3 FD and Pyramid , we use the official inference code and configurations .",0
9116,"For SSH , we use multi-scale testing with the short side containing 500 , 800 , 1200 and 1600 pixels , and for S 3 FD , we execute the official evaluation code with both multi-scale testing and horizontal flip .",0
9117,Pyramid,0
9118,Box takes a similar testing configuration as S 3 FD .,0
9119,"As shown in , our detector can outperform SSH , S 3 FD and PyramidBox significantly with a smaller inference time .",0
9120,"Based on that , using horizontal flip can further improve the performance slightly .",0
9121,"In terms of GPU memory usage , our method uses only a half of what PyramidBox occupies , while achieving better performance .",0
9122,"Ours * in indicates our method without extra small scales in inference , i.e. , evaluated with scales [ 600 , 1000 , 1400 ] .",0
9123,"It is only 6.5 % faster than evaluation with [ 100 , 300 , 600 , 1000 , 1400 ] ( 1.59 compared with 1.70 ) .",0
9124,"This proves that although our face detector is only trained on small faces , it can perform well on large faces , by simply shrinking the testing image with negligible computation overhead .",0
9125,Conclusion,0
9126,"To conclude , we propose a novel face detector to focus on learning small faces on hard images , which achieves the state - of - the - art performance on all popular face detection datasets .",0
9127,"We propose a hard image mining strategy by dynamically assigning difficulty scores to training images , and re-sampling subsets with hard images for training before each epoch .",0
9128,"We also design a single shot face detector with only one detection feature map , to train and test on small faces .",0
9129,"With these designs , our model can put more attention on learning small hard faces instead of memorizing change of scales .",0
9130,"Extensive experiments and ablations have been done to show the effectiveness of our method , and our face detector achieves the state - of - the - art performance on all popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces and AFW .",0
9131,Our face detector also enjoys faster multi-scale inference speed and less GPU memory usage .,0
9132,"Our proposed method are flexible and can be applied to other backbones and tasks , which we remain as future work .",0
9133,title,0
9134,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,1
9135,abstract,0
9136,This paper describes a simple but competitive unsupervised system for hypernym discovery .,1
9137,"The system uses skip - gram word embeddings with negative sampling , trained on specialised corpora .",0
9138,Candidate hypernyms for an input word are predicted based on cosine similarity scores .,0
9139,Two sets of word embedding models were trained separately on two specialised corpora : a medical corpus and a music industry corpus .,0
9140,Our system scored highest in the medical domain among the competing unsupervised systems but performed poorly on the music industry domain .,0
9141,Our approach does not depend on any external data other than raw specialised corpora .,0
9142,Introduction,0
9143,The SemEval-2018 shared task on Hypernymy Discovery sought to study approaches for identifying words that hold a hypernymic relation .,0
9144,Two words have a hypernymic relation if one of the words belongs to a taxonomical class that is more general than that of the other word .,0
9145,"For example , the word vehicle belongs to a more general taxonomical class than car does , as car is a type of vehicle .",0
9146,Hypernymy can be seen as an is - a relationship .,0
9147,Hypernymy has been studied from different angles in the natural language processing literature as it is related to the human cognitive ability of generalis ation .,0
9148,"This shared task differs from recent taxonomy evaluation tasks by concentrating on Hypernym Discovery : the task of predicting ( discovering ) n hypernym candidates for a given input word , within the vocabulary of a specific domain .",0
9149,This shared task provided a general language domain vocabulary and two specialised domain vocabularies in English : medical and music indus - try .,0
9150,"For each vocabulary , a reference corpus was also supplied .",0
9151,"In addition to these English vocabularies , general language domain vocabularies for Spanish and Italian were also provided .",0
9152,The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains .,1
9153,"Word embeddings trained on large corpora have been shown to capture semantic relations between words , including hypernym -hyponym relations .",0
9154,The word embeddings built and used by the system presented here exploit this property .,0
9155,"Although these word embeddings do not distinguish one semantic relation from another , we expect that true hypernyms will constitute a significant proportion of the predicted candidate hypernyms .",0
9156,"Indeed , we show that for the medical domain subtask , our system beats the other unsupervised systems , although it still ranks behind the supervised systems .",0
9157,"Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general , our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand - annotated data , and from the expectation that they are able to generalise more easily to unseen hypernym - hyponym pairs .",1
9158,The rest of this system description paper is organised as follows :,0
9159,Section 2 briefly surveys the relevant literature and explains the reasons for choosing to use a particular flavour of word embeddings .,0
9160,Section 3 describes the components of the system and its settings .,0
9161,Section 4 summarises the results and offers some insights behind the numbers .,0
9162,Section 5 concludes and proposes avenues for future work .,0
9163,924,0
9164,Modern neural methods for natural language processing ( NLP ) use pre-trained word embeddings as fixed - sized vector representations of lexical units in running text as input data .,0
9165,"However , as mentioned previously , word embedding vectors can be used on their own to measure semantic relations between words in an unsupervised manner by , for example , taking the cosine similarity of two word embedding vectors for which semantic similarity is to be measured .",0
9166,There are several competing approaches for producing word embedding vectors .,0
9167,"One such approach is skip - gram with negative sampling ( SGNS ) , introduced by as part of their Word2 Vec software package .",0
9168,"The skip - gram approach assumes that a focus word occurring in text depends on its context words ( the words the focus word co-occurs with inside a fixed - sized window ) , but that those context words occur independently of each other .",0
9169,This conditional independence assumption in the context words makes computation more efficient and produces vectors that work well in practice .,0
9170,"The negative sampling portion of the algorithm is away of producing "" negative "" context words for the focus word by simply drawing random words from the corpus .",0
9171,"These random words are assumed to be "" bad "" context words for the focus word .",0
9172,Our team indeed implemented a variant of the Hypervec method but failed to obtain better per-formance scores on the training set than those obtained by using traditional SGNS ( see Section 4 ) .,0
9173,"Whilst it is possible that a software bug in our implementation could be the cause of this lower performance , we decided to submit the SGNS results to the official shared task due to time constraints .",0
9174,System Description,0
9175,"Our system consists of two components : a trainer that learns word vectors using an implementation of the Skip - Gram with Negative Sampling algorithm , and a predictor that outputs ( predicts ) the top 10 hypernyms of an input word based on the trained vectors .",0
9176,These two components and their settings are described here .,0
9177,Trainer,0
9178,"The trainer is a modification of Py - Torch SGNS 1 , a freely available implementation of the Skip - Gram with Negative Sampling algorithm .",0
9179,"One set of vectors per specialised corpus ( medicine and music industry ) were trained on a vocabulary that consists of the 100,000 most frequent words in each corpus , using a word window of 5 words to the left and 5 words to the right of a sliding focus word .",0
9180,The windows do not cross sentence boundaries .,0
9181,"For negative sampling , 20 words were randomly selected from the vocabulary based on their frequency 2 .",0
9182,All vectors had a dimensionality of 300 .,0
9183,Predictor,0
9184,"For each input word in the test file , the predictor attempts to produce 10 candidate hypernyms based on the vectors it learned during training .",0
9185,"If there is no vector for an input word , no output for that word is given .",0
9186,"If the input word is a multiword expression , then the learned vectors for the individual component words are retrieved and averaged together .",0
9187,This averaged vector is interpreted to represent the input multiword expression .,0
9188,"After a vector is retrieved ( or computed , in the case of averaged multiword expressions ) , pairwise cosine similarities are taken between this vector and all other vectors ( i.e. the vectors corresponding to the other 99,999 most frequent words ) .",0
9189,The words represented by the 10 highest ranking cosine similarities are output as the 10 candidate hypernyms for the input word or multiword expression .,0
9190,"As can be seen , our system is completely unsupervised as it does not require corpora with tagged examples of words holding hypernym - hyponym relations or any external linguistic or taxonomical resources .",0
9191,"shows the results for our SGNS - based approach , which was submitted to the official shared task ( SGNS ) , and for our Hypervec variant ( HV ) , which was not submitted .",0
9192,Results,1
9193,Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision ( MAP ) of 8.13 .,1
9194,"However , it ranked first place among all the unsupervised systems on this subtask .",1
9195,"On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .",1
9196,"We believe that one reason why the music industry scores are so much lower than the medical results is due to our system not producing an output for 233 of the music industry input words ( 45 % of the total ) , compared to the 128 medical input words ( 26 % ) it failed to predict .",0
9197,"Another aspect that seems to work against our system is its simplistic way of handling multiword expressions , namely by averaging together the individual word 's vectors .",0
9198,"The total number of multiword expressions in the medical test set is 264 , slightly higher than in the music test set , which contains 220 multiword expressions .",0
9199,"Similarly , our system does not have away of predicting multiword expressions as hypernym candidates , as it can only output the unigrams for which it has vector representations .",0
9200,"82 % of the medical domain input words have at least one hypernym that is a multi -word expression , whilst 92 % of the music industry domain input words have multi-word expression hypernyms .",0
9201,Conclusions and Future Work,0
9202,"We presented a simple but competitive unsupervised system to predict hypernym candidates for input words , based on cosine similarity scores of word embedding vectors trained on specialised corpora .",0
9203,Unsupervised systems in general tend to have lower performance than supervised systems as they lack explicit information to train on .,0
9204,"So we are encouraged that our system beat other unsupervised systems on one corpus , as this gives us more avenues to explore .",0
9205,One such avenue is to revisit our Hypervec implementation .,0
9206,We suspect that it might require more training epochs than the traditional SGNS method in order to achieve reasonable results .,0
9207,"We also seek to experiment with refining pre-trained SGNS word embeddings with Hypervec , rather than training word embeddings from scratch using Hypervec directly .",0
9208,Another avenue to explore involves incorporating taxonomical information into our word embeddings .,0
9209,One way to achieve this is by retrofitting pre-trained SGNS word embeddings with information derived from existing taxonomies like WordNet .,0
9210,Another way of incorporating taxonomical information is by generating a pseudo - corpus via a random walkover such a taxonomy and then learn SGNS word embeddings in the usual way .,0
9211,"These approaches ( Hypervec , retrofitting and taxonomy random - walk ) however , would relax the unsupervised constraint we followed in our implementation .",0
9212,"So yet another avenue to explore is to instead apply different similarity functions that might be more sensitive to the one - way , generalspecific nature of hypernymic relationships between words .",0
9213,title,0
9214,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,1
9215,abstract,0
9216,"This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 "" Hypernym Discovery "" , Subtask 1 : "" General - Purpose Hypernym Discovery "" , which tries to produce a ranked list of hypernyms for a specific term .",0
9217,We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .,0
9218,The results show that the application of these patterns leads to a higher score than using the traditional lexical patterns .,0
9219,Introduction,0
9220,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,1
9221,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,1
9222,"The paper is structured in 4 sections : this section presents existing approaches for automatic extraction of hypernymy relations , Section 2 contains the current system architecture .",0
9223,"The next section presents the web interface of the project , and , finally , Section 4 briefly analyses the results and drafts some conclusions .",0
9224,"Since language is a "" vital organ "" , constantly evolving and changing overtime , there are many words which lose one of their meanings or attach a new meaning .",0
9225,"For instance , when searching the word "" apple "" in WordNet , it appears defined as "" fruit with red or yellow or green skin and sweet to tart crisp whitish flesh "" and "" native Eurasian tree widely cultivated in many varieties for it s firm rounded edible fruits "" but searching in British National Corpus 1 , we will remark that the term is used more frequently as a named entity ( referring to a "" company "" ) .",0
9226,"From this point of view , we consider that developing a system for hypernym discovery that uses linguistic features from a corpus could be more useful for this task than using a manuallycrafted taxonomy .",0
9227,"It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .",0
9228,"Also , detecting hypernymy relations is an important task in NLP , which has been pursued for over two decades , and it is addressed in the literature using two complementary approaches : rule - based and distributional methods .",0
9229,Rule - based methods base the decision on the lexicosyntactic paths connecting the joint occurrences of two or more terms in a corpus .,0
9230,"In the case of supervised distributional methods , term - pair is represented using some combination of the terms ' embedding vectors .",0
9231,"This challenge has been shown to directly help in downstream applications such automatic hypernymy detection is useful for NLP tasks such as : taxonomy creation , recognizing textual entailment , text generation , Question Answering systems , semantic search , Natural Language Inference , Coreference Resolution and many others .",0
9232,"Traditional procedures to evaluate taxonomies have focused on measuring the quality of the edges , i.e. , assessing the quality of the is - a relations .",0
9233,This process typically consists of extracting a random sample of edges and manually labeling them by human judges .,0
9234,"In addition to the manual effort required to perform this evaluation , this procedure is not easily replicable from taxonomy to taxonomy ( which would most likely include different sets of concepts ) , and do not reflect the over all quality of a taxonomy .",0
9235,"Moreover , some taxonomy learning approaches link their concepts to existing resources such as Wikipedia .",0
9236,A new Approach to Detect Hypernymy Relation,0
9237,The main purpose of this project was to identify the best ( set of ) candidate hypernyms for a certain term from the given corpus 2 .,0
9238,"In our system , we considered the rule - based approach and , in order to extract the corresponding patterns , we used syntactic dependencies relations ( Universal Dependencies Parser 3 ) .",0
9239,"Below , we present our method of extracting hypernyms from text :",0
9240,"2 For this subtask , we used the 3 - billion - word UMBC corpus , which consists of paragraphs extracted from the web as part of the Stanford WebBase Project .",0
9241,This is a very large corpus containing information from different domains .,0
9242,3 Universal Dependencies ( UD ) is a framework for crosslinguistically consistent grammatical annotation and an open community effort with over 200 contributors producing more than 100 treebanks in over 60 languages .,0
9243,Tokenization : sentence boundaries are detected and punctuation signs are separated from words ;,0
9244,Part - of - speech tagging : the process of assigning a part - of - speech or lexical class marker to each word in a corpus .,0
9245,"Words in natural languages usually encode many pieces of information , such as : what the word "" means "" in the real world , what categories , if any , the word belongs to , what is the function of the word in the sentence ?",0
9246,Many language processing applications need to extract the information encoded in the words .,0
9247,"Parsers which analyze sentence structure need to know / check agreement between : subjects and verbs , adjectives and nouns , determiners and nouns , etc .",0
9248,Information retrieval systems benefit from know what the stem of a word is .,0
9249,Machine translation systems need to analyze words to their components and generate words with specific features in the target language .,0
9250,Dependency parsing : the syntactic parsing of a sentence consists of finding the correct syntactic structure of that sentence in a given formalism / grammar .,0
9251,"Dependency parsing structure consists of lexical items , linked by binary asymmetric relations called dependencies .",0
9252,"It is interested in grammatical relations between individual words ( governing & dependent words ) , it does not propose a recursive structure , rather a network of relations .",0
9253,"These relations can also have labels and the phrasal nodes are missing in the dependency structure , when compared to constituency structure .",0
9254,One of the boosts for this approach was to develop new dependency patterns for identifying hypernymy relations from text thatare based on dependency relations .,0
9255,The increased popularity and the universal inventory of categories and guidelines ( which facilitate annotation across languages ) of Universal Dependencies determined us to use this resource in order to automatically extract the hypernyms from the corpus .,0
9256,Figure1 : Project 's architecture,0
9257,"In this manner , we managed to compress a list of 44 lexico- syntactic patterns used for the hypernyms extraction 4 in only 8 dependencies patterns .",0
9258,"In the next lines , we present few examples of lexico - syntactic patterns that were replaced by dependencies patterns :",0
9259,"{X and other Y ; X or other Y ; X and any other Y ; X and some other Y ; Y other than X ; X like other Y ; Y other than X } replaced by X "" amod "" Y;",0
9260,{ X is a Y ; X was a Y ; X area Y ; X are Y ; X will be a Y ; X is an adj Y ; X was a adj .,0
9261,Y ; X area adj .,0
9262,Y ; X was a adj .,0
9263,Y;,0
9264,"which are similar to X ; Y which is similar to X } replaced by X "" nmod "" Y.",0
9265,"Because we used syntactic dependencies relations ( no lexical patterns were involved ) , our system is language independent .",0
9266,"Unfortunately , the limited hardware resources determined us to run 4 http://webdatacommons.org/is adb/lrec2016.pdf our system only in English but we are looking forward to running it in both Spanish and Italian .",0
9267,The Web Interface,0
9268,The interface 5 was implemented in the form of a website .,0
9269,The site is backed by a Mongodb data base .,0
9270,"When a user types in a query and hits enter a post request is sent and the backend will do some processing on the query ( tokenizing , lemmatizing ) and then search in the data base .",0
9271,The results are then sent back to the user where they are rendered .,0
9272,Results,1
9273,We consider that a qualitative way of analyzing our system is to look at which relations are more productive .,0
9274,presents the percentages of the most representative syntactic relations which we have identified .,0
9275,"While some relations have not been very fruitful ( such as X "" obj "" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .",1
9276,"The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .",1
9277,"The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .",1
9278,"An issue that we have noticed was that the given vocabulary was quite restrictive , for instance , it contains words like "" above - water "" , "" artesian water "" , "" bath water "" etc. , but it does n't contain the word "" water "" ( we had a case when our system identified the word "" water "" as a hypernym and it was a correct hypernym , but due to the fact that the vocabulary does n't contain the word "" water "" , it can not be evaluated ) and many other examples like this .",0
9279,title,0
9280,abstract,0
9281,"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings .",1
9282,"Our system took first place in subtasks ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",0
9283,Introduction,0
9284,"Natural language phenomena are extremely sparse by their nature , whereas continuous word embeddings employ dense representations of words .",0
9285,Turning these dense representations into a much sparser form can help in focusing on most salient parts of word representations .,0
9286,Sparsity - based techniques often involve the coding of a large number of signals over the same dictionary .,0
9287,"Sparse , overcomplete representations have been motivated in various domains as away to increase separability and interpretability and stability in the presence of noise .",0
9288,Non-negativity has also been argued to be advantageous for interpretability .,0
9289,"As illustrates this in the language domain , where sparse features are interpreted as lexical attributes , "" to describe the city of Pittsburgh , one might talk about phenomena typical of the city , like erratic weather and large bridges .",0
9290,"It is redundant and inefficient to list negative properties , like the absence of the Statue of Liberty "" .",0
9291,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,0
9292,Here we apply sparse feature pairs to hypernym extraction .,1
9293,"The role of an attribute pair i , j ? ? ( q ) ? ( h ) ( where q is the query word , h is the hypernym candidate , and ?( w ) is the index of a non -zero component in the sparse representations of w ) is similar to interaction terms in regression , see section 2 for details .",0
9294,Sparse representation is related to hypernymy in various natural ways .,1
9295,One of them is through Formal concept Analysis ( FCA ) .,1
9296,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,0
9297,Our submissions experiment with formal concept analysis tool by .,0
9298,"See the next section for a description of formal concept lattices , and how hypernyms can be found in them .",0
9299,"Another natural formulation is related to hierarchical sparse coding , where trees describe the order in which variables "" enter the model "" ( i.e. , take non - zero values ) .",1
9300,"A node may take a non-zero value only if it s ancestors also do : the dimensions that correspond to top level nodes should focus on "" general "" meaning components thatare present in most words .",0
9301,offer an implementation that is efficient for gigaword corpora .,0
9302,Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice .,1
9303,"The task evaluated systems on their ability to extract hypernyms for query words in five subtasks ( three languages , English , Italian , and Spanish , and two domains , medical and music ) .",0
9304,Queries have been categorized as concepts or entities .,0
9305,"Results were reported for each category separately as well as in combined form , thus resulting in 5 3 combinations .",0
9306,"Our system took first place in subtasks ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",0
9307,Detailed results for our system appear in section 3 .,0
9308,Our source code is available online 1 .,0
9309,Formal concept analysis,0
9310,Formal concept Analysis ( FCA ) is the mathematization of concept and conceptual hierarchy .,0
9311,"In FCA terminology , a context is a set of objects O , a set of attributes A , and a binary incidence relation I ?",0
9312,O A between members of O and A .,0
9313,"In our application , I associates a word w ?",0
9314,O to the indices of its non -zero sparse coding coordinates i ?,0
9315,"A. FCA finds formal concepts , pairs O , A of object sets and attribute sets ( O ? O , A ? A ) such that A consists of the shared attributes of objects in O ( and no more ) , and O consists of the objects in O that have all the attributes in A ( and no more ) .",0
9316,"( There is a closure - operator related to each FCA context , for which O and A are closed sets iff O , A is a concept . )",0
9317,O is called the extent and A is the intent of the concept .,0
9318,"There is an order defined in the context : if A 1 , B 1 and A 2 , B 2 are concepts in C ,",0
9319,The concept order forms a lattice .,0
9320,The smallest concept whose extent contains a word is said to introduce the object .,0
9321,We expect that h will be a hypernym of q iff n ( q ) ? n ( h ) where n ( w ) denotes the node in the concept lattice that introduces w .,0
9322,The closedness of extents and intents has an important structural consequence .,0
9323,Adding attributes to A ( e.g. responses of additional neurons ) will very probably grow the model .,0
9324,"However , the original concepts will be embedded as a substructure in the larger lattice , with their ordering relationships preserved .",0
9325,We use the popular skip - gram ( SG ) approach to train d = 100 dimensional dense distributed word representations for each sub-corpus .,0
9326,"The word embeddings are trained over the text corpora provided by the shared task organizers with the default training parameters of word2vec ( w2 v ) , i.e. a window size of 10 and 25 negative samples for each positive context .",0
9327,We derived multi-token units by relying on the word2 phrase software accompanying the w2 v toolkit .,0
9328,An additional source for identifying multitoken units in the training corpora was the list of potential hypernyms released for each subtask by the shared task organizers .,0
9329,Given the dense embedding matrix W x ?,0
9330,"R d| Vx | , for some subcorpus of the shared task x ? { 1A , 1 B , 1C , 2A , 2B } , where | V x | is the size of the vocabulary and dis set to 100 .",0
9331,"As a subsequent step , we turn W x into sparse word vectors akin to by solving for",0
9332,"where C refers to the convex set of R dk matrices consisting of d-dimensional columns vectors with norm at most 1 , and ?",0
9333,contains the sparse coefficients for the elements of the vocabulary .,0
9334,The only difference compared to is that here we ensure a non-negativity constraint over the elements of ?.,0
9335,For the elements of the vocabulary we ran the formal concept analysis tool of 2 .,0
9336,"In order to keep the size of the DAG outputted by the FCA algorithm manageable , we only included the query words and those hypernyms in the analysis which occur in the training dataset for the corpora .",0
9337,"As we will see in the next section , this restriction turns out to be very useful .",0
9338,"Next , we determine a handful of features for a pair of expressions ( q , h) consisting of a query q and its potential hypernym h. provides an overview of the features employed for a pair ( q , h ) .",0
9339,We denote with q and h the 100 dimensional dense vectorial representations of q and h.,0
9340,"Additionally , we denote with Q and H the sequence of tokens constituting the query and hypernym phrases .",0
9341,"Finally , we refer to the set of basis vectors ( in the FCA terminology , attributes ) which are assigned non -zero weights in the reconstruction of the vectorial representation of q and h as ? ( q ) and ?( h ) .",0
9342,It is also considered as a feature ( isFrequentHypernym ) whether a particular candidate hypernym h belongs to the top - 50 most frequent hypernyms for the category of q ( i.e. concept or entity ) .,0
9343,Modeling the two categories separately played an important role in the success of our systems .,0
9344,Three additional features are defined for incorporating the concept lattice output by FCA .,0
9345,"With n ( w ) denoting the concept that introduces w , i.e. the most specific location within the DAG for w , our features indicate whether n ( q ) ( 1 ) coincides with that of h , ( 2 ) is the parent ( immediate successor ) for that of h , or ( 3 ) is the child ( immediate predictions ) for that of h. Parents , and even the inverse relation , proved to be more predictive than the conceptually motivated q ? h. In Table 1 , n 1 ? n 2 denotes that n 1 is an immediate predecessor of n 2 .",0
9346,"We will see in post-evaluation ablation experiments , where we refer to the above three features as the FCA features , that they were not useful in our submissions .",0
9347,"At submission time , this feature did notwork properly .",0
9348,The attribute,0
9349,"Pair ij s above , our most important features , are indicator features for every possible interaction term between the sparse coefficients in ?.",0
9350,"That means that for a pair of words ( q , h) we defined ?( q ) ? ( h ) , i.e. candidates get assigned with the Cartesian product derived from the indices of the non -zero coefficients in ?.",0
9351,"Note that this feature template induces k 2 features , with k being the number of basis vectors introduced in the dictionary matrix D according to Eq. 1 .",0
9352,In order to rank potential hypernym candidates over the test set we trained a logistic regression classifier for concepts and entities utilizing the sklearn package with the regularization parameter defaulting to 1.0 .,0
9353,"For each appropriate ( q , h ) pair of words for which h is a hypernym of q , we generated a number of negative samples ( q , h ) , such that the training data does not include h as a valid hypernym for q .",0
9354,"For a given query q , belonging to either of the concept or entity category , we sampled h from those hypernyms which were included as a valid hypernym in the training data with respect to some q = q query phrase .",0
9355,"When making predictions for the hypernyms of a query , we relied on our query type sensitive logistic regression model to determine the ranking of the hypernym candidates .",0
9356,In our official submission we treated such phrases to rank which were included in the training data for being a proper hypernym at least once .,0
9357,"After the appropriate model ranked the hypernym candidates , we selected the top 15 ranked candidates and applied a post-ranking heuristic over them , i.e. reordered them according to their background frequency from the training corpus .",0
9358,Our assumption here is that more frequent words tend to refer to more general concepts and more general hypernymy relations potentially tend to be more easily detectable than more specialized ones .,0
9359,Results,0
9360,Our submissions,0
9361,Our submissions were based on k = 200 dimensional sparse vectors computed from unit - normed 100 - dimensional dense vectors with ? =,0
9362,. 3 . The sum of the two dimensions motivates our group name .,0
9363,"For training the regression model with negative samples , 50 false hypernyms were sampled for each query q in the training dataset .",0
9364,"One of our : Baseline results , most frequent training hypernyms .",0
9365,We ( upper ) consider the most frequent hypernym in the given query type ( concept or entity ) .,0
9366,"For comparison , we also show the MFH baseline provided by the organizers ( lower ) that is based on the most frequent hypernyms in general .",0
9367,"submissions involved attribute pairs , the other not .",0
9368,Both submissions used the conceptually motivated but practically harmful FCA - based features .,0
9369,shows submission results .,0
9370,"The figures that can be reproduced with the code in the project repo ( reprd ) is slightly different from our official submissions ( offic ) for two reasons : because the implementation of isFreqHyp contained a bug , and because of the natural randomness in negative sampling .",0
9371,"For reproducibility , we report result without the isFreqHyp feature .",0
9372,The randomness introduced by negative sampling is now factored out by random seeding . :,0
9373,"Number of in - vocabulary ( and out - ofvocabulary , OOV ) queries per query type .",0
9374,The ratio of the latter is also shown .,0
9375,Query type sensitive baselining,0
9376,"Our submission with attribute pairs achieved first place in categories ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",1
9377,This is in part due to our good choice of a fallback solution in the case of OOV queries : we applied a category - sensitive baseline returning the most frequent train hypernym in the corresponding query type ( concept or entity ) .,0
9378,"shows how frequently we had to rely on this fallback , and shows the corresponding pure baseline results .",0
9379,Post - evaluation analysis,0
9380,"After the evaluation closed , we conducted ablation experiments the results of which are included in .",0
9381,"In these experiments , we investigated the contribution of the features derived from sparse attribute pairs and FCA .",0
9382,These ablation experiments corroborate the importance of features derived from sparse attribute pairs and reveal that turning off FCA - based features does not hurt performance at all .,0
9383,For this reason - even though our official shared task submission included FCArelated features - we no longer employed them in our post-evaluation experiments .,0
9384,training .,0
9385,"In our post evaluation experiments we investigated the effects of generating more negative samples , i.e. we regarded all the valid hypernyms over the training set - not being a proper hypernym for q - as h upon the creation of the ( q , h ) negative training instances .",0
9386,This latter strategy is referenced as ns = all in .,0
9387,In our official submission we regarded only those hypernyms as potential candidates to rank during test time which occurred at least once as a correct hypernym in the training data .,0
9388,We call this strategy as candidate filtering .,0
9389,"Historically , we applied this restriction to speedup the FCA algorithm because this way the size of the concept lattice could be made smaller .",0
9390,"As there are valid hypernyms on the test set which never occurred in the training data , our official submission would not be able to obtain a perfect score even in theory .",0
9391,Table 7 contains the best possible metrics on the test set that we could achieve when candidate filtering is applied .,0
9392,In our post evaluation experiments we also investigated the effects of turning this kind of filtering step off .,0
9393,"As illustrates , however , our scores degrade after turning candidate filtering off .",0
9394,Our post evaluation experiments in gest that it is advantageous to apply sparse representation of more expressive power ( i.e. a higher number of basis vectors ) .,0
9395,Generating more negative samples also provides some additional performance boost .,0
9396,"These previous observations hold irrespective whether candidate filtering is employed or not , however , their effects are more pronounced when hypernym candidates are not filtered .",0
9397,"Finally , we report our post-evaluation results for all the subtasks and compare them to the official scores of the best performing systems in .",0
9398,"It can be seen from these enhanced results for category "" all "" ( concepts and entities mixed ) that we would win ( 1B ) Italian and ( 1C ) Spanish .",0
9399,"Our post-evaluation system - which only differs from our participating system that it fixes the calculation of a features , does not rely on FCA - based features and uses k = 1000 - would also place third in the rest of the subtasks .",0
9400,Conclusion,0
9401,In this paper we experimented with the integration of sparse word representations into the task of hypernymy discovery .,0
9402,"We strived to utilize sparse word representations in two ways , i.e. via building concept lattices using formal concept analysis and modeling the hypernymy relation with the help of interaction terms .",0
9403,"While our former approach for deriving formal concepts from sparse word representations was not successful , the interaction terms derived from sparse word representations proved to be highly beneficial .",0
9404,title,0
9405,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,1
9406,abstract,0
9407,"The fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation , most of which rely on word distribution .",0
9408,"We investigate an extensive number of such unsupervised measures , using several distributional semantic models that differ by context type and feature weighting .",0
9409,We analyze the performance of the different methods based on their linguistic motivation .,0
9410,"Comparison to the state - of - the - art supervised methods shows that while supervised methods generally outperform the unsupervised ones , the former are sensitive to the distribution of training instances , hurting their reliability .",0
9411,"Being based on general linguistic hypotheses and independent from training data , unsupervised measures are more robust , and therefore are still useful artillery for hypernymy detection .",0
9412,Introduction,0
9413,"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy .",1
9414,"Such effort is motivated by the role this semantic relation plays in a large number of tasks , such as taxonomy creation and recognizing textual entailment .",0
9415,"The task has appeared to be , however , a challenging one , and the numerous approaches proposed to tackle it have often shown limitations .",0
9416,"Early corpus - based methods have exploited patterns that may indicate hypernymy ( e.g. "" animals such as dogs "" ) , but the recall limitation of this approach , requiring both words to co-occur in a sentence , motivated the development of methods that rely on adaptations of the distributional hypothesis .",0
9417,"The first distributional approaches were unsupervised , assigning a score for each ( x , y ) wordpair , which is expected to be higher for hypernym pairs than for negative instances .",0
9418,"Evaluation is performed using ranking metrics inherited from information retrieval , such as Average Precision ( AP ) and Mean Average Precision ( MAP ) .",0
9419,Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis and the distributional informativeness hypothesis .,0
9420,"In the last couple of years , the focus of the research community shifted to supervised distributional methods , in which each ( x , y) word - pair is represented by a combination of x and y's word vectors ( e.g. concatenation or difference ) , and a classifier is trained on these resulting vectors to predict hypernymy .",0
9421,"While the original methods were based on count - based vectors , in recent years they have been used with word embeddings , and have gained popularity thanks to their ease of use and their high performance on several common datasets .",0
9422,"However , there have been doubts on whether they can actually learn to recognize hypernymy .",0
9423,"Additional recent hypernymy detection methods include a multimodal perspective , a supervised method using unsupervised measure scores as features , and a neural method integrating path - based and distributional information .",0
9424,"In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection , using several distributional semantic models that differ by context type and feature weighting .",1
9425,Some measure vari - ants and context - types are tested for the first time .,0
9426,"We demonstrate that since each of these measures captures a different aspect of the hypernymy relation , there is no single measure that consistently performs well in discriminating hypernymy from different semantic relations .",0
9427,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .",1
9428,We also compare the unsupervised measures to the state - of - the - art supervised methods .,1
9429,"We show that supervised methods outperform the unsupervised ones , while also being more efficient , computed on top of low - dimensional vectors .",0
9430,"At the same time , however , our analysis reassesses previous findings suggesting that supervised methods do not actually learn the relation between the words , but only characteristics of a single word in the pair .",0
9431,"Moreover , since the features in embedding - based classifiers are latent , it is difficult to tell what the classifier has learned .",0
9432,"We demonstrate that unsupervised methods , on the other hand , do account for the relation between words in a pair , and are easily interpretable , being based on general linguistic hypotheses .",0
9433,Distributional Semantic Spaces,0
9434,We created multiple distributional semantic spaces that differ in their context type and feature weighting .,0
9435,"As an underlying corpus we used a concatenation of the following two corpora : uk WaC ( Ferraresi , 2007 ) , a 2 - billion word corpus constructed by crawling the .uk domain , and WaCkypedia EN ) , a 2009 dump of the English Wikipedia .",0
9436,"Both corpora include POS , lemma and dependency parse annotations .",0
9437,"Our vocabulary ( of target and context words ) includes only nouns , verbs and adjectives that occurred at least 100 times in the corpus .",0
9438,Context Type,0
9439,We use several context types :,0
9440,Window - based contexts :,0
9441,"the contexts of a target word w i are the words surrounding it in a ksized window : w i?k , ... , w i ?1 , w i + 1 , ... , w i+k .",0
9442,"If the context - type is directional , words occurring before and after w i are marked differently , i.e. : w i?k /l , ... , w i ?1 / l , w i + 1 /r , ... , w i+k /r .",0
9443,Out - of - vocabulary words are filtered out before applying the window .,0
9444,"We experimented with window sizes 2 and 5 , directional and indirectional ( win2 , win2d , win5 , win5d ) .",0
9445,"Dependency - based contexts : rather than adjacent words in a window , we consider neighbors in a dependency parse tree .",0
9446,The contexts of a target word w i are its parent and daughter nodes in the dependency tree ( dep ) .,0
9447,"We also experimented with a joint dependency context inspired by , in which the contexts of a target word are the parent - sister pairs in the dependency tree ( joint ) .",0
9448,See for an illustration .,0
9449,Feature Weighting,0
9450,Each distributional semantic space is spanned by a matrix Min which each row corresponds to a target word while each column corresponds to a context .,0
9451,"The value of each cell M i , j represents the association between the target word w i and the context c j .",0
9452,We experimented with two feature weightings :,0
9453,Frequency - raw frequency ( no weighting ) :,0
9454,"M i , j is the number of co-occurrences of w i and c j in the corpus .",0
9455,"Positive PMI ( PPMI ) - pointwise mutual information ( PMI ) is defined as the log ratio between the joint probability of wand c and the product of In addition , one of the measures we used ( Santus et al. , 2014 ) required a third feature weighting :",0
9456,Positive LMI ( PLMI ) - positive local mutual information ( PLMI ) .,0
9457,PPMI was found to have a bias towards rare events .,0
9458,"PLMI simply balances PPMI by multiplying it by the co-occurrence frequency of wand c : P LM I ( w , c ) = f req ( w , c ) PP M I (w , c ) .",0
9459,Unsupervised Hypernymy Detection Measures,0
9460,"We experiment with a large number of unsupervised measures proposed in the literature for distributional hypernymy detection , with some new variants .",0
9461,"In the following section , v x and v y denote x and y's word vectors ( rows in the matrix M ) .",0
9462,We consider the scores as measuring to what extent y is a hypernym of x ( x ? y ) .,0
9463,Similarity Measures,0
9464,"Following the distributional hypothesis , similar words share many contexts , thus have a high similarity score .",0
9465,"Although the hypernymy relation is asymmetric , similarity is one of its properties .",0
9466,Cosine Similarity ),0
9467,A symmetric similarity measure :,0
9468,A symmetric similarity measure that quantifies the ratio of shared contexts to the contexts of each word :,0
9469,"APSyn A symmetric measure that computes the extent of intersection among the N most related contexts of two words , weighted according to the rank of the shared contexts ( with N as a hyper -parameter ) :",0
9470,"AP Syn ( x , y ) = ? c?N ( vx ) ?N ( vy ) 1 rankx ( c ) + ranky ( c )",0
9471,2,0
9472,Inclusion Measures,0
9473,"According to the distributional inclusion hypothesis , the prominent contexts of a hyponym ( x ) are expected to be included in those of its hypernym ( y ) .",0
9474,Weeds Precision,0
9475,A directional precision - based similarity measure .,0
9476,This measure quantifies the weighted inclusion of x 's contexts by y's contexts :,0
9477,cosWeeds Geometric mean of cosine similarity and Weeds precision :,0
9478,"Clarke DE ) Computes degree of inclusion , by quantifying weighted coverage of the hyponym 's contexts by those of the hypernym :",0
9479,balAPinc Balanced average precision inclusion .,0
9480,is an adaptation of the average precision measure from information retrieval for the inclusion hypothesis .,0
9481,"N y is the number of non -zero contexts of y and P ( r ) is the precision at rank r , defined as the ratio of shared contexts with y among the top r contexts of x. rel ( c ) is the relevance of a context c , set to 0 if c is not a context of y , and to 1 ?",0
9482,"ranky ( c ) Ny+1 otherwise , where rank y ( c ) is the rank of the context c in y's sorted vector .",0
9483,"Finally ,",0
9484,is the geometric mean of APinc and Lin similarity .,0
9485,invCL,0
9486,Measures both distributional inclusion of x in y and distributional non-inclusion of yin x:,0
9487,Informativeness Measures,0
9488,"According to the distributional informativeness hypothesis , hypernyms tend to be less informative than hyponyms , as they are likely to occur in more general contexts than their hyponyms .",0
9489,SLQS SLQS ( x ? y),0
9490,= 1 ? E x E y,0
9491,The informativeness of a word x is evaluated as the median entropy of its top N contexts :,0
9492,is the entropy of context c.,0
9493,SLQS Sub,0
9494,A new variant of SLQS based on,0
9495,"the assumption that if y is judged to be a hypernym of x to a certain extent , then x should be judged to be a hyponym of y to the same extent ( which is not the case for regular SLQS ) .",0
9496,This is achieved by subtraction :,0
9497,"SLQS and SLQS Sub have 3 hyper-parameters : i ) the number of contexts N ; i i ) whether to use median or average entropy among the top N contexts ; and iii ) the feature weighting used to sort the contexts by relevance ( i.e. , PPMI or PLMI ) .",0
9498,"SLQS Row Differently from SLQS , SLQS",0
9499,"Row computes the entropy of the target rather than the average / median entropy of the contexts , as an alternative way to compute the generality of a word .",0
9500,2,0
9501,"In addition , parallel to SLQS we tested SLQS Row with subtraction , SLQS Row Sub .",0
9502,RCTC,0
9503,Ratio of change in topic coherence :,0
9504,"where t x are the top N contexts of x , considered as x 's topic , and t x\y are the top N contexts of x which are not contexts of y. T C ( A ) is the topic coherence of a set of words A , defined as the median pairwise PMI scores between words in A .",0
9505,N is a hyper - parameter .,0
9506,"The measure is based on the assumptions that excluding y's contexts from x 's increases the coherence of the topic , while excluding x 's contexts from y's decreases the coherence of the topic .",0
9507,"We include this measure under the informativeness inclusion , as it is based on a similar hypothesis .",0
9508,Reversed Inclusion Measures,0
9509,"These measures are motivated by the fact that , even though - being more general - hypernyms are expected to occur in a larger set of contexts , sentences like "" the vertebrate barks "" or "" the mammal arrested the thieves "" are not common , since hyponyms are more specialized and are hence more appropriate in such contexts .",0
9510,"On the other hand , hyponyms are likely to occur in broad contexts ( e.g. eat , live ) , where hypernyms are also appropriate .",0
9511,"In this sense , we can define the reversed inclusion hypothesis : "" hypernym 's contexts are likely to be included in the hyponym 's contexts "" .",0
9512,The following variants are tested for the first time .,0
9513,Reversed Weeds,0
9514,RevW eeds ( x ? y) = W eeds ( y ? x ),0
9515,Datasets,0
9516,"We use four common semantic relation datasets : BLESS , EVALution , Lenci / Benotto , and Weeds .",0
9517,"The datasets were constructed either using knowledge resources ( e.g. WordNet , Wikipedia ) , crowdsourcing or both .",0
9518,The semantic relations and the size of each dataset are detailed in We split each dataset randomly to 90 % test and 10 % validation .,0
9519,"The validation sets are used to tune the hyper - parameters of several measures : SLQS ( Sub ) , APSyn and RCTC .",0
9520,Experiments,1
9521,Comparing Unsupervised Measures,1
9522,"In order to evaluate the unsupervised measures described in Section 3 , we compute the measure scores for each ( x , y) pair in each dataset .",0
9523,"We first measure the method 's ability to discriminate hypernymy from all other relations in the dataset , i.e. by considering hypernyms as positive instances , and other word pairs as negative instances .",0
9524,"In addition , we measure the method 's ability to discriminate hypernymy from every other relation in the dataset by considering one relation at a time .",0
9525,"For a relation R we consider only ( x , y) pairs thatare annotated as either hypernyms ( positive instances ) or R ( negative instances ) .",0
9526,We rank the pairs according to the measure score and compute average precision ( AP ) at k = 100 and k = all .,0
9527,5 lated differently in each sense .,0
9528,We consider y as a hypernym of x if hypernymy holds in some of the words ' senses .,0
9529,"Therefore , when a pair is assigned both hypernymy and another relation , we only keep it as hypernymy .",0
9530,We tried several cut-offs and chose the one that seemed to be more informative in distinguishing between the unsupervised measures .,0
9531,"reports the best performing measure ( s ) , with respect to AP @ 100 , for each relation in each dataset .",0
9532,"The first observation is that there is no single combination of measure , context type and feature weighting that performs best in discriminating hypernymy from all other relations .",0
9533,"In order to better understand the results , we focus on the second type of evaluation , in which we discriminate hypernyms from each other relation .",0
9534,"The results show preference to the syntactic context - types ( dep and joint ) , which might be explained by the fact that these contexts are richer ( as they contain both proximity and syntactic information ) and therefore more discriminative .",1
9535,"In feature weighting there is no consistency , but interestingly , raw frequency appears to be successful in hypernymy detection , contrary to previously reported results for word similarity tasks , where PPMI was shown to outperform it .",1
9536,The new SLQS variants are on top of the list in many settings .,1
9537,"In particular they perform well in discriminating hypernyms from symmetric relations ( antonymy , synonymy , coordination ) .",1
9538,"The measures based on the reversed inclusion hypothesis performed inconsistently , achieving perfect score in the discrimination of hypernyms from unrelated words , and performing well in few other cases , always in combination with syntactic contexts .",0
9539,"Finally , the results show that there is no single combination of measure and parameters that performs consistently well for all datasets and classification tasks .",0
9540,"In the following section we analyze the best combination of measure , context type and feature weighting to distinguish hypernymy from any other relation .",0
9541,Best Measure Per Classification Task,0
9542,We considered all relations that occurred in two datasets .,0
9543,"For such relation , for each dataset , we ranked the measures by their AP @ 100 score , selecting those with score ?",0
9544,0.8 . 6 displays the intersection of the datasets ' best measures .,0
9545,Hypernym vs. Meronym,0
9546,The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts .,0
9547,"We conjecture that the windowbased contexts are less effective since they capture topical context words , that might be shared also among holonyms and their meronyms ( e.g. car will occur with many of the neighbors of wheel ) .",0
9548,"However , since meronyms and holonyms often have different functions , their functional contexts , which are expressed in the syntactic context - types , are less shared .",0
9549,"This is where they mostly differ from hyponym - hypernym pairs , which are of the same function ( e.g. cat is a type of animal ) .",0
9550,shows that SLQS performs well in this task on BLESS .,0
9551,"This is contrary to previous findings that suggested that SLQS is weak in discriminating between hypernyms and meronyms , as in many cases the holonym is more general than the meronym .",0
9552,The surprising result could be explained by the nature of meronymy in this dataset : most holonyms in BLESS are rather specific words .,0
9553,"BLESS was built starting from 200 basic level concepts ( e.g. goldfish ) used as the x words , to which y words in different relations were associated ( e.g. eye , for meronymy ; animal , for hypernymy ) .",0
9554,"x words represent hyponyms in the hyponym - hypernym pairs , and should therefore not be too general .",0
9555,"Indeed , SLQS assigns high scores to hyponym - hypernym pairs .",0
9556,"At the same time , in the meronymy relation in BLESS , x is the holonym and y is the meronym .",0
9557,"For consistency with EVALution , we switched those pairs in BLESS , placing the meronym in the x slot and the holonym in they slot .",0
9558,"As a consequence , after the switching , holonyms in BLESS are usually rather specific words ( e.g. , there are no holonyms like animal and vehicle , as these words were originally in they slot ) .",0
9559,"In most cases , they are not more general than their meronyms ( ( eye , goldfish ) ) , yielding low SLQS scores which are easy to separate from hypernyms .",0
9560,"We note that this is a weakness of the BLESS dataset , rather than a strength of the measure .",0
9561,"For instance , on EVALution , SLQS performs worse ( ranked only as high as 13th ) , as this dataset has no such restriction on the basic level concepts , and may contain pairs like ( eye , animal ) .",0
9562,Hypernym vs. Attribute,0
9563,Symmetric similarity measures computed on syntactic contexts succeed to discriminate between hypernyms and attributes .,0
9564,"Since attributes are syntactically different from hypernyms ( in attributes , y is an adjective ) , it is unsurprising that they occur in different syntactic contexts , yielding low similarity scores .",0
9565,"nearly 50 % of the SLQS false positive pairs were meronymholonym pairs , in many of which the holonym is more general than the meronym by definition , e.",0
9566,Hypernym vs. Antonym,0
9567,"In all our experiments , antonyms were the hardest to distinguish from hypernyms , yielding the lowest performance .",0
9568,We found that SLQS performed reasonably well in this setting .,0
9569,"However , the measure variations , context types and feature weightings were not consistent across datasets .",0
9570,"SLQS relies on the assumption that y is a more general word than x , which is not true for antonyms , making it the most suitable measure for this setting .",0
9571,"Hypernym vs. Synonym SLQS performs well also in discriminating between hypernyms and synonyms , in which y is also not more general than x .",0
9572,"We observed that in the joint context type , the difference in SLQS scores between synonyms and hypernyms was the largest .",0
9573,This may stem from the restrictiveness of this context type .,0
9574,"For instance , among the most salient contexts we would expect to find informative contexts like drinks milk for cat and less informative ones like drinks water for animal , whereas the nonrestrictive single dependency context drinks would probably be present for both .",0
9575,"Another measure that works well is invCL : interestingly , other inclusion - based measures assign high scores to ( x , y) when y includes many of x 's contexts , which might be true also for synonyms ( e.g. elevator and lift share many contexts ) .",0
9576,"in - vCL , on the other hand , reduces with the ratio of y's contexts included in x , yielding lower scores for synonyms .",0
9577,Hypernym vs. Coordination,0
9578,We found no consistency among BLESS and Weeds .,0
9579,"On Weeds , inclusion - based measures ( ClarkeDE , invCL and Weeds ) showed the best results .",0
9580,"The best performing measures on BLESS , however , were variants of SLQS , that showed to perform well in cases where the negative relation is symmetric ( antonym , synonym and coordination ) .",0
9581,"The difference could be explained by the nature of the datasets : the BLESS test set contains 1,185 hypernymy pairs , with only 129 distinct ys , many of which are general words like animal and object .",0
9582,"The Weeds test set , on the other hand , was intentionally constructed to contain an over all unique yin each pair , and therefore contains much more specific ys ( e. ) .",0
9583,"For this reason , generality - based measures perform well on BLESS , and struggle with Weeds , which is handled better using inclusion - based measures .",0
9584,Comparison to State - of - the - art Supervised Methods,1
9585,"For comparison with the state - of - the - art , we evaluated several supervised hypernymy detection methods , based on the word embeddings of x and y: concatenation v x ?",0
9586,"v y , difference v y ? v x , and ASYM .",0
9587,"We downloaded several pretrained embeddings , and trained a logistic regression classifier to predict hypernymy .",0
9588,"We used the 90 % portion ( originally the test set ) as the train set , and the other 10 % ( originally the validation set ) as a test set , reporting the best results among different vectors , method AP@100 original AP@100 switched ?",0
9589,"supervised concat , word2vec , L1 0.995 0.575 - 0.42 unsupervised",0
9590,"cosWeeds , win2d , ppmi 0.818 0.882 + 0.064 : Average Precision ( AP ) at k = 100 of the best supervised and unsupervised methods for hypernym vs. random - n , on the original BLESS validation set and the validation set with the artificially added switched hypernym pairs .",0
9591,method and regularization factor .,0
9592,"8 displays the performance of the best classifier on each dataset , in a hypernym vs. a single relation setting .",0
9593,"We also re-evaluated the unsupervised measures , this time reporting the results on the validation set ( 10 % ) for comparison .",0
9594,"The over all performance of the embeddingbased classifiers is almost perfect , and in particular the best performance is achieved using the concatenation method with either GloVe or the dependency - based embeddings .",1
9595,"As expected , the unsupervised measures perform worse than the embedding - based classifiers , though generally not bad on their own .",1
9596,"These results may suggest that unsupervised methods should be preferred only when no training data is available , leaving all the other cases to supervised methods .",0
9597,"This is , however , not completely true .",0
9598,"As others previously noticed , supervised methods do not actually learn the relation between x and y , but rather separate properties of either xor y. named this the "" lexical memorization "" effect , i.e. memorizing that certain ys tend to appear in many positive pairs ( prototypical hypernyms ) .",0
9599,"On that account , the Weeds dataset has been designed to avoid such memorization , with every word occurring once in each slot of the relation .",0
9600,"While the performance of the supervised methods on this dataset is substantially lower than their performance on other datasets , it is yet well above the random baseline which we might expect from a method that can only memorize words it has seen during training .",0
9601,9,0
9602,This is an indication that supervised methods can abstract away from the words .,0
9603,"Indeed , when we repeated the experiment with a lexical split of each dataset , i.e. , such that the train and test set consist of distinct vocabularies , we found that the supervised methods ' performance did not decrease dramatically , in contrast to the 8 In our preliminary experiments we also trained other classifiers used in the distributional hypernymy detection literature ( SVM and SVM + RBF kernel ) , that performed similarly .",0
9604,"We report the results for logistic regression , since we use the prediction probabilities to measure average precision .",0
9605,The dataset is balanced between its two classes .,0
9606,findings of .,0
9607,The large performance gaps reported by might be attributed to the size of their training sets .,0
9608,"Their lexical split discarded around half of the pairs in the dataset and split the rest of the pairs equally to train and test , resulting in a relatively small train set .",0
9609,"We performed the split such that only around 30 % of the pairs in each dataset were discarded , and split the train and test sets with a ratio of roughly 90 / 10 % , obtaining large enough train sets .",0
9610,"Our experiment suggests that rather than memorizing the verbatim prototypical hypernyms , the supervised models might learn that certain regions in the vector space pertain to prototypical hypernyms .",0
9611,"For example , device ( from the BLESS train set ) and appliance ( from the BLESS test set ) are two similar words , which are both prototypical hypernyms .",0
9612,"Another interesting observation was recently made by : they showed that when dependency - based embeddings are used , supervised distributional methods trace x and y's separate occurrences in different slots of Hearst patterns .",0
9613,"Whether supervised methods only memorize or also learn , it is more consensual that they lack the ability to capture the relation between x and y , and that they rather indicate how likely y ( x ) is to be a hypernym ( hyponym ) .",0
9614,"While this information is valuable , it can not be solely relied upon for classification .",0
9615,"To better understand the extent of this limitation , we conducted an experiment in a similar manner to the switched hypernym pairs in .",0
9616,"We used BLESS , which is the only dataset with random pairs .",0
9617,"For each hypernym pair ( x 1 , y 1 ) , we sampled a wordy 2 that participates in another hypernym pair ( x 2 , y 2 ) , such that ( x 1 , y 2 ) is not in the dataset , and added ( x 1 , y 2 ) as a random pair .",0
9618,"We added 139 new pairs to the validation set , such as ( rifle , animal ) and ( salmon , weapon ) .",0
9619,We then used the best supervised and unsupervised methods for hypernym vs. randomn on BLESS to re-classify the revised validation set .,0
9620,displays the experiment results .,0
9621,"The switched hypernym experiment paints a much less optimistic picture of the embeddings ' actual performance , with a drop of 42 points in average precision .",0
9622,121 out of the 139 switched hypernym pairs were falsely classified as hypernyms .,0
9623,Examining they words of these pairs reveals general words that appear in many hypernym pairs ( e..,0
9624,"The unsupervised measure was not similarly affected by the switched pairs , and the performance even slightly increased .",0
9625,"This result is not surprising , since most unsupervised measures aim to capture aspects of the relation between x and y , while not relying on information about one of the words in the pair .",0
9626,10,0
9627,Discussion,0
9628,The results in Section 5 suggest that a supervised method using the unsupervised measures as features could possibly be the best of both worlds .,0
9629,"We would expect it to be more robust than embeddingbased methods on the one hand , while being more informative than any single unsupervised measure on the other hand .",0
9630,"Such a method was developed by , however using mostly features that describe a single word , e.g. frequency and entropy .",0
9631,It was shown to be competitive with the state - of - theart supervised methods .,0
9632,"With that said , it was also shown to be sensitive to the distribution of training examples in a specific dataset , like the embeddingbased methods .",0
9633,"We conducted a similar experiment , with a much larger number of unsupervised features , namely the various measure scores , and encountered the same issue .",0
9634,"While the performance was good , it dropped dramatically when the model was tested on a different test set .",0
9635,"We conjecture that the problem stems from the currently available datasets , which are all somewhat artificial and biased .",0
9636,"Supervised methods which are strongly based on the relation between the words , e.g. those that rely on path - based information , manage to overcome the bias .",0
9637,"Distributional methods , on the other hand , are based on a weaker notion of the relation between words , hence are more prone to overfit the distribution of training instances in a specific dataset .",0
9638,"In the future , we hope that new 10 Turney and Mohammad have also shown that unsupervised methods are more robust than supervised ones in a transfer - learning experiment , when the "" training data "" was used to tune their parameters .",0
9639,"datasets will be available for the task , which would be drawn from corpora and will reflect more realistic distributions of words and semantic relations .",0
9640,Conclusion,0
9641,We performed an extensive evaluation of unsupervised methods for discriminating hypernyms from other semantic relations .,0
9642,"We found that there is no single combination of measure and parameters which is always preferred ; however , we suggested a principled linguistic - based analysis of the most suitable measure for each task that yields consistent performance across different datasets .",0
9643,"We investigated several new variants of existing methods , and found that some variants of SLQS turned out to be superior on certain tasks .",0
9644,"In addition , we have tested for the first time the joint context type , which was found to be very discriminative , and might hopefully benefit other semantic tasks .",0
9645,"For comparison , we evaluated the state - of the - art supervised methods on the datasets , and they have shown to outperform the unsupervised ones , while also being efficient and easier to use .",0
9646,"However , a deeper analysis of their performance demonstrated that , as previously suggested , these methods do not capture the relation between x and y , but rather indicate the "" prior probability "" of either word to be a hyponym or a hypernym .",0
9647,"As a consequence , supervised methods are sensitive to the distribution of examples in a particular dataset , making them less reliable for real - world applications .",0
9648,"Being motivated by linguistic hypotheses , and independent from training data , unsupervised measures were shown to be more robust .",0
9649,"In this sense , unsupervised methods can still play a relevant role , especially if combined with supervised methods , in the decision whether the relation holds or not .",0
9650,title,0
9651,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,1
9652,abstract,0
9653,This report describes the system developed by the CRIM team for the hypernym discovery task at SemEval 2018 .,0
9654,This system exploits a combination of supervised projection learning and unsupervised pattern - based hypernym discovery .,0
9655,It was ranked first on the 3 sub - tasks for which we submitted results .,0
9656,Introduction,0
9657,The goal of the hypernym discovery task at Sem - Eval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms .,0
9658,A query can be either a concept ( e.g. cocktail or epistemology ) or a named entity ( e.g. Craig Anderson or City of Whitehorse ) .,0
9659,Two types of data were provided to train the systems : a large unlabeled text corpus and a small training set of examples comprising a query and its hypernyms .,0
9660,More details on this task maybe found in the task description paper .,0
9661,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .",1
9662,"These two approaches are described in Sections 2 and 3 , then Section 4 describes our hybrid system and Section 5 presents our results .",0
9663,Pattern - Based Hypernym Discovery,0
9664,Pattern - based approaches to relation extraction have been discussed in the literature for quite sometime ( see surveys by and ) .,0
9665,"They can be used to discover various relations , including domain - specific ones and more general ones , such as hypernymy .",0
9666,"The patternbased approach to hypernym discovery was pioneered by , who defined specific textual patterns ( e.g. Y such as X ) to mine hyponym / hypernym pairs from corpora .",0
9667,"This approach is known to suffer from low recall because it assumes that hyponym / hypernym pairs will occur together in one of these patterns , which is often not the case .",0
9668,"For instance , using the training data of sub - task 1 A , we found that the majority of training pairs never co-occur within the same paragraph in corpus 1 A , let alone within a pattern that suggests hypernymy .",0
9669,"To increase recall , we extend the basic patternbased approach to hypernym discovery in two ways .",0
9670,"First , we identify co-hyponyms for each query and add the hypernyms discovered for these terms to those found for the query .",0
9671,"These cohyponyms are identified using patterns , and filtered based on distributional similarity using the embeddings described in Section 3.3 .",0
9672,"Furthermore , we discover additional hypernyms using a method based on the following assumptions : most multi-word expressions are compositional , and the prevailing head - modifier relation is hypernymy .",0
9673,"The co-hyponym patterns we use are limited to enumeration patterns ( e.g. X 1 , X 2 and X 3 ) .",0
9674,"For hypernyms , we use an extended set of Hearst - like patterns which we selected empirically ( e.g. Y such as X , Y other than X , not all Y are X , Y including X , Y especially X , Y like X , Y for example X , Y which includes X , X are also Y , X are all Y , not Y so much as X ) .",0
9675,"Our pattern - based hypernym discovery algorithm can be defined as follows : given a query q , 1 . Create the empty set Q , which will contain an extended set of queries .",0
9676,2 .,0
9677,Search for the co-hyponym patterns in the corpus to discover co-hyponyms of q .,0
9678,Add these to Q and store their frequency ( number of times a given co -hyponym was found using these patterns ) .,0
9679,3 .,0
9680,Score each co-hyponym q ?,0
9681,Q by multiplying the frequency of q by the cosine similarity of the embeddings of q and q .,0
9682,"Rank the co-hyponyms in Q according to this score , keep the top n , 1 and discard the rest .",0
9683,4 .,0
9684,Add the original query q to Q.,0
9685,5 .,0
9686,Create the empty set of hypernyms H q .,0
9687,6 .,0
9688,For each query q ?,0
9689,"Q , search for the hypernym patterns in the corpus to discover hypernyms of q .",0
9690,Add these to H q .,0
9691,7 .,0
9692,"Add the head of each term in H q to this set , as well as the head of the original query q.",0
9693,8 .,0
9694,Score each candidate c ?,0
9695,"H q by multiplying its normalized frequency 2 by the cosine similarity between the embeddings of c and q , and rank the candidates according to this score .",0
9696,"Although the pattern - based search for both cohyponyms and hypernyms can find terms not included in the provided vocabulary ( which could also be useful ) , we discarded out - of - vocabulary terms because we had not learned embeddings for them .",0
9697,Learning Projections for Hypernym Discovery,0
9698,Several supervised learning approaches based on word embeddings have recently been developed for the task of hypernym detection and the related task of hypernym discovery .,0
9699,The general idea is to learn a function that takes as input the word embeddings of a query q and a candidate hypernym hand outputs the likelihood that there is a hypernymy relationship between q and h.,0
9700,"To discover hypernyms for a given query q ( rather than classify a given pair of words ) , we apply this decision function to all candidate hypernyms , and select the most likely candidates ( or all those classified as hypernyms ) .",0
9701,This decision function can be learned in a supervised fashion using examples of pairs of words thatare related by hypernymy and pairs thatare not .,0
9702,"The supervised model can take as input a combination of the embeddings of q and h , and different ways of combining the embeddings for this purpose have been used .",0
9703,"In related work , models have been proposed that learn to project the embedding of q such that its projection is close to that of its hypernym h.",0
9704,This has been termed projection learning .,0
9705,The decision function is then based on how close the projection of q is to a given candidate h. introduced a model that learns multiple projection matrices representing different kinds of hypernymy relationships .,0
9706,"In this model , each ( q , h) pair is first assigned to a cluster based on their vector offsets , then projection matrices are learned for each cluster .",0
9707,"Based on this work , Yamane et al .",0
9708,proposed a model that jointly learns the clusters and projection matrices .,0
9709,"We use a similar method to learn projections for hypernym discovery , but our approach differs from that of in several ways : our model performs a soft clustering of query - hypernym pairs rather than a hard clustering , and we modified the training algorithm in several ways .",0
9710,The Model,0
9711,"Given a query q and a candidate hypernym h , the model retrieves their embeddings e q , eh ?",0
9712,R d 1 using a lookup table .,0
9713,These embeddings were learned beforehand on a large unlabeled text corpus ( i.e. the corpora provided for this task ) .,0
9714,The embedding e q is then multiplied by a 3 - D tensor containing k square projection matrices ?,0
9715,i ?,0
9716,R dd for i ?,0
9717,"{ 1 , . . . , k} , producing a matrix P ?",0
9718,R kd containing the projections of e q :,0
9719,The model then checks how close each of the k projections of e q are toe h by taking the dot product :,0
9720,The column vector s ?,0
9721,"R k1 is then fed to an affine transformation and a sigmoid activation function ( in other words , a logistic regression classifier ) to obtain an estimate of the likelihood that q and hare related by hypernymy :",0
9722,"To discover the hypernyms of a given query , we compute the likelihood y for all candidates and select the top - ranked ones .",0
9723,The Training Algorithm,0
9724,"We train the model using negative sampling : for each positive example of a ( query , hypernym ) pair in the training data , we generate a fixed number m of negative examples by replacing the hypernym with a word randomly drawn from the vocabulary .",0
9725,We then train the model to output a likelihood ( y ) close to 1 for positive examples and close to 0 for negative examples .,0
9726,This is accomplished by minimizing the binary cross - entropy of the positive and negative training examples .,0
9727,"For a particular example , this is computed as follows :",0
9728,"where q is a query , h is a candidate hypernym , t is the target ( 1 for positive examples , 0 for negative ) , and y is the likelihood predicted by the model .",0
9729,"If we sum H for every example in the training set D ( containing both the positive and negative examples ) , we obtain the cost function J = ( q , h , t ) ?",0
9730,D,0
9731,"H ( q , h , t ) .",0
9732,"This function is minimized by gradient descent , using the Adam optimizer 4 ( Kingma and Ba , 2014 ) .",0
9733,A few details of the setup we use for training are worth mentioning :,0
9734,We use a fixed number of projections ( k ) rather than the dynamic clustering algorithm of .,0
9735,"For our official runs , we used k = 24 .",0
9736,The word embeddings are normalized to unitlength before training .,0
9737,"For the initialization of the projection matrices , we add random noise to an identity matrix , which means that at first , the projections of a query are simply k randomly corrupted copies of the query 's embedding .",0
9738,Dropout is applied to the embeddings e q and eh and the query projections P .,0
9739,"For regularization , we also use gradient clipping , as well as early stopping .",0
9740,3 Different ways of selecting the negative examples for this purpose have been proposed .,0
9741,See. We use ?1 = ?2 = 0.9 .,0
9742,"We sample positive examples using a function based on the frequency of the hypernyms in the training data , such that we subsample ( q , h) pairs where h occurs often in the training data .",0
9743,"The probability of sampling ( q , h) is given by :",0
9744,where freq ( h ) returns the frequency of h in the training data .,0
9745,5,0
9746,"The word embeddings are optimized ( or "" fine - tuned "" ) during training .",0
9747,"We use a multi-task learning setup whereby we train two separate logistic regression classifiers , each with their own parameters W and b , and use one for queries thatare named entities , and the other for queries thatare concepts .",0
9748,The rest of the parameters ( i.e. the projection matrices ? ) are shared .,0
9749,The various hyperparameters mentioned above were tuned on the trial set ( i.e. development set ) provided for sub - task 1A .,0
9750,The Word Embeddings,0
9751,"We learned term embeddings for all queries and candidates using the pre-tokenized corpora provided for sub - tasks 1 A , 2 A , and 2B .",0
9752,"We preprocessed the corpora by converting all characters to lowercase and replacing multi-word terms found in the vocabulary ( candidates and lowercased queries ) with a single token , starting with trigrams , then bigrams .",0
9753,We then used the skipgram algorithm with negative sampling to learn the embeddings .,0
9754,Data Augmentation,0
9755,"For one of our 2 runs , we experimented with a method to add synthetic examples to the positive examples in the training set provided ( D ) .",0
9756,This was meant to provide additional training data and avoid overfitting the embeddings of the words in the training set .,0
9757,We use 2 heuristics to generate these synthetic examples :,0
9758,1 .,0
9759,"Given a positive example ( q , h ) ? D , add ( q , h) to the positive examples , where q is the nearest neighbour of q , based on the cosine similarity of the embeddings of all the words in the vocabulary .",0
9760,This was motivated by the observation that nearest neighbours were often co-hyponyms .,0
9761,2 .,0
9762,"Given a query q and the set H q containing the hypernyms of q according to the training data , compute the ?",0
9763,"nearest neighbours of each hypernym in H q , and for each neighbour that is shared by at least 2 of the hypernyms in H q , add that neighbour to H q .",0
9764,8,0
9765,"Negative examples are generated for each of the synthetic examples , as with the actual positive examples in the training set .",0
9766,Hybrid Hypernym,0
9767,Discovery,0
9768,Our hybrid approach to hypernym discovery combines supervised projection learning and unsupervised pattern - based hypernym discovery ( see Sections 2 and 3 ) .,0
9769,"To combine the outputs of the 2 systems , we take the top 100 candidates according to each , 9 normalize their scores and sum them , then rerank the candidates according to this new score .",0
9770,"This reranking function favours candidates found by both systems , but also gives a chance to strong candidates found by a single system .",0
9771,Experiments and Results,0
9772,"We submitted 2 runs on 3 of the 5 sub -tasks : 1A ( general ) , 2A ( medical ) , and 2B ( music ) .",0
9773,The system outputs its top 15 predictions in all cases .,0
9774,"The difference between the 2 runs is that for run 1 , we used data augmentation ( see Section 3.4 ) to train the supervised system - the same unsupervised output was used for both runs .",0
9775,"We also submitted one run for cross - evaluation ( training on 1A , but testing on 2A or 2B ) .",0
9776,"First , we added the queries and candidates of 2A or 2 B to those of 1A be-fore training embeddings on the corpus of 1 A .",0
9777,10 These embeddings were used to train the supervised model on the 1A training data .,0
9778,We then combined the predictions of the supervised and unsupervised models on test set 2A / 2B .,0
9779,A summary of our system 's results is shown in .,0
9780,"This table shows the mean average precision ( MAP ) , mean reciprocal rank ( MRR ) and precision at rank 1 ( P@1 ) of our system and those of the 2 strongest baselines which were computed by the task organizers .",0
9781,The first is a supervised baseline 12 and the second is based on the most frequent hypernyms in the training data .,0
9782,"For more details , see ) .",0
9783,Our hybrid system was ranked 1st on all three sub - tasks for which we submitted runs .,1
9784,"As shown in , the scores obtained using this system are much higher than the strongest baselines for this task .",1
9785,"Furthermore , it is likely that we could improve our scores on 2A and 2B , since we only tuned the system on 1A .",1
9786,"If we compare runs 1 and 2 of our hybrid system , we see that data augmentation improved our scores slightly on 1A and 2B , and increased them by several points on 2A .",1
9787,"Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup , so training our system on general - purpose data produced better results on a domain - specific test set than a strong , supervised baseline trained on the domain - specific data .",1
9788,also shows the scores we would have obtained on the test set if we had used only the unsupervised ( pattern - based ) or supervised ( projection learning ) parts of our system .,0
9789,"Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task , and even outperformed the supervised baseline on 2A .",1
9790,"Combining the outputs of the 2 systems improves the best score of either system on all test sets , sometimes by as much as 10 points .",0
9791,"Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only ( compare runs 1 and 2 ) , although our tests on the trial set suggested it would also have a positive effect on our 1A scores .",0
9792,"Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .",1
9793,"One possible explanation is that adding the synthetic examples makes the errors of the supervised system more different from those of the unsupervised system , and that this in turn makes the ensemble method more beneficial , but we have n't looked into this .",0
9794,Ablation Tests,0
9795,"To assess the influence of different aspects of the supervised system and its training algorithm , we carried out a few simple ablation tests on subtask 1 A .",1
9796,The baseline for these tests is our supervised projection learning system - we did not apply pattern - based hypernym discovery for any of these tests .,0
9797,We used the setup of run 1 ( with data augmentation ) and used the trial set for early stopping .,0
9798,"We conducted the following tests ( one by one , without combining any of the ablations ) :",0
9799,1 .,0
9800,No subsampling : we sample positive examples uniformly from the training set .,0
9801,2 .,0
9802,"No MTL : instead of multi - task learning ( MTL ) , we use a single classifier for both named entities and concepts .",0
9803,3 .,0
9804,Random init : the weights of ?,0
9805,"are initialized randomly , instead of adding random noise to an identity matrix .",0
9806,4 .,0
9807,Single projection : k = 1 instead of 24 .,0
9808,5 .,0
9809,Single neg .,0
9810,example : m = 1 instead of 10 .,0
9811,6 .,0
9812,Frozen embeddings : the word embeddings are not fine - tuned during training .,0
9813,The results obtained on test set 1A are shown in .,0
9814,"These results show that 2 of the techniques we used , namely subsampling and multitask learning , actually harmed our system 's performance on test set 1 A , although our experiments on the trial set suggested that they would be beneficial .",1
9815,This maybe due to the small size of the trial set ( i.e. 50 queries ) or some difference in the underlying distributions of the trial and test sets .,0
9816,"On the other hand , fine - tuning the word embeddings during training seems to be one of the keys to the success of this approach , as are the use of multiple projection matrices , and the sampling of multiple negative examples for each positive example .",1
9817,The way we initialize ?,0
9818,also seems to have helped quite a bit .,0
9819,MAP MRR P@1,0
9820,It is important to remember that a more thorough exploration of hyperparameter space would produce results very different from those of simple ablation tests such as these .,0
9821,"It is worth noting that our supervised model outperforms the supervised baseline provided for this task ( see ) even when it exploits a single projection matrix , however the difference in scores between these 2 systems is only 2 or 3 points , depending on the evaluation metric .",0
9822,"We should also note that the supervised model is prone to overfitting , and we found early stopping to be particularly important .",1
9823,Qualitative Analysis,0
9824,"We manually inspected the results of run 1 on some of the 1A test queries to get a better idea of the ability of the model to discover hypernyms , and to identify potential sources of errors .",0
9825,shows a few of these test cases .,0
9826,"Below we will outline a few of our observations , and will refer repeatedly to examples in The model is indeed able to discover valid hypernyms for both concepts and named entities .",0
9827,"It seems that it can even handle very low - frequency queries in some cases ( Suzy Favor Hamilton occurs only 5 times in the corpus ) , but we have not had the chance to investigate how sensitive the model is to term frequency .",0
9828,Lexical memorization can sometimes be observed .,0
9829,"For example , person is the most frequent hypernym in the 1A training data , and the model often predicts this candidate incorrectly , even when it s other top predictions are completely unrelated ( e.g. tenpence ) .",0
9830,"The model can discover hypernyms of different senses of the same query ( e.g. aquamarine , for which the top 15 predictions contain the valid hypernyms spectral color and primary color ) , and it sometimes discovers hypernyms for senses thatare not represented in the gold standard ( e.g. there is a college named Swarthmore , and hypostasis has senses related to linguistics and philosophy ) .",0
9831,It is likely that the senses that dominate the model 's top predictions for a given query are it s most frequent senses in the corpus .,0
9832,"The case of vegetarian suggests that syntactic ambiguity is a source of errors : the predicted hypernyms include some that might be considered valid for the query vegetarian food , where vegetarian is an adjective , but not for the noun vegetarian .",0
9833,"Lastly , the model sometimes confuses concepts and named entities ( e.g. Local Group , which refers to a group of galaxies ) .",0
9834,Preserving the true case of the characters in the corpus would mitigate this issue .,0
9835,Concluding Remarks,0
9836,Our approach to hypernym discovery combines a novel supervised projection learning algorithm and an unsupervised pattern - based algorithm which exploits co-hyponyms in its search for hypernyms .,0
9837,"This hybrid approach produced very good results on the hypernym discovery task , and was ranked first on all 3 sub - tasks for which we submitted results .",0
9838,title,0
9839,Supervised Distributional Hypernym Discovery via Domain Adaptation,1
9840,abstract,0
9841,Lexical taxonomies are graph - like hierarchical structures that provide a formal representation of knowledge .,0
9842,Most knowledge graphs to date rely on is -a ( hypernymic ) relations as the backbone of their semantic structure .,0
9843,"In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .",0
9844,"By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .",0
9845,"We evaluate several configurations of our approach , training with information derived from a manually created knowledge base , along with hypernymic relations obtained from Open Information Extraction systems .",0
9846,The integration of both sources of knowledge yields the best over all results according to both automatic and manual evaluation on ten different domains .,0
9847,Introduction,0
9848,"Lexical taxonomies ( taxonomies henceforth ) are graph - like hierarchical structures where terms are nodes , and are typically organized over a predefined merging or splitting criterion .",0
9849,"By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .",0
9850,"In fact , taxonomies have enabled a remarkable number of novel NLP techniques , e.g. the contribution of WordNet to lexical semantics as well as various tasks , from word sense dis ambiguation to information retrieval , question answering and textual entailment .",0
9851,"To date , the application of taxonomies in NLP has consisted mainly of , on one hand , formally representing a domain of knowledge ( e.g. Food ) , and , on the other hand , constituting the semantic backbone of large - scale knowledge repositories such as ontologies or Knowledge Bases ( KBs ) .",0
9852,"In domain knowledge formalization , prominent work has made use of the web ( Kozareva and Hovy , 2010 ) , lexico - syntactic patterns , syntactic evidence , graph - based algorithms or popularity of web sources .",0
9853,"As for enabling large - scale knowledge repositories , this task often tackles the additional problem of dis ambiguating word senses and entity mentions .",0
9854,"Notable approaches of this kind include Yago , WikiTaxonomy , and the Wikipedia Bitaxonomy .",0
9855,"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .",1
9856,"Taxonomy learning is roughly based on a twostep process , namely is -a ( hypernymic ) relation de-tection , and graph induction .",0
9857,The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics .,0
9858,"It has been addressed by means of pattern - based methods 1 , clustering ) and graph - based approaches .",0
9859,"Moreover , work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings .",0
9860,"In this area , supervised approaches , arguably the most popular nowadays , learn a feature vector between term - hypernym vector pairs and train classifiers to predict hypernymic relations .",0
9861,"These pairs maybe represented either as a concatenation of both vectors , difference , dot -product , or including additional linguistic information for LSTMbased learning .",0
9862,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .",1
9863,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",1
9864,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,1
9865,( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,1
9866,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .",1
9867,"In addition to pattern - based , other terms like path - based or rule - based are also used .",0
9868,2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .,1
9869,"Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .",0
9870,"Evaluation ( both manual and automatic ) shows that we can effectively replicate the Wikidata is - a branch , and capture previously unseen relations in other reference taxonomies ( YAGO or WIBI ) .",0
9871,Related Work,0
9872,Pattern - based methods for hypernym identification exploit the joint co-ocurrence of term and hypernym in text corpora .,0
9873,"Building upon Hearst 's patterns , these approaches have focused on , for instance , exploiting templates for harvesting candidate instances which are ranked via mutual information , training a classifier with WordNet hypernymic relations combined with syntactic dependencies , or applying a doubly - anchored method , which queries the web with two semantically related terms for collecting domainspecific corpora .",0
9874,"Syntactic information is also used for supervised definition and hypernym extraction , or together with Wikipedia - specific heuristics .",0
9875,"One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window , which strongly hinders their recall .",0
9876,"Higher recall can be achieved thanks to distributional methods , as they do not have co-occurrence requirements .",0
9877,"In addition , they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy , but also cause - effect or entity - origin ) .",0
9878,"However , they are often more imprecise and seem to perform best in discovering broader semantic relations .",0
9879,"One way to surmount the issue of generality was proposed by , who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space .",0
9880,"As shown empirically in Fu et al. 's original work , the hypernymic relation that holds for the pair ( dragonfly , insect ) differs from the one of e.g. ( carpenter , man ) .",0
9881,"Prior to training , their system addresses this discrepancy via k-means clustering using a held - out development set for tuning .",0
9882,The previously described methods for hypernym and taxonomy learning operate inherently at the surface level .,0
9883,"This is partly due to the way evaluation is conducted , which is often limited to very specific domains with no integrative potential ( e.g. taxonomies in food , science or equipment from ) , or restricted to lists of word pairs .",0
9884,"Hence , a drawback of surface - level taxonomy learning , apart from ambiguity issues , is that they require additional and error - prone steps to identify semantic clusters .",0
9885,"Alternatively , recent advances in OIE based on dis ambiguation and deeper semantic analysis have shown their potential to construct taxonomized dis ambiguated resources both at node and at relation level .",0
9886,"However , in addition to their inherently broader scope , OIE approaches are designed to achieve high coverage , and hence they tend to produce noisier data compared to taxonomy learning systems .",0
9887,"In our sense - based approach , instead , not only do we leverage an unambiguous vector representation for hypernym discovery , but we also take advantage of a domain - wise clustering strategy to directly obtain specific term - hypernym training pairs , thereby substantially refining this step .",0
9888,"Additionally , we exploit the complementary knowledge of OIE systems by incorporating high - confidence relation triples drawn from OIE - derived resources , yielding the best average configuration as evaluated on ten different domains of knowledge .",0
9889,Preliminaries,0
9890,"TAXOEMBED leverages the vast amounts of training data available from structured and unstructured knowledge resources , along with the mapping among these resources and a state - of - the - art vector representation of word senses .",0
9891,"BabelNet 3 ( Navigli and Ponzetto , 2012 ) constitutes our sense inventory , as it is currently the largest single multilingual repository of named en - 3",0
9892,"http://babelnet.org tities and concepts , integrating various resources such as WordNet , Wikipedia or Wikidata .",0
9893,"As in WordNet , BabelNet is structured in synsets .",0
9894,Each synset is composed of a set of words ( lexicalizations or senses ) representing the same meaning .,0
9895,"For instance , the synset referring to the members of a business organization is represented by the set of senses firm , house , business firm .",0
9896,BabelNet contains around 14M synsets in total .,0
9897,"We exploit BabelNet 4 as ( 1 ) A repository for the manually - curated hypernymic relations included in Wikidata ; ( 2 ) A semantic pivot of the integration of several OIE systems into one single resource , namely KB - UNIFY ; and ( 3 ) A sense inventory for the SENSEMBED vector representations .",0
9898,In the following we provide further details about each of these resources .,0
9899,Training Data,0
9900,Wikidata 5 ) is a document - oriented semantic data base operated by the Wikimedia Foundation with the goal of providing a common source of data that can be used by other Wikimedia projects .,0
9901,"Our initial training set W consists of the hypernym branch of Wikidata , specifically the version included in BabelNet .",0
9902,Each term - hypernym ?,0
9903,"Wis in fact a pair of BabelNet synsets , e.g. the synset for Apple ( with the company sense ) , and the concept company .",0
9904,"KB - UNIFY 6 ( Delli Bovi et al. , 2015 a ) ( KB - U ) is a knowledge - based approach , based on BabelNet , for integrating the output of different OIE systems into a single unified and dis ambiguated knowledge repository .",0
9905,"The unification algorithm takes as input a set K of OIE - derived resources , each of which is modeled as a set of entity , relation , entity triples , and comprises two subsequent stages : in the first dis ambiguation stage , each KB in K is linked to the sense inventory of Babel Net by dis ambiguating its relation argument pairs ; in the following alignment stage , equivalent relations across different KB in K are merged together .",0
9906,"As a result , KB - U generates a KB of triples where arguments are linked to the corresponding BabelNet synsets , and relations are replaced by relation synsets of semantically similar OIE - derived relation patterns .",0
9907,"The original experimental setup of KB - UNIFY included NELL as one of its input resources : since NELL features its own manually - built taxonomic structure and relation type inventory ( hence it s own is - a relation type ) , we identified the relation synset containing NELL 's is - a 7 and then drew from the unified KB all the corresponding triples , which we denote as K .",0
9908,"These triples constitute , similarly as in the previous case , a set of term - hypernym pairs automatically extracted from OIE - derived resources , with a dis ambiguation confidence of above 0.9 according to the dis ambiguation strategy described in the original paper .",0
9909,"Initially , | W| = 5,301,867 and | K | = 1,358,949 .",0
9910,Sense vectors,0
9911,SENSEMBED 8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm .,0
9912,"Vectors in the SENSEMBED space , denoted as S , are latent continuous representations of word senses based on the Word2Vec architecture , which was applied on a dis ambiguated Wikipedia corpus .",0
9913,Each vector v ?,0
9914,"S represents a BabelNet sense , i.e. a synset along with one of its lexicalizations ( e.g. album_chart_bn:00002488 n ) .",0
9915,"This differs from unsupervised approaches that learn sense representations from text corpora only and are not mapped to any lexical resource , limiting their application in our task .",0
9916,Methodology,0
9917,Our approach can be summarized as follows .,0
9918,"First , we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C ( Section 4.1 ) .",0
9919,"Then , we expand the training set by exploiting the different lexicalizations available for each BabelNet synset ( Section 4.2 ) .",0
9920,"Finally , we learn a cluster - wise linear projection ( a hypernym transformation matrix ) over all pairs ( term-hypernym ) of the expanded training set ( Section 4.3 ) .",0
9921,"induced semantic clusters via kmeans , where k was tuned on a development set .",0
9922,"Instead , we aim at learning a function sensitive to a predefined knowledge domain , under the assumption that vectors clustered with this criterion are likely to exhibit similar semantic properties ( e.g. similarity ) .",0
9923,"First , we allocate each synset into its most representative domain , which is achieved by exploiting the set of thirty four domains available in the Wikipedia featured articles page 9 .",0
9924,"Warfare , transport , or music are some of these domains .",0
9925,In the Wikipedia featured articles page each domain is composed of 128 Wikipedia pages on average .,0
9926,"Then , in order to expand the set of concepts associated with each domain , we leverage NASARI 10 ( Camacho - Collados et al. , 2015 ) , a distributional approach that has been used to construct explicit vector representations of BabelNet synsets .",0
9927,Domain Clustering,0
9928,Our goal is to associate BabelNet synsets with domains .,0
9929,"To this end , we follow Camacho - Collados et al. and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text .",0
9930,"Finally , given a BabelNet synset b , we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors , selecting the domain leading to the highest similarity score :",0
9931,where,0
9932,"Dis the set of all thirty - three domains , d is the vector of the domain d ?",0
9933,"D , b is the vector of the BabelNet synset b , and WO refers to the Weighted Overlap comparison measure , which is defined as follows :",0
9934,"where rank w , vi is the rank of the word win the vector vi according to its weight , and O is the set of overlapping words between the two vectors .",0
9935,"In order to have a highly reliable set of domain labels , those synsets whose maximum similarity score is below a certain threshold are not annotated with any domain .",0
9936,"We fixed the threshold to 0.35 , which provided a fine balance between precision ( estimated in around 85 % ) and recall in our development set .",0
9937,By following this approach almost 2 million synsets are labelled with a domain .,0
9938,Training Data Expansion,0
9939,"Prior to training our model , we benefit from the fact that a given BabelNet synset maybe associated with a fixed number of lexicalizations or senses , i.e. different ways of referring to the same concept , to expand our set of training pairs .",0
9940,"For instance , the synset b associated with the concept mu-sic_album is represented by the set of lexicalizations L b = { album , music_album . . . album_project }.",0
9941,We take advantage of this synset representation to expand each term - hypernym synset pair .,0
9942,"For each term - hypernym pair , both concepts are expanded to their given lexicalizations and thus , each synset pair term - hypernym in the training data is expanded to a set of | L t |.| L h | sense training pairs .",0
9943,"This expansion step results in much larger sets W * and K * , where | W * | = 18,291,330 and | K * | = 15,362,268 .",0
9944,"Specifically , they are 3 and 11 times bigger than the original training sets described in Section 3.1 .",0
9945,"These numbers are higher than those reported in recent approaches for hypernym detection , which exploited Chinese semantic thesauri along with manual validation of hypernym pairs ( obtaining a total of 1,391 instances ) , or pairs from knowledge resources such as Wikidata , Yago , WordNet and DBpedia , where the maximum reported split for training data ( 70 % ) amounted to 49,475 pairs .",0
9946,Learning a Hypernym Detection Matrix,0
9947,"The gist of our approach lies on the property of current semantic vector space models to capture relations between vectors , in our case hypernymy .",0
9948,"This can be found even in disjoint spaces , where this property has been exploited for machine translation or language normalization .",0
9949,"For our purposes , however , instead of learning a global linear transformation function in two spaces over a broad relation like hypernymy , we learn a function sensitive to a given do - main of knowledge .",0
9950,"Thus , our training data becomes restricted to those term - hypernym BabelNet sense pairs",0
9951,Under the intuition that there exists a matrix ?,0
9952,"so that yd = ? x d , we learn a transformation matrix for each domain cluster",0
9953,Cd by minimizing :,0
9954,"Then , for any unseen term x d , we obtain a ranked list of the most likely hypernyms of its lexicalization vectors x j d , using as measure cosine similarity :",0
9955,"At this point , we have associated with each sense vector a ranked list of candidate hypernym vectors .",0
9956,"However , in the ( frequent ) cases in which one synset has more than one lexicalization , we need to condense the results into one single list of candidates , which we achieve with a simple ranking function ? ( ) , which we compute as ?( v ) = cos ( v , ? C x d ) rank ( v ) , where rank ( v ) is the rank of v according to its cosine similarity with ?",0
9957,C x d .,0
9958,The above operations allow us to cast the hypernym detection task as a ranking problem .,0
9959,"This is also particularly interesting to enable a flexible evaluation framework where we can combine highly demanding metrics for the quality of the candidate given at a certain rank , as well as other measures which consider the rank of the first valid retrieved candidate .",0
9960,Evaluation,0
9961,"The performance of TAXOEMBED is evaluated by conducting several experiments , both automatic and manual .",0
9962,"Specifically , we assess its ability to return valid hypernyms for a given unseen term with a held - out evaluation dataset of 250 Wikidata termhypernym pairs ( Section 5.1 ) .",0
9963,"In addition , we assess the extent to which TAXOEMBED is able to correctly identify hypernyms outside of Wikidata ( Section 5.2 ) .",0
9964,Experiment 1 : Automatic Evaluation,0
9965,Experimental setting,0
9966,"For this experiment we use two configurations of TAXOEMBED : the first one includes 25 k domainwise expanded training pairs ( TaxE 25 k ) , whereas the second one adds 1 k pairs from KB - U ( TaxE 25 k + K d ) .",0
9967,We randomly extract 200 test BabelNet synsets ( 20 per domain ) whose hypernyms are missing in Wikidata .,0
9968,"We compare against a number of taxonomy learning and Information Extraction systems , namely , WiBi and DefIE .",1
9969,Yago and WiBi are used as upper bounds due to the nature of their hypernymic relations .,0
9970,They include a great number of manually - encoded taxonomies ( e.g. exploiting WordNet and Wikipedia categories ) .,0
9971,Yago derives its taxonomic relations from an automatic mapping between WordNet and Wikipedia categories .,0
9972,"WiBi , on the other hand , exploits , among a number of different Wikipedia - specific heuristics , categories and the syntactic structure of the introductory sentence of Wikipedia pages .",0
9973,"Finally , DefIE is an automaic OIE system relying on the syntactic structure of pre-dis ambiguated definitions 13 .",1
9974,Three annotators manually evaluated the validity of the hypernyms extracted by each system ( one per test instance ) .,0
9975,shows the results of TAXOEMBED and all comparison systems .,1
9976,"As expected , Yago and WiBi achieve the best over all results .",1
9977,"However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .",1
9978,"However , our model does not perform particularly well on media and physics .",1
9979,In most domains our model is able to discover novel hypernym relations thatare not captured by any other system ( e.g. therapy for radiation treatment planning in the health domain or decoration for molding in the art domain ) .,0
9980,Results and discussion,0
9981,"In fact , the overlap between our approach and the remaining systems is actually quite small ( on average less than 25 % with all of them on the Extra - Coverage experiment ) .",0
9982,"This is mainly due to the fact that TAXOEMBED only exploits distributional information and does not make use of predefined syntactic heuristics , suggesting that the information it provides and the rule - based comparison systems maybe complementary .",0
9983,We foresee a potential avenue focused on combining a supervised distributional approach such as TAXOEMBED with syntacticallymotivated systems such as Wibi or Yago .,0
9984,This combination of a distributional system and manual patterns was already introduced by Shwartz et al .,0
9985,( 2016 ) on the hypernym detection task with highly encouraging results .,0
9986,Experiment 2 : Extra-Coverage,0
9987,In this experiment we evaluate the performance of TAXOEMBED on instances not included in Wikidata .,0
9988,We describe the experimental setting in Section 5.2.1 and present the results in Section 5.2.2 .,0
9989,Conclusion,0
9990,"We have presented TAXOEMBED , a supervised taxonomy learning framework exploiting the property that was observed in , namely that there exists , for a given domain - specific terminology , a shared linear projection among termhypernym pairs .",0
9991,"We showed how this can be used to learn a hypernym transformation matrix for discovering novel is - a relations , which are the backbone of lexical taxonomies .",0
9992,"First , we allocate almost 2M BabelNet synsets into a predefined domain of knowledge .",0
9993,"Then , we collect training data both from a manually constructed knowledge base ( Wiki - : Precision , recall and F - Measure between TAXOEMBED , two taxonomy learning systems ( Yago and WiBi ) , and a pattern - based approach that performs hypernym extraction ( DefIE ) .",0
9994,"data ) , and from OIE systems .",0
9995,"We substantially expand our initial training set by expanding both terms and hypernyms to all their available senses , and in a last step , to their corresponding dis ambiguated vector representations .",0
9996,Evaluation shows that the general trend is that our hypernym matrix improves as we increase training data .,0
9997,"Our best domain - wise configuration combines 25 k training pairs from Wikidata and additional pairs from an OIE - derived KB , achieving promising results .",0
9998,"The domains in which the addition of the OIE - based information contributed the most are education , transport and media .",0
9999,"For instance , in the case of education , this maybe due to the over representation of the North American educational system in IE systems like NELL .",0
10000,"We accompany this quantitative evaluation with manual assessment of precision of false positives , and an analysis of the potential coverage comparing it with knowledge taxonomies like Yago or WiBi , and with DefIE , a quasi - OIE system .",0
10001,Future Work,0
10002,For future work we are planning to apply this strategy to learn large - scale semantic relations beyond hypernymy .,0
10003,This may constitute a first step towards a global and fully automatic ontology learning system .,0
10004,"In the context of semantic web , we would like to include semantic parsers and distant supervision to our algorithm in order to capture n-ary relations between pairs of concepts to further create and improve existing KBs .",0
10005,"As mentioned in Section 5.2.2 , we are also planning to combine our distributional approach with rule - based heuristics , following the line of work introduced by .",0
10006,"Finally , we see potential in the domain clustering approach for improving graph - based taxonomy learning systems , as it can serve as a weighting measure as to how pertinent a given set of concepts in a taxonomy are for a specific domain .",0
10007,title,0
10008,abstract,0
10009,Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus .,1
10010,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .",0
10011,"Given a test hyponym word , we get its hypernym lists by computing the similarities between the hyponym word and words in the training data , and fill the test word 's hypernym lists with the hypernym list in the training set of the nearest similarity distance to the test word .",0
10012,"In SemEval 2018 task9 , our results , achieve 1st on Spanish , 2nd on Italian , 6th on English in the metric of MAP .",0
10013,Introduction,0
10014,"Hypernymy relationship plays a critical role in language understanding because it enables generalization , which lies at the core of human cognition ) .",0
10015,"It has been widely used in various NLP applications ) , from word sense dis ambiguation ) to information retrieval ) , question answering ) and textual entailment ) .",0
10016,"To date , the hypernymy relation also plays an important role in Knowledge Base Construction task .",0
10017,"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not .",1
10018,This has led to criticisms regarding its oversimplification .,0
10019,"In the SemEval 2018 Task 9 , the task has shifted to "" Hypernym Discovery "" , i.e. , given the search space of a domain 's vocabulary and an input hyponym , discover its best ( set of ) candidate hypernyms .",0
10020,"In this paper , the content is organized as follows :",0
10021,"Section 2 gives an introduction to the related work ; Section 3 describes our methods for this task , including word embedding projection learning as the baseline and the nearest - neighbourbased method as the submission result ; The experimental results are presented in Section 4 .",0
10022,We conclude the paper with Section 5 .,0
10023,Related Work,0
10024,The work of identifying hypernymy relationship can be categorized from different aspects according to the learning methods and the task formulization .,0
10025,"The earlier work ) formalized the task as an unsupervised hypernym discovery task , i.e. , none hyponym - hypernyms pairs ( x , y) are given as the training data .",0
10026,handcrafted a set of lexico - syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus .,0
10027,trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym - hypernym pairs .,0
10028,Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus .,0
10029,"Variations of were later used in tasks such as taxonomy construction ; ; ) , analogy identification ( Turney ( 2006 ) ) , and definition extraction ; Navigli and Velardi ( 2010 ) ) .",0
10030,A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs .,0
10031,Distributional methods are developed to overcome this limitation .,0
10032,developed symmetric similarity measures to detect hypernym in an unsupervised manner. ; employed directional measures based on the distributional inclusion hypothesis .,0
10033,"More recent work ; ) introduces new measures , based on the distributional informativeness hypothesis. ; learn directly the word embeddings which are optimized for capturing the hypernymy relationship .",0
10034,The supervised methods include ; .,0
10035,"These methods were originally wordcount - based , but can be easily adapted using word embeddings ; ) .",0
10036,"However , it was criticized that the supervised methods only learn prototypical hypernymy ) .",0
10037,3 Hyponym - hypernym Discovery method,0
10038,Preprocessing,0
10039,"For the corpus and the train / gold / test data , we have two preprocessing steps :",0
10040,1 ) Lowercase all the words ;,0
10041,"2 ) Concatenate the phrases ( hyponym or hypernym composed with more than one word ) which occur in the training set or the test set with underline , i.e. , "" executive president "" is replaced by "" executive president "" .",0
10042,It is quite useful for training word embedding models because we want to treat phrases as single words .,0
10043,"If there are multiple phrases in one sentence , we generate multiple sentences , one per phrase .",0
10044,"For example , "" executive president "" and "" vice executive president "" both exist in the corpus sentence "" Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry . "" .",0
10045,"After preprocessing , two more sentences are generated and included in the training corpus for word embeddings :",0
10046,"Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry .",0
10047,"Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry .",0
10048,"The size of the original corpus has increased after the preprocessing step , e.g. , The English corpus has increased from ? 18G to ?32G .",0
10049,Word Embedding,0
10050,We train our word embedding models using the Google word2vec ) tool 3 on the preprocessed corpus .,0
10051,We employ the skipgram model since the skip - gram model is shown to perform best in identifying semantic relations among words .,0
10052,The trained word embeddings are used in the projection learning and nearestneighbour based method .,0
10053,Method based on Projection Learning,0
10054,The intuition of this method is to assume that there is a linear transformation in the embedding space which maps hyponyms to their correspondent hypernyms .,0
10055,"We first learn a projection matrix from the training data , then apply the matrix to the test data .",0
10056,"Our method is similar to that described in , the main idea can be summarized as follows :",0
10057,1 .,0
10058,"Give a word x and its hypernym y , assuming there exists a linear projection matrix ?",0
10059,to meet y = ?x.,0
10060,We need to learn a approximate ?,0
10061,using the following equation to minimize the MSE loss :,0
10062,2 .,0
10063,Learn the piecewise linear projection by clustering the training data into different groups according to the vector offsets .,0
10064,"The motivation for the clustering is two - fold : firstly , the hypernym - hyponym relation is diverse , e.g. , offset from "" carpenter "" and "" laborer "" is distant from the one from "" gold fish "" to "" fish "" ; Secondly , if a hyponym x has many hypernyms ( or hierarchical hypernyms ) , we ca n't use a single transition matrix ?",0
10065,to project x to different hyperny m y .,0
10066,So a piecewise projection learning is needed in each individual group .,0
10067,"Thus , the optimization goal can be formalized as follows :",0
10068,Where N k is the number of word pairs in the k th cluster C k .,0
10069,3 .,0
10070,Learn the threshold ?,0
10071,"k for each cluster , by assumming that positive ( hyponmy - hypernmy ) pairs can locate in radius ?",0
10072,while negative pairs can not :,0
10073,Where d stands for the euclidean distance .,0
10074,4 .,0
10075,"Once the piecewise projection and the threshold is learned , given a new hyponym x , all of the hypernym candidates ys from the vocabulary are paired with x .",0
10076,The pairs are assigned to the proper cluster by the vector offset ( y - x ) .,0
10077,According to the threshold ?,0
10078,"in that group , it can be decided whether ( x , y) is a reasonable hyponym - hypernym pair .",0
10079,Method Based on Nearest Neighbors,0
10080,We noticed that the hypernyms are often very distant from the correspondent hyponyms in the embedding space .,0
10081,"Meanwhile , hyponyms which are close to each other often share the same hypernyms .",0
10082,We propose a simple yet effective approach based on this observation .,0
10083,"Suppose the training set H consists of a number of hyponyms and their correspondent hypernyms where the distance function measures the similarity between Hypo i and x , HypoN is the list of words from the training set sorted according to their distances to x .",0
10084,Consine similarity in the embedding space is used for the distance function in our setup .,0
10085,"According to the requirements of Task 9 , only the top 15 of Hyper ( x ) are submitted for evaluation .",0
10086,Evaluation,0
10087,Experimental Setup,0
10088,Word2vec is used to produce the word embeddings .,1
10089,The skip - gram model ( - cbow 0 ) is used with the embedding dimension set to 300 ( - size 300 ) .,1
10090,The other options are by default .,0
10091,We use 10 fold cross validation to evaluate both methods on the provided training data .,0
10092,The results are shown in,0
10093,Results Based on Projection Learning,1
10094,"For the projection learning method , we followed experimental settings described in .",0
10095,"The negative ( hyponym , hypernym ) pairs are randomly sampled from the vocabulary .",0
10096,The training set consists of the negative pairs and the positive pairs in 3:1 ratio .,0
10097,"By using the same evaluating metrics as PRF in the cited paper , our best F - value on the validation set is 0.68 ( the paper result is 0.73 ) when the best cluster number is 2 and the threshold is ( 17.7 , 17.3 ) .",1
10098,"We apply the learned projection matrices and thresholds on the validation data , extract out the candidate hypernyms from the given vocabulary and truncate the top 15 candidates by sorting them according to the d ( ?",0
10099,"k x , y) /?",0
10100,k scores .,0
10101,"The generated results are not very promising , see",0
10102,"This projection learning method performs not very well on task9 , we think the most probable reason is that in , the problem is formalized as a classification problem , in which the ( hyponym , hypernym ) pairs are given .",1
10103,"However , our task is formalized as a hypernym discovery problem given only hyponmys .",0
10104,"This task might be inherently much harder than the classification task ; a second reason might be related to the relative small amount of training data , i.e. , ? 7500 training pairs in total .",0
10105,Results Based on NN,1
10106,The results are shown in from row 2 to row 5 .,0
10107,shows the results evaluated on the test data .,0
10108,The performance evaluated using either cross validation or the test data is much worse than that of a typical hypernym prediction task reported by .,1
10109,This illustrates that hypernym discovery is indeed a much harder task than the hypernym prediction task .,0
10110,"Although the method proposed by us is quite simple , our submissions are the 1st on Spanish , the 2nd on Italian , the 6th on English , ranked by the metric of MAP .",1
10111,This proves the effectiveness of the method .,0
10112,"Compared with the results got by cross validation , the performance evaluated on the test data ) dropped significantly on English ( MAP dropped by 4 % ) and Italian ( MAP dropped by 8 % ) , but increased by a margin on Spanish ( MAP increased by 3.6 % ) .",1
10113,"We consider that it is due to the properties of provided data , i.e. , the hypernyms in the test set are similar to those in the training set for Spanish , but dissimilar for English or Italian .",0
10114,The performance drop for English and Italian exposes one of the main drawbacks of our method : the method can not discover the hypernyms that have never occurred in the training set .,0
10115,"To overcome this shortcoming , using syntactic patterns to extract hyponym - hypernym with high confidence can be employed to enlarge the training set .",0
10116,We leave this to the future work .,0
10117,Conclusion,0
10118,In this paper we describe two methods we have tried out for the hypernym discovery task in Se-mEval 2018 .,0
10119,We extended the method originally proposed for hypernym prediction by as a baseline system .,0
10120,However the performance of this method is poor .,0
10121,"The nearestneighbor - based method is relatively simple , yet quite effective .",0
10122,"We analyzed the experimental results , reveal some shortcomings , and propose a potential extension to future improvement .",0
10123,title,0
10124,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,1
10125,abstract,0
10126,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",1
10127,We introduce a neural network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases .,0
10128,"The evaluated models include convolutional neural network , long - short term memory network , gated recurrent unit and recurrent convolutional neural network .",0
10129,"We also explore different embedding methods , including word embedding and sense embedding for better performance .",0
10130,"Recently , neural network ( NN ) models have shown competitive or even better results than traditional linear models with handcrafted sparse fea -",0
10131,Introduction,0
10132,Hypernym - hyponym relationship is an is - a semantic relation between terms as shown in Table,0
10133,1 .,0
10134,"Various natural language processing ( NLP ) tasks , especially those semantically intensive ones aiming for inference and reasoning with generalization capability , such as question answering and textual entailment , can benefit from identifying semantic relations between words beyond synonymy .",0
10135,The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,0
10136,"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not .",1
10137,"A hypernym detection system should be capable of learning taxonomy and lexical semantics , including pattern - based methods and graph - based approaches .",0
10138,"However , our concerned task , hypernym discovery , is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work .",0
10139,"The other challenge is representation for terms , including words and phrases , where the phrase embedding could not be obtained byword embeddings directly .",0
10140,A simple method is to average the inner word embeddings to form the phrase embedding .,0
10141,"However , this is too coarse since each word might share different weights .",0
10142,"Current systems like ( Espinosa - Anke et al. , 2016 a ) commonly discover hypernymic relations by exploiting linear transformation matrix in embedding space , where the embedding should contain words and phrases , resulting to be parameter - exploded and hard to train .",0
10143,"Besides , these systems might be insufficient to obtain the deep relationships between terms .",0
10144,tures .,0
10145,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",1
10146,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .",1
10147,The rest of the paper is organized as follows :,0
10148,"Section 2 briefly describes our system , Section 3 shows our experiments on the hyperym discovery task including the general - purpose and domainspecific one .",0
10149,Section 4 concludes this paper .,0
10150,System Overview,0
10151,"Our hypernym discovery system can be roughly split into two parts , Term Embedding and Hypernym Relationship Learning .",0
10152,"We first train term embeddings , either using word embedding or sense embedding to represent each word .",0
10153,"Then , neural networks are used to discover and rank the hypernym candidates forgiven terms .",0
10154,Embedding,0
10155,"To use deep neural networks , symbolic data needs to be transformed into distributed representations .",0
10156,We use Glove toolkit to train the word embeddings using UMBC corpus .,0
10157,"Moreover , in order to perform word sense induction and dis ambiguation , the word embedding could be transformed to sense embedding , which is induced from exhisting word embeddings via clustering of ego - networks of related words .",0
10158,"Thus , each input word or phrase is embedded into vector sequence , w = {x 1 , x 2 , . . . , x l } where l denotes the sequence length .",0
10159,"If the input term is a word , then l = 1 while for phrases , l means the number of words .",0
10160,Hypernym Learning,0
10161,"Previous work like TAXOEMBED ( Espinosa - Anke et al. , 2016 a ) uses transformation matrix for hypernm relationship learning , which might be not optimal due to the lack of deeper nonlinear fea - ture extraction .",0
10162,"Thus , we empirically survey various neural networks to represent terms in latent space .",0
10163,"After obtaining the representation for input term and all the candidate hypernyms , to give the ranked hypernym list , the cosine similarity between the term and the candidate hypernym is computed by ,",0
10164,where x i and y i denote the two concerned vectors .,0
10165,"Our candidate neural networks include Convolutional Neural Network ( CNN ) , Long - short Term Memory network ( LSTM ) , Gated Recurrent Unit ( GRU ) and Recurrent Convolutional Neural Network ( RCNN ) .",0
10166,GRU,0
10167,The structure of GRU used in this paper are described as follows .,0
10168,where denotes the element - wise multiplication .,0
10169,"rt and z tare the reset and update gates respectively , andh t the hidden states .",0
10170,LSTM LSTM unit is defined as follows .,0
10171,where ?,0
10172,"stands for the sigmoid function , represents element - wise multiplication and",0
10173,"ht are the input gates , forget gates , memory cells , output gates and the current state , respectively .",0
10174,CNN,0
10175,"Convolutional neural networks have also been successfully applied to various NLP tasks , in which the temporal convolution operation and associated filters map local chunks ( windows ) of the input into a feature representation .",0
10176,"Concretely , let n denote the filter width , filter matrices [ W 1 , W 2 , . . . , W k ] with several variable sizes [ l 1 , l 2 , . . . , l k ] are utilized to perform the convolution operations for input embeddings .",0
10177,"For the sake of simplicity , we will explain the procedure for only one embedding sequence .",0
10178,The embedding will be transformed to sequences c j ( j ?,0
10179,"[ 1 , k ] ) :",0
10180,where [ i : i + l j ?,0
10181,1 ] indexes the convolution window .,0
10182,"Additionally , we apply wide convolution operation between embedding layer and filter matrices , because it ensures that all weights in the filters reach the entire sentence , including the words at the margins .",0
10183,A one - max - pooling operation is adopted after convolution and the output vector sis obtained through concatenating all the mappings for those k filters .,0
10184,"In this way , the model can capture the critical features in the sentence with different filters .",0
10185,RCNN,0
10186,"Since some input terms are phrases , whose member words share different weights .",0
10187,"In RCNN , an adaptive gated decay mechanism is used to weight the words in the convolution layer .",0
10188,"Following , we introduce neural gates similar ?",0
10189,to LSTMs to specify when and how to average the observed signals .,0
10190,The resulting architecture integrates recurrent networks with nonconsecutive convolutions :,0
10191,"where c 1 t , c 2 t , , c n tare accumulator vectors that store weighted averages of 1 - gram to n-gram features .",0
10192,"For discriminative training , we use a maxmargin framework for learning ( or fine - tuning ) parameters ?.",0
10193,"Specifically , a scoring function ? ( , ; ? ) is defined to measure the semantic similarity between the corresponding representations of input term and hypernym .",0
10194,"Let p = {p 1 , ...p n } denote the hypernym corpus and h ?",0
10195,"p is the ground - truth hypernym to the term ti , the optimal parameters ?",0
10196,are learned by minimizing the maxmargin loss :,0
10197,"where ?(. , . ) denotes a non-negative margin and ?( p i , a ) is a small constant when a = pi and 0 otherwise .",0
10198,Experiment,0
10199,"In the following experiments , besides the neural networks , we also simply average the embeddings of an input phrase as our baseline to discover the relationship of terms and their corresponding hypernyms for comparison ( denoted as term embedding averaging , TEA ) .",0
10200,Setting,0
10201,Our hypernym discovery experiments include general - purpose substask for English and domainspecific ones for medical and music .,0
10202,Our evaluation is based on the following information retrieval metrics :,0
10203,"Mean Average Precision ( MAP ) , Mean Reciprocal Rank ( MRR ) , Precision at 1 ( P@1 ) , Precision at 3 ( P@3 ) , Precision at 5 ( P@5 ) , Precision at 15 ( P@15 ) .",0
10204,"For the sake of computational efficiency , we simply average the sense embedding if a word has more than one sense embedding ( among various domains ) .",0
10205,Our model was implemented using the Theano 1 .,1
10206,The diagonal variant of Ada - Grad is used for neural network training .,1
10207,We tune the hyper - parameters with the following range of values : learning rate ?,1
10208,"{ 1 e ? 3 , 1 e ? 2 } , dropout probability ? { 0.1 , 0.2 } , CNN filter width ? { 2 , 3 , 4 }.",1
10209,The hidden dimension of all neural models are 200 .,1
10210,The batch size is set to 20 and the word embedding and sense embedding sizes are set to 300 .,1
10211,"All of our models are trained on a single GPU ( NVIDIA GTX 980 Ti ) , with roughly 1.5h for general - purpose subtask for English and 0.5h domain - specific domain - specific ones for medical and music .",1
10212,We run all our models up to 50 epoch and select the best result in validation . :,0
10213,Gold standard evaluation on domain - specific subtask .,0
10214,""" Embed "" is short for "" Embedding "" .",0
10215,Result and analysis,0
10216,all the metrics .,0
10217,This result indicates simply averaging the embedding of words in a phrase is not an appropriate solution to represent a phrase .,0
10218,"Convolution or recurrent gated mechanisms in either CNN - based ( CNN , RCNN ) or RNN ( GRU , LSTM ) based neural networks could essentially be helpful of modeling the semantic connections between words in a phrase , and guide the networks to discover the hypernym relationships .",1
10219,"We also observe CNN - based network performance is better than RNN - based , which indicates local features between words could be more important than long - term dependency in this task where the term length is up to trigrams .",1
10220,"To investigate the performance of neural models on specific domains , we conduct experiments on medical and medicine subtask .",1
10221,shows the result .,0
10222,"All the neural models outperform term embedding averaging in terms of all the metrics and CNN - based network also performs better than RNN - based ones in most of the metrics using word embedding , which verifies our hypothesis in the general - purpose task .",1
10223,"Compared with word embedding , the sense embedding shows a much poorer result though they work closely in generalpurpose subtask .",1
10224,"The reason might be the simple averaging of sense embedding of various domains for a word , which may introduce too much noise and bias the over all sense representation .",0
10225,This also discloses that modeling the sense embedding of specific domains could be quite important for further improvement .,0
10226,Conclusion,0
10227,"In this paper , we introduce a neural network architecture for the hypernym discovery task and empirically study various neural network models to model the representations in latent space for words and phrases .",0
10228,Experiments on three subtasks show the neural models can yield satisfying results .,0
10229,"We also evaluate the performance of word embedding and sense embedding , showing that in domainspecific tasks , sense embedding could be much more volatile .",0
10230,References,0
10231,title,0
10232,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,1
10233,abstract,0
10234,"In this paper , we present our proposed system ( EXPR ) to participate in the hypernym discovery task of SemEval 2018 .",0
10235,The task addresses the challenge of discovering hypernym relations from a text corpus .,0
10236,Our proposal is a combined approach of path - based technique and distributional technique .,0
10237,We use dependency parser on a corpus to extract candidate hypernyms and represent their dependency paths as a feature vector .,0
10238,The feature vector is concatenated with a feature vector obtained using Wikipedia pre-trained term embedding model .,0
10239,The concatenated feature vector fits a supervised machine learning method to learn a classifier model .,0
10240,This model is able to classify new candidate hypernyms as hypernym or not .,0
10241,Our system performs well to discover new hypernyms not defined in gold hypernyms .,0
10242,Introduction,0
10243,"Hypernymy is an important lexical - semantic relation that is useful for many applications such as question answering , machine translation , information retrieval , and soon .",0
10244,"In addition , hypernym relations are the backbone for building ontologies .",0
10245,Various methods have been proposed to detect hypernym relation from text corpora .,0
10246,Most of these techniques are either path - based techniques or distributional techniques .,0
10247,"In path - based methods , the detection of hypernym relations is based on the lexico - syntactic paths connecting a pair of terms in a corpus .",0
10248,"Conversely , distributional methods are based on the distribution of term pair contexts .",0
10249,Most of these methods were unsupervised .,0
10250,"Recently , focus shifted towards supervised methods .",0
10251,This task inherits complexity and is far from being solved .,0
10252,The SemEval organizers address the same task but with a novel formulation .,0
10253,They reformulate the task from hypernym detection into hypernym discovery .,0
10254,"This novel formulation makes the task more realistic in terms of actual downstream application , while also enabling the benefits of information retrieval evaluation metrics .",0
10255,Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not .,0
10256,Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space .,0
10257,The task is divided into two subtasks : General - Purpose Hypernym Discovery and Domain - Specific Hypernym Discovery .,0
10258,"The first consists of discovering hypernym in a general - purpose corpus , thus the SemEval organizers provide the participants with data for three languages : English , Italian , and Spanish .",0
10259,"The second consists of discovering hypernym in a domain - specific corpus , thus they provide the participants with data for two specific domains : Medical and Music .",0
10260,"The data contains a list of training terms along with gold hypernyms , a list of testing terms , and a vocabulary search space .",0
10261,The term is either a concept or an entity .,0
10262,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",1
10263,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .",1
10264,The model predicts if a term and its candidate hypernym are hypernym related or not .,1
10265,Related Work,0
10266,Most of the previous approaches for hypernymy detection are either path - based ( patterns ) or distributional based .,0
10267,"Recently , some approaches are taking advantages of the combination of pathbased and distributional techniques .",0
10268,Path - Based,0
10269,Path - based approaches are heuristic methods that predict hypernymy between a pair of terms if they match a particular pattern in a sentence of the corpus .,0
10270,These patterns are either manually identified or automatically extracted .,0
10271,"Approaches based on handcrafted patterns yield a good precision , but their recall is very low .",0
10272,"Approaches based on automatic learning of patterns achieve better performance by a small improvement in terms of precision and a considerable improvement in terms of recall , but the main limitation of these approaches is the sparsity of the feature space .",0
10273,Distributional,0
10274,"Distributional approaches predict hypernym relations between terms based on their distributional representation , by either unsupervised or supervised models .",0
10275,The early unsupervised distributional models are based on symmetric measures .,0
10276,"Later , asymmetric measures are introduced based on the Distributional Inclusion Hypothesis ( DIH ) .",0
10277,"More recent , ; introduce new measures based on assumption that DIH is not correct for all cases .",0
10278,"While , most of the supervised models rely on term embedding to represent the feature vector between the terms x and y .",0
10279,Various vector representations have been used such as concatenation x ?,0
10280,y and difference y ? x .,0
10281,"More recent , ; suggested that models rely on term embedding are useful to indicate similarity between words , not to indicate hypernymy relations .",0
10282,"Consequently , they learn their own term embedding models thatare more relevant to indicate hypernym relations .",0
10283,Combined Approaches,0
10284,Combined approaches of distributional and lexicosyntactic paths are proposed based on the assumption that distributional approaches and path - based approaches have certain complementary properties .,0
10285,"To our best knowledge , there are little works on integrating them .",0
10286,The recent work on integrat - ing them is proposed by .,0
10287,"They use along short - term memory ( LSTM ) network to encode dependency paths into a feature vector , then they concatenate the feature vector by the term embedding vectors of term x and term y.",0
10288,System Description,0
10289,"As a preliminary step , we split each corpus into a training corpus and a testing corpus .",0
10290,"Training corpus is a corpus of all sentences that contains training data terms ( Concept / Entity ) , while testing corpus is a corpus of all sentences that contains testing data terms ( Concept / Entity ) .",0
10291,Some sentences may contain training and testing data terms .,0
10292,These sentences will exist in both training and testing corpus .,0
10293,Candidate Hypernyms,0
10294,The first step in the system is to extract candidate hypernyms for the given training and testing data terms from a training corpus and a testing corpus respectively .,0
10295,We consider a term as a candidate hypernym if :,0
10296,1 .,0
10297,The term and its candidate occur in the same sentence .,0
10298,2 .,0
10299,The candidate exists in the vocabulary list .,0
10300,3 .,0
10301,The term and its candidate are noun phrases .,0
10302,4 .,0
10303,The term and its candidate are linked by short dependency path .,0
10304,We consider a dependency path as short if it does n't exceed two grammatical dependency relations .,0
10305,"Using the short dependency path , we are capable representing paths similar to Hearst Patterns and other patterns .",0
10306,"For example of short dependency paths , the dependency path between X and Y in the sentence S 1 "" X such as Y "" is { nmod:such as ( X , Y ) } and in the sentence S 2 "" X includes Y "" is {nsubj ( includes , X ) , dobj ( includes , Y ) }.",0
10307,We use Stanford dependency parser 1 to extract dependency paths .,0
10308,Feature Vector,0
10309,The feature vector used to learn a model capable of predicting hypernym relations between a term and a candidate hypernym consists of the concatenation of two vectors : the first one is a vector extracted using a path - based technique while the second is extracted using a distributional technique .,0
10310,The path - based vector consists of a set of features representing the short dependency path between a term y and its candidate hypernym x .,0
10311,"The feature set is : [ T ag ( x ) , GRel ( x ) , HR , F req , T ag ( y ) , GRel ( y ) ] .",0
10312,"T ag ( x ) and T ag ( y ) are the POS tag of x and y , GRel ( x ) and GRel ( y ) are the grammatical dependency relation of x and y , HR is the hypernym ratio of a dependency path and it is equal to the number of occurrences of a dependency path when indicating hypernm relation divided by the total occurrences of the same dependency path , and F req is the relative frequency of a dependency path and it is equal to the occurrence of a dependency path divided by the total occurrences of all dependency paths .",0
10313,HR =,0
10314,hypernym DP occurrences DP occurrences F req = DP occurrences T otal DP s occurrences,0
10315,"For a distributional based vector , We use pretrained 300 dimensional Word2Vec 2 term embeddings , trained on Wikipedia .",0
10316,We apply the difference between the embedding vector of term y and the embedding vector of term x ( y ? x ) .,0
10317,The term is either a single word or a multi-word expression .,0
10318,Model Learning and Hypernym Discovery,0
10319,"In each training corpus , we extract a set of candidate hypernyms for each training term and label them if they are hypernym related or not using the gold hypernym data .",0
10320,"Next , we represent each term and its candidate hypernym by a concatenated feature vector .",0
10321,These concatenated vectors are used for training the model .,0
10322,"The classification method we used is SVM 3 with RBF kernel ( C = 1.0 , gamma = 1 / F eature Size ) .",0
10323,"The training dataset was unbalanced , the ratio of hypernym instances w.r.t. not hypernym is less than 0.05 .",0
10324,"To represent the two categories ( hypernym and not hypernym ) in the training set , we improved this ratio to 0.2 by random elimination of not hypernym instances ( 20 % hypernym instances and 80 % not hypernym instances ) .",0
10325,The classifier model is then used to discover hypernyms from a set of candidate hypernyms extracted from a testing corpus for each testing term by predicting if a term and its candidate hypernym are hypernym related or not .,0
10326,Each predicted hypernym is associated with a probability value .,0
10327,These values are used as ranking values to select the best fifteen hypernyms for each term ( from higher to lower probability ) .,0
10328,Results and Analysis,1
10329,"We submit our systems predictions for three corpora : English , Medical , and Music .",0
10330,"The table 1 ( a , b and c) below shows the result of our system and other supervised systems to discover hypernyms for Concept terms only .",0
10331,"For the three corpora , our system performs better than STJU system , and it performs better than the MFH system on the English corpora .",1
10332,"In addition , the result shows that our system performs well in discovering new hypernyms not defined in the gold hypernyms where it yields good False Positive values in the three corpora and we achieve the best False Positive value in Medical corpus The evaluation results of our system and other supervised systems .",1
10333,Our system result was beneath the expectation .,0
10334,"By a short look into the output result files , we notice a lot of empty lines , meaning that our system was unable to discover any hypernym for a lot of terms and unexpectedly these terms correspond to all entity terms .",0
10335,"In other words , our system lacks the ability to discover hypernyms for entity terms .",0
10336,"The table 2 ( a , b and c) below shows the coverage of Wikipedia pre-trained term embedding model ( TEM ) and the coverage of candidate hypernym extraction ( CHE ) for the training and testing terms of the three corpora .",0
10337,The table shows that our system is unable to discover hypernyms for a considerable number of terms due to two main reasons .,0
10338,"The first reason is that Wikipedia pre-trained term embedding model is limited in coverage , where many terms ( Concepts / Entities ) are not covered by the pre-trained embeddings , which leads to failure to discover hypernyms for these terms .",0
10339,"For example , the term embedding ( TEM ) coverage of Medical Testing terms is 249 ( 50 % ) , which means the system is unable to discover hypernyms for 251 ( 50 % ) terms not covered by the pre-trained term embedding .",0
10340,The second reason is that some conditions used to extract candidate hypernyms restrict the number of candidate hypernyms .,0
10341,"For instance , the condition of the existence of a short dependency link between the term and its candidate causes the system to miss many candidate hypernyms if they are not linked by a short dependency path with the terms .",0
10342,"In addition , the term and its candidate hypernym must occur as noun phrases in the sentence .",0
10343,"This condition leads to failure to extract candidate hypernyms for some entity terms that ca n't be identified as noun phrases in the corpus such as "" Up All Night "" , "" Someday Came Suddenly "" , "" Now What "" , etc .",0
10344,"As shown in the table 2 , the candidate hypernym extraction ( CHE ) coverage for English testing terms is 950 ( 63 % ) , that means our system is unable to extract any candidate hypernym for 550 ( 37 % ) terms ( 398 entities and 152 concepts ) .",1
10345,"Furthermore , our system suffers from a major computational issue when applied to a large corpus .",0
10346,Parsing the corpus took to long and failed to complete before the submission deadline .,0
10347,"Approximately , we processed 50 % sentences of English corpus and 80 % sentences of Music corpus , while we processed all sentences of Medical corpus .",0
10348,This explains why the performance of our system on Medical corpus is better than its performance on the two others corpora .,0
10349,Conclusion,0
10350,"In this paper , we presented our proposed system ( EXPR ) that is a combination of path - based technique and distributional technique to participate in Hypernym Discovery task of SemEval 2018 .",0
10351,"In this work , two feature vectors were extracted and concatenated : the first one is obtained using dependency parser on sentences and the second vector is obtained using pre-trained term embedding .",0
10352,A supervised classifier model based on SVM is built using training dataset composed of concatenated vectors .,0
10353,This model is used to discover hypernyms for new terms .,0
10354,The result was good but did nt fulfill our ambition due to several issues .,0
10355,Our future work is to improve our approach for hypernym discovery by solving several issues .,0
10356,We believe that relying on term embedding model learned from the corpus provided in this task maybe a good choice .,0
10357,"In addition , we will work on the definition of a new dependency links not only those defined in this paper .",0
10358,"Also , we will work to propose an unsupervised approach by using sequential pattern mining technique to automatically extract frequent sequential pattern between hyponym terms and their given hypernyms from the corpus .",0
10359,title,0
10360,An improved neural network model for joint POS tagging and dependency parsing,1
10361,abstract,0
10362,We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing .,1
10363,"Our model extends the well - known BIST graph - based dependency parser ( Kiperwasser and Goldberg , 2016 ) by incorporating a BiLSTM - based tagging component to produce automatically predicted POS tags for the parser .",0
10364,"On the benchmark English Penn treebank , our model obtains strong UAS and LAS scores at 94.51 % and 92.87 % , respectively , producing 1.5 + % absolute improvements to the BIST graph - based parser , and also obtaining a state - of - the - art POS tagging accuracy at 97.97 % .",0
10365,"Furthermore , experimental results on parsing 61 "" big "" Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe ( Straka and Strakov , 2017 ) with 0.8 % higher average POS tagging score and 3.6 % higher average LAS score .",0
10366,"In addition , with our model , we also obtain state - of - the - art downstream task scores for biomedical event extraction and opinion analysis applications .",0
10367,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,1
10368,Introduction,0
10369,"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) .",1
10370,"In general , dependency parsing models can be categorized as graph - based and transition - based .",0
10371,"Most traditional graph - or transition - based models define a set of core and combined features , while recent stateof - the - art models propose neural network architectures to handle feature - engineering .",0
10372,Most traditional and neural network - based parsing models use automatically predicted POS tags as essential features .,0
10373,"However , POS taggers are not perfect , resulting in error propagation problems .",0
10374,"Some work has attempted to avoid using POS tags for dependency parsing , however , to achieve the strongest parsing scores these methods still require automatically assigned POS tags .",0
10375,"Alternatively , joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling .",0
10376,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .",1
10377,Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,1
10378,"In particular , this tagging component generates predicted POS tags for the parser component .",0
10379,duces a 1.5 + % absolute improvement over the BIST graph - based parser with a strong UAS score of 94.51 % and LAS score of 92.87 % ; and also obtaining a state - of - the - art POS tagging accuracy of 97.97 % .,0
10380,"In addition , multilingual parsing experiments from raw texts on 61 "" big "" Universal Dependencies treebanks show that our model outperforms the baseline UDPipe with 0.8 % higher average POS tagging score , 3.1 % higher UAS and 3.6 % higher LAS .",0
10381,"Furthermore , experimental results on downstream task applications show that our joint model helps produce state - of - the - art scores for biomedical event extraction and opinion analysis .",0
10382,Our joint model,0
10383,This section presents our model for joint POS tagging and graph - based dependency parsing .,0
10384,illustrates the architecture of our joint model which can be viewed as a two - component mixture of a tagging component and a parsing component .,0
10385,"Given word tokens in an input sentence , the tagging component uses a BiLSTM to learn "" latent "" feature vectors representing these word tokens .",0
10386,Then the tagging component feeds these feature vectors into a multilayer perceptron with one hidden layer ( MLP ) to predict POS tags .,0
10387,"The parsing component then uses another BiLSTM to learn another set of latent feature representations , based on both the input word tokens and the predicted POS tags .",0
10388,These latent feature representations are fed into a MLP to decode dependency arcs and another MLP to label the predicted dependency arcs .,0
10389,Word vector representation,0
10390,"Given an input sentence s consisting of n word tokens w 1 , w 2 , ... , w n , we represent each i th word w i in s by a vector e i .",0
10391,We obtain e i by concatenating word embedding e ( W ) w i and character - level word embedding e ( C ) w i :,0
10392,"Here , each word type win the training data is represented by a real - valued word embedding e ( W ) w .",0
10393,"Given the word type w consisting of k characters w = c 1 c 2 ... c k where each j th character in w is represented by a character embedding c j , we use a sequence BiLSTM ( BiLSTM seq ) to learn its character - level vector representation .",0
10394,"The input to BiLSTM seq is the sequence of k character embeddings c 1 :k , and the output is a concatenation of outputs of a forward LSTM ( LSTM f ) reading the input in its regular order and a reverse LSTM ( LSTM r ) reading the input in reverse :",0
10395,Tagging component,0
10396,"We feed the sequence of vectors e 1:n with an additional context position index i into another BiL - STM ( BiLSTM pos ) , resulting in latent feature vectors v",0
10397,We use a MLP with softmax output ( MLP pos ) on top of the BiLSTM pos to predict POS tag of each word in s .,0
10398,The number of nodes in the output layer of this MLP pos is the number of POS tags .,0
10399,"Given v ( pos ) i , we compute an output vector as :",0
10400,Based on output vectors ?,0
10401,"i , we then compute the cross - entropy objective loss L POS ( t , t ) , in whicht and tare the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s , respectively .",0
10402,"Our tagging component thus can be viewed as a simplified version of the POS tagging model proposed by , without their additional auxiliary loss for rare words .",0
10403,Parsing component,0
10404,"Assume that p 1 , p 2 , ... , p n are the predicted POS tags produced by the tagging component for the input words .",0
10405,We represent each i th predicted POS tag by a vector embedding e ( P ) pi .,0
10406,We then create a sequence of vectors x 1:n in which each x i is produced by concatenating the POS tag embedding e ( P ) pi and the word vector representation e i :,0
10407,"We feed the sequence of vectors x 1:n with an additional index i into a BiLSTM ( BiLSTM dep ) , resulting in latent feature vectors vi as follows :",0
10408,"Based on latent feature vectors vi , we follow a common arc -factored parsing approach to decode dependency arcs .",0
10409,"In particular , a dependency tree can be formalized as a directed graph .",0
10410,An arc - factored parsing approach learns the scores of the arcs in the graph ) .,0
10411,"Here , we score an arc by using a MLP with a one - node output layer ( MLP arc ) on top of the BiLSTM dep :",0
10412,where ( v i * v j ) and |v i ?,0
10413,"v j | denote the elementwise product and the absolute element - wise difference , respectively ; and vi and v j are correspondingly the latent feature vectors associating to the i th and j th words in s , computed by Equation 5 .",0
10414,"Given the arc scores , we use the Eisner ( 1996 ) 's decoding algorithm to find the highest scoring projective parse tree :",0
10415,"where Y ( s ) is the set of all possible dependency trees for the input sentence s while score arc ( h , m ) measures the score of the arc between the head h th word and the modifier m th word in s.",0
10416,"Following , we compute a margin - based hinge loss L ARC with loss - augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree .",0
10417,"For predicting dependency relation type of a head - modifier arc , we use another MLP with softmax output ( MLP rel ) on top of the BiLSTM dep .",0
10418,"Here , the number of the nodes in the output layer of this MLP rel is the number of dependency relation types .",0
10419,"Given an arc ( h , m ) , we compute an output vector as :",0
10420,"Based on output vectors v ( h , m ) , we also compute another cross - entropy objective loss L REL for relation type prediction , using only the gold labeled parse tree .",0
10421,"Our parsing component can be viewed as an extension of the BIST graph - based dependency model , where we additionally incorporate the character - level vector representations of words .",0
10422,Joint model training,0
10423,"The training objective loss of our joint model is the sum of the POS tagging loss L POS , the structure loss L ARC and the relation labeling loss L REL :",0
10424,"The model parameters , including word embeddings , character embeddings , POS embeddings , three one - hidden - layer MLPs and three BiLSTMs , are learned to minimize the sum L of the losses .",0
10425,"Most neural network - based joint models for POS tagging and dependency parsing are transition - based approaches , while our model is a graph - based method .",0
10426,"In addition , the joint model JMT ) defines its dependency parsing task as ahead selection task which produces a probability distribution over possible heads for each word .",0
10427,Our model is the successor of the joint model jPTDP v1.0 which is also a graph - based method .,0
10428,"However , unlike our model , jPTDP v1.0 uses a BiLSTM to learn "" shared "" latent feature vectors which are then used for both POS tagging and dependency parsing tasks , rather than using two separate layers .",0
10429,"As mentioned in Section 4 , our model generally outperforms j PTDP v1.0 with 2.5 + % LAS improvements on universal dependencies ( UD ) treebanks .",0
10430,Implementation details,0
10431,"Our model is released as jPTDP v2.0 , available at https://github.com/datquocnguyen/",0
10432,jPTDP .,0
10433,Our jPTDP v 2.0 is implemented using DYNET v2.0 with a fixed random seed .,1
10434,1,0
10435,"Word embeddings are initialized either randomly or by pre-trained word vectors , while character and POS tag embeddings are randomly initialized .",1
10436,"For learning character - level word embeddings , we use one - layer BiLSTM seq , and set the size of LSTM hidden states to be equal to the vector size of character embeddings .",0
10437,We apply dropout with a 67 % keep probability to the inputs of BiLSTMs and MLPs .,1
10438,"Following and , we also apply word dropout to learn an embedding for unknown words : we replace each word token w appearing # ( w ) times in the training set with a special "" unk "" symbol with probability punk ( w ) = 0.25 0.25 + # ( w ) .",1
10439,This procedure only involves the word embedding part in the input word vector representation .,0
10440,"We optimize the objective loss using Adam ( Kingma and Ba , 2014 ) with an initial learning rate at 0.001 and no mini-batches .",1
10441,"For training , we run for 30 epochs , and restart the Adam optimizer and anneal its initial learning rate at a proportion of 0.5 every 10 epochs .",0
10442,We evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch .,0
10443,"We choose the model with the highest mixed accuracy on the development set , which is then applied to the test set for the evaluation phase .",0
10444,"For all experiments presented in this paper , we use 100 - dimensional word embeddings , 50 - dimensional character embeddings and 100 dimensional POS tag embeddings .",1
10445,We also fix the number of hidden nodes in MLPs at 100 .,1
10446,"Due to limited computational resource , for experiments presented in Section 3 , we perform a minimal grid search of hyper - parameters to select the number of BiLSTM pos and BiLSTM dep layers from { 1 , 2 } and the size of LSTM hidden states in each layer from { 128 , 256 } .",0
10447,"For experiments presented in sections 4 and 5 , we fix the number of BiLSTM layers at 2 and the size of hidden states at 128 .",0
10448,Experiments on English Penn treebank,0
10449,Experimental setup :,0
10450,We evaluate our model using the English WSJ Penn treebank .,0
10451,"We follow a standard data split to use sections 02 - 21 for training , Section 22 for development and Section 23 for test , employing the Stanford conversion toolkit v 3.3.0 to generate dependency trees with Stanford basic dependencies .",0
10452,Word embeddings are initialized by 100 dimensional Glo Ve word vectors pre-trained on Wikipedia and Gigaword .,0
10453,2,0
10454,"As mentioned in Section 2.5 , we perform a minimal grid search of hyper - parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256 - dimensional LSTM hidden states ( in , we present scores obtained on the development set when using 2 BiLSTM layers ) .",0
10455,Main results : The first 11 rows present scores of dependency parsers in which POS tags were predicted by using an external POS tagger such as the Stanford tagger .,0
10456,The last 6 rows present scores for joint models .,0
10457,"Clearly , our model produces very competitive parsing results .",1
10458,"In particular , our model obtains a UAS score at 94.51 % and a LAS score at 92.87 % which are about 1.4 % and 1.9 % absolute higher than UAS and LAS scores of the BIST graph - based model , respectively .",1
10459,"Our model also does better than the previous transition - based joint models in , and , while obtaining similar UAS and LAS scores to the joint model JMT proposed by .",0
10460,We achieve 0.9 % lower parsing scores than the state - of - the - art dependency parser of .,1
10461,"While also a BiLSTM - and graph - based model , it uses a more sophisticated attention mechanism "" biaffine "" for better decoding dependency arcs and relation types .",0
10462,"In future work , we will extend our model with the biaffine attention mechanism to investigate the benefit for our model .",0
10463,"Other differences are that they use a higher dimensional representation than ours , but rely on predicted POS tags .",0
10464,"We also obtain a state - of - the - art POS tagging accuracy at 97.97 % on the test Section 23 , which is about 0.4 + % higher than those by , and .",1
10465,Other previous joint models did not mention their specific POS tagging accuracies .,0
10466,4 4 UniMelb in the CoNLL 2018 shared task on UD parsing,0
10467,Our UniMelb team participated with jPTDP v2.0 in the CoNLL 2018 shared task on parsing 82 treebank test sets ( in 57 languages ) from raw text to universal dependencies .,0
10468,"The 82 treebanks are taken from UD v 2.2 , where 61 / 82 test sets are for "" big "" UD treebanks for which both training and development data sets are available and 5 / 82 test sets are extra "" parallel "" test sets in languages where another big treebank exists .",0
10469,"In addition , 7 / 82 test sets are for "" small "" UD treebanks for which development data is not available .",0
10470,The remaining 9 / 82 sets are in low - resource languages without training data or with a few gold - annotation sample sentences .,0
10471,"For the 7 small treebanks without development data available , we split training data into two parts with a ratio 9:1 , and then use the larger part for training and the smaller part for development .",0
10472,"For each big or small treebank , we train a joint model for universal POS tagging and dependency parsing , using a fixed random seed and a fixed set .",0
10473,""" UPOS "" denotes the universal POS tagging score .",0
10474,""" All "" , "" Big "" , "" PUD "" , "" Small "" and "" Low "" refer to the macro-average scores over all 81 , 61 big treebank , 5 parallel , 7 small treebank and 9 lowresource treebank test sets , respectively .",0
10475,""" goldseg . "" denotes the scores of our jPTDP v2.0 model regarding gold segmentation , detailed in .",0
10476,of hyper - parameters as mentioned in Section 2.5 .,0
10477,"We evaluate the mixed accuracy on the development set after each training epoch , and select the model with the highest mixed accuracy .",0
10478,"For parsing from raw text to universal dependencies , we employ CoNLL - U test files preprocessed by the baseline UDPipe 1.2 .",0
10479,"Here , we utilize the tokenization , word and sentence segmentation predicted by UD - Pipe 1.2 .",0
10480,"For 68 big and small treebank test files , we use the corresponding trained joint models .",0
10481,"We use the joint models trained for cs pdt , en ewt , fi tdt , ja gsd and sv talbanken to process 5 parallel test files cs pud , en pud , fi pud , ja modern and sv pud , respectively .",0
10482,"Since we do not focus on low - resource languages , we employ the baseline UDPipe 1.2 to process 9 low - resource treebank test files .",0
10483,The final test runs are carried out on the TIRA platform .,0
10484,presents our results in the CoNLL 2018 shared task on multilingual parsing from raw texts to universal dependencies .,0
10485,"Over all 82 test sets , we outperform the baseline UDPipe 1.2 with 0.6 % absolute higher average UPOS F1 score and 2.5 + % higher average UAS and LAS F1 scores .",0
10486,"In particular , for the "" big "" category consisting of 61 treebank test sets , we obtain 0.8 % higher UPOS and 3.1 % higher UAS and 3.6 % higher LAS than UDPipe 1.2 .",0
10487,Our ( UniMelb ) official LAS - based rank is at 14 th place while the baseline UDPipe 1.2 is at 18 th place over total 26 participating systems .,0
10488,"However , it is difficult to make a clear comparison between our jPTDP v2.0 and the parsing models used in other top systems .",0
10489,"Several better participating systems simply reuse the state - of - theart biaffine dependency parser , constructing ensemble models or developing treebank concatenation strategies to obtain larger training data , which is likely to produce better scores than ours .",0
10490,Recall that the shared task focuses on parsing from raw texts .,0
10491,"Most higher - ranking systems aim to improve the pre-processing steps of tokenization 7 , word 8 and sentence 9 segmentation , resulting in significant improvements in final parsing scores .",0
10492,"For example , in the CoNLL 2017 shared task on UD parsing , UDPipe 1.2 obtained 0.1 + % higher average tokenization and word segmentation scores and 0.2 % higher average sentence segmentation score than UDPipe 1.1 , resulting in 1 + % improvement in the final average LAS F1 score while both UDPipe 1.2 and UDPipe 1.1 shared exactly the same remaining components .",0
10493,"Utilizing better pre-processors , as used in other participating systems , should likewise improve our final parsing scores .",0
10494,"In , we also present our average UPOS , UAS and LAS accuracies with respect to ( w.r.t. ) gold - standard tokenization , word and sentence segmentation .",0
10495,"For more details and future comparison , presents the UPOS , UAS and LAS scores w.r.t. gold - standard segmentation , obtained by jPTDP v2.0 on each UD v2.2-CoNLL 2018 shared task test set .",0
10496,"Compared to the scores presented in : UPOS , UAS and LAS scores computed on all tokens of our jPTDP v2.0 model regarding goldstandard segmentation on 73 CoNLL - 2018 shared task test sets "" Big "" , "" PUD "" and "" Small "" - UD v2.2 .",0
10497,"[ p ] and [ s ] denote the "" PUD "" extra parallel and small test sets , respectively .",0
10498,"For each treebank , a joint model is trained using a fixed set of hyper - parameters as mentioned in Section 2.5 .",0
10499,UniMelb in the EPE 2018 campaign,0
10500,Our UniMelb team also participated with jPTDP v2.0 in the 2018 Extrinsic Parser Evaluation ( EPE ) campaign .,0
10501,"The EPE 2018 campaign runs in collaboration with the CoNLL 2018 shared task , which aims to evaluate dependency parsers by comparing their performance on three downstream tasks : biomedical event extraction , negation resolution and opinion analysis .",0
10502,"Here , participants only need to provide parsing outputs of English raw texts used in these downstream tasks ; the campaign organizers then compute end - to - end downstream task .",0
10503,""" SP17 "" denotes the F 1 scores obtained by the EPE 2017 system Stanford - Paris with respect to ( w.r.t. ) the Stanford basic dependencies .",0
10504,"The subscript in the SP17 column denotes the F 1 scores obtained by Stanford - Paris w.r.t. the UD - v1 - enhanced type of dependency representations , in which the average F1 score at 60.51 is the highest one at EPE 2017 .",0
10505,ning our trained model on the pre-processed tokenized and sentence - segmented data provided by the campaign on the TIRA platform .,0
10506,presents the results we obtained for three downstream tasks at EPE 2018 .,0
10507,"Since we employed external training data , our obtained scores are not officially ranked .",0
10508,"In total 17 participating teams , we obtained the highest average F 1 score over the three downstream tasks ( i.e. , we ranked first , unofficially ) .",0
10509,"In particular , we achieved the highest F 1 scores for both biomedical event extraction and opinion analysis .",0
10510,Our results maybe high because the training data we used is larger than the English UD treebanks used by other teams .,0
10511,also presents scores from the Stanford - Paris team ) - the first - ranked team at EPE 2017 .,0
10512,"Both EPE 2017 and 2018 campaigns use the same downstream task setups , therefore the downstream task scores are directly comparable .",0
10513,Note that Stanford - Paris employed the state - of - the - art biaffine dependency parser with larger training data .,0
10514,"In particular , Stanford - Paris not only used the WSJ sections 02 - 21 and the training split of the GENIA treebank ( as we did ) , but also included the Brown corpus .",0
10515,"The downstream application of negation resolution requires parsing of fiction , which is one the genres included in the Brown corpus .",0
10516,Hence it is reasonable that the Stanford - Paris team produced better negation resolution scores than we did .,0
10517,"However , in terms of the Stanford basic dependencies , while we employ a less accurate parsing model with smaller training data , we obtain higher downstream task scores for event extraction and opinion analysis than the Stanford - Paris team .",0
10518,"Consequently , better intrinsic parsing performance does not always imply better extrinsic downstream application performance .",0
10519,"Similar observations on the biomedical event extraction and opinion analysis tasks can also be found in and , respectively .",0
10520,"Further investigations of this pattern requires much deeper understanding of the architecture of the downstream task systems , which is left for future work .",0
10521,Conclusion,0
10522,"In this paper , we have presented a novel neural network model for joint POS tagging and graphbased dependency parsing .",0
10523,"On the benchmark English WSJ Penn treebank , our model obtains strong parsing scores UAS at 94.51 % and LAS at 92.87 % , and a state - of - the - art POS tagging accuracy at 97.97 % .",0
10524,"We also participated with our joint model in the CoNLL 2018 shared task on multilingual parsing from raw texts to universal dependencies , and obtained very competitive results .",0
10525,"Specifically , using the same CoNLL - U files pre-processed by UDPipe , our model produced 0.8 % higher POS tagging , 3.1 % higher UAS and 3.6 % higher LAS scores on average than UDPipe on 61 big UD treebank test sets .",0
10526,"Furthermore , our model also helps obtain state - of - the - art downstream task scores for the biomedical event extraction and opinion analysis applications .",0
10527,We believe our joint model can serve as a new strong baseline for both intrinsic POS tagging and dependency parsing tasks as well as for extrinsic downstream applications .,0
10528,Our code and pre-trained models are available at : https :// github.com/datquocnguyen/jPTDP .,0
10529,title,0
10530,Globally Normalized Transition - Based Neural Networks,1
10531,abstract,0
10532,"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results .",1
10533,"Our model is a simple feed - forward neural network that operates on a task - specific transition system , yet achieves comparable or better accuracies than recurrent models .",0
10534,We discuss the importance of global as opposed to local normalization : a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models .,0
10535,Introduction,0
10536,Neural network approaches have taken the field of natural language processing ( NLP ) by storm .,0
10537,"In particular , variants of long short - term memory ( LSTM ) networks have produced impressive results on some of the classic NLP tasks such as part - of - speech tagging , syntactic parsing and semantic role labeling .",0
10538,One might speculate that it is the recurrent nature of these models that enables these results .,0
10539,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .",1
10540,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..",1
10541,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",1
10542,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .",1
10543,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,1
10544,In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models .,0
10545,"Lookahead features can partially mitigate this discrepancy , but can not fully compensate for it - a point to which we return later .",0
10546,"To empirically demonstrate the effectiveness of global normalization , we evaluate our model on part - of - speech tagging , syntactic dependency parsing and sentence compression ( Section 4 ) .",0
10547,"Our model achieves state - of - the - art accuracy on all of these tasks , matching or outperforming LSTMs while being significantly faster .",0
10548,In particular for dependency parsing on the Wall Street Journal we achieve the best - ever published unlabeled attachment score of 94.61 % .,0
10549,"As discussed in more detail in Section 5 , we also outperform previous structured training approaches used for neural network transitionbased parsing .",0
10550,"Our ablation experiments show that we outperform and because we do global backpropagation training of all model parameters , while they fix the neural network parameters when training the global part of their model .",0
10551,We also outperform despite using a smaller beam .,0
10552,"To shed additional light on the label bias problem in practice , we provide a sentence compression example where the local model completely fails .",0
10553,"We then demonstrate that a globally normalized parsing model without any lookahead features is almost as accurate as our best model , while a locally normalized model loses more than 10 % absolute in accuracy because it can not effectively incorporate evidence as it becomes available .",0
10554,"Finally , we provide an open - source implementation of our method , called SyntaxNet , 1 which we have integrated into the popular TensorFlow 2 framework .",0
10555,"We also provide a pre-trained , state - of - the art English dependency parser called "" Parsey McParseface , "" which we tuned for a balance of speed , simplicity , and accuracy .",0
10556,Model,0
10557,"At its core , our model is an incremental transitionbased parser .",0
10558,To apply it to different tasks we only need to adjust the transition system and the input features .,0
10559,Transition System,0
10560,"Given an input x , most often a sentence , we define :",0
10561,A set of states S ( x ) .,0
10562,A special start state s ? S ( x ) .,0
10563,"A set of allowed decisions A ( s , x ) for all s ?",0
10564,"S ( x ) . A transition function t( s , d , x ) returning a new state s ?",0
10565,for any decision d ?,0
10566,"A ( s , x ) .",0
10567,We will use a function ?,0
10568,to compute the score of decision din state s for input x .,0
10569,The vector ?,0
10570,"contains the model parameters and we assume that ?( s , d , x ; ? ) is differentiable with respect to ?.",0
10571,"In this section , for brevity , we will drop the dependence of x in the functions given above , simply writing S , A ( s ) , t( s , d ) , and ?.",0
10572,Throughout this work we will use transition systems in which all complete structures for the same input x have the same number of decisions n ( x ) ( or n for brevity ) .,0
10573,"In dependency parsing for example , this is true for both the arc-standard and arc-eager transition systems , where for a sentence x of length m , the number of deci-sions for any complete parse is n ( x ) = 2 m. 3",0
10574,"A complete structure is then a sequence of decision / state pairs ( s 1 , d 1 ) . . . ( s n , d n ) such that s 1 = s , d i ?",0
10575,"S (s i ) for i = 1 . . . n , and s i + 1 = t ( s i , d i ) .",0
10576,We use the notation d 1:j to refer to a decision sequence d 1 . . . d j .,0
10577,"We assume that there is a one - to - one mapping between decision sequences d 1:j?1 and states s j : that is , we essentially assume that a state encodes the entire history of decisions .",0
10578,"Thus , each state can be reached by a unique decision sequence from s .",0
10579,"We will use decision sequences d 1:j?1 and states interchangeably : in a slight abuse of notation , we define ?( d 1:j?1 , d ; ? ) to be equal to ?( s , d ; ? ) where sis the state reached by the decision sequence d 1:j?1 .",0
10580,"The scoring function ?( s , d ; ? ) can be defined in a number of ways .",0
10581,"In this work , following Chen and Manning ( 2014 ) , , and , we define it via a feedforward neural network as",0
10582,"Here ? ( l ) are the parameters of the neural network , excluding the parameters at the final layer . ?",0
10583,( d ) are the final layer parameters for decision d. ? ( s ; ? ( l ) ) is the representation for state s computed by the neural network under parameters ? .,0
10584,Note that the score is linear in the parameters ? ( d ) .,0
10585,We next describe how softmax - style normalization can be performed at the local or global level .,0
10586,Global vs. Local Normalization,0
10587,"In the Chen and Manning ( 2014 ) style of greedy neural network parsing , the conditional probability distribution over decisions d j given context d 1:j? 1 is defined as Each Z L ( d 1:j?1 ; ? ) is a local normalization term .",0
10588,The probability of a sequence of decisions d 1:n is,0
10589,.,0
10590,(,0
10591,Beam search can be used to attempt to find the maximum of Eq .,0
10592,( 2 ) with respect to d 1:n .,0
10593,"The additive scores used in beam search are the logsoftmax of each decision , ln p ( d j |d 1:j?1 ; ? ) , not the raw scores ?( d 1:j?1 , d j ; ? ) .",0
10594,"In contrast , a Conditional Random Field ( CRF ) defines a distribution p G ( d 1:n ) as follows :",0
10595,where,0
10596,and D n is the set of all valid sequences of decisions of length n. Z G ( ? ) is a global normalization term .,0
10597,The inference problem is now to find Beam search can again be used to approximately find the argmax .,0
10598,Training,0
10599,Training data consists of inputs x paired with gold decision sequences d * 1:n .,0
10600,We use stochastic gradient descent on the negative log - likelihood of the data under the model .,0
10601,"Under a locally normalized model , the negative log-likelihood is",0
10602,whereas under a globally normalized model it is,0
10603,A significant practical advantange of the locally normalized cost Eq. ( 4 ) is that the local partition function Z Land its derivative can usually be computed efficiently .,0
10604,"In contrast , the Z G term in Eq. ( 5 ) contains a sum over d ?",0
10605,1:n ?,0
10606,D n that is in many cases intractable .,0
10607,"To make learning tractable with the globally normalized model , we use beam search and early updates .",0
10608,"As the training sequence is being decoded , we keep track of the location of the gold path in the beam .",0
10609,"If the gold path falls out of the beam at step j , a stochastic gradient step is taken on the following objective :",0
10610,"Here the set B j contains all paths in the beam at step j , together with the gold path prefix d * 1 : j .",0
10611,"It is straightforward to derive gradients of the loss in Eq. ( 6 ) and to back - propagate gradients to all levels of a neural network defining the score ?( s , d ; ? ) .",0
10612,"If the gold path remains in the beam throughout decoding , a gradient step is performed using B n , the beam at the end of decoding .",0
10613,The Label Bias Problem,0
10614,"Intuitively , we would like the model to be able to revise an earlier decision made during search , when later evidence becomes available that rules out the earlier decision as incorrect .",0
10615,"At first glance , it might appear that a locally normalized model used in conjunction with beam search or exact search is able to revise earlier decisions .",0
10616,"However the label bias problem ( see , pages 222 - 226 , , , Smith and Johnson ) means that locally normalized models often have a very weak ability to revise earlier decisions .",0
10617,"This section gives a formal perspective on the label bias problem , through a proof that globally normalized models are strictly more expressive than locally normalized models .",0
10618,The theorem was originally proved 5 by .,0
10619,The example underlying the proof gives a clear illustration of the label bias problem .,0
10620,6 Global Models can be Strictly More Expressive than Local Models Consider a tagging problem where the task is to map an input sequence x 1:n to a decision sequence d 1:n .,0
10621,"First , consider a locally normalized model where we restrict the scoring function to access only the first i input symbols x 1:i when scoring decision d i .",0
10622,We will return to this restriction soon .,0
10623,The scoring function ?,0
10624,"can bean otherwise arbitrary function of the tuple d 1:i? 1 , d i , x 1:i :",0
10625,.,0
10626,"Second , consider a globally normalized model",0
10627,This model again makes use of a scoring function,0
10628,Define P L to be the set of all possible distributions p L ( d 1:n | x 1 :n ) under the local model obtained as the scores ?,0
10629,vary .,0
10630,"Similarly , define PG to be the set of all possible distributions p G ( d 1:n | x 1 :n ) under the global model .",0
10631,"Here a "" distribution "" is a function from a pair ( x 1 :n , d 1:n ) to a probability p ( d 1:n | x 1 :n ) .",0
10632,Our main result is the following : Theorem 3.1 See also .,0
10633,"P L is a strict subset of PG , that is P LP G .",0
10634,To prove this we will first prove that P L ?,0
10635,PG .,0
10636,This step is straightforward .,0
10637,"We then show that PG P L ; that is , there are distributions in PG thatare not in P L .",0
10638,The proof that PG P L gives a clear illustration of the label bias problem .,0
10639,Proof that P L ?,0
10640,PG :,0
10641,"We need to show that for any locally normalized distribution p L , we can 6 Smith and Johnson ( 2007 ) cite Michael Collins as the source of the example underlying the proof .",0
10642,Note that the theorem refers to conditional models of the form p ( d1:n | x1 :n ) with global or local normalization .,0
10643,"Equivalence ( or non-equivalence ) results for joint models of the form p ( d1 : n , x 1:n ) are quite different : for example results from and imply that weighted context - free grammars ( a globally normalized joint model ) and probabilistic context - free grammars ( a locally normalized joint model ) are equally expressive .",0
10644,construct a globally normalized model p G such that p G = p,0
10645,L .,0
10646,"Consider a locally normalized model with scores ?( d 1:i?1 , d i , x 1:i ) .",0
10647,Define a global model p G with scores,0
10648,Then it is easily verified that,0
10649,"for all x 1 :n , d 1:n .",0
10650,In proving PG P L we will use a simple problem where every example seen in training or test data is one of the following two tagged sentences :,0
10651,Note that the input x 2 = b is ambiguous : it can take tags B or D .,0
10652,"This ambiguity is resolved when the next input symbol , cor e , is observed .",0
10653,"Now consider a globally normalized model , where the scores ?( d 1:i? 1 , d i , x 1:i ) are defined as follows .",0
10654,Define T as the set,0
10655,", ( e , E ) } of ( word , tag ) pairs seen in the data .",0
10656,We define,0
10657,where ?,0
10658,"is the single scalar parameter of the model , and ? = 1 if ?",0
10659,"is true , 0 otherwise .",0
10660,Proof that PG P L :,0
10661,We will construct a globally normalized model p G such that there is no locally normalized model such that p L = p G .,0
10662,"Under the definition in Eq. , it is straightforward to show that",0
10663,"In contrast , under any definition for",0
10664,"The inequality p L ( B|A , a b ) + p L ( D|A , a b ) ? 1 then immediately implies Eq. ( 9 ) .",0
10665,"It follows that for sufficiently large values of ? , we have p G ( A B C|a b c ) + p G ( A D E|a b e ) > 1 , and given Eq. ( 9 ) it is impossible to define a locally normalized model with",0
10666,"Under the restriction that scores ?( d 1:i? 1 , d i , x 1:i ) depend only on the first i input symbols , the globally normalized model is still able to model the data in Eq. , while the locally normalized model fails ( see Eq. 9 ) .",0
10667,"The ambiguity at input symbol b is naturally resolved when the next symbol ( c or e ) is observed , but the locally normalized model is notable to revise its prediction .",0
10668,"It is easy to fix the locally normalized model for the example in Eq. ( 7 ) by allowing scores ?( d 1:i?1 , d i , x 1:i+ 1 ) that take into account the input symbol x i + 1 .",0
10669,"More generally we can have a model of the form ?( d 1:i?1 , d i , x 1:i+k ) where the integer k specifies the amount of lookahead in the model .",0
10670,"Such lookahead is common in practice , but insufficient in general .",0
10671,"For every amount of lookahead k , we can construct examples that can not be modeled with a locally normalized model by duplicating the middle input bin ( 7 ) k + 1 times .",0
10672,"Only a local model with scores ?( d 1:i? 1 , d i , x 1:n ) that considers the entire input can capture any distribution p ( d 1:n | x 1 :n ) : in this case the decomposition p L ( d 1:n | x 1:n ) = n i = 1 p L ( d i |d 1:i? 1 , x 1:n ) makes no independence assumptions .",0
10673,"However , increasing the amount of context used as input comes at a cost , requiring more powerful learning algorithms , and potentially more training data .",0
10674,"For a detailed analysis of the tradeoffs between structural features in CRFs and more powerful local classifiers without structural constraints , see ; in these experiments local classifiers are unable to reach the performance of CRFs on problems such as parsing and named entity recognition where structural constraints are important .",0
10675,"Note that there is nothing to preclude an approach that makes use of both global normalization and more powerful scoring functions ?( d 1:i? 1 , d i , x 1 :n ) , obtaining the best of both worlds .",0
10676,The experiments that follow make use of both .,0
10677,Experiments,1
10678,"To demonstrate the flexibility and modeling power of our approach , we provide experimental results on a diverse set of structured prediction tasks .",0
10679,"We apply our approach to POS tagging , syntactic dependency parsing , and sentence compression .",0
10680,"While directly optimizing the global model defined by Eq. ( 5 ) works well , we found that training the model in two steps achieves the same precision much faster : we first pretrain the network using the local objective given in Eq. ( 4 ) , and then perform additional training steps using the global objective given in Eq. ( 6 ) .",0
10681,We pretrain all layers except the softmax layer in this way .,0
10682,"We purposefully abstain from complicated hand engineering of input features , which might improve performance further .",0
10683,We use the training recipe from for each training stage of our model .,0
10684,"Specifically , we use averaged stochastic gradient descent with momentum , and we tune the learning rate , learning rate schedule , momentum , and early stopping time using a separate held - out corpus for each task .",0
10685,We tune again with a different set of hyperparameters for training with the global objective .,0
10686,Part of Speech Tagging,1
10687,"Part of speech ( POS ) tagging is a classic NLP task , where modeling the structure of the output is important for achieving state - of - the - art performance .",0
10688,Data & Evaluation .,0
10689,"We follow , where a large news collection is used to heuristically generate compression instances .",0
10690,"Our final corpus contains about 2.3 M compression instances : we use 2 M examples for training , 130 k for development and 160 k for the final test .",0
10691,"We report per-token F 1 score and per-sentence accuracy ( A ) , i.e. percentage of instances that fully match the golden compressions .",0
10692,Following we also run a human evaluation on 200 sentences where we ask the raters to score compressions for readability ( read ) and informativeness ( info ) on a scale from 0 to 5 .,0
10693,Model Configuration .,0
10694,The transition system for sentence compression is similar to POS tagging : we scan sentences from left - to - right and label each token as keep or drop .,0
10695,"We extract features from words , POS tags , and dependency labels from a window of tokens centered on the in - put , as well as features from the history of predictions .",0
10696,We use a single hidden layer of size 400 .,0
10697,Results .,1
10698,shows our sentence compression results .,0
10699,Our globally normalized model again significantly outperforms the local model .,1
10700,Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5 .,1
10701,"We also compare to the sentence compression system from , a 3 - layer stacked LSTM which uses dependency label information .",0
10702,"The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings , but our model is roughly 100 faster .",0
10703,All compressions kept approximately 42 % of the tokens on average and all the models are significantly better than the automatic extractions ( p < 0.05 ) .,0
10704,Model Configuration .,0
10705,"Inspired by the integrated POS tagging and parsing transition system of , we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack .",0
10706,"We extract the following features on a window 3 tokens centered at the current focus token : word , cluster , character n- gram up to length 3 .",0
10707,We also extract the tag predicted for the previous 4 tokens .,0
10708,The Results .,0
10709,In we compare our model to a linear CRF and to the compositional characterto - word LSTM model of .,0
10710,The CRF is a first - order linear model with exact inference and the same emission features as our model .,0
10711,"It additionally also has transition features of the word , cluster and character n-gram up to length 3 on both endpoints of the transition .",0
10712,The results for were solicited from the authors .,0
10713,Our local model already compares favorably against these methods on average .,0
10714,"Using beam search with a locally normalized model does not help , but with global normalization it leads to a 7 % reduction in relative error , empirically demonstrating the effect of label bias .",1
10715,"The set of character ngrams feature is very important , increasing average accuracy on the CoNLL '09 datasets by about 0.5 % absolute .",1
10716,This shows that characterlevel modeling can also be done with a simple feed - forward network without recurrence .,0
10717,Dependency Parsing,1
10718,In dependency parsing the goal is to produce a directed tree representing the syntactic structure of the input sentence .,0
10719,Results .,1
10720,"Even though we do not use tri-training , our model compares favorably to the 94.26 % LAS and 92.41 % UAS reported by with tri-training .",1
10721,"As we show in Sec. 5 , these gains can be attributed to the full backpropagation training that differentiates our approach from that of and .",0
10722,Our results also significantly outperform the LSTM - based approaches of .,1
10723,Sentence Compression,0
10724,Our final structured prediction task is extractive sentence compression .,0
10725,Discussion,0
10726,We derived a proof for the label bias problem and the advantages of global models .,0
10727,We then emprirically verified this theoretical superiority by demonstrating state - of - the - art performance on three different tasks .,0
10728,In this section we situate and compare our model to previous work and provide two examples of the label bias problem in practice .,0
10729,Related Neural CRF,0
10730,Work,0
10731,Neural network models have been been combined with conditional random fields and globally normalized models before .,0
10732,and Le describe global training of neural network models for structured prediction problems .,0
10733,add a non-linear neural network layer to a linearchain CRF and Do and Artires ( 2010 ) apply a similar approach to more general Markov network structures .,0
10734,and introduce recurrence into the model and finally combine CRFs and LSTMs .,0
10735,"These neural CRF models are limited to sequence labeling tasks where exact inference is possible , while our model works well when exact inference is intractable .",0
10736,Related Transition - Based Parsing Work,0
10737,"For early work on neural - networks for transition - based parsing , see .",0
10738,"Our work is closest to the work of , and ; in these approaches global normalization is added to the local model of .",0
10739,"Empirically , achieves the best performance , even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features .",0
10740,Their model is therefore limited in its ability to revise the predictions of the locally normalized model .,0
10741,In we show that full backpropagation training all the way to the word embeddings is very important and significantly contributes to the performance of our model .,0
10742,We also compared training under the CRF objective with a Perceptron - like hinge loss between the gold and best elements of the beam .,0
10743,"When we limited the backpropagation depth to training only the top layer ? ( d ) , we found negligible differences in accuracy : 93.20 % and 93.28 % for the CRF objective and hinge loss respectively .",0
10744,"However , when training with full backpropagation the CRF accuracy is 0.2 % higher and training converged more than 4 faster .",0
10745,"perform full backpropagation training like us , but even with a much larger beam , their performance is significantly lower than ours .",0
10746,"We also apply our model to two additional tasks , while they experiment only with dependency parsing .",0
10747,"Finally , introduce recurrent components and additional techniques like maxviolation updates for a corresponding constituency parsing model .",0
10748,"In contrast , our model does not require any recurrence or specialized training .",0
10749,Label Bias in Practice,0
10750,We observed several instances of severe label bias in the sentence compression task .,0
10751,"Although using beam search with the local model outperforms greedy inference on average , beam search leads the local model to occasionally produce empty compressions ) .",0
10752,It is important to note that these are not search errors : the empty compression has higher probability under p L than the prediction from greedy inference .,0
10753,"However , the more expressive globally normalized model does not suffer from this limitation , and correctly gives the empty compression almost zero probability .",0
10754,We also present some empirical evidence that the label bias problem is severe in parsing .,0
10755,"We trained models where the scoring functions in parsing at position i in the sentence are limited to A number of authors have considered modified training procedures for greedy models , or for locally normalized models .",0
10756,"introduce Searn , an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions .",0
10757,describe improvements to a greedy parsing approach that makes use of methods from imitation learning to augment the training set .,0
10758,"Note that these methods are focused on greedy models : they are unlikely to solve the label bias problem when used in conjunction with beam search , given that the problem is one of expressivity of the underlying model .",0
10759,"More recent work has augmented locally normalized models with correctness probabilities or error states , effectively adding a step after every decision where the probability of correctness of the resulting structure is evaluated .",0
10760,"This gives con-siderable gains over a locally normalized model , although performance is lower than our full globally normalized approach .",0
10761,Conclusions,0
10762,"We presented a simple and yet powerful model architecture that produces state - of - the - art results for POS tagging , dependency parsing and sentence compression .",0
10763,Our model combines the flexibility of transition - based algorithms and the modeling power of neural networks .,0
10764,Our results demonstrate that feed - forward network without recurrence can outperform recurrent models such as LSTMs when they are trained with global normalization .,0
10765,We further support our empirical findings with a proof showing that global normalization helps the model overcome the label bias problem from which locally normalized models suffer .,0
10766,title,0
10767,Training with Exploration Improves a Greedy Stack LSTM Parser,0
10768,abstract,0
10769,"We adapt the greedy stack LSTM dependency parser of Dyer et al. ( 2015 ) to support a training - with - exploration procedure using dynamic oracles ( Goldberg and Nivre , 2013 ) instead of assuming an error - free action history .",0
10770,"This form of training , which accounts for model predictions at training time , improves parsing accuracies .",1
10771,We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser .,0
10772,Introduction,0
10773,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .",1
10774,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .",1
10775,The state is of unbounded size .,0
10776,presented a parser in which the parser 's unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks .,0
10777,"Coupled with a recursive tree composition function , the feature representation is able to capture information from the entirety of the state , without resorting to locality assumptions that were common in most other transition - based parsers .",0
10778,"The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update , and retain an over all linear parsing time .",0
10779,"The was trained to maximize the likelihood of gold - standard transition sequences , given words .",0
10780,"At test time , the parser makes greedy decisions according to the learned model .",0
10781,"Although this setup obtains very good performance , the training and testing conditions are mismatched in the following way : at training time the historical context of an action is always derived from the gold standard ( i.e. , perfectly correct past actions ) , but at test time , it will be a model prediction .",0
10782,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .",1
10783,"To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .",1
10784,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .",1
10785,We show that the technique can be used to improve the strong parser of Dyer et al .,0
10786,Parsing Model and Parameter Learning,0
10787,Our departure point is the parsing model described by .,0
10788,"We do not describe the model in detail , and refer the reader to the original work .",0
10789,"At each stage t of the parsing process , the parser state is encoded into a vector pt , which is used to compute the probability of the parser action at time t as :",0
10790,"where g z is a column vector representing the ( output ) embedding of the parser action z , and q z is a bias term for action z .",0
10791,"The set A (S , B ) represents the valid transition actions that maybe taken in the current state .",0
10792,"Since pt encodes information about all previous decisions made by the parser , the chain rule gives the probability of any valid sequence of parse transitions z conditional on the input :",0
10793,"The parser is trained to maximize the conditional probability of taking a "" correct "" action at each parsing state .",0
10794,"The definition of what constitutes a "" correct "" action is the major difference between a static oracle as used by and the dynamic oracle explored here .",0
10795,"Regardless of the oracle , our training implementation constructs a computation graph ( nodes that represent values , linked by directed edges from each function 's inputs to its outputs ) for the negative log probability for the oracle transition sequence as a function of the current model parameters and uses forward - and backpropagation to obtain the gradients respect to the model parameters .",0
10796,Training with Static Oracles,0
10797,"With a static oracle , the training procedure computes a canonical reference series of transitions for each gold parse tree .",0
10798,"It then runs the parser through this canonical sequence of transitions , while keeping track of the state representation pt at each step t , as well as the distribution over transitions p ( z t | pt ) which is predicted by the current classifier for the state representation .",0
10799,"Once the end of the sentence is reached , the parameters are updated towards maximizing the likelihood of the reference transition sequence ( Equation 2 ) , which equates to maximizing the probability of the correct transition , p ( z gt | pt ) , at each state along the path .",0
10800,Training with Dynamic Oracles,0
10801,"In the static oracle case , the parser is trained to predict the best transition to take at each parsing step , assuming all previous transitions were correct .",0
10802,"Since the parser is likely to make mistakes at test time and encounter states it has not seen during training , this training criterion is problematic .",0
10803,"Instead , we would prefer to train the parser to behave optimally even after making a mistake ( under the constraint that it can not backtrack or fix any previous decision ) .",0
10804,"We thus need to include in the training examples states that result from wrong parsing decisions , together with the optimal transitions to take in these states .",0
10805,"To this end we reconsider which training examples to show , and what it means to behave optimally on these training examples .",0
10806,The framework of training with exploration using dynamic oracles suggested by provides answers to these questions .,0
10807,"While the application of dynamic oracle training is relatively straightforward , some adaptations were needed to accommodate the probabilistic training objective .",0
10808,These adaptations mostly follow .,0
10809,Dynamic Oracles .,0
10810,"A dynamic oracle is the component that , given a gold parse tree , provides the optimal set of possible actions to take for any valid parser state .",0
10811,"In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path , the dynamic oracle is well defined for states that result from parsing mistakes , and they may produce more than a single gold action for a given state .",0
10812,"Under the dynamic oracle framework , an action is said to be optimal for a state if the best tree that can be reached after taking the action is no worse ( in terms of accuracy with respect to the gold tree ) than the best tree that could be reached prior to taking that action .",0
10813,"define the arcdecomposition property of transition systems , and show how to derive efficient dynamic oracles for transition systems thatare arc-decomposable .",0
10814,2,0
10815,"Un- fortunately , the arc-standard transition system does not have this property .",0
10816,"While it is possible to compute dynamic oracles for the arc-standard system , the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack .",0
10817,"As the dynamic oracle has to be queried for each parser state seen during training , the use of this dynamic oracle will make the training runtime several times longer .",0
10818,"We chose instead to switch to the arc-hybrid transition system , which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O ( 1 ) dynamic oracle , resulting in only negligible increase to training runtime .",0
10819,We implemented the dynamic oracle to the arc-hybrid system as described by .,0
10820,Training with Exploration .,0
10821,"In order to expose the parser to configurations thatare likely to result from incorrect parsing decisions , we make use of the probabilistic nature of the classifier .",0
10822,"During training , instead of following the gold action , we sample the next transition according to the output distribution the classifier assigns to the current configuration .",0
10823,"Another option , taken by Goldberg and Nivre , is to follow the one - best action predicted by the classifier .",0
10824,"However , initial experiments showed that the onebest approach did notwork well .",0
10825,"Because the neural network classifier becomes accurate early on in the training process , the one - best action is likely to be correct , and the parser is then exposed to very few error states in its training process .",0
10826,"By sampling from the predicted distribution , we are effectively increasing the chance of straying from the gold path during training , while still focusing on mistakes that receive relatively high parser scores .",0
10827,"We believe further formal analysis of this method will reveal connections to reinforcement learning and , perhaps , other methods for learning complex policies .",0
10828,"Taking this idea further , we could increase the number of error-states observed in the training process by changing the sampling distribution so as to bias it toward more low - probability states .",0
10829,"We do this by raising each probability to the power of arcs A , if each arc in A can be derived from p , then a valid tree structure containing all of the arcs in A can also be derived from p.",0
10830,"This is a sufficient condition , but whether it is necessary is unknown ; hence the question of an efficient , O ( 1 ) dynamic oracle for the augmented system is open .",0
10831,?,0
10832,( 0 < ? ?,0
10833,1 ) and re-normalizing .,0
10834,"This transformation keeps the relative ordering of the events , while shifting probability mass towards less frequent events .",0
10835,"As we show below , this turns out to be very beneficial for the configurations that make use of external embeddings .",0
10836,"Indeed , these configurations achieve high accuracies and sharp class distributions early on in the training process .",0
10837,The parser is trained to maximize the likelihood of a correct action z g at each parsing state pt according to Equation 1 .,0
10838,"When using the dynamic oracle , a state pt may admit multiple correct actions z g = {z g i , . . . , z g k }.",0
10839,"Our objective in such cases is the marginal likelihood of all correct actions , 3",0
10840,( 3 ),0
10841,Experiments,1
10842,Following the same settings of Chen and Manning ( 2014 ) and we report results 4 in the English PTB and Chinese CTB - 5 .,0
10843,The score achieved by the dynamic oracle for English is 93.56 UAS .,1
10844,This is remarkable given that the parser uses a completely greedy search procedure .,0
10845,"Moreover , the Chinese score establishes the state - of - the - art , using the same settings as Chen and Manning ( 2014 ) .",1
10846,"and Andor et al. , respectively .",0
10847,A '16 - beam is the parser with beam larger than 1 by Andor et al ..,0
10848,Bold numbers indicate the best results among the greedy parsers .,0
10849,"The error - exploring dynamic - oracle training always improves over static oracle training controlling for the transition system , but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle .",0
10850,Flattening the sampling distribution ( ? = 0.75 ) is especially beneficial when training with pretrained word embeddings .,0
10851,In order to be able to compare with similar greedy parsers we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task .,0
10852,"Since some of the treebanks contain nonprojective sentences and arc-hybrid does not allow nonprojective trees , we use the pseudo -projective approach .",0
10853,We used predicted partof - speech tags provided by the CoNLL 2009 shared task organizers .,0
10854,"We also include results with pretrained word embeddings for English , Chinese , German , and Spanish following the same training setup as ; for English and Chinese we used the same pretrained word embeddings as in Table 1 , for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version",0
10855,3 .,0
10856,See.,0
10857,Related Work,0
10858,"Training greedy parsers on non-gold outcomes , facilitated by dynamic oracles , has been explored by several researchers in different ways",0
10859,"We report the performance of these parsers in the most comparable setup , that is , with beam size 1 or greedy search ..",0
10860,"More generally , training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto - search frameworks .",0
10861,Directly modeling the probability of making a mistake has also been explored for parsing .,0
10862,"Generally , the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test - time prediction errors .",0
10863,"Solutions based on curriculum learning , expected loss training , and reinforcement learning have been proposed .",0
10864,"Finally , abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search ( Andor et al. , , and has been analyzed as well , including for parsing ) .",0
10865,presented stack LSTMs and used them to implement a transition - based dependency parser .,0
10866,The parser uses a greedy learning strategy which potentially provides very high parsing speed while still achieving state - of - the - art results .,0
10867,"We have demonstrated that improvement by training the greedy parser on non-gold outcomes ; dynamic oracles improve the stack LSTM parser , achieving 93.56 UAS for English , maintaining greedy search .",0
10868,Conclusions,0
10869,title,0
10870,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,0
10871,abstract,0
10872,We introduce two first - order graph - based dependency parsers achieving a new state of the art .,0
10873,The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition - based parsers with different random initializations .,0
10874,We cast this approach as minimum Bayes risk decoding ( under the Hamming cost ) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity .,0
10875,"The second parser is a "" distillation "" of the ensemble into a single model .",0
10876,"We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment , thereby avoiding the intractable crossentropy computations required by applying standard distillation objectives to problems with structured outputs .",0
10877,"The first - order distillation parser matches or surpasses the state of the art on English , Chinese , and German .",0
10878,Introduction,0
10879,"Neural network dependency parsers achieve state of the art performance , but training them involves gradient descent on non-convex objectives , which is unstable with respect to initial parameter values .",0
10880,"For some tasks , an ensemble of neural networks from different random initializations has been found to improve performance over individual models .",0
10881,"In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .",0
10882,"We give a probabilistic interpretation to the ensemble parser ( with a minor modification ) , viewing it as an instance of minimum Bayes risk inference .",1
10883,We propose that dis agreements among the ensemble 's members maybe taken as a signal that an attachment decision is difficult or ambiguous .,0
10884,"Ensemble parsing is not a practical solution , however , since an ensemble of N parsers requires",0
10885,"N times as much computation , plus the runtime of finding consensus .",0
10886,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .",1
10887,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .",1
10888,"The application of distilliation to structured prediction is , to our knowledge , new , as is the idea of empirically estimating cost functions .",0
10889,"The distilled model performs almost as well as the ensemble consensus and much better than ( i ) a strong LSTM FOG parser trained using the conventional Hamming cost function , ( ii ) recently published strong , and ( iii ) many higher - order graph - based parsers .",0
10890,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",1
10891,The code to reproduce our results is publicly available .,0
10892,1,0
10893,Notation and Definitions,0
10894,"Let x = x 1 , . . . , x n denote an n -length sentence .",0
10895,"A dependency parse for x , denoted y , is a set of tuples , where h is the index of ahead , m the index of a modifier , and a dependency label ( or relation type ) .",0
10896,Most dependency parsers are constrained to return y that form a directed tree .,0
10897,"A first - order graph - based ( FOG ; also known as "" arc-factored "" ) dependency parser exactly solve ?",0
10898,"where T ( x ) is the set of directed trees over x , and sis a local scoring function that considers only a single dependency arc at a time .",0
10899,"( We suppress dependency labels ; there are various ways to incorporate them , discussed later . )",0
10900,"To define s , used hand - engineered features of the surrounding and in - between context of x hand x m ; more recently , Kiperwasser and Goldberg ( 2016 ) used a bidirectional LSTM followed by a single hidden layer with non-linearity .",0
10901,"The exact solution to Eq. 1 can be found using a minimum ( directed ) spanning tree algorithm or , under a projectivity constraint , a dynamic programming algorithm , in O ( n 2 ) or O ( n 3 ) runtime , respectively .",0
10902,We refer to parsing with a minimum spanning tree algorithm as MST parsing .,0
10903,"An alternative that runs in linear time is transition - based parsing , which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree .",0
10904,Such parsers can return a solution in a faster O ( n ) asymptotic runtime .,0
10905,"Unlike FOG parsers , transition - based parsers allow the use of scoring functions with history - based features , so that attachment decisions can interact more freely ; the best performing parser at the time of this writing employ neural networks .",0
10906,"Let h y ( m ) denote the parent of x min y ( using a special null symbol when m is the root of the tree ) , and h y ( m ) denotes the parent of x min the predicted tree y .",0
10907,"Given two dependency parses of the same sentence , y and y , the Hamming cost is",0
10908,"This cost underlies the standard dependency parsing evaluation scores ( unlabeled and labeled attachment scores , henceforth UAS and LAS ) .",0
10909,"More generally , a cost function C maps pairs of parses for the same sentence to non-negative values interpreted as the cost of mistaking one for the other , and a firstorder cost function ( FOC ) is one that decomposes by attachments , like the Hamming cost .",0
10910,"Given a cost function C and a probabilistic model that defines p ( y | x ) , minimum Bayes risk ( MBR ) decoding is defined b ?",0
10911,"Under the Hamming cost , MBR parsing equates algorithmically to FOG parsing with s ( h , m , x ) = p ( ( h , m ) ?",0
10912,"Y | x ) , the posterior marginal of the attachment under p.",0
10913,This is shown by linearity of expectation ; see also .,0
10914,"Apart from MBR decoding , cost functions are also used for discriminative training of a parser .",0
10915,"For example , suppose we seek to estimate the parameters ?",0
10916,of scoring function S ? .,0
10917,One approach is to minimize the structured hinge loss of a training dataset D with respect to ?:,0
10918,"Intuitively , this amounts to finding parameters that separate the model score of the correct parse from any wrong parse by a distance proportional to the cost of the wrong parse .",0
10919,"With regularization , this is equivalent to the structured support vector machine , and if S ? is ( sub ) differentiable , many algorithms are available .",0
10920,"Variants have been used extensively in training graph - based parsers , which typically make use of Hamming cost , so that the inner max can be solved efficiently using FOG parsing with a slightly revised local scoring function :",0
10921,4 ) Plugging this into Eq. 1 is known as costaugmented parsing .,0
10922,Consensus and Minimum Bayes Risk,0
10923,"Despite the recent success of neural network dependency parsers , most prior works exclusively report single - model performance .",0
10924,"Ensembling neural network models trained from different random starting points is a standard technique in a variety of problems , such as machine translation and constituency parsing .",0
10925,We aim to investigate the benefit of ensembling independently trained neural network dependency parsers by applying the parser ensembling method of Sagae and Lavie ( 2006 ) to a collection of N strong neural network base parsers .,0
10926,"Here , each base parser is an instance of the greedy , transition - based parser of , known as the stack LSTM parser , trained from a different random initial estimate .",0
10927,"Given a sentence x , the consensus FOG parser ( Eq. 1 ) defines score s ( h , m , x ) as the number of base parsers that include the attachment ( h , m ) , which we denote votes ( h , m ) .",0
10928,2,0
10929,"An example of this scoring function with an ensemble of 20 models is shown in We assign to dependency ( h , m ) the label most frequently selected by the base parsers that attach m to h.",0
10930,"Next , note that if we let s ( h , m , x ) = votes ( h , m ) / N , this has no effect on the parser ( we have only scaled by a constant factor ) .",0
10931,"We can therefore view s as a posterior marginal , and the ensemble parser as an MBR parser ( Eq. 2 ) .",0
10932,Experiment .,0
10933,"We consider this approach on the Stanford dependencies version 3.3.0 ( De Marneffe and Manning , 2008 )",0
10934,Penn Treebank task .,0
10935,"As noted , the base parsers instantiate the greedy stack LSTM parser .",0
10936,"3 shows that ensembles , even with small N , strongly outperform a single stack LSTM parser .",0
10937,"Our ensembles of greedy , locally normalized parsers perform comparably to the best previously reported , due to , which uses a beam ( width 32 ) for training and decoding .",0
10938,What is Ensemble Uncertainty ?,0
10939,"While previous works have already demonstrated the merit of ensembling in dependency parsing , usually with diverse base parsers , we consider whether the posterior marginals estimated b? p ( ( h , m ) ?",0
10940,"Y | x ) = votes ( h , m) / N can be interpreted .",0
10941,"We conjecture that dis agreement among base parsers about whereto attach x m ( i.e. , uncertainty in the posterior ) is a sign of difficulty or am - Sentence :",0
10942,"It will go for work ranging from refinery modification to changes in the distribution system , including the way service stations pump fuel into cars .",0
10943,x :,0
10944,An ambiguous sentence from the training set and the posteriors 4 of various possible parents for including .,0
10945,"The last two columns are , respectively , the contributions to the distillation cost CD ( explained in 5.1 , Eq. 5 ) and the standard Hamming cost C H .",0
10946,"The most probable head under the ensemble is changes , which is also the correct answer .",0
10947,biguity .,0
10948,"If this is true , then the ensemble provides information about which confusions are more or less reasonable - information we will exploit in our distilled parser ( 5 ) .",0
10949,"A complete linguistic study is out of scope here ; instead , we provide a motivating example before empirically validating our conjecture .",0
10950,shows an example where there is considerable dis agreement among base parsers over the attachment of a word ( including ) .,0
10951,"We invite the reader to attempt to select the correct attachment and gauge the difficulty of doing so , before reading on .",0
10952,"Regardless of whether our intuition that this is an inherently difficult and perhaps ambiguous case is correct , it is uncontroversial to say that the words in the sentence not listed , which received zero votes ( e.g. , both instances of the ) , are obviously implausible attachments .",0
10953,Our next idea is to transform ensemble uncertainty into a new estimate of cost - a replacement,0
10954,"5 Distilling the Ensemble Despite its state of the art performance , our ensemble requires N parsing calls to decode each sentence .",0
10955,"To reduce the computational cost , we introduce a method for "" distilling "" the ensemble 's knowledge into a single parser , making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model .",0
10956,"While models that combine the outputs of other parsing models have been proposed before , these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first - stage models at test - time .",0
10957,"Creating a cost function from a data analysis procedure is , to our knowledge , a new idea .",0
10958,The idea is attractive because cost functions are model - agnostic ; they can be used with any parser amenable to discriminative training .,0
10959,"Further , only the training procedure changes ; parsing at test time does not require consulting the ensemble at all , avoiding the costly application of the N parsers to new data , unlike model combination techniques like stacking and beam search .",0
10960,Distilling an ensemble of classifiers into one simpler classifer that behaves similarly is due to and ; they were likewise motivated by a desire to create a simpler model that was cheaper to run at test time .,0
10961,"In their work , the ensemble provides a probability distribution over labels for each input , and this predicted distribution serves as the training target for the distilled model ( a sum of two cross entropies objective is used , one targeting the empirical training distribution and the other targeting the ensemble 's posterior distribution ) .",0
10962,"This can be contrasted with the supervision provided by the training data alone , which conventionally provides a single correct label for each instance .",0
10963,"These are respectively called "" soft "" and "" hard "" targets .",0
10964,We propose a novel adaptation of the soft target idea to the structured output case .,0
10965,"Since a sentence has an exponential ( in it s length ) number of parses , representing the posterior distribution over parses predicted by the ensemble is nontrivial .",0
10966,"We solve this problem by taking a single parse from each model , representing the N - sized ensemble 's parse distribution using N samples .",0
10967,"Second , rather than considering uncertainty at the level of complete parse trees ( which would be analogous to the classification case ) or larger structures , we instead consider uncertainty about individual attachments , and seek to "" soften "" the attachment targets used in training the parser .",0
10968,"An illustration for the prepositional phrase attachment ambiguity in , taken from the ensemble output for the sentence , is shown in .",0
10969,Soft targets allow us to encode the notion that mistaking woman as the parent of with is less bad than attaching with to John or telescope .,0
10970,Hard targets alone do not capture this information .,0
10971,Distillation Cost Function,0
10972,The natural place to exploit this additional information when training a parser is in the cost function .,0
10973,"When incorporated into discriminative training , the Hamming cost encodes hard targets : the correct attachment should receive a higher score than all incorrect ones , with the same margin .",0
10974,"Our distillation cost function aims to reduce the cost of decisions that - based on the ensemble uncertainty - appear to be more difficult , or where there maybe multiple plausible attachments .",0
10975,"Let ? ( h , m ) =",0
10976,"Our new cost function is defined by CD ( y , y ) =",0
10977,"Recall that y denotes the correct parse , according to the training data , and y is a candidate parse .",0
10978,This function has several attractive properties :,0
10979,1 .,0
10980,"When a word x m has more than one plausible ( according to the ensemble ) but incorrect ( according to the annotations ) attachment , each one has a diminished cost ( relative to Hamming cost and all implausible attachments ) .",0
10981,2 . The correct attachment ( according to the goldstandard training data ) always has zero cost since h y ( m ) = h y ( m ) and Eq. 5 cancels out .,0
10982,"3 . When the ensemble is confident , cost for its choice ( s ) is lower than it would be under Hamming cost - even when the ensemble is wrong .",0
10983,"This means that we are largely training the distilled parser to simulate the ensemble , including mistakes and correct predictions .",0
10984,This encourages the model to replicate the state of the art ensemble performance .,0
10985,"4 . Further , when the ensemble is perfectly confident and correct , every incorrect attachment has a cost of 1 , just as in Hamming cost .",0
10986,5 . The cost of any attachment is bounded above by the proportion of votes assigned to the correct attachment .,0
10987,One way to understand this cost function is to imagine that it gives the parser more ways to achieve a zero - cost 5 attachment .,0
10988,The first is to correctly attach a word to its correct parent .,0
10989,"The second is to predict a parent that the ensemble prefers to the correct parent , i.e. , ?( h y ( m ) , m ) < ? ( h y ( m ) , m ) .",0
10990,"Any other decision will incur a non-zero cost that is proportional to the implausibility of the attachment , according to the ensemble .",0
10991,Hence the model is supervised both by the hard targets in the training data annotations and the soft targets from the ensemble .,0
10992,"While it may seem counter - intuitive to place zero cost on an incorrect attachment , recall that the cost is merely a margin that must separate the scores of parses containing correct and incorrect arcs .",0
10993,"In contrast , the loss ( in our case , the structured hinge loss ) is the "" penalty "" the learner tries to minimize while training the graph - based parser , which depends on both the cost and model score as defined in Equation 3 .",0
10994,"When an incorrect arc is preferred by the ensemble over the gold arc ( hence assigned a cost / margin of 0 ) , the model will still incur a loss if s ( h y ( m ) , m , x ) < s (h y ( m ) , m , x ) .",0
10995,"In other words , the score of any incorrect arc ( including those strongly preferred by the ensemble ) can not be higher than the score of the gold arc .",0
10996,"The learner only incurs 0 loss if s ( h y ( m ) , m , x ) ? s ( h y ( m ) , m , x ) .",0
10997,"This means that the gold score and the predicted score can have a margin of 0 ( i.e. , have the same score and incur no loss ) when the ensemble is highly confident of that prediction , but the score of the correct parse can not be lower regardless of how confident the ensemble is ( hence the objective does not encourage incorrect trees at the expense of gold ones ) .",0
10998,"In the example in , we show the ( additive ) contribution to the distillation cost by each attachment decision ( column labeled "" new cost "" ) .",0
10999,"Note that more plausible attachments according to the ensemble have a lower cost than less plausible ones ( e.g. , the cost for modification is less than system , though both are incorrect ) .",0
11000,"While in the last line stations received no votes in the ensemble ( implausible attachment ) , its contribution to the cost is bounded by the proportion of votes for correct attachment .",0
11001,"The intuition is that , when the ensemble is not certain of the correct answer , it should not assign a large cost to implausible attachments .",0
11002,"In contrast , Hamming cost would assign a cost of 1 ( column labeled "" Hamming "" ) in all incorrect cases .",0
11003,Distilled Parser,0
11004,Our distilled parser is trained discriminatively with the structured hinge loss ( Eq. 3 ) .,0
11005,This is a natural choice because it makes the cost function explicit and central to learning .,0
11006,"6 Further , because our ensemble 's posterior gives us information about each attachment individually , the cost function we construct can be first - order , which simplifies training with exact inference .",0
11007,"This approach to training a model is wellstudied for a FOG parser , but not for a transitionbased parser , which is comprised of a collection of classifiers trained to choose good sequences of transitions - not to score whole trees for good attachment accuracy .",0
11008,"Transition - based approaches are therefore unsuitable for our proposed distillation cost function , even though they are asymptotically faster .",0
11009,"We proceed with a FOG parser ( with Eisner 's algorithm for English and Chinese , and MST for German since it contains a considerable number of non-projective trees ) as the distilled model .",0
11010,"Concretely , we use a bidirectional LSTM followed by a hidden layer of non-linearity to calculate the scoring function s ( h , m , x ) , following Kiperwasser and Goldberg ( 2016 ) with minor modifications .",0
11011,"The bidirectional LSTM maps each word xi to a vectorx i that embeds the word in context ( i.e. , x 1:i?1 and x i + 1:n ) .",0
11012,Local attachment scores are given by :,0
11013,"where the model parameters are v , W , and b , plus the bidirectional LSTM parameters .",0
11014,We will refer to this parsing model as neural FOG .,0
11015,"Our model architecture is nearly identical to that of Kiperwasser and Goldberg ( 2016 ) , with two primary differences .",0
11016,"The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings , allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation .",0
11017,"7 Unlike Kiperwasser and Goldberg ( 2016 ) , we did not observe any degradation by incorporating the pretrained vectors .",0
11018,"Second , we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer .",0
11019,"While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes , we find that this additional per-epoch decay consistently improves performance across all settings and languages .",0
11020,Experiments,0
11021,"We ran experiments on the English PTB - SD version 3.3.0 , Penn Chinese Treebank , and German CoNLL 2009 tasks .",0
11022,Experimental settings .,0
11023,We used the standard splits for all languages .,0
11024,"Like and , we use predicted tags with the Stanford tagger for English and gold tags for Chinese .",0
11025,For German we use the predicted tags provided by the CoNLL 2009 shared task organizers .,0
11026,"All models were augmented with pretrained structured - skipgram embeddings ; for English we used the Gigaword corpus and 100 dimensions , for Chinese Gigaword and 80 , and for German WMT 2010 monolingual data and 64 .",0
11027,Hyperparameters .,0
11028,The hyperparameters for neural FOG are summarized in .,0
11029,For the Adam optimizer we use the default settings in the CNN neural network library .,0
11030,8,0
11031,"Since the ensemble is used to obtain the uncertainty on the training set , it is imperative that the stack LSTMs do not overfit the training set .",0
11032,"To address this issue , we performed five - way jackknifing of the training data for each stack LSTM model to obtain the training data uncertainty under the ensemble .",0
11033,"To obtain the ensemble uncertainty on each language , we use 21 base models for English ( see footnote 4 ) , 17 for Chinese , and 11 for German .",0
11034,Speed .,0
11035,One potential drawback of using a quadratic or cubic time parser to distill an ensemble of linear - time transition - based models is speed .,0
11036,Our FOG model is implemented using the same CNN library as the stack LSTM transition - based parser .,0
11037,"On the same single - thread CPU hardware , the distilled MST parser 9 parses 20 sentences per second without any pruning , while a single stack LSTM model 8 https://github.com/clab/cnn.git",0
11038,9 The runtime of the Hamming - cost bidirectional LSTM FOG parser is the same as the distilled parser .,0
11039,is only three times faster at 60 sentences per second .,0
11040,"Running an ensemble of 20 stack LSTMs is at least 20 times slower ( without multi-threading ) , not including consensus parsing .",0
11041,"In the end , the distilled parser is more than ten times faster than the ensemble pipeline .",0
11042,Accuracy .,0
11043,All scores are shown in .,0
11044,"First , consider the neural FOG parser trained with Hamming cost ( C H in the second - to - last row ) .",1
11045,"This is a very strong benchmark , outperforming many higherorder graph - based and neural network models on all three datasets .",1
11046,"Nonetheless , training the same model with distillation cost gives consistent improvements for all languages .",1
11047,"For English , we see that this model comes close to the slower ensemble it was trained to simulate .",1
11048,"For Chinese , it achieves the best published scores , for German the best published UAS scores , and just after Bohnet and Nivre ( 2012 ) for LAS .",1
11049,Effects of Pre-trained Word Embedding .,0
11050,"As an ablation study , we ran experiments on English without pre-trained word embedding , both with the Hamming and distillation costs .",0
11051,"The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS , compared to 93.6 UAS and 91.1 LAS for the model with distillation cost .",0
11052,This result further showcases the consistent improvements from using the distillation cost across different settings and languages .,0
11053,"We conclude that "" soft targets "" derived from ensemble uncertainty offer useful guidance , through the distillation cost function and discriminative training of a graph - based parser .",0
11054,"Here we consid - : Dependency parsing performance on English , Chinese , and German tasks .",0
11055,"The "" P ? "" column indicates the use of pretrained word embeddings .",0
11056,Reranking / blend indicates that the reranker score is interpolated with the base model 's score .,0
11057,Note that previous works might use different predicted tags for English .,0
11058,"We report accuracy without punctuation for English and Chinese , and with punctuation for German , using the standard evaluation script in each case .",0
11059,We only consider systems that do not use additional training data .,0
11060,"The best over all results are indicated with bold ( this was achieved by the ensemble of greedy stack LSTMs in Chinese and German ) , while the best non-ensemble model is denoted with an underline .",0
11061,"The sign indicates the use of predicted tags for Chinese in the original publication , although we report accuracy using gold Chinese tags based on private correspondence with the authors .",0
11062,"ered a FOG parser , though future work might investigate any parser amenable to training to minimize a cost - aware loss like the structured hinge .",0
11063,Related Work,0
11064,Our work on ensembling dependency parsers is based on Sagae and Lavie and ; an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing .,0
11065,"proposed a similar model combination with random initializations for phrase - structure parsing , using products of constituent marginals .",0
11066,The local optima in his base model 's training objective arise from latent variables instead of neural networks ( in our case ) .,0
11067,"Model distillation was proposed by , who used a single neural network to simulate a large ensemble of classifiers .",0
11068,"More recently , showed that a single shal - low neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object detection .",0
11069,"Our work is closer to , in the sense that we do not simply compress the ensemble and hit the "" soft target , "" but also the "" hard target "" at the same time 10 .",0
11070,These previous works only used model compression and distillation for classification ; we extend the work to a structured prediction problem ( dependency parsing ) .,0
11071,"similarly used an ensemble of other parsers to guide the prediction of a seed model , though in a different context of "" ambiguityaware "" ensemble training to re-lexicalize a transfer model for a target language .",0
11072,We similarly use an ensemble of models as a supervision for a sin - gle model .,0
11073,"By incorporating the ensemble uncertainty estimates in the cost function , our approach is cheaper , not requiring any marginalization during training .",0
11074,"An additional difference is that we learn from the gold labels ( "" hard targets "" ) rather than only ensemble estimates on unlabeled data .",0
11075,"proposed a distillation model at the sequence level , with application in sequence - to - sequence neural machine translation .",0
11076,There are two primary differences with this work .,0
11077,"First , we use a global model to distill the ensemble , instead of a sequential one .",0
11078,"Second , aim to distill a larger model into a smaller one , while we propose to distill an ensemble instead of a single model .",0
11079,Conclusions,0
11080,We demonstrate that an ensemble of 20 greedy stack LSTMs can achieve state of the art accuracy on English dependency parsing .,0
11081,"This approach corresponds to minimum Bayes risk decoding , and we conjecture that the arc attachment posterior marginals quantify a notion of uncertainty that may indicate difficulty or ambiguity .",0
11082,"Since running an ensemble is computationally expensive , we proposed discriminative training of a graph - based model with a novel cost function that distills the ensemble uncertainty .",0
11083,Deriving a cost function from a statistical model and extending distillation to structured prediction are new contributions .,0
11084,"This distilled model , trained to simulate the slower ensemble parser , improves over the state of the art on Chinese and German .",0
11085,title,0
11086,Structured Training for Neural Network Transition - Based Parsing,1
11087,abstract,0
11088,We present structured perceptron training for neural network transition - based dependency parsing .,1
11089,We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .,0
11090,"Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .",0
11091,"On the Penn Treebank , our parser reaches 94. 26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .",0
11092,We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy .,0
11093,Introduction,0
11094,Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,0
11095,"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers .",1
11096,Transition - based parsers have been shown to provide a good balance between efficiency and accuracy .,0
11097,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",1
11098,"In greedy models , a classifier is used to independently decide which transition to take based on local features of the current parse configuration .",0
11099,This classifier typically uses hand - engineered features and is trained on individual transitions extracted from the gold transition sequence .,0
11100,"While extremely fast , these greedy models typically suffer from search errors due to the inability to recover from incorrect decisions .",0
11101,showed that a beamsearch decoding algorithm utilizing the structured perceptron training algorithm can greatly improve accuracy .,0
11102,"Nonetheless , significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph - based parsers , and only by incorporating graph - based scoring functions were able to exceed the accuracy of graph - based approaches .",0
11103,"In contrast to these carefully hand - tuned approaches , recently presented a neural network version of a greedy transition - based parser .",0
11104,"In their model , a feedforward neural network with a hidden layer is used to make the transition decisions .",0
11105,"The hidden layer has the power to learn arbitrary combinations of the atomic inputs , thereby eliminating the need for hand - engineered features .",0
11106,"Furthermore , because the neural network uses a distributed representation , it is able to model lexical , part - of - speech ( POS ) tag , and arc label similarities in a continuous space .",0
11107,"However , although their model outperforms its greedy hand - engineered counterparts , it is not competitive with state - of - the - art dependency parsers thatare trained for structured search .",0
11108,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .",1
11109,"Training and testing on the Penn Treebank , our transition - based parser achieves 93.99 % unlabeled ( UAS ) / 92.05 % labeled ( LAS ) attachment accuracy , outperforming the 93.22 % UAS / 91.02 % LAS of and 93.27 UAS / 91.19 LAS of .",0
11110,"In addition , by incorporating unlabeled data into training , we further improve the accuracy of our model to 94.26 % UAS / 92.41 % LAS ( 93.46 % UAS / 91.49 % LAS for our greedy model ) .",0
11111,"In our approach we start with the basic structure of , but with a deeper architecture and improvements to the optimization procedure .",0
11112,These modifications ( Section 2 ) increase the performance of the greedy model by as much as 1 % .,0
11113,"As in prior work , we train the neural network to model the probability of individual parse actions .",1
11114,"However , we do not use these probabilities directly for prediction .",0
11115,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .",1
11116,"On the Penn Treebank , this structured learning approach significantly improves parsing accuracy by 0.8 % .",0
11117,An additional contribution of this work is an effective way to leverage unlabeled data .,0
11118,"Neural networks are known to perform very well in the presence of large amounts of training data ; however , obtaining more expert - annotated parse trees is very expensive .",0
11119,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .",1
11120,"This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .",1
11121,"By adding 10 million automatically parsed tokens to the training data , we improve the accuracy of our parsers by almost ? 1.0 % on web domain data .",0
11122,We provide an extensive exploration of our model in Section 5 through ablative analysis and other retrospective experiments .,0
11123,One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper .,0
11124,"Finally , we also note that neural network representations have along history in syntactic parsing ; however , like , our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients .",0
11125,Our work is also not the first to apply structured training to neural networks ( see e.g . Features Extracted early updates ( section 3 ) .,0
11126,Structured learning reduces bias and significantly improves parsing accuracy by 0.6 % .,0
11127,"We demonstrate empirically that beam search based on the scores from the neural network does notwork as well , perhaps because of the label bias problem .",0
11128,A second contribution of this work is an effective way to leverage unlabeled data and other parsers .,0
11129,Neural networks are known to perform very well in the presence of large amounts of training data .,0
11130,It is however unlikely that the amount of hand parsed data will increase significantly because of the high cost for syntactic annotations .,0
11131,To this end we generate large quantities of high - confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees .,0
11132,"This idea comes from tri-training and while applicable to other parsers as well , we show that it benefits neural network parsers more than models with discrete features .",0
11133,Adding 10 million automatically parsed tokens to the training data improves the accuracy of our parsers further by 0.7 % .,0
11134,"Our final greedy parser achieves an unlabeled attachment score ( UAS ) of 93 . 46 % on the Penn Treebank test set , while a model with a beam of size 8 produces an UAS of 94.08 % ( section 4 .",0
11135,"To the best of our knowledge , these are some of the very best dependency accuracies on this corpus .",0
11136,We provide an extensive exploration of our model in section 5 .,0
11137,In ablation experiments we tease apart our various contributions and modeling choices in order to shed some light on what matters in practice .,0
11138,"Neural network representations have been used in structured models before , and have also been used for syntactic parsing , alas with fairly complex architectures and constraints .",0
11139,Our work on the other hand introduces a general approach for structured perceptron training with a neural network representation and achieves stateof - the - art parsing results for English .,0
11140,Neural Network Model,0
11141,"In this section , we describe the architecture of our model , which is summarized in .",0
11142,"Note that we separate the embedding processing to a distinct "" embedding layer "" for clarity of presentation .",0
11143,Our model is based upon that of and we discuss the differences between our model and theirs in detail at the end of this section .,0
11144,We use the arc-standard transition system .,0
11145,Feature,0
11146,Groups ( 2014 ) and we discuss the differences between our model and theirs in detail at the end of this section .,0
11147,Input layer,0
11148,"Given a parse configuration c ( consisting of a stack sand a buffer b ) , we extract a rich set of discrete features which we feed into the neural network .",0
11149,"Following Chen and Manning ( 2014 ) , we group these features by their input source : words , POS tags , and arc labels .",0
11150,"The features extracted for each group are represented as a sparse F V matrix X , where V is the size of the vocabulary of the feature group and F is the number of features .",0
11151,The value of element X f v is 1 if the f 'th feature takes on value v.,0
11152,We produce three input matrices :,0
11153,"X word for words features , X tag for POS tag features , and X label for arc labels , with F word = F tag = 20 and F label = 12 ( ) .",0
11154,"For all feature groups , we add additional special values for "" ROOT "" ( indicating the POS or word of the root token ) , "" NULL "" ( indicating no valid feature value could be computed ) or "" UNK "" ( indicating an out - of - vocabulary item ) .",0
11155,Embedding layer,0
11156,"The first learned layer h 0 in the network transforms the sparse , discrete features X into a dense , continuous embedded representation .",0
11157,"For each feature group X g , we learn a V g D g embedding matrix E g that applies the conversion :",0
11158,where we apply the computation separately for each group g and concatenate the results .,0
11159,"Thus , the embedding layer has E = g F g D g outputs , which we reshape to a vector h 0 .",0
11160,We can choose the embedding dimensionality D for each group freely .,0
11161,"Since POS tags and arc labels have much smaller vocabularies , we show in our experiments ( Section 5.1 ) that we can use smaller D tag and D label , without a loss in accuracy .",0
11162,Hidden layers,0
11163,We experimented with one and two hidden layers composed of M rectified linear ( Relu ) units .,0
11164,Each unit in the hidden layers is fully connected to the previous layer :,0
11165,where W 1 is a M 1 E weight matrix for the first hidden layer and W i are M i M i ?1 matrices for all subsequent layers .,0
11166,The weights bi are bias terms .,0
11167,Relu layers have been well studied in the neural network literature and have been shown to work well for a wide domain of problems .,0
11168,"Through most of development , we kept M i = 200 , but we found that significantly increasing the number of hidden units improved our results for the final comparison .",0
11169,2.4 Relationship to Our model is clearly inspired by and based on the work of .,0
11170,"There area few structural differences : ( 1 ) we allow for much smaller embeddings of POS tags and labels , ( 2 ) we use Relu units in our hidden layers , and we use a deeper model with two hidden layers .",0
11171,"Somewhat to our surprise , we found these changes combined with an SGD training scheme ( Section 3.1 ) during the "" pre-training "" phase of the model to lead to an almost 1 % accuracy gain over .",0
11172,This trend held despite carefully tuning hyperparameters for each method of training and structure combination .,0
11173,"Our main contribution from an algorithmic perspective is our training procedure : as described in the next section , we use the structured perceptron for learning the final layer of our model .",0
11174,We thus present a novel way to leverage a neural network representation in a structured prediction setting .,0
11175,Semi-Supervised,0
11176,Structured Learning,0
11177,"In this work , we investigate a semi-supervised structured learning scheme that yields substantial improvements in accuracy over the baseline neural network model .",0
11178,There are two complementary contributions of our approach : ( 1 ) incorporating structured learning of the model and ( 2 ) utilizing unlabeled data .,0
11179,"In both cases , we use the neural network to model the probability of each parsing action y as a soft - max function taking the final hidden layer as its input :",0
11180,where ?,0
11181,y is a M i dimensional vector of weights for classy and i is the index of the final hidden layer of the network .,0
11182,At a high level our approach can be summarized as follows :,0
11183,"First , we pre-train the network 's hidden representations by learning probabilities of parsing actions .",0
11184,"Fixing the hidden representations , we learn an additional final output layer using the structured perceptron that uses the output of the network 's hidden layers .",0
11185,In practice this improves accuracy by ? 0.6 % absolute .,0
11186,"Next , we show that we can supplement the gold data with a large corpus of high quality automatic parses .",0
11187,We show that incorporating unlabeled data in this way improves accuracy by as much as 1 % absolute .,0
11188,Backpropagation,0
11189,Pretraining,0
11190,"To learn the hidden representations , we use mini-batched averaged stochastic gradient descent ( ASGD ) with momentum to learn the parameters ?",0
11191,"of the network ,",0
11192,We use backpropagation to minimize the multinomial logistic loss :,0
11193,where ?,0
11194,"is a regularization hyper - parameter over the hidden layer parameters ( we use ? = 10 ? 4 in all experiments ) and j sums over all decisions and configurations {y j , c j } extracted from gold parse trees in the dataset .",0
11195,The specific update rule we apply at iteration t is as follows :,0
11196,where the descent direction gt is computed by a weighted combination of the previous direction g t?1 and the current gradient ? L ( ? t ) .,0
11197,The parameter ?,0
11198,"[ 0 , 1 ) is the momentum parameter while ?",0
11199,t is the traditional learning rate .,0
11200,"In addition , since we did not tune the regularization parameter ? , we apply a simple exponential step - wise decay to ? t ; for every ?",0
11201,"rounds of updates , we multiply ? t = 0.96 ?",0
11202,t?1 .,0
11203,The final component of the update is parameter averaging : we maintain averaged parameters,0
11204,where ?,0
11205,t is an averaging weight that increases from 0.1 to 0.9999 with 1/t .,0
11206,"Combined with averaging , careful tuning of the three hyperparameters , ? 0 , and ?",0
11207,using heldout data was crucial in our experiments .,0
11208,Structured Perceptron Training,0
11209,"Given the hidden representations , we now describe how the perceptron can be trained to utilize these representations .",0
11210,The perceptron algorithm with early updates ) requires a feature - vector definition ?,0
11211,"that maps a sentence x together with a configuration c to a feature vector ?( x , c) ?",0
11212,Rd .,0
11213,There is a one - to - one mapping between configurations c and decision sequences y 1 . . . y j?1 for any integer j ?,0
11214,1 : we will use c and y 1 . . . y j?1 interchangeably .,0
11215,"For a sentence x , define GEN ( x ) to be the set of parse trees for x .",0
11216,Each y ?,0
11217,GEN ( x ) is a sequence of decisions y 1 . . .,0
11218,y m for some integer m .,0
11219,We use Y to denote the set of possible decisions in the parsing model .,0
11220,For each decision y ?,0
11221,Y we assume a parameter vector v ( y ) ?,0
11222,Rd .,0
11223,These parameters will be trained using the perceptron .,0
11224,"In decoding with the perceptron - trained model , we will use beam search to attempt to find :",0
11225,"Thus each decision y j receives a score : v ( y j ) ? ( x , y 1 . . . y j?1 ) .",0
11226,"In the perceptron with early updates , the parameters v ( y ) are trained as follows .",0
11227,"On each training example , we run beam search until the goldstandard parse tree falls out of the beam .",0
11228,1 Define j to be the length of the beam at this point .,0
11229,"A structured perceptron update is performed using the gold - standard decisions y 1 . . . y j as the target , and the highest scoring ( incorrect ) member of the beam as the negative example .",0
11230,"A key idea in this paper is to use the neural network to define the representation ?( x , c ) .",0
11231,"Given the sentence x and the configuration c , assuming two hidden layers , the neural network defines values for h 1 , h 2 , and P ( y ) for each decision y .",0
11232,"We experimented with various definitions of ? ( Section 5.2 ) and found that ?( x , c ) = [ h 1 h 2 P (y ) ] ( the concatenation of the outputs from both hidden layers , as well as the probabilities for all decisions y possible in the current configuration ) had the best accuracy on development data .",0
11233,"Note that it is possible to continue to use backpropagation to learn the representation ?( x , c ) during perceptron training ; however , we found using ASGD to pre-train the representation always led to faster , more accurate results in preliminary experiments , and we left further investigation for future work .",0
11234,Incorporating Unlabeled Data,0
11235,"Given the high capacity , non-linear nature of the deep network we hypothesize that our model can be significantly improved by incorporating more data .",0
11236,One way to use unlabeled data is through unsupervised methods such as word clusters ; we follow and use pretrained word embeddings to initialize our model .,0
11237,The word embeddings capture similar distributional information as word clusters and give consistent improvements by providing a good initialization and information about words not seen in the treebank data .,0
11238,"However , obtaining more training data is even more important than a good initialization .",0
11239,One potential way to obtain additional training data is by parsing unlabeled data with previously trained models .,0
11240,"and showed that iteratively re-training a single model ( "" self - training "" ) can be used to improve parsers in certain settings ; built on this work and showed that a slow and accurate parser can be used to "" up - train "" a faster but less accurate parser .",0
11241,"In this work , we adopt the "" tri-training "" approach of : Two parsers are used to process the unlabeled corpus and only sentences for which both parsers produced the same parse tree are added to the training data .",0
11242,"The intuition behind this idea is that the chance of the parse being correct is much higher when the two parsers agree : there is only one way to be correct , while there are many possible incorrect parses .",0
11243,"Of course , this reasoning holds only as long as the parsers suffer from different biases .",0
11244,We show that tri-training is far more effective than vanilla up - training for our neural network model .,0
11245,"We use same setup as , intersecting the output of the BerkeleyParser , and a reimplementation of ZPar as our baseline parsers .",0
11246,"The two parsers agree only 36 % of the time on the tune set , but their accuracy on those sentences is 97. 26 % UAS , approaching the inter annotator agreement rate .",0
11247,"These sentences are of course easier to parse , having an average length of 15 words , compared to 24 words for the tune set over all .",0
11248,"However , because we only use these sentences to extract individual transition decisions , the shorter length does not seem to hurt their utility .",0
11249,We generate 10 7 tokens worth of new parses and use this data in the backpropagation stage of training .,0
11250,Experiments,0
11251,In this section we present our experimental setup and the main results of our work .,0
11252,Experimental Setup,0
11253,We conduct our experiments on two English language benchmarks : ( 1 ) the standard Wall Street Journal ( WSJ ) part of the Penn Treebank and a more comprehensive union of publicly available treebanks spanning multiple domains .,0
11254,"For the WSJ experiments , we follow standard practice and use sections 2 - 21 for training , section 22 for development and section 23 as the final test set .",0
11255,"Since there are many hyperparameters in our models , we additionally use section 24 for tuning .",0
11256,We convert the constituency trees to Stanford style dependencies ( De using version 3.3.0 of the converter .,0
11257,"We use a CRF - based POS tagger to generate 5 fold jack - knifed POS tags on the training set and predicted tags on the dev , test and tune sets ; our tagger gets comparable accuracy to the Stanford POS tagger with 97 . 44 % on the test set .",0
11258,"We report unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) excluding punctuation on predicted POS tags , as is standard for English .",0
11259,"For the second set of experiments , we follow the same procedure as above , but with a more diverse dataset for training and evaluation .",0
11260,"Following , we use ( in addition to the WSJ ) , the OntoNotes corpus version 5 , the English Web Treebank , and the updated and corrected Question Treebank .",0
11261,We train on the union of each corpora 's training set and test on each domain separately .,0
11262,"We refer to this setup as the "" Treebank Union "" setup .",0
11263,"In our semi-supervised experiments , we use the corpus from as our source of unlabeled data .",0
11264,"We process it with the Berkeley - Parser , a latent variable constituency parser , and a reimplementation of ZPar , a transition - based parser with beam search .",0
11265,Both parsers are included as baselines in our evaluation .,0
11266,We select the first 10 7 tokens for which the two parsers agree as additional training data .,0
11267,"For our tri-training experiments , we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser .",0
11268,This increases POS,0
11269,Method,0
11270,UAS LAS,0
11271,Beam,0
11272,Graph-based 92.88 90.71 n/ a 92.89 90.55 n/ a 93.22 91.02 n/ a Transition - based 93.00 90.95 32 93.27 91.19 40 91.80 89.60 1 S-LSTM : Final WSJ test set results .,0
11273,We compare our system to state - of - the - art graph - based and transition - based dependency parsers .,0
11274,denotes our own re-implementation of the system so we could compare tri-training on a competitive baseline .,0
11275,All methods except and were run using predicted tags from our POS tagger .,0
11276,"For reference , the accuracy of the Berkeley constituency parser ( after conversion ) is 93.61 % UAS / 91.51 % LAS .",0
11277,accuracy slightly to 97.57 % on the WSJ .,0
11278,Model Initialization & Hyperparameters,0
11279,"In all cases , we initialized W i and ?",0
11280,randomly using a Gaussian distribution with variance 10 ?4 .,0
11281,"We used fixed initialization with bi = 0.2 , to ensure that most Relu units are activated during the initial rounds of training .",0
11282,"We did not systematically compare this random scheme to others , but we found that it was sufficient for our purposes .",0
11283,"For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings .",0
11284,We used the publicly available word2vec 2 tool to learn CBOW embeddings following the sample configuration provided with the tool .,1
11285,"For words not appearing in the unsupervised data and the special "" NULL "" etc. tokens , we used random initialization .",0
11286,In preliminary experiments we found no difference between training the word embeddings on 1 billion or 10 billion tokens .,0
11287,We therefore trained the word embeddings on the same corpus we used for tri-training .,0
11288,"We tron layer , we used ?( x , c ) = [ h 1 h 2 P ( y ) ] ( concatenation of all intermediate layers ) .",0
11289,All hyperparameters ( including structure ) were tuned using Section 24 of the WSJ only .,0
11290,"When not tri-training , we used hyperparameters of ? = 0.2 , ? 0 = 0.05 , = 0.9 , early stopping after roughly 16 hours of training time .",0
11291,"With the tri-training data , we decreased ?",0
11292,"0 = 0.05 , increased ? = 0.5 , and decreased the size of the network to M 1 = 1024 , M 2 = 256 for run-time efficiency , and trained the network for approximately 4 days .",0
11293,"For the Treebank Union setup , we set M 1 = M 2 = 1024 for the standard training set and for the tri-training setup .",0
11294,"shows our final results on the WSJ test set , and shows the cross - domain results from the Treebank Union .",0
11295,We compare to the best dependency parsers in the literature .,0
11296,"For and , we use reported results ; the other baselines were run by Bernd Bohnet using version 3.3.0 of the Stanford dependencies and our predicted POS tags for all datasets to make comparisons as fair as possible .",0
11297,"On the WSJ and Web tasks , our parser outperforms all dependency parsers in our comparison by a substantial margin .",0
11298,"The Question ( QTB ) dataset is more sensitive to the smaller beam size we use in order to train the models in a reasonable time ; if we increase to B = 32 at inference time only , our perceptron performance goes up to 92.29 % LAS .",0
11299,"Since many of the baselines could not be directly compared to our semi-supervised approach , we re-implemented and trained on the tri-training corpus .",0
11300,"Although tritraining did help the baseline on the dev set , test set performance did not improve significantly .",0
11301,"In contrast , it is quite exciting to see that after tri-training , even our greedy parser is more accurate than any of the baseline dependency parsers and competitive with the Berkeley - Parser used to generate the tri-training data .",0
11302,"As expected , tri-training helps most dramatically to increase accuracy on the Treebank Union setup with diverse domains , yielding 0.4 - 1.0 % absolute LAS improvement gains for our most accurate model .",0
11303,Results,1
11304,"Unfortunately we are notable to compare to several semi-supervised dependency parsers that achieve some of the highest reported accuracies on the WSJ , in particular and .",0
11305,These parsers use the dependency conversion and the accuracies are therefore not directly comparable .,0
11306,"The highest of these is , with a reported accuracy of 94.22 % UAS .",1
11307,"Even though the UAS is not directly comparable , it is typically similar , and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ .",1
11308,Discussion,0
11309,"In this section , we investigate the contribution of the various components of our approach through ablation studies and other systematic experiments .",0
11310,"We tune on Section 24 , and use Section 22 for comparisons in order to not pollute the official test set ( Section 23 ) .",0
11311,We focus on UAS as we found the LAS scores to be strongly correlated .,0
11312,"Unless otherwise specified , we use 200 hidden units in each layer to be able to run more ablative experiments in a reasonable amount of time .",0
11313,Impact of Network Structure,0
11314,"In addition to initialization and hyperparameter tuning , there are several additional choices about model structure and size a practitioner faces when implementing a neural network model .",0
11315,We explore these questions and justify the particular choices we use in the following .,0
11316,Note that we do Figure 2 : Effect of hidden layers and pre-training on variance of random restarts .,0
11317,"Initialization was either completely random or initialized with word2vec embeddings ( "" Pretrained "" ) , and either one or two hidden layers of size 200 were used .",0
11318,Each point represents maximization over a small hyperparameter grid with early stopping based on WSJ tune set UAS score .,0
11319,"D word = 64 , D tag , D label = 16 . not use a beam for this analysis and therefore do not train the final perceptron layer .",0
11320,This is done in order to reduce training times and because the trends persist across settings .,0
11321,Variance reduction with pre-trained embeddings .,0
11322,"Since the learning problem is nonconvex , different initializations of the parameters yield different solutions to the learning problem .",0
11323,"Thus , for any given experiment , we ran multiple random restarts for every setting of our hyperparameters and picked the model that performed best using the held - out tune set .",0
11324,We found it important to allow the model to stop training early if tune set accuracy decreased .,0
11325,"We visualize the performance of 32 random restarts with one or two hidden layers and with and without pretrained word embeddings in , and a summary of the figure in .",0
11326,"While adding a second hidden layer results in a large gain on the tune set , there is no gain on the dev set if pre-trained embeddings are not used .",0
11327,"In fact , while the over all UAS scores of the tune set and dev set are strongly correlated (? = 0.64 , p < 10 ?10 ) , they are not significantly correlated if pre-trained embeddings are not used (? = 0.12 , p > 0.3 ) .",0
11328,"This suggests that an additional benefit of pre-trained embeddings , aside from allowing learning to reach a more accurate solution , is to push learning towards a solution that generalizes to more data .",0
11329,Diminishing returns with increasing embedding dimensions .,0
11330,"For these experiments , we fixed one embedding type to a high value and reduced the dimensionality of all others to very small values .",0
11331,"The results are plotted in , suggesting larger embeddings do not significantly improve results .",0
11332,We also ran tri-training on a very compact model with D word = 8 and D tag = D label = 2 ( 8 fewer parameters than our full model ) which resulted in 92.33 % UAS accuracy on the dev set .,0
11333,"This is comparable to the full model without tri-training , suggesting that more training data can compensate for fewer parameters .",0
11334,Increasing hidden units yields large gains .,0
11335,"For these experiments , we fixed the embedding sizes D word = 64 , D tag = D label = 32 and tried increasing and decreasing the dimensionality of the hidden layers on a logarthmic scale .",0
11336,"Improvements in accuracy did not appear to saturate even with increasing the number of hidden units by an order of magnitude , though the network became too slow to train effectively past M = 2048 .",0
11337,"These results suggest that there are still gains to be made by increasing the efficiency of larger networks , even for greedy shift - reduce parsers .",0
11338,Impact of Structured Perceptron,0
11339,We now turn our attention to the importance of structured perceptron training as well as the impact of different latent representations .,0
11340,Bias reduction through structured training .,0
11341,"To evaluate the impact of structured training , we compare using the estimates P ( y ) from the neural network directly for beam search to using the activations from all layers as features in the structured perceptron .",0
11342,"Using the probability estimates directly is very similar to , where a maximum - entropy model was used to model the distribution over possible actions at each parser state , and beam search was used to search for the highest probability parse .",0
11343,A known problem with beam search in this setting is the label - bias problem .,0
11344,shows the impact of using structured perceptron training over using the softmax function during beam search as a function of the beam size used .,0
11345,"For reference , our reimplementation of is trained equivalently for each setting .",0
11346,We also show the impact on beam size when tri-training is used .,0
11347,"Although the beam does marginally improve accuracy for the softmax model , much greater gains are achieved when perceptron training is used .",0
11348,Using all hidden layers crucial for structured perceptron .,0
11349,We also investigated the impact of connecting the final perceptron layer to all prior hidden layers ) .,0
11350,Our results suggest that all intermediate layers of the network are indeed discriminative .,0
11351,"Nonetheless , aggregating all of their activations proved to be the most effective representation for the structured perceptron .",0
11352,"This suggests that the representations learned by the network collectively contain the information re - quired to reduce the bias of the model , but not when filtered through the softmax layer .",0
11353,"Finally , we also experimented with connecting both hidden layers to the softmax layer during backpropagation training , but we found this did not significantly affect the performance of the greedy model .",0
11354,Impact of Tri-Training,0
11355,"To evaluate the impact of the tri-training approach , we compared to up - training with the Berkely - Parser alone .",0
11356,The results are summarized in for the greedy and perceptron neural net models as well as our reimplementated baseline .,0
11357,"For our neural network model , training on the output of the BerkeleyParser yields only modest gains , while training on the data where the two parsers agree produces significantly better results .",0
11358,"This was especially pronounced for the greedy models : after tri-training , the greedy neural network model surpasses the BerkeleyParser in accuracy .",0
11359,It is also interesting to note that up - training improved results far more than tri-training for the baseline .,0
11360,"We speculate that this is due to the alack of diversity in the tri-training data for this model , since the same baseline model was intersected with the BerkeleyParser to generate the tritraining data .",0
11361,Error Analysis,0
11362,"Regardless of tri-training , using the structured perceptron improved error rates on some of the common and difficult labels : ROOT , ccomp , cc , conj , and nsubj all improved by > 1 % .",0
11363,We inspected the learned perceptron weights v for the softmax probabilities P ( y ) ( see Appendix ) and found that the perceptron reweights the softmax probabilities based on common confusions ; e.g. a strong negative weight for the action RIGHT ( ccomp ) given the softmax model outputs RIGHT ( conj ) .,0
11364,"Note without the hidden layer , the perceptron was notable to reweight the softmax probabilities to account for the greedy model 's biases .",0
11365,Conclusion,0
11366,We presented a new state of the art in dependency parsing : a transition - based neural network parser trained with the structured perceptron and ASGD .,0
11367,We then combined this approach with unlabeled data and tri-training to further push state - of - the - art in semi-supervised dependency parsing .,0
11368,"Nonetheless , our ablative analysis suggests that further gains are possible simply by scaling up our system to even larger representations .",0
11369,"In future work , we will apply our method to other languages , explore end - to - end training of the system using structured learning , and scale up the method to larger datasets and network structures .",0
11370,title,0
11371,From POS tagging to dependency parsing for biomedical event extraction,1
11372,abstract,0
11373,Background :,0
11374,"Given the importance of relation or event extraction from biomedical research publications to support knowledge capture and synthesis , and the strong dependency of approaches to this information extraction task on syntactic information , it is valuable to understand which approaches to syntactic processing of biomedical text have the highest performance .",0
11375,"We perform an empirical study comparing state - of - the - art traditional feature - based and neural network - based models for two core natural language processing tasks of part - of - speech ( POS ) tagging and dependency parsing on two benchmark biomedical corpora , GENIA and CRAFT .",0
11376,"To the best of our knowledge , there is no recent work making such comparisons in the biomedical context ; specifically no detailed analysis of neural models on this data is available .",0
11377,"Experimental results show that in general , the neural models outperform the feature - based models on two benchmark biomedical corpora GENIA and CRAFT .",0
11378,"We also perform a task - oriented evaluation to investigate the influences of these models in a downstream application on biomedical event extraction , and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance .",0
11379,"We have presented a detailed empirical study comparing traditional feature - based and neural network - based models for POS tagging and dependency parsing in the biomedical context , and also investigated the influence of parser selection for a biomedical event extraction downstream task .",0
11380,Availability of data and material :,0
11381,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,1
11382,Background,0
11383,"The biomedical literature , as captured in the parallel repositories of PubMed ( abstracts ) and PubMed Central ( full text articles ) , is growing at a remarkable rate of over one million publications per year .",0
11384,Effort to catalog the key research results in these publications demands automation .,0
11385,Hence extraction of relations and events from the published literature has become a key focus of the biomedical natural language processing community .,0
11386,"Methods for information extraction typically make use of linguistic information , with a specific emphasis on the value of dependency parses .",0
11387,"A number of linguistically - annotated resources , notably including the GENIA and CRAFT corpora , have been * Correspondence : dqnguyen@unimelb.edu.au ,",0
11388,"The University of Melbourne Karin.Verspoor@unimelb.edu.au ,",0
11389,The University of Melbourne https://www.ncbi.nlm.nih.gov/pubmed,0
11390,"https://www.ncbi.nlm.nih.gov/pmc produced to support development and evaluation of natural language processing ( NLP ) tools over biomedical publications , based on the observation of the substantive differences between these domain texts and general English texts , as captured in resources such as the Penn Treebank thatare standardly used for development and evaluation of syntactic processing tools .",0
11391,Recent work on biomedical relation extraction has highlighted the particular importance of syntactic information .,0
11392,"Despite this , that work , and most other related work , has simply adopted a tool to analyze the syntactic characteristics of the biomedical texts without consideration of the appropriateness of the tool for these texts .",0
11393,"A commonly used tool is the Stanford CoreNLP dependency parser , although domain - adapted parsers ( e.g. ) are sometimes used .",0
11394,Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data .,0
11395,Given the significant improvements in parsing performance in the last arXiv : 1808.03731v2 [ cs. CL ] 2 Jan 2019,0
11396,Pre-trained POS tagger Retrained POS tagger,0
11397,Pre-trained parser,0
11398,Retrained parser,0
11399,"Event extraction few years , thanks to renewed attention to the problem and exploration of neural methods , it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts .",0
11400,"In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .",1
11401,"We also present detailed results on the precursor task of POS tagging , since parsing depends heavily on POS tags .",0
11402,"Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .",1
11403,We find that differences in over all intrinsic parser performance do not consistently explain differences in information extraction performance .,0
11404,Experimental methodology,0
11405,"In this section , we present our empirical approach to evaluate different POS tagging and dependency parsing models on benchmark biomedical corpora .",0
11406,illustrates our experimental flow .,0
11407,"In particular , we compare pre-trained and retrained POS taggers , and investigate the effect of these pre-trained and retrained taggers in pre-trained parsing models ( in the first five rows of ) .",0
11408,We then compare the performance of retrained parsing models to the pre-trained ones ( in the last ten rows of ) .,0
11409,"Finally , we investigate the influence of pre-trained and retrained parsing models in the biomedical event extraction task ( in ) .",0
11410,Datasets,0
11411,We use two biomedical corpora : GENIA and CRAFT .,0
11412,"GENIA includes abstracts from PubMed , while CRAFT includes full text publications .",0
11413,It has been observed that there are substantial linguistic differences between the abstracts and the corresponding full text publications ; hence it is important to consider both contexts when assessing NLP tools in biomedical domain .,0
11414,"The GENIA corpus contains 18K sentences ( ? 486K words ) from 1,999 Medline abstracts , which are manually annotated following the Penn Treebank ( PTB ) bracketing guidelines .",0
11415,"On this treebank , we use the training , development and test split from .",0
11416,We then use the Stanford constituent - to - dependency conversion toolkit ( v3.5.1 ) to generate dependency trees with basic Stanford dependencies .,0
11417,The CRAFT corpus includes 21 K sentences ( ? 561K words ) from 67 full - text biomedical journal articles .,0
11418,These sentences are syntactically annotated using an extended PTB tag set .,0
11419,"Given this extended set , the Stanford conversion toolkit is not suitable for generating dependency trees .",0
11420,"Hence , a dependency treebank using the CoNLL 2008 dependencies was produced from the CRAFT treebank using ClearNLP ; we directly use this dependency treebank in our experiments .",0
11421,"We use sentences from the first 6 files ( PubMed IDs : 11532192-12585968 ) for development and sentences from the next 6 files ( PubMed IDs : 12925238-15005800 ) for testing , while the the remaining 55 files are used for training .",0
11422,"gives an overview of the experimental datasets , while details corpus statistics .",0
11423,We also include out - of - vocabulary ( OOV ) rate in .,0
11424,"OOV rate is relevant because if a word has not been observed in the training data at all , the tagger / parser is limited to using contextual clues to resolve the label ( i.e. it has observed no prior usage of the word during training and hence has no experience with the word to draw on ) .",0
11425,POS tagging models,0
11426,We compare SOTA feature - based and neural networkbased models for POS tagging over both GENIA and CRAFT .,0
11427,We consider the following :,0
11428,MarMoT is a well - known generic CRF framework as well as a leading POS and morphological tagger .,0
11429,NLP4 J's POS tagging model is a dynamic feature induction model that automatically optimizes feature combinations .,0
11430,NLP4J is the successor of ClearNLP .,0
11431,https://nlp.stanford.edu/~mcclosky/biomedical.html,0
11432,http://bionlp-corpora.sourceforge.net/CRAFT,0
11433,http://cistern.cis.lmu.de/marmot,0
11434,https://emorynlp.github.io/nlp4j/components/,0
11435,part-of-speech-tagging.html,0
11436,"Statistics by the most frequent dependency and overlapped POS labels , sentence length ( i.e. number of words in the sentence ) and relative dependency distances i ?",0
11437,j from a dependent w i to it s head w j .,0
11438,"In addition , % G and % C denote the occurrence proportions in GENIA and CRAFT , respectively .",0
11439,Dependency labels POS tags,0
11440,is a sequence labeling model which extends a standard BiLSTM neural network with a CRF layer . BiLSTM - CRF + CNN - char extends the model BiLSTM - CRF with character - level word embeddings .,0
11441,"For each word token , its character - level word embedding is derived by applying a CNN to the word 's character sequence .",0
11442,"BiLSTM - CRF + LSTM - char also extends the BiLSTM - CRF model with character - level word embeddings , which are derived by applying a BiL - STM to each word 's character sequence .",0
11443,"For the three BiLSTM - CRF - based sequence labeling models , we use a performance - optimized implementation from .",0
11444,"As detailed later in the POS tagging results section , we use NLP4J - POS to predict POS tags on development and test sets and perform 20 - way jackknifing to generate POS tags on the training set for dependency parsing .",0
11445,Dependency parsers,0
11446,"Our second study assesses the performance of SOTA dependency parsers , as well as commonly used parsers , on biomedical texts .",0
11447,"Prior work on the CRAFT treebank identified the domain - retrained ClearParser , now part of the NLP4J toolkit , as a top - performing system for dependency parsing over that data .",0
11448,It remains the best performing non-neural model for dependency parsing .,0
11449,"In particular , we compare the following parsers : https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf",0
11450,"The Stanford neural network dependency parser ( Stanford - NNdep ) is a greedy transitionbased parsing model which concatenates word , POS tag and arc label embeddings into a single vector , and then feeds this vector into a multilayer perceptron with one hidden layer for transition classification .",0
11451,"NLP4J 's dependency parsing model ( NLP4Jdep ) is a transition - based parser with a selectional branching method that uses confidence estimates to decide when employing a beam . jPTDP v 1 is a joint model for POS tagging and dependency parsing , which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing .",0
11452,"jPTDP can be viewed as an extension of the graph - based dependency parser bmstparser , replacing POS tag embeddings with LSTM - based character - level word embeddings .",0
11453,"For jPTDP , we train with gold standard POS tags . The Stanford "" Biaffine "" parser v 1 extends bmstparser with biaffine classifiers to predict dependency arcs and labels , obtaining the highest parsing result to date on the benchmark English PTB .",0
11454,"The Stanford Biaffine parser v2 , further extends v 1 with LSTM - based character - level word embeddings , obtaining the highest result ( i.e. , 1 st place ) at the CoNLL 2017 shared task on multilin - https://nlp.stanford.edu/software/nndep.shtml",0
11455,https://emorynlp.github.io/nlp4j/components/,0
11456,dependency-parsing.html,0
11457,https://github.com/datquocnguyen/jPTDP,0
11458,gual dependency parsing .,0
11459,We use the Stanford Biaffine parser v 2 in our experiments .,0
11460,Implementation details,0
11461,We use the training set to learn model parameters while we tune the model hyper - parameters on the development set .,0
11462,Then we report final evaluation results on the test set .,0
11463,The metric for POS tagging is the accuracy .,0
11464,The metrics for dependency parsing are the labeled attachment score ( LAS ) and unlabeled attachment score ( UAS ) :,0
11465,LAS is the proportion of words which are correctly assigned both dependency arc and label while UAS is the proportion of words for which the dependency arc is assigned correctly .,0
11466,"For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .",1
11467,These pre-trained vectors were obtained by training the Word2Vec skip - gram model on a PubMed abstract corpus of 3 billion word tokens .,0
11468,"For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .",1
11469,"For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .",1
11470,"We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",0
11471,Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .,0
11472,"For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .",1
11473,"For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .",1
11474,"We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .",1
11475,Other hyper - parameters are set at their default values .,0
11476,"For Stanford - Biaffine , we use default hyper - parameter values .",0
11477,These default values can be considered as optimal ones as they helped producing the highest scores for 57 test sets ( including English test sets ) and second highest scores for 14 test sets over total 81 test sets across 45 different languages at the CoNLL 2017 shared task .,0
11478,https://github.com/tdozat/Parser-v2,0
11479,POS tagging accuracies on the test set with gold tokenization .,0
11480,[ ] denotes a result with a pre-trained POS tagger .,0
11481,We do not provide accuracy results of the pre-trained POS taggers on CRAFT because CRAFT uses an extended PTB POS tag set ( i.e. there are POS tags in CRAFT thatare not defined in the original PTB POS tag set ) .,0
11482,Corpus - level accuracy differences of at least 0.17 % in GENIA and 0.26 % in CRAFT between two POS tagging models are significant at p ? 0.05 .,0
11483,"Here , we compute sentence - level accuracies , then use paired t- test to measure the significance level .",0
11484,"Main results , when trained on 90 % of the GENIA corpus ( cf. our 85 % training set ) .",0
11485,It does not support a ( re ) - training process .,0
11486,Model,0
11487,POS tagging results,1
11488,"In general , we find that the six retrained models produce competitive results .",0
11489,"BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .",1
11490,jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .,1
11491,"In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .",1
11492,"NLP4J - POS uses additional features based on Brown clusters and pre-trained word vectors learned from a large external corpus , providing useful extra information .",0
11493,BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .,1
11494,"Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .",0
11495,Parsing results on the test set with predicted POS tags and gold tokenization ( except [ G ] which denotes results when employing gold POS tags in both training and testing phases ) .,0
11496,""" Without punctuation "" refers to results excluding punctuation and other symbols from evaluation .",0
11497,""" Exact match "" denotes the percentage of sentences whose predicted trees are entirely correct .",0
11498,"0.3 % absolute improvements to BiLSTM - CRF on GE - NIA and CRAFT , respectively , resulting in the highest accuracies on both experimental corpora .",0
11499,"Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .",1
11500,The larger improvements on GENIA and CRAFT show that character - level word embeddings are specifically useful to capture rare or unseen words in biomedical text data .,0
11501,"Character - level word embeddings are useful for morphologically rich languages , and although English is not morphologically rich , the biomedical domain contains a wide variety of morphological variants of domain - specific terminology .",0
11502,"Words tagged incorrectly are largely associated with gold tags NN , JJ and NNS ; many are abbreviations which are also out - of - vocabulary .",0
11503,It is typically difficult for character - level word embeddings to capture those unseen abbreviated words .,0
11504,"On both GENIA and CRAFT , BiLSTM - CRF with character - level word embeddings obtains the highest accuracy scores .",1
11505,These are just 0.1 % absolute higher than the accuracies of NLP4J - POS .,0
11506,Note that small variations in POS tagging performance are not a critical factor in parsing performance .,0
11507,"In addition , we find that NLP4J - POS obtains 30 - time faster training and testing speed .",0
11508,"Hence for the dependency parsing task , we use NLP4J - POS to perform 20 - way jackknifing to generate POS tags on training data and to predict POS tags on development and test sets .",0
11509,Overall dependency parsing results,1
11510,We present the LAS and UAS scores of different parsing models in .,0
11511,"The first five rows show parsing results on the GENIA test set of "" pre-trained "" parsers .",0
11512,"The first two rows present scores of the pre-trained Stanford NNdep and Biaffine v 1 models with POS tags predicted by the pre-trained Stanford tagger , while the next two rows 3 - 4 present scores of these pretrained models with POS tags predicted by NLP4J - POS .",0
11513,"Both pre-trained NNdep and Biaffine models were trained on a dependency treebank of 40K sentences , which was converted from the English PTB sections 2 - 21 .",0
11514,"The fifth row shows scores of BLLIP + Bio , the BLLIP reranking constituent parser with an improved self - trained biomedical parsing model .",0
11515,"We use the Stanford conversion toolkit ( v3.5.1 ) to generate dependency trees with the basic Stanford dependencies and use the data split on GENIA as used in , therefore parsing scores are comparable .",0
11516,The remaining rows show results of our retrained dependency parsing models .,0
11517,"On GENIA , among pre-trained models , BLLIP obtains highest results .",1
11518,"This model , unlike the other pretrained models , was trained using GENIA , so this result is unsurprising .",0
11519,The pre-trained Stanford - Biaffine ( v1 ) model produces lower scores than the pre-trained Stanford - NNdep model on GENIA .,1
11520,It is also unsurprising because the pre-trained Stanford - Biaffine utilizes pre-trained word vectors which were learned from newswire corpora .,0
11521,Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .,1
11522,"Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the",0
11523,Parsing result analysis,0
11524,Here we present a detailed analysis of the parsing results obtained by the retrained models with predicted POS tags .,0
11525,"For simplicity , the following more detailed analyses report LAS scores , computed without punctuation .",0
11526,Using UAS scores or computing with punctuation does not reveal any additional information .,0
11527,presents LAS scores by sentence length in bins of length 10 .,0
11528,"As expected , all parsers produce better results for shorter sentences on both corpora ; longer sentences are likely to have longer dependencies which are typically harder to predict precisely .",0
11529,"Scores drop by about 10 % for sentences longer than 50 words , relative to short sentences <= 10 words .",0
11530,"Exceptionally , on GENIA we find lower scores for the shortest sentences than for the sentences from 11 to 20 words .",0
11531,"This is probably because abstracts tend not to contain short sentences : ( i ) as shown in , the proportion of sentences in the first bin is very low at 3.5 % on GE - NIA ( cf. 17.8 % on CRAFT ) , and ( ii ) sentences in the first bin on GENIA are relatively long , with an average length of 9 words ( cf. 5 words in CRAFT ) .",0
11532,shows LAS ( F1 ) scores corresponding to the dependency distance i ?,0
11533,"j , between a dependent w i and it s head w j , where i and j are consecutive indices of words in a sentence .",0
11534,"Short dependencies are often modifiers of nouns such as determiners or adjectives or pronouns modifying their direct neighbors , while longer dependencies typically represent modifiers of the root or the main verb .",0
11535,All parsers obtain higher scores for left dependencies than for right dependencies .,0
11536,This is not completely unexpected as English is strongly head - initial .,0
11537,"In addition , the gaps between LSTM - based models ( i.e. Stanford - Biaffine and jPTDP ) and non -LSTM models ( i.e. NLP4J - dep and Stanford - NNdep ) are larger for the long dependencies than for the shorter ones , as LSTM architectures can preserve long range information .",0
11538,"On both corpora , higher scores are also associated with shorter distances .",0
11539,"There is one surprising exception : on GENIA , in distance bins of ? 4 , ? 5 and < ?5 , Stanford - Biaffine and jPTDP obtain higher scores for longer distances .",0
11540,This may result from the structural characteristics of sentences in the GENIA corpus .,0
11541,Table 5 details the scores of Stanford - Biaffine in terms of the most frequent dependency labels in these leftmost dependency bins .,0
11542,We find amod and nn are the two most difficult to predict dependency relations ( the same finding applied to jPTDP ) .,0
11543,"They appear much more frequently in the bins ? 4 and ? 5 than in bin < ?5 , explaining the higher over all score for bin < ?5 . present LAS scores for the most frequent dependency relation types on GENIA and CRAFT , respectively .",0
11544,"In most cases , Stanford - Biaffine obtains the highest score for each relation type on both corpora with the following exceptions : on GENIA , jPTDP gets the highest results to aux , dep and nn ( as well as nsubjpass ) , while NLP4J - dep and NNdep obtain the highest scores for auxpass and num , respectively .",0
11545,"On GENIA the labels associated with the highest average LAS scores ( generally > 90 % ) are amod , aux , auxpass , det , dobj , mark , nsubj , nsubjpass , pobj and root whereas on CRAFT they are NMOD , OBJ , PMOD , PRD , ROOT , SBJ , SUB and VC .",0
11546,"These labels either correspond to short dependencies ( e.g. aux , auxpass and VC ) , have strong lexical indications ( e.g. det , pobj and PMOD ) , or occur very often ( e.g. amod , subj , NMOD and SBJ ) .",0
11547,"Those relation types with the lowest LAS scores ( generally < 70 % ) are dep on GENIA and DEP , LOC , PRN and TMP on CRAFT ; dep / DEP are very general labels while LOC , PRN and TMP are among the least frequent labels .",0
11548,Those types also associate to the biggest variation of obtained accuracy across parsers ( > 8 % ) .,0
11549,"In addition , the coordination - related labels cc , conj / CONJ and COORD show large variation across parsers .",0
11550,These 9 mentioned relation labels generally correspond to long dependencies .,0
11551,"Therefore , it is not surprising that BiLSTM - based models Stanford - Biaffine and jPTDP can produce much higher accuracies on these labels than non -LSTM models NLP4J - dep and NNdep .",0
11552,Sentence length,0
11553,Dependency distance,0
11554,Dependency label,0
11555,"The remaining types are either relatively rare labels ( e.g. appos , num and AMOD ) or more frequent labels but with a varied distribution of dependency distances ( e.g. advmod , nn , and ADV ) .",0
11556,POS tag of the dependent analyzes the LAS scores by the most frequent POS tags ( across two corpora ) of the dependent .,0
11557,Stanford - Biaffine achieves the highest scores on all these tags except TO where the traditional featurebased model NLP4J - dep obtains the highest score ( TO is relatively rare tag in GENIA and is the least frequent tag in CRAFT among tags listed in ) .,0
11558,"Among listed tags VBG is the least and second least frequent one in GENIA and CRAFT , respectively , and generally associates to longer dependency distances .",0
11559,"So , it is reasonable that the lowest scores we obtain on both corpora are accounted for by VBG .",0
11560,"The coordinating conjunction tag CC also often corresponds to long dependencies , thus resulting in biggest ranges across parsers on both GENIA and CRAFT .",0
11561,The results for CC are consistent with the results obtained for the dependency labels cc in and COORD in because they are coupled to each other .,0
11562,"On the remaining POS tags , we generally find similar patterns across parsers and corpora , except for IN and VB where parsers produce 8 + % higher scores for IN on GENIA than on CRAFT , and vice versa producing 9 + % lower scores for VB on GENIA .",0
11563,"This is because on GENIA , IN is mostly coupled with the dependency label prep at a rate of 90 % ( thus their corresponding LAS scores in tables 8 and 6 are consistent ) , while on CRAFT IN is coupled to a more varied distribution of dependency labels such as ADV with a rate at 20 % , LOC at 14 % , NMOD at 40 % and TMP at 5 % .",0
11564,"Regarding VB , on CRAFT it usually associates to a short dependency distance of 1 word ( i.e. head and dependent words are next to each other ) with a rate at 80 % , and to a distance of 2 words at 15 % , while on GENIA it associates with longer dependency distances with a rate at 17 % for the distance of 1 word , 31 % for the distance of 2 words and 34 % for a distance of > 5 words .",0
11565,"So , parsers obtain much higher scores for VB on CRAFT than on GENIA .",0
11566,Error analysis,0
11567,"We analyze token - level parsing errors that occur consistently across all parsers ( i.e. the intersection set of errors ) , and find that there are few common error patterns .",0
11568,The first one is related to incorrect POS tag prediction ( 8 % of the intersected parsing errors on GENIA and 12 % on CRAFT are coupled with incorrect predicted POS tags ) .,0
11569,"For example , the word token "" domains "" is the head of the phrase "" both the POU ( S ) and POU ( H ) domains "" in .",0
11570,"We also have two OOV word tokens "" POU ( S ) "" and "" POU ( H ) "" which abbreviate "" POU-specific "" and "" POU homeodomain "" , respectively .",0
11571,"NLP4J - POS ( as well as all other POS taggers ) produced an incorrect tag of NN rather than adjective ( JJ ) for "" POU ( S ) "" .",0
11572,"As "" POU ( S ) "" is predicted to be a noun , all parsers make an incorrect prediction that it is the phrasal head , thus also resulting in errors to remaining dependent words in the phrase .",0
11573,"The second error type occurs on noun phrases such as "" the Oct -1 - responsive octamer sequence AT - GCAAAT "" ( in ) and "" the herpes simplex virus Oct - 1 coregulator VP16 "" , commonly referred to as appositive structures , where the second to last noun ( i.e. "" sequence "" and "" coregulator "" ) is considered to be the phrasal head , rather than the last noun .",0
11574,"However , such phrases are relatively rare and all parsers predict the last noun as the head .",0
11575,The third error type is related to the relation labels dep / DEP .,0
11576,"We manually re-annotate every case where all parsers agree on the dependency label for a dependency arc with the same dependency label , where this label dis agrees with the gold label dep / DEP ( these cases are about 3.5 % of the parsing errors intersected across all parsers on GENIA and 0.5 % on CRAFT ) .",0
11577,"Based on this manual review , we find that about 80 % of these cases appear to be labelled correctly , despite not agreeing with the gold standard .",0
11578,"In other words , the gold standard appears to be in error in these cases .",0
11579,"This result is not completely unexpected because when converting from constituent treebanks to dependency treebanks , the general dependency label dep / DEP is usually assigned due to limitations in the automatic conversion toolkit .",0
11580,Parser comparison on event extraction,0
11581,We present an extrinsic evaluation of the four dependency parsers for the downstream task of biomedical event extraction .,0
11582,Evaluation setup,0
11583,"Previously , Miwa et al.",0
11584,"adopted the BioNLP 2009 shared task on biomedical event extraction to compare the task - oriented performance of six "" pretrained "" parsers with 3 different types of dependency representations .",0
11585,"However , their evaluation setup requires use of a currently unavailable event extraction system .",0
11586,"Fortunately , the extrinsic parser evaluation ( EPE 2017 ) shared task aimed to evaluate different dependency representations by comparing their performance on downstream tasks , including a biomedical event extraction task .",0
11587,"We thus follow the experimental setup used there ; employing the Turku Event Extraction System ( TEES , ) to assess the impact of parser differences on biomedical relation extraction .",0
11588,"EPE 2017 uses the BioNLP 2009 shared task dataset , which was derived from the GENIA treebank corpus ( 800 , 150 and 260 abstract files used for BioNLP 2009 training , development and test , respectively ) .",0
11589,We only need to provide dependency parses of raw texts using the pre-processed tokenized and sentencesegmented data provided by the EPE 2017 shared task .,0
11590,"For the Stanford - Biaffine , NLP4 J - dep and Stanford - NNdep parsers that require predicted POS tags , we use the retrained NLP4J - POS model to generate POS https://github.com/jbjorne/TEES/wiki/EPE-2017 678 of 800 training , 132 of 150 development and 248 of 260 test files are included in the GENIA treebank training set .",0
11591,tags .,0
11592,We then produce parses using retrained dependency parsing models .,0
11593,"TEES is then trained for the BioNLP 2009 Task 1 using the training data , and is evaluated on the development data ( gold event annotations are only available to public for training and development sets ) .",0
11594,"To obtain test set performance , we use an online evaluation system .",0
11595,The online evaluation system for the BioNLP 2009 shared task is currently not available .,0
11596,"Therefore , we employ the online evaluation system for the BioNLP 2011 shared task with the "" abstracts only "" option .",0
11597,The score is reported using the approximate span & recursive evaluation strategy .,0
11598,"presents the intrinsic UAS and LAS ( F1 ) scores on the pre-processed segmented BioNLP 2009 development sentences ( i.e. scores with respect to predicted segmentation ) , for which these sentences contain event interactions .",0
11599,These scores are higher than those presented in because most part of the BioNLP 2009 dataset is extracted from the GENIA treebank training set .,0
11600,"Although gold event annotations in the BioNLP 2009 test set are not available to public , it is likely that we would obtain the similar intrinsic UAS and LAS scores on the pre-processed segmented test sentences containing event interactions .",0
11601,compares parsers with respect to the EPE 2017 biomedical event extraction task .,0
11602,The first row presents the score of the Stanford&Paris team ; the highest official score obtained on the test set .,0
11603,"Their system used the Stanford - Biaffine parser ( v2 ) trained on a dataset combining PTB , Brown corpus , and GE - NIA treebank data .",0
11604,The second row presents our score for the pre-trained BLLIP +,0
11605,Bio model ; remaining rows show scores using re-trained parsing models .,0
11606,Impact of parsing on event extraction,0
11607,"The results for parsers trained with the GENIA treebank ( Rows 1 - 6 , ) are generally higher than http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi",0
11608,"The EPE 2017 shared task focused on evaluating different dependency representations in downstream tasks , not on comparing different parsers .",0
11609,"Therefore each participating team employed only one parser , either a dependency graph or tree parser .",0
11610,"Only the Stanford&Paris team employ GENIA data , obtaining the highest biomedical event extraction score . for parsers trained on CRAFT .",0
11611,This is logical because the BioNLP 2009 shared task dataset was a subset of the GENIA corpus .,0
11612,"However , we find that the differences in intrinsic parsing results as presented in tables 4 and 10 do not consistently explain the differences in extrinsic biomedical event extraction performance , extending preliminary related observations in prior work .",0
11613,"Among the four dependency parsers trained on GENIA , Stanford - Biaffine , jPTDP and NLP4J - dep produce similar event extraction scores on the development set , while on the the test set jPTDP and NLP4 Jdep obtain the lowest and highest scores , respectively .",0
11614,"also summarizes the results with the dependency structures only ( i.e. results without dependency relation labels ; replacing all predicted dependency labels by "" UNK "" before training TEES ) .",0
11615,"In most cases , compared to using dependency labels , event extraction scores drop on the development set ( except NLP4J - dep trained on CRAFT ) , while they increase on the test set ( except NLP4J - dep trained on GENIA and Stanford - NNdep trained on CRAFT ) .",0
11616,"Without dependency labels , better event extraction scores on the development set corresponds to better scores on the test set .",0
11617,"In addition , the differences in these event extraction scores without dependency labels are more consistent with the parsing performance differences than the scores with dependency labels .",0
11618,These findings show that variations in dependency representations strongly affect event extraction performance .,0
11619,"Some ( predicted ) dependency labels are likely to be particularly useful for extracting events , while others hurt performance .",0
11620,"Also , investigating ?",0
11621,20 frequent dependency labels in each dataset as well as some possible combinations between them could lead to an enormous number of additional experiments .,0
11622,We believe a detailed analysis of the interaction between those labels in a downstream application task deserves another research paper with a more careful analysis .,0
11623,"Here , one contribution of our paper could be seen to be that we highlight the need for further research in this direction .",0
11624,Conclusion,0
11625,We have presented a detailed empirical study comparing SOTA traditional feature - based and neural network - based models for POS tagging and dependency parsing in the biomedical context .,0
11626,"In general , the neural models outperform the feature - based models on two benchmark biomedical corpora GENIA and CRAFT .",0
11627,"In particular , BiLSTM - CRF - based models with character - level word embeddings produce highest POS tagging accuracies which are slightly better than NLP4J - POS , while the Stanford - Biaffine parsing model obtains significantly better result than other parsing models .",0
11628,"We also investigate the influence of parser selection for a biomedical event extraction downstream task , and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance .",0
11629,Whether this pattern holds for other information extraction tasks is left as future work .,0
11630,title,0
11631,Stack - Pointer Networks for Dependency Parsing,1
11632,abstract,0
11633,We introduce a novel architecture for dependency parsing : stack - pointer networks ( STACKPTR ) .,0
11634,"Combining pointer networks ( Vinyals et al. , 2015 ) with an internal stack , the proposed model first reads and encodes the whole sentence , then builds the dependency tree top - down ( from root - to - leaf ) in a depth - first fashion .",0
11635,The stack tracks the status of the depthfirst search and the pointer networks select one child for the word at the top of the stack at each step .,0
11636,"The STACKPTR parser benefits from the information of the whole sentence and all previously derived subtree structures , and removes the leftto - right restriction in classical transitionbased parsers .",0
11637,"Yet , the number of steps for building any ( including non-projective ) parse tree is linear in the length of the sentence just as other transition - based parsers , yielding an efficient decoding algorithm with O ( n 2 ) time complexity .",0
11638,"We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas , and achieve state - of - theart performance on 21 of them .",0
11639,Introduction,0
11640,"Dependency parsing , which predicts the existence and type of linguistic dependency relations between words , is a first step towards deep language understanding .",0
11641,"Its importance is widely recognized in the natural language processing ( NLP ) community , with it benefiting a wide range of NLP applications , such as coreference resolution Work done while at Carnegie Mellon University .",0
11642,"2016 ) , sentiment analysis , machine translation , information extraction , word sense dis ambiguation , and low - resource languages processing .",0
11643,"There are two dominant approaches to dependency parsing : local and greedy transitionbased algorithms , and the globally optimized graph - based algorithms .",0
11644,Transition - based dependency parsers read words sequentially ( commonly from left - to - right ) and build dependency trees incrementally by making series of multiple choice decisions .,0
11645,The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence .,0
11646,"The challenge , however , is that the decision made at each step is based on local information , leading to error propagation and worse performance compared to graph - based parsers on root and long dependencies .",0
11647,Previous studies have explored solutions to address this challenge .,0
11648,Stack LSTMs are capable of learning representations of the parser state thatare sensitive to the complete contents of the parser 's state .,0
11649,proposed a globally normalized transition model to replace the locally normalized classifier .,0
11650,"However , the parsing accuracy is still behind state - of - the - art graph - based parsers .",0
11651,"Graph - based dependency parsers , on the other hand , learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring tree .",0
11652,"Incorporating this global search algorithm with distributed representations learned from neural networks , neural graph - based parsers have achieved the state - of - the - art accuracies on a number of treebanks in different languages .",0
11653,"Nevertheless , these models , while accurate , are usually slow ( e.g. decoding is O ( n 3 ) time complexity for first - order models and higher polynomials for higherorder models ) .",0
11654,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .",1
11655,"STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .",1
11656,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .",1
11657,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .",1
11658,"This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .",0
11659,"We evaluate our parser on 29 treebanks across 20 languages and different dependency annotation schemas , and achieve state - of - the - art performance on 21 of them .",0
11660,The contributions of this work are summarized as follows :,0
11661,"( i ) We propose a neural network architecture for dependency parsing that is simple , effective , and efficient .",0
11662,( ii ) Empirical evaluations on benchmark datasets over 20 languages show that our method achieves state - of - the - art performance on 21 different treebanks 1 . ( iii ) Comprehensive error analysis is conducted to compare the proposed method to a strong graph - based baseline using biaffine attention .,0
11663,Background,0
11664,"We first briefly describe the task of dependency parsing , setup the notation , and review Pointer Networks .",0
11665,Dependency Parsing and Notations,0
11666,Dependency trees represent syntactic relationships between words in the sentences through labeled directed edges between head words and their dependents .,0
11667,"shows a dependency tree for the sentence , "" But there were no buyers "" .",0
11668,"In this paper , we will use the following notation :",0
11669,"Input : x = {w 1 , . . . , w n } represents a generic sentence , where w i is the ith word .",0
11670,"Output : y = {p 1 , p 2 , , pk } represents a generic ( possibly non-projective ) dependency tree , where each path pi = $ , w i , 1 , w i , 2 , , w i , l i is a sequence of words from the root to a leaf .",0
11671,""" $ "" is an universal virtual root that is added to each tree .",0
11672,Stack : ?,0
11673,"denotes a stack configuration , which is a sequence of words .",0
11674,We use ?|w to represent a stack configuration that pushes word w into the stack ?.,0
11675,Children : ch ( w i ) denotes the list of all the children ( modifiers ) of word w i .,0
11676,Pointer Networks,0
11677,Pointer Networks ( PTR - NET ) area variety of neural network capable of learning the conditional probability of an output sequence with elements thatare discrete tokens corresponding to positions in an input sequence .,0
11678,This model can not be trivially expressed by standard sequence - to - sequence networks due to the variable number of input positions in each sentence .,0
11679,PTR - NET solves the problem by using attention as a pointer to select a member of the input sequence as the output .,0
11680,"Formally , the words of the sentence x are fed one - by - one into the encoder ( a multiple - layer bidirectional RNN ) , producing a sequence of encoder hidden states s i .",0
11681,"At each time step t , the decoder ( a uni-directional RNN ) receives the input from last step and outputs decoder hidden state ht .",0
11682,The attention vector at is calculated as follows :,0
11683,"where score ( , ) is the attention scoring function , which has several variations such as dot -product , concatenation , and biaffine .",0
11684,"PTR - NET regards the attention vector at as a probability distribution over the source words , i.e. it uses at i as pointers to select the input elements .",0
11685,3 Stack - Pointer Networks,0
11686,Overview,0
11687,"Similarly to PTR - NET , STACKPTR first reads the whole sentence and encodes each word into the encoder hidden state s i .",0
11688,The internal stack ?,0
11689,is always initialized with the root symbol $ .,0
11690,"At each time step t , the decoder receives the input vector corresponding to the top element of the stack ?",0
11691,"( the headword w p where p is the word index ) , generates the hidden state ht , and computes the attention vector at using Eq. ( 1 ) .",0
11692,"The parser chooses a specific position c according to the attention scores in at to generate a new dependency arc ( w h , w c ) by selecting w c as a child of w h .",0
11693,"Then the parser pushes w c onto the stack , i.e. ? ? ?|w c , and goes to the next step .",0
11694,"At one step if the parser points w h to itself , i.e. c = h , it indicates that all children of the headword w h have already been selected .",0
11695,Then the parser goes to the next step by popping w h out of ?.,0
11696,"At test time , in order to guarantee a valid dependency tree containing all the words in the input sentences exactly once , the decoder maintains a list of "" available "" words .",0
11697,"At each decoding step , the parser selects a child for the current headword , and removes the child from the list of available words to make sure that it can not be selected as a child of other head words .",0
11698,"For head words with multiple children , it is possible that there is more than one valid selection for each time step .",0
11699,"In order to define a deterministic decoding process to make sure that there is only one ground - truth choice at each step ( which is necessary for simple maximum likelihood estimation ) , a predefined order for each ch ( w i ) needs to be introduced .",0
11700,"The predefined order of children can have different alternatives , such as leftto - right or inside - out 2 .",0
11701,"In this paper , we adopt the inside - out order 3 since it enables us to utilize second - order sibling information , which has been proven beneficial for parsing performance ( see 3.4 for details ) .",0
11702,( b ) depicts the architecture of STACKPTR and the decoding procedure for the example sentence in ( a ) .,0
11703,Encoder,0
11704,The encoder of our parsing model is based on the bi-directional LSTM - CNN architecture ( BLSTM - CNNs ) where CNNs encode character - level information of a word into its character - level repre-sentation and BLSTM models context information of each word .,0
11705,"Formally , for each word , the CNN , with character embeddings as inputs , encodes the character - level representation .",0
11706,Then the character - level representation vector is concatenated with the word embedding vector to feed into the BLSTM network .,0
11707,"To enrich word - level information , we also use POS embeddings .",0
11708,"Finally , the encoder outputs a sequence of hidden states s i .",0
11709,Decoder,0
11710,The decoder for our parser is a uni-directional LSTM .,0
11711,"Different from previous work which uses word embeddings of the previous word as the input to the decoder , our decoder receives the encoder hidden state vector ( s i ) of the top element in the stack ? ( see ( b ) ) .",0
11712,"Compared to word embeddings , the encoder hidden states contain more contextual information , benefiting both the training and decoding procedures .",0
11713,"The decoder produces a sequence of decoder hidden states hi , one for each decoding step .",0
11714,Higher - order Information,0
11715,"As mentioned before , our parser is capable of utilizing higher - order information .",0
11716,"In this paper , we incorporate two kinds of higher - order structures grandparent and sibling .",0
11717,"A sibling structure is ahead word with two successive modifiers , and a grandparent structure is a pair of dependencies connected head - to - tail : 0 12 3 456 2782 96 56 986 2 5214 3 77543 9 5 2 52",0
11718,cc c d efgfhefhij kl mn l hoop qheirl n e s pl t kl mn l hoop qhet kl mn l ho,0
11719,"To utilize higher - order information , the decoder 's input at each step is the sum of the encoder hidden states of three words :",0
11720,where ?,0
11721,"t is the input vector of decoder at time t and h , g , s are the indices of the headword and its grandparent and sibling , respectively .",0
11722,illustrates the details .,0
11723,"Here we use the element - wise sum operation instead of concatenation because it does not increase the dimension of the input vector ? t , thus introducing no additional model parameters .",0
11724,Biaffine Attention Mechanism,0
11725,"For attention score function ( Eq. ( 1 ) ) , we adopt the biaffine attention mechanism :",0
11726,"where W , U , V , bare parameters , denoting the weight matrix of the bi-linear term , the two weight vectors of the linear terms , and the bias vector .",0
11727,"As discussed in , applying a multilayer perceptron ( MLP ) to the output vectors of the BLSTM before the score function can both reduce the dimensionality and overfitting of the model .",0
11728,We follow this work by using a one - layer perceptron to s i and hi with elu as its activation function .,0
11729,"Similarly , the dependency label classifier also uses a biaffine function to score each label , given the headword vector ht and child vector s i as inputs .",0
11730,"Again , we use MLPs to transform ht and s i before feeding them into the classifier .",0
11731,Training Objectives,0
11732,"The STACKPTR parser is trained to optimize the probability of the dependency trees given sentences : P ? ( y|x ) , which can be factorized as :",0
11733,where ?,0
11734,represents model parameters .,0
11735,p < i denotes the preceding paths that have already been generated .,0
11736,"c i , j represents the jth word in pi and c i , <j denotes all the proceeding words on the path pi .",0
11737,"Thus , the STACKPTR parser is an autoregressive model , like sequence - to - sequence models , but it factors the distribution according to a top - down tree structure as opposed to a left - to - right chain .",0
11738,"We define P ? ( c i , j | c i , < j , p < i , x ) = at , where attention vector at ( of dimension n) is used as the distribution over the indices of words in a sentence .",0
11739,Arc Prediction,0
11740,"Our parser is trained by optimizing the conditional likelihood in Eq ( 2 ) , which is implemented as the cross - entropy loss .",0
11741,Label Prediction,0
11742,We train a separated multiclass classifier in parallel to predict the dependency labels .,0
11743,"Following , the classifier takes the information of the headword and its child as features .",0
11744,The label classifier is trained simultaneously with the parser by optimizing the sum of their objectives .,0
11745,Discussion,0
11746,Time Complexity .,0
11747,"The number of decoding steps to build a parse tree for a sentence of length n is 2 n ? 1 , linear inn .",0
11748,"Together with the attention mechanism ( at each step , we need to compute the attention vector at , whose runtime is O ( n ) ) , the time complexity of decoding algorithm is O ( n 2 ) , which is more efficient than graph - based parsers that have O ( n 3 ) or worse complexity when using dynamic programming or maximum spanning tree ( MST ) decoding algorithms .",0
11749,Top - down Parsing .,0
11750,"When humans comprehend a natural language sentence , they arguably do it in an incremental , left - to - right manner .",0
11751,"However , when humans consciously annotate a sentence with syntactic structure , they rarely ever process in fixed left - to - right order .",0
11752,"Rather , they start by reading the whole sentence , then seeking the main predicates , jumping back - and - forth over the sentence and recursively proceeding to the subtree structures governed by certain head words .",0
11753,"Our parser follows a similar kind of annotation process : starting from reading the whole sentence , and processing in a top - down manner by finding the main predicates first and only then search for sub -trees governed by them .",0
11754,"When making latter decisions , the parser has access to the entire structure builtin earlier steps .",0
11755,Implementation Details,0
11756,Pre-trained Word Embeddings .,0
11757,"For all the parsing models in different languages , we initialize word vectors with pretrained word embeddings .",0
11758,"For Chinese , Dutch , English , German and Spanish , we use the structured - skipgram embeddings .",0
11759,For other languages we use Polyglot embeddings .,0
11760,Optimization .,0
11761,Parameter optimization is performed with the Adam optimizer with ? 1 = ? 2 = 0.9 . We choose an initial learning rate of ? 0 = 0.001 .,0
11762,The learning rate ?,0
11763,is annealed by multiplying a fixed decay rate ? =,0
11764,0.75 when parsing performance stops increasing on validation sets .,0
11765,"To reduce the effects of "" gradient exploding "" , we use gradient clipping of 5.0 .",0
11766,Dropout Training .,0
11767,"To mitigate overfitting , we apply dropout .",0
11768,"For BLSTM , we use recurrent dropout with a drop rate of 0.33 between hidden states and 0.33 between layers .",0
11769,"Following , we also use embedding dropout with a rate of 0.33 on all word , character , and POS embeddings .",0
11770,Hyper-Parameters .,0
11771,Some parameters are chosen from those reported in .,0
11772,"We use the same hyper - parameters across the models on different treebanks and languages , due to time constraints .",0
11773,The details of the chosen hyper - parameters for all experiments are summarized in Appendix A.,0
11774,Experiments,0
11775,Setup,0
11776,"We evaluate our STACKPTR parser mainly on three treebanks : the English Penn Treebank ( PTB version 3.0 ) , the Penn Chinese Treebank ( CTB version 5.1 ) , and the German CoNLL 2009 corpus .",0
11777,We use the same experimental settings as .,0
11778,"To make a thorough empirical comparison with previous studies , we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency ( UD )",0
11779,"Treebanks 4 . For the CoNLL Treebanks , we use the English treebank from CoNLL - 2008 shared task and all 13 treebanks from CoNLL - 2006 shared task .",0
11780,The experimental settings are the same as .,0
11781,"For UD Treebanks , we select 12 languages .",0
11782,The details of the treebanks and experimental settings are in 4.5 and Appendix B.,0
11783,"Evaluation Metrics Parsing performance is measured with five metrics : unlabeled attachment score ( UAS ) , labeled attachment score ( LAS ) , unlabeled complete match ( UCM ) , labeled complete match ( LCM ) , and root accuracy ( RA ) .",0
11784,"Following previous work , we report results excluding punctuations for Chinese and English .",0
11785,"For each experiment , we report the mean values with corresponding standard deviations over 5 repetitions .",0
11786,Baseline,0
11787,"For fair comparison of the parsing performance , we re-implemented the graph - based Deep Biaffine ( BIAF ) parser , which achieved state - of - the - art results on a wide range of languages .",1
11788,"Our re-implementation adds character - level information using the same LSTM - CNN encoder as our model ( 3.2 ) to the original BIAF model , which boosts its performance on all languages .",0
11789,Main Results,1
11790,We first conduct experiments to demonstrate the effectiveness of our neural architecture by comparing with the strong baseline BIAF .,0
11791,"We compare the performance of four variations of our model with different decoder inputs - Org , + gpar , + sib and Full - where the Org model utilizes only the encoder hidden states of head words , while the + gpar and + sib models augments the original one with grandparent and sibling information , respectively .",0
11792,The Full model includes all the three information as inputs .,0
11793,"illustrates the performance ( five metrics ) of different variations of our STACKPTR parser together with the results of baseline BIAF re-implemented by us , on the test sets of the three languages .",0
11794,"On UAS and LAS , the Full variation of STACKPTR with decoding beam size 10 outperforms BIAF on Chinese , and obtains competitive performance on English and German .",1
11795,"An interesting observation is that the Full model achieves the best accuracy on English and Chinese , while performs slightly worse than + sib on German .",1
11796,This shows that the importance of higher - order information varies in languages .,0
11797,"On LCM and UCM , STACKPTR significantly outperforms BIAF on all languages , showing the superiority of our parser on complete sentence parsing .",1
11798,The results of our parser on RA are slightly worse than BIAF .,1
11799,"More details of results are provided in Appendix C. illustrates the UAS and LAS of the four versions of our model ( with decoding beam size 10 ) on the three treebanks , together with previous top - performing systems for comparison .",0
11800,Note that the results of STACKPTR and our reimplementation of BIAF are the average of 5 repetitions instead of a single run .,0
11801,"Our Full model significantly outperforms all the transition - based parsers on all three languages , and achieves better results than most graph - based parsers .",0
11802,Our and our re-implementation .,0
11803,"For STACKPTR and our re-implementation of BiAF , we report the average over 5 runs .",0
11804,"re-implementation of BIAF obtains better performance than the original one in , demonstrating the effectiveness of the character - level information .",1
11805,"Our model achieves state - of - the - art performance on both UAS and LAS on Chinese , and best UAS on English .",1
11806,"On German , the performance is competitive with BIAF , and significantly better than other models .",1
11807,Comparison with Previous Work,0
11808,Error Analysis,0
11809,"In this section , we characterize the errors made by BIAF and STACKPTR by presenting a number of experiments that relate parsing errors to a set of linguistic and structural properties .",0
11810,"For simplicity , we follow and report labeled parsing metrics ( either accuracy , precision , or recall ) for all experiments .",0
11811,Length and Graph Factors,0
11812,"Following McDonald and Nivre ( 2011 ) , we analyze parsing errors related to structural factors .",0
11813,Sentence Length .,0
11814,shows the accuracy of both parsing models relative to sentence lengths .,0
11815,"Consistent with the analysis in , STACKPTR tends to perform better on shorter sentences , which make fewer parsing decisions , significantly reducing the chance of error propagation .",0
11816,Dependency Length .,0
11817,measures the precision and recall relative to dependency lengths .,0
11818,"While the graph - based BIAF parser still performs better for longer dependency arcs and transition - based STACKPTR parser does better for shorter ones , the gap between the two systems is marginal , much smaller than that shown : UAS and LAS on 14 treebanks from CoNLL shared tasks , together with several state - of - the - art parsers .",0
11819,"Bi - Att is the bi-directional attention based parser , and Neuro MST is the neural MST parser . in .",0
11820,"One possible reason is that , unlike traditional transition - based parsers that scan the sentence from left to right , STACKPTR processes in a top - down manner , thus sometimes unnecessarily creating shorter dependency arcs first .",0
11821,Root Distance .,0
11822,plots the precision and recall of each system for arcs of varying distance to the root .,0
11823,"Different from the observation in McDonald and Nivre ( 2011 ) , STACKPTR does not show an obvious advantage on the precision for arcs further away from the root .",0
11824,"Furthermore , the STACKPTR parser does not have the tendency to over-predict root modifiers reported in .",0
11825,This behavior can be explained using the same reasoning as above : the fact that arcs further away from the root are usually constructed early in the parsing algorithm of traditional transition - based parsers is not true for the STACKPTR parser .,0
11826,Effect of POS,0
11827,Embedding,0
11828,The only prerequisite information that our parsing model relies on is POS tags .,0
11829,"With the goal of achieving an end - to - end parser , we explore the effect of POS tags on parsing performance .",0
11830,"We run experiments on PTB using our STACKPTR parser with gold - standard and predicted POS tags , and without tags , respectively .",0
11831,STACKPTR in these experiments is the Full model with beam = 10 . gives results of the parsers with different versions of POS tags on the test data of PTB .,0
11832,"The parser with gold - standard POS tags significantly outperforms the other two parsers , showing that dependency parsers can still benefit from accurate POS information .",0
11833,"The parser with predicted ( imperfect ) POS tags , however , performs even slightly worse than the parser without using POS tags .",0
11834,"It illustrates that an end - to - end parser that does n't rely on POS information can obtain competitive ( or even better ) performance than parsers using imperfect predicted POS tags , even if the POS tagger is relative high accuracy ( accuracy > 97 % in this experiment on PTB ) .",0
11835,"summarizes the parsing results of our model on the test sets of 14 treebanks from the CoNLL shared task , along with the state - of - theart baselines .",0
11836,"Along with BIAF , we also list the performance of the bi-directional attention based Parser ( Bi - Att ) and the neural MST parser ( NeuroMST ) for comparison .",0
11837,"Our parser achieves state - of - theart performance on both UAS and LAS on eight languages - Arabic , Czech , English , German , Portuguese , Slovene , Spanish , and Swedish .",0
11838,"On Bulgarian and Dutch , our parser obtains the best UAS .",0
11839,"On other languages , the performance of our parser is competitive with BIAF , and significantly better than others .",0
11840,"The only exception is Japanese , on which NeuroMST obtains the best scores .",0
11841,Experiments on Other Treebanks,0
11842,CoNLL,0
11843,Treebanks,0
11844,UD Treebanks,0
11845,For UD,0
11846,"Treebanks , we select 12 languages - Bulgarian , Catalan , Czech , Dutch , English , French , German , Italian , Norwegian , Romanian , Russian and Spanish .",0
11847,"For all the languages , we adopt the standard training / dev / test splits , and use the universal POS tags provided in each treebank .",0
11848,"The statistics of these corpora are provided in Appendix B. summarizes the results of the STACKPTR parser , along with BIAF for comparison , on both the development and test datasets for each language .",0
11849,"First , both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages - all with UAS are higher than 90 % .",0
11850,"On nine languages - Catalan , Czech , Dutch , English , French , German , Norwegian , Russian and Spanish - STACKPTR outperforms BIAF for both UAS and LAS .",0
11851,"On Bulgarian , STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF .",0
11852,"On Italian and Romanian , BIAF obtains marginally better parsing performance than STACKPTR .",0
11853,Conclusion,0
11854,"In this paper , we proposed STACKPTR , a transition - based neural network architecture , for dependency parsing .",0
11855,"Combining pointer networks with an internal stack to track the status of the top - down , depth - first search in the decoding procedure , the STACKPTR parser is able to capture information from the whole sentence and all the previously derived subtrees , removing the leftto - right restriction in classical transition - based parsers , while maintaining linear parsing steps , w.r.t the length of the sentences .",0
11856,"Experimental re-sults on 29 treebanks show the effectiveness of our parser across 20 languages , by achieving state - of the - art performance on 21 corpora .",0
11857,There are several potential directions for future work .,0
11858,"First , we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively .",0
11859,"Another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words , instead of using a predefined fixed order .",0
11860,summarizes the chosen hyper - parameters used for all the experiments in this paper .,0
11861,Some parameters are chosen directly or similarly from those reported in .,0
11862,"We use the same hyper - parameters across the models on different treebanks and languages , due to time constraints .",0
11863,Layer,0
11864,Hyper,0
11865,title,0
11866,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,1
11867,abstract,0
11868,We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) .,0
11869,"Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors .",0
11870,"The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing .",0
11871,We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .,0
11872,"The resulting parsers have very simple architectures , and match or surpass the state - of - the - art accuracies on English and Chinese .",0
11873,Introduction,0
11874,"The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( "" deep learning "" ) literature .",0
11875,Modern approaches to dependency parsing can be broadly categorized into graph - based and transition - based parsers ) .,0
11876,Graph - based parsers treat parsing as a search - based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees .,0
11877,"Transition - based parsers treat parsing as a sequence of actions that produce a parse tree , and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process .",0
11878,"Perhaps the simplest graph - based parsers are arc-factored ( first order ) models , in which the scoring function for a tree decomposes over the individual arcs of the tree .",0
11879,"More elaborate models look at larger ( overlapping ) parts , requiring more sophisticated inference and training algorithms .",0
11880,"The basic transition - based parsers work in a greedy manner , performing a series of locally - optimal decisions , and boast very fast parsing speeds .",0
11881,More advanced transition - based parsers introduce some search into the process using a beam or dynamic programming .,0
11882,"Regardless of the details of the parsing framework being used , a crucial step in parser design is choosing the right feature function for the underlying statistical model .",0
11883,"Recent work ( see Section 2.2 for an overview ) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models , enabling the modeler to focus on a small set of "" core "" features and leaving it up to the machine - learning machinery to come up with good feature combinations .",0
11884,"However , the need to carefully define a set of core features remains .",0
11885,"For example , the work of uses 18 different elements in its feature function , while the work of uses 21 different elements .",0
11886,"Other works , notably and , propose more sophisticated feature representations , in which the feature engineering is replaced with architecture engineering .",0
11887,"In this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering .",0
11888,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .",1
11889,"The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .",0
11890,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .",1
11891,"Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .",1
11892,"If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box , our proposal results in a pleasingly simple feature extractor .",0
11893,"We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .",1
11894,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .",1
11895,"To the best of our knowledge , we are the first to perform such end - to - end training of a structured prediction model and a recurrent feature extractor for non-sequential outputs .",0
11896,"Aside from the novelty of the BiLSTM feature extractor and the end - to - end structured training , we rely on existing models and techniques from the parsing and structured prediction literature .",0
11897,"We stick to the simplest parsers in each categorygreedy inference for the transition - based architecture , and a first - order , arc -factored model for the graph - based architecture .",0
11898,"Despite the simplicity of the parsing architectures and the feature functions , we achieve near state - of - the - art parsing accuracies in both English ( 93.1 UAS ) and Chinese ( 86.6 UAS ) , using a first - order parser with two features and while training solely on Treebank data , without relying on semi-supervised signals such as pre-trained word embeddings , word - clusters , or tech - 1 Structured training of sequence tagging models over RNNbased representations was explored by and niques such as tri-training .",0
11899,"When also including pre-trained word embeddings , we obtain further improvements , with accuracies of 93.9 UAS ( English ) and 87.6 UAS ( Chinese ) for a greedy transition - based parser with 11 features , and 93.6 UAS ( En ) / 87.4 ( Ch ) for a greedy transitionbased parser with 4 features .",0
11900,Background and Notation,0
11901,Notation,0
11902,"We use x 1:n to denote a sequence of n vectors x 1 , , x n . F ? ( ) is a function parameterized with parameters ?.",0
11903,We write FL ( ) as shorthand for F ?,0
11904,L - an instantiation of F with a specific set of parameters ?,0
11905,L .,0
11906,"We use to denote a vector concatenation operation , and v[i ] to denote an indexing operation taking the ith element of a vector v.",0
11907,Feature Functions in Dependency Parsing,0
11908,"Traditionally , state - of - the - art parsers rely on linear models over hand - crafted feature functions .",0
11909,"The feature functions look at core components ( e.g. "" word on top of stack "" , "" leftmost child of the second - totop word on the stack "" , "" distance between the head and the modifier words "" ) , and are comprised of several templates , where each template instantiates a binary indicator function over a conjunction of core elements ( resulting in features of the form "" word on top of stack is X and leftmost child is Y and . . . "" ) .",0
11910,The design of the feature function - which components to consider and which combinations of components to include - is a major challenge in parser design .,0
11911,"Once a good feature function is proposed in a paper it is usually adopted in later works , and sometimes tweaked to improve performance .",0
11912,"Examples of good feature functions are the feature - set proposed by for transitionbased parsing ( including roughly 20 core components and 72 feature templates ) , and the featureset proposed by for graphbased parsing , with the paper listing 18 templates for a first - order parser , while the first order featureextractor in the actual implementation 's code ( MST - Parser 2 ) includes roughly a hundred feature templates .",0
11913,"The core features in a transition - based parser usually look at information such as the word - identity and part - of - speech ( POS ) tags of a fixed number of words on top of the stack , a fixed number of words on the top of the buffer , the modifiers ( usually leftmost and right - most ) of items on the stack and on the buffer , the number of modifiers of these elements , parents of words on the stack , and the length of the spans spanned by the words on the stack .",0
11914,"The core features of a first - order graph - based parser usually take into account the word and POS of the head and modifier items , as well as POS - tags of the items around the head and modifier , POS tags of items between the head and modifier , and the distance and direction between the head and modifier .",0
11915,Related Research Efforts,0
11916,"Coming up with a good feature - set for a parser is a hard and time consuming task , and many researchers attempt to reduce the required manual effort .",0
11917,The work of suggests a low - rank tensor representation to automatically find good feature combinations .,0
11918,suggest a kernel - based approach to implicitly consider all possible feature combinations over sets of core-features .,0
11919,"The recent popularity of neural networks prompted a move from templates of sparse , binary indicator features to dense core feature encodings fed into non-linear classifiers .",0
11920,"encode each core feature of a greedy transition - based parser as a dense low - dimensional vector , and the vectors are then concatenated and fed into a nonlinear classifier ( multi - layer perceptron ) which can potentially capture arbitrary feature combinations .",0
11921,"showed further gains using the same approach coupled with a somewhat improved set of core features , a more involved network architecture with skip - layers , beam search - decoding , and careful hyper - parameter tuning .",0
11922,apply a similar methodology to graph - based parsing .,0
11923,"While the move to neural - network classifiers alleviates the need for hand - crafting featurecombinations , the need to carefully define a set of core features remain .",0
11924,"For example , the feature representation in Chen and Manning ( 2014 ) is a concatenation of 18 word vectors , 18 POS vectors and 12 dependency - label vectors .",0
11925,The above works tackle the effort in hand - crafting effective feature combinations .,0
11926,"A different line of work attacks the feature - engineering problem by suggesting novel neural - network architectures for encoding the parser state , including intermediatelybuilt subtrees , as vectors which are then fed to nonlinear classifiers .",0
11927,Titov and Henderson encode the parser state using incremental sigmoid - belief .,0
11928,"In the work of , the entire stack and buffer of a transition - based parser are encoded as a stack - LSTMs , where each stack element is itself based on a compositional representation of parse trees .",0
11929,"encode each tree node as two compositional representations capturing the inside and outside structures around the node , and feed the representations into a reranker .",0
11930,"A similar reranking approach , this time based on convolutional neural networks , is taken by .",0
11931,"Finally , in we present an Easy - First parser based on a novel hierarchical - LSTM tree encoding .",0
11932,"In contrast to these , the approach we present in this work results in much simpler feature functions , without resorting to elaborate network architectures or compositional tree representations .",0
11933,Work by employs a sequence - to - sequence with attention architecture for constituency parsing .,0
11934,"Each token in the input sentence is encoded in a deep - BiLSTM representation , and then the tokens are fed as input to a deep - LSTM that predicts a sequence of bracketing actions based on the already predicted bracketing as well as the encoded BiLSTM vectors .",0
11935,A trainable attention mechanism is used to guide the parser to relevant BiLSTM vectors at each stage .,0
11936,This architecture shares with ours the use of BiLSTM encoding and end - to - end training .,0
11937,The sequence of bracketing actions can be interpreted as a sequence of Shift and Reduce operations of a transition - based parser .,0
11938,"However , while the parser of Vinyals et al .",0
11939,"relies on a trainable attention mechanism for focusing on specific BiLSTM vectors , parsers in the transition - based family we use in Section 4 use a human designed stack and buffer mechanism to manually direct the parser 's attention .",0
11940,"While the effectiveness of the trainable attention approach is impressive , the stack - and - buffer guidance of transitionbased parsers results in more robust learning .",0
11941,"Indeed , work by , published while working on the camera - ready version of this paper , show that the same methodology as ours is highly effective also for greedy , transition - based constituency parsing , surpassing the beam - based architecture of when trained on the Penn Treebank dataset and without using orthogonal methods such as ensembling and up - training .",0
11942,Bidirectional Recurrent Neural Networks,0
11943,Recurrent neural networks ( RNNs ) are statistical learners for modeling sequential data .,0
11944,An RNN allows one to model the ith element in the sequence based on the past - the elements x 1:i up to and including it .,0
11945,The RNN model provides a framework for conditioning on the entire history x 1:i without resorting to the Markov assumption which is traditionally used for modeling sequences .,0
11946,"RNNs were shown to be capable of learning to count , as well as to model line lengths and complex phenomena such as bracketing and code indentation .",0
11947,"Our proposed feature extractors are based on a bidirectional recurrent neural network ( BiRNN ) , an extension of RNNs that take into account both the past x 1:i and the future x i:n .",0
11948,We use a specific flavor of RNN called along short - term memory network ( LSTM ) .,0
11949,"For brevity , we treat RNN as an abstraction , without getting into the mathematical details of the implementation of the RNNs and LSTMs .",0
11950,"For further details on RNNs and LSTMs , the reader is referred to and .",0
11951,"The recurrent neural network ( RNN ) abstraction is a parameterized function RNN ? ( x 1 :n ) mapping a sequence of n input vectors x 1:n , x i ?",0
11952,"Rd in to a sequence of n output vectors h 1:n , hi ?",0
11953,R dout .,0
11954,"Each output vector hi is conditioned on all the input vectors x 1 :i , and can bethought of as a summary of the prefix x 1:i of x 1:n .",0
11955,"In our notation , we ignore the intermediate vectors h 1:n?1 and take the output of RNN ? ( x 1 :n ) to be the vector h n .",0
11956,"A bidirectional RNN is composed of two RNNs , RNN F and RNN R , one reading the sequence in its regular order , and the other reading it in reverse .",0
11957,"Concretely , given a sequence of vectors x 1:n and a desired index i , the function BIRNN ? ( x 1 :n , i ) is defined as :",0
11958,"The vector vi = BIRNN ( x 1 :n , i ) is then a representation of the ith item in x 1:n , taking into account both the entire history x 1:i and the entire future x i:n by concatenating the matching RNNs .",0
11959,We can view the BiRNN encoding of an item i as representing the item i together with a context of an infinite window around it .,0
11960,Computational Complexity,0
11961,Computing the BiRNN vectors encoding of the ith element of a sequence x 1:n requires O ( n ) time for computing the two RNNs and concatenating their outputs .,0
11962,A naive approach of computing the bidirectional representation of all n elements result in O ( n 2 ) computation .,0
11963,"However , it is trivial to compute the BiRNN encoding of all sequence items in linear time by pre-computing RNN F ( x 1:n ) and RNN R ( x n:1 ) , keeping the intermediate representations , and concatenating the required elements as needed .",0
11964,BiRNN,0
11965,Training,0
11966,"Initially , the BiRNN encodings vi do not capture any particular information .",0
11967,"During training , the encoded vectors vi are fed into further network layers , until at some point a prediction is made , and a loss is incurred .",0
11968,"The back - propagation algorithm is used to compute the gradients of all the parameters in the network ( including the BiRNN parameters ) with respect to the loss , and an optimizer is used to update the parameters according to the gradients .",0
11969,The training procedure causes the BiRNN function to extract from the input sequence x 1:n the relevant information for the task task at hand .,0
11970,Going deeper,0
11971,"We use a variant of deep bidirectional RNN ( or k-layer BiRNN ) which is composed of k BiRNN functions BIRNN 1 , , BIRNN k that feed into each other : the output BIRNN ( x 1 :n , 1 ) , . . . , BIRNN ( x 1 :n , n ) of BIRNN becomes the input of BIRNN + 1 .",0
11972,Stacking BiRNNs in this way has been empirically shown to be effective .,0
11973,"In this work , we use BiRNNs and deep - BiRNNs interchangeably , specifying the number of layers when needed .",0
11974,"Historical Notes RNNs were introduced by Elman , and extended to BiRNNs by .",0
11975,The LSTM variant of RNNs is due to .,0
11976,"BiLSTMs were recently popularized by , and deep BiRNNs were introduced to NLP by , who used them for sequence tagging .",0
11977,"In the context of parsing , and use a BiLSTM sequence tagging model to assign a CCG supertag for each token in the sentence .",0
11978,feeds the resulting supertags sequence into an A * CCG parser .,0
11979,"adds an additional layer of LSTM which receives the BiLSTM representation together with the k-best supertags for each word and outputs the most likely supertag given previous tags , and then feeds the predicted supertags to a discriminitively trained parser .",0
11980,"In both works , the BiLSTM is trained to produce accurate CCG supertags , and is not aware of the global parsing objective .",0
11981,Our Approach,0
11982,We propose to replace the hand - crafted feature functions in favor of minimally - defined feature functions which make use of automatically learned Bidirectional LSTM representations .,0
11983,"Given n-words input sentence s with words w 1 , . . . , w n together with the corresponding POS tags t 1 , . . . , tn , 4 we associate each word w i and POS ti with embedding vectors e ( w i ) and e (t i ) , and create a sequence of input vectors x 1:n in which each xi is a concatenation of the corresponding word and POS vectors :",0
11984,The embeddings are trained together with the model .,0
11985,"This encodes each word in isolation , disregarding its context .",0
11986,"We introduce context by representing each input element as its ( deep ) BiLSTM vector , vi :",0
11987,Our feature function ?,0
11988,is then a concatenation of a small number of BiLSTM vectors .,0
11989,The exact feature function is parser dependent and will be discussed when discussing the corresponding parsers .,0
11990,"The resulting feature vectors are then scored using a non-linear function , namely a multi - layer perceptron with one hidden layer ( MLP ) :",0
11991,"Beside using the BiLSTM - based feature functions , we make use of standard parsing techniques .",0
11992,"Crucially , the BiLSTM is trained jointly with the rest of the parsing objective .",0
11993,This allows it to learn representations which are suitable for the parsing task .,0
11994,Consider a concatenation of two BiLSTM vectors ( v i v j ) scored using an MLP .,0
11995,"The scoring function has access to the words and POS - tags of vi and v j , as well as the words and POS - tags of the words in an infinite window surrounding them .",0
11996,"As LSTMs are known to capture length and sequence position information , it is very plausible that the scoring function can be sensitive also to the distance between i and j , their ordering , and the sequential material between them .",0
11997,Parsing - time Complexity,0
11998,"Once the BiLSTM is trained , parsing is performed by first computing the BiLSTM encoding vi for each word in the sentence ( a linear time operation ) .",0
11999,5,0
12000,"Then , parsing proceeds as usual , where the feature extraction involves a concatenation of a small number of the pre-computed vi vectors .",0
12001,Transition - based Parser,0
12002,We begin by integrating the feature extractor in a transition - based parser .,0
12003,We follow the notation in .,0
12004,"The Scoring : transition - based parsing framework assumes a transition system , an abstract machine that processes sentences and produces parse trees .",0
12005,The transition system has a set of configurations and a set of transitions which are applied to configurations .,0
12006,"When parsing a sentence , the system is initialized to an initial configuration based on the input sentence , and transitions are repeatedly applied to this configuration .",0
12007,"After a finite number of transitions , the system arrives at a terminal configuration , and a parse tree is read off the terminal configuration .",0
12008,"In a greedy parser , a classifier is used to choose the transition to take in each configuration , based on features extracted from the configuration itself .",0
12009,The parsing algorithm is presented in Algorithm 1 below .,0
12010,"Given a sentence s , the parser is initialized with the configuration c ( line 2 ) .",0
12011,"Then , a feature function ? ( c ) represents the configuration c as a vector , which is fed to a scoring function SCORE assigning scores to ( configuration , transition ) pairs .",0
12012,"SCORE scores the possible transitions t , and the highest Algorithm 1 Greedy transition - based parsing 1 : Input : sentence s = w 1 , . . . , x w , t 1 , . . . , tn , parameterized function SCORE ? ( ) with parameters ?. scoring transitiont is chosen ( line 4 ) .",0
12013,"The transition t is applied to the configuration , resulting in a new parser configuration .",0
12014,"The process ends when reaching a final configuration , from which the resulting parse tree is read and returned ( line 6 ) .",0
12015,"Transition systems differ by the way they define configurations , and by the particular set of transitions available to them .",0
12016,"A parser is determined by the choice of a transition system , a feature function ?",0
12017,and a scoring function SCORE .,0
12018,Our choices are detailed below .,0
12019,The Arc-Hybrid System,0
12020,Many transition systems exist in the literature .,0
12021,"In this work , we use the archybrid transition system , which is similar to the more popular arc-standard system , but for which an efficient dynamic oracle is available .",0
12022,"In the arc-hybrid system , a configuration c = ( ? , ? , T ) consists of a stack ? , a buffer ? , and a set T of dependency arcs .",0
12023,Both the stack and the buffer hold integer indices pointing to sentence elements .,0
12024,"Given a sentence s = w 1 , . . . , w n , t 1 , . . . , tn , the system is initialized with an empty stack , an empty arc set , and ? = 1 , . . . , n , ROOT , where ROOT is the special root index .",0
12025,"Any configuration c with an empty stack and a buffer containing only ROOT is terminal , and the parse tree is given by the arc set Tc of c .",0
12026,"The archybrid system allows 3 possible transitions , SHIFT , LEFT and RIGHT , defined as :",0
12027,The SHIFT transition moves the first item of the buffer ( b 0 ) to the stack .,0
12028,"The LEFT transition removes the first item on top of the stack ( s 0 ) and attaches it as a modifier to b 0 with label , adding the arc ( b 0 , s 0 , ) .",0
12029,"The RIGHT transition removes s 0 from the stack and attaches it as a modifier to the next item on the stack ( s 1 ) , adding the arc ( s 1 , s 0 , ) .",0
12030,Scoring Function,0
12031,"Traditionally , the scoring func -",0
12032,The linearity of SCORE required the feature function ? ( ) to encode non-linearities in the form of combination features .,0
12033,We follow and replace the linear scoring model with an MLP .,0
12034,Simple Feature Function,0
12035,The feature function ? ( c ) is typically complex ( see Section 2.1 ) .,0
12036,Our feature function is the concatenated BiLSTM vectors of the top 3 items on the stack and the first item on the buffer .,0
12037,"I.e. , for a configuration c = ( . . . |s 2 | s 1 |s 0 , b 0 | . . . , T ) the feature extractor is defined as :",0
12038,"This feature function is rather minimal : it takes into account the BiLSTM representations of s 1 , s 0 and b 0 , which are the items affected by the possible transitions being scored , as well as one extra stack context s 2 . 6 depicts transition scoring with our architecture and this feature function .",0
12039,"Note that , unlike previous work , this feature function does not take into account T , the already built structure .",0
12040,The high parsing accuracies in the experimental sections suggest that the BiLSTM encoding is capable of estimating a lot of the missing information based on the provided stack and buffer elements and the sequential content between them .,0
12041,"While not explored in this work , relying on only four word indices for scoring an action results in very compact state signatures , making our proposed feature representation very appealing for use in transition - based parsers that employ dynamic - programming search .",0
12042,Extended Feature Function,0
12043,One of the benefits of the greedy transition - based parsing framework is precisely its ability to look at arbitrary features from the already built tree .,0
12044,"If we allow somewhat less minimal feature function , we could add the BiLSTM vectors corresponding to the right - most and leftmost modifiers of s 0 , s 1 and s 2 , as well as the leftmost modifier of b 0 , reaching a total of 11 BiLSTM vectors .",0
12045,We refer to this as the extended feature set .,0
12046,"As we 'll see in Section 6 , using the extended set does indeed improve parsing accuracies when using pre-trained word embeddings , but has a minimal effect in the fully - supervised case .",0
12047,7,0
12048,"6 An additional buffer context is not needed , as b 1 is by definition adjacent to b0 , a fact that we expect the BiLSTM encoding of b0 to capture .",0
12049,"In contrast , b 0 , s 0 , s 1 and s 2 are not necessarily adjacent to each other in the original sentence .",0
12050,We did not experiment with other feature configurations .,0
12051,"It is well possible that not all of the additional 7 child encodings are needed for the observed accuracy gains , and that a smaller feature set will yield similar or even better improvements .",0
12052,Details of the Training Algorithm,0
12053,The training objective is to set the score of correct transitions above the scores of incorrect transitions .,0
12054,"We use a margin - based objective , aiming to maximize the margin between the highest scoring correct action and the highest scoring incorrect action .",0
12055,The hinge loss at each parsing configuration c is defined as :,0
12056,where A is the set of possible transitions and G is the set of correct ( gold ) transitions at the current stage .,0
12057,"At each stage of the training process the parser scores the possible transitions A , incurs a loss , selects a transition to follow , and moves to the next configuration based on it .",0
12058,"The local losses are summed throughout the parsing process of a sentence , and the parameters are updated with respect to the sum of the losses at sentence boundaries .",0
12059,The gradients of the entire network ( including the MLP and the BiLSTM ) with respect to the sum of the losses are calculated using the backpropagation algorithm .,0
12060,"As usual , we perform several training iterations over the training corpus , shuffling the order of sentences in each iteration .",0
12061,Error - Exploration and Dynamic Oracle Training,0
12062,"We follow ; in using error exploration training with a dynamic - oracle , which we briefly describe below .",0
12063,"At each stage in the training process , the parser assigns scores to all the possible transitions t ?",0
12064,A .,0
12065,"It then selects a transition , applies it , and moves to the next step .",0
12066,Which transition should be followed ?,0
12067,A common approach follows the highest scoring transition that can lead to the gold tree .,0
12068,"However , when training in this way the parser sees only configurations that result from following correct actions , and as a result tends to suffer from error propagation at test time .",0
12069,"Instead , in error-exploration training the parser follows the highest scoring action in A during training even if this action is incorrect , exposing it to configurations that result from erroneous decisions .",0
12070,This strategy requires defining the set G such that the correct actions to take are well - defined also for states that can not lead to the gold tree .,0
12071,Such a set G is called a dynamic oracle .,0
12072,We perform error-exploration training using the dynamic - oracle defined by .,0
12073,Aggressive Exploration,0
12074,"We found that even when using error-exploration , after one iteration the model remembers the training set quite well , and does not make enough errors to make error-exploration effective .",0
12075,"In order to expose the parser to more errors , we follow an aggressive - exploration scheme : we sometimes follow incorrect transitions also if they score below correct transitions .",0
12076,"Specifically , when the score of the correct transition is greater than that of the wrong transition but the difference is smaller than a margin constant , we chose to follow the incorrect action with probability p agg ( we use p agg = 0.1 in our experiments ) .",0
12077,Summary,0
12078,"The greedy transition - based parser follows standard techniques from the literature ( margin - based objective , dynamic oracle training , error exploration , MLP - based non-linear scoring function ) .",0
12079,"We depart from the literature by replacing the hand - crafted feature function over carefully selected components of the configuration with a concatenation of BiLSTM representations of a few prominent items on the stack and the buffer , and training the BiLSTM encoder jointly with the rest of the network .",0
12080,Graph - based Parser,0
12081,Graph - based parsing follows the common structured prediction paradigm :,0
12082,Given an input sentence s ( and the corresponding sequence of vectors x 1:n ) we look for the highest - :,0
12083,Illustration of the neural model scheme of the graph - based parser when calculating the score of a given parse tree .,0
12084,The parse tree is depicted below the sentence .,0
12085,"Each dependency arc in the sentence is scored using an MLP that is fed the BiLSTM encoding of the words at the arc's end points ( the colors of the arcs correspond to colors of the MLP inputs above ) , and the individual arc scores are summed to produce the final score .",0
12086,All the MLPs share the same parameters .,0
12087,"The figure depicts a single - layer BiLSTM , while in practice we use two layers .",0
12088,"When parsing a sentence , we compute scores for all possible n 2 arcs , and find the best scoring tree using a dynamic - programming algorithm .",0
12089,scoring parse tree yin the space Y ( s ) of valid dependency trees over s .,0
12090,"In order to make the search tractable , the scoring function is decomposed to the sum of local scores for each part independently .",0
12091,"In this work , we focus on arc-factored graph based approach presented in .",0
12092,Arc-factored parsing decomposes the score of a tree to the sum of the score of its head - modifier arcs,0
12093,Given the scores of the arcs the highest scoring projective tree can be efficiently found using .,0
12094,McDonald et al .,0
12095,"and most subsequent work estimate the local score of an arc by a linear model parameterized by a weight vector w , and a feature function ?( s , h , m ) assigning a sparse feature vector for an arc linking modifier m to head h.",0
12096,We follow and replace the linear scoring function with an MLP .,0
12097,"The feature extractor ?( s , h , m ) is usually complex , involving many elements ( see Section 2.1 ) .",0
12098,"In contrast , our feature extractor uses merely the BiLSTM encoding of the headword and the mod-",0
12099,The architecture is illustrated in .,0
12100,Training,0
12101,The training objective is to set the score function such that correct tree y is scored above incorrect ones .,0
12102,"We use a margin - based objective , aiming to maximize the margin between the score of the gold tree y and the highest scoring incorrect tree y .",0
12103,We define a hinge loss with respect to a gold tree y as :,0
12104,Each of the tree scores is then calculated by activating the MLP on the arc representations .,0
12105,"The entire loss can viewed as the sum of multiple neural networks , which is sub-differentiable .",0
12106,We calculate the gradients of the entire network ( including to the BiLSTM encoder and word embeddings ) .,0
12107,Labeled Parsing,0
12108,"Up to now , we described unlabeled parsing .",0
12109,"A possible approach for adding labels is to score the combination of an unlabeled arc ( h , m ) and it s label by considering the label as part of the arc ( h , m , ) .",0
12110,"This results in | Labels | | Arcs | parts that need to be scored , leading to slow parsing speeds and arguably a harder learning problem .",0
12111,"Instead , we chose to first predict the unlabeled structure using the model given above , and then predict the label of each resulting arc .",0
12112,"Using this approach , the number of parts stays small , enabling fast parsing .",0
12113,"The labeling of an arc ( h , m ) is performed using the same feature representation ?( s , h , m ) fed into a different MLP predictor :",0
12114,As before we use a margin based hinge loss .,0
12115,The labeler is trained on the gold trees .,0
12116,9 The BiLSTM encoder responsible for producing v hand v m is shared with the arc-factored parser : the same BiLSTM encoder is used in the parer and the labeler .,0
12117,This sharing of parameters can be seen as an instance of multi-task learning .,0
12118,"As we show in Section 6 , the sharing is effective : training the BiLSTM feature encoder to be good at predicting arc-labels significantly improves the parser 's unlabeled accuracy .",0
12119,Loss augmented inference,0
12120,"In initial experiments , the network learned quickly and overfit the data .",0
12121,"In order to remedy this , we found it useful to use loss augmented inference .",0
12122,The intuition behind loss augmented inference is to update against trees which have high model scores and are also very wrong .,0
12123,This is done by augmenting the score of each part not belonging to the gold tree by adding a constant to its score .,0
12124,"Formally , the loss transforms as follows :",0
12125,Speed improvements,0
12126,The arc-factored model requires the scoring of n 2 arcs .,0
12127,"Scoring is performed using an MLP with one hidden layer , resulting inn 2 matrix - vector multiplications from the input to the hidden layer , and n 2 multiplications from the hidden to the output layer .",0
12128,"The first n 2 multiplications involve larger dimensional input and output vectors , and are the most time consuming .",0
12129,"Fortunately , these can be reduced to 2n multiplications and n 2 vector additions , by observing that the multiplication",0
12130,where W 1 and W 1 are are the first and second half of the matrix W and reusing the products across different pairs .,0
12131,Summary,0
12132,"The graph - based parser is straightforward first - order parser , trained with a marginbased hinge - loss and loss - augmented inference .",0
12133,"We depart from the literature by replacing the handcrafted feature function with a concatenation of BiLSTM representations of the head and modifier words , and training the BiLSTM encoder jointly with the structured objective .",0
12134,We also introduce a novel multi-task learning approach for labeled parsing by training a second - stage arc - labeler sharing the same BiLSTM encoder with the unlabeled parser .,0
12135,Experiments and Results,0
12136,We evaluated our parsing model on English and Chinese data .,0
12137,For comparison purposes we follow the setup of .,0
12138,"Data For English , we used the Stanford Dependency ( SD ) ( de Marneffe and Manning , 2008 ) conversion of the Penn Treebank ; Weiss 15 ; Pei15 : ; Dyer15 ; Ballesteros 16 ; LeZuidema14 ; Zhu15 :.",0
12139,same predicted POS - tags as used in ; .,0
12140,This dataset contains a few non-projective trees .,0
12141,Punctuation symbols are excluded from the evaluation .,0
12142,"For Chinese , we use the Penn Chinese Treebank 5.1 ( CTB5 ) , using the train / test / dev splits of with gold partof - speech tags , also following .",0
12143,"When using external word embeddings , we also use the same data as .",0
12144,10 Implementation Details,0
12145,"The parsers are implemented in python , using the PyCNN toolkit 11 for neural network training .",1
12146,The code is available at the github repository https://github.com/elikip / bist -parser .,1
12147,"We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer .",1
12148,"Unless otherwise noted , we use the default values provided by PyCNN ( e.g. for random initialization , learning rates etc ) .",0
12149,We thank Dyer et al. for sharing their data with us .,0
12150,11 https://github.com/clab/cnn/tree/,0
12151,master / pycnn,0
12152,The word and POS embeddings e ( w i ) and e ( p i ) are initialized to random values and trained together with the rest of the parsers ' networks .,0
12153,"In some experiments , we introduce also pre-trained word embeddings .",0
12154,"In those cases , the vector representation of a word is a concatenation of its randomlyinitialized vector embedding with its pre-trained word vector .",0
12155,Both are tuned during training .,0
12156,"We use the same word vectors as in During training , we employ a variant of word dropout , and replace a word with the unknown - word symbol with probability that is inversely proportional to the frequency of the word .",0
12157,A word w appearing # ( w ) times in the training corpus is replaced with the unknown symbol with probability punk ( w ) = ? # ( w ) +? .,0
12158,If a word was dropped the external embedding of the word is also dropped with probability 0.5 .,0
12159,"We train the parsers for up to 30 iterations , and choose the best model according to the UAS accuracy on the development set .",0
12160,Hyperparameter Tuning,0
12161,"We performed a very minimal hyper - parameter search with the graph - based parser , and use the same hyper - parameters for both parsers .",0
12162,"The hyper- parameters of the final networks used for all the reported experiments are detailed in Main Results lists the test - set accuracies of our best parsing models , compared to other state - of the - art parsers from the literature .",0
12163,"It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors .",1
12164,"When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems thatare not using external resources , including the third - order TurboParser .",1
12165,"The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of and the Stack - LSTM parser of , as well as the same parser when trained using a dynamic oracle .",1
12166,Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese .,1
12167,"Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades .",1
12168,"We are not sure why this happens , and leave the exploration of effective semi-supervised parsing with the graph - based model for future work .",0
12169,"The greedy parser does manage to benefit from the external embeddings , and using them we also see gains from moving from the simple to the extended feature set .",0
12170,"Both feature sets result in very competitive re - 12 Unfortunately , many papers still report English parsing results on the deficient Yamada and Matsumoto head rules ( PTB - YM ) rather than the more modern Stanford - dependencies ( PTB - SD ) .",0
12171,"We note that the PTB - YM and PTB - SD results are not strictly comparable , and in our experience the PTB - YM results are usually about half a UAS point higher .",0
12172,"sults , with the extended feature set yielding the best reported results for Chinese , and ranked second for English , after the heavily - tuned beam - based parser of .",0
12173,Additional Results,0
12174,"We perform some ablation experiments in order to quantify the effect of the different components on our best models Loss augmented inference is crucial for the success of the graph - based parser , and the multi-task learning scheme for the arc-labeler contributes nicely to the unlabeled scores .",0
12175,Dynamic oracle training yields nice gains for both English and Chinese .,0
12176,Conclusion,0
12177,"We presented a pleasingly effective approach for feature extraction for dependency parsing based on a BiLSTM encoder that is trained jointly with the parser , and demonstrated its effectiveness by integrating it into two simple parsing models : a greedy transition - based parser and a globally optimized first - order graph - based parser , yielding very competitive parsing accuracies in both cases .",0
12178,title,0
12179,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,1
12180,abstract,0
12181,This paper builds off recent work from Kiperwasser & Goldberg ( 2016 ) using neural attention in a simple graph - based dependency parser .,0
12182,"We use a larger but more thoroughly regularized parser than other recent BiLSTM - based approaches , with biaffine classifiers to predict arcs and labels .",0
12183,"Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages , achieving 95.7 % UAS and 94.1 % LAS on the most popular English PTB dataset .",0
12184,"This makes it the highest - performing graph - based parser on this benchmarkoutperforming Kiperwasser & Goldberg ( 2016 ) by 1.8 % and 2.2 % - and comparable to the highest performing transition - based parser ( Kuncoro et al. , 2016 ) , which achieves 95.8 % UAS and 94.6 % LAS .",0
12185,"We also show which hyperparameter choices had a significant effect on parsing accuracy , allowing us to achieve large gains over other graph - based approaches .",0
12186,INTRODUCTION,0
12187,"Dependency parsers - which annotate sentences in a way designed to be easy for humans and computers alike to understand - have been found to be extremely useful for a sizable number of NLP tasks , especially those involving natural language understanding in someway .",0
12188,"However , frequent incorrect parses can severely inhibit final performance , so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks .",0
12189,The current state - of - the - art transition - based neural dependency parser substantially outperforms many much simpler neural graph - based parsers .,0
12190,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .",1
12191,"Furthermore , we compare models trained with different architectures and hyperparameters to motivate our approach empirically .",0
12192,The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .,0
12193,BACKGROUND AND RELATED WORK,0
12194,"Transition - based parsers - such as shift - reduce parsers - parse sentences from left to right , maintaining a "" buffer "" of words that have not yet been parsed and a "" stack "" of words whose head has not been seen or whose dependents have not all been fully parsed .",0
12195,"At each step , transition - based parsers can access and manipulate the stack and buffer and assign arcs from one word to another .",0
12196,"One can then train any multi-class machine learning classifier on features extracted from the stack , buffer , and previous arc actions in order to predict the next action .",0
12197,Chen & Manning ( 2014 ) make the first successful attempt at incorporating deep learning into a transition - based dependency parser .,0
12198,"At each step , the ( feedforward ) network assigns a probability to each action the parser can take based on word , tag , and label embeddings from certain words root / ROOT Casey / NNP hugged / VBD",0
12199,Kim / NNP root nsubj dobj :,0
12200,"A dependency tree parse for Casey hugged Kim , including part - of - speech tags and a special root token .",0
12201,Directed edges ( or arcs ) with labels ( or relations ) connect the verb to the root and the arguments to the verb head .,0
12202,on the stack and buffer .,0
12203,"A number of other researchers have attempted to address some limitations of Chen & Manning 's Chen & Manning parser by augmenting it with additional complexity : and augment it with a beam search and a conditional random field loss objective to allow the parser to "" undo "" previous actions once it finds evidence that they may have been incorrect ; and and instead use LSTMs to represent the stack and buffer , getting state - of - the - art performance by building in a way of composing parsed phrases together .",0
12204,Transition - based parsing processes a sentence sequentially to buildup a parse tree one arc at a time .,0
12205,"Consequently , these parsers do n't use machine learning for directly predicting edges ; they use it for predicting the operations of the transition algorithm .",0
12206,"Graph - based parsers , by contrast , use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree ( MST ) from these weighted edges .",0
12207,present a neural graph - based parser ( in addition to a transition - based one ) that uses the same kind of attention mechanism as for machine translation .,0
12208,"In Kiperwasser & Goldberg 's 2016 model , the ( bidirectional ) LSTM 's recurrent output vector for each word is concatenated with each possible head 's recurrent vector , and the result is used as input to an MLP that scores each resulting arc .",0
12209,The predicted tree structure at training time is the one where each word depends on its highestscoring head .,0
12210,"Labels are generated analogously , with each word 's recurrent output vector and it s gold or predicted head word 's recurrent vector being used in a multi -class MLP .",0
12211,"Similarly , include a graph - based dependency parser in their multi -task neural model .",0
12212,"In addition to training the model with multiple distinct objectives , they replace the traditional MLP - based attention mechanism that use with a bilinear one ( but still using an MLP label classifier ) .",0
12213,This makes it analogous to Luong et al. 's 2015 proposed attention mechanism for neural machine translation .,0
12214,"likewise propose a graph - based neural dependency parser , but in a way that attempts to circumvent the limitation of other neural graph - based parsers being unable to condition the scores of each possible arc on previous parsing decisions .",0
12215,"In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word , they have additional , unidirectional recurrent networks ( leftto - right and right - to - left ) that keep track of the probabilities of each previous arc , and use these together to predict the scores for the next arc .",0
12216,PROPOSED DEPENDENCY PARSER,0
12217,DEEP BIAFFINE ATTENTION,0
12218,"We make a few modifications to the graph - based architectures of , , and , shown in : we use biaffine attention instead of bilinear or traditional MLP - based attention ; we use a biaffine dependency label classifier ; and we apply dimension - reducing MLPs to each recurrent output vector r i before applying the biaffine transformation .",0
12219,1,0
12220,"The choice of biaffine rather than bilinear or MLP mechanisms makes the classifiers in our model analogous to traditional affine classifiers , which use an affine transformation over a single LSTM output state r i ( or other vector input ) to predict the vector of scores s i for all classes ( 1 ) .",0
12221,We can think of the proposed biaffine attention mechanism as being a traditional affine,0
12222,"Figure 2 : BiLSTM with deep biaffine attention to score each possible head for each dependent , applied to the sentence "" Casey hugged Kim "" .",0
12223,We reverse the order of the biaffine transformation here for clarity .,0
12224,"classifier , but using a ( d d ) linear transformation of the stacked LSTM output RU ( 1 ) in place of the weight matrix W and a ( d 1 ) transformation Ru ( 2 ) for the bias term b.",0
12225,"In addition to being arguably simpler than the MLP - based approach ( involving one bilinear layer rather than two linear layers and a nonlinearity ) , this has the conceptual advantage of directly modeling both the prior probability of a word j receiving any dependents in the term r ? j u ( 2 ) and the likelihood of j receiving a specific dependent i in the term r ? j U ( 1 ) r i .",0
12226,"Analogously , we also use a biaffine classifier to predict dependency labels given the gold or predicted heady i ( 3 ) .",0
12227,Fixed - class biaffine classifier,0
12228,"( 3 ) This likewise directly models each of the prior probability of each class , the likelihood of a class given just word i ( how probable a word is to take a particular label ) , the likelihood of a class given just the headword y i ( how probable a word is to take dependents with a particular label ) , and the likelihood of a class given both word i and it s head ( how probable a word is to take a particular label given that word 's head ) .",0
12229,Applying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision .,0
12230,"That is , every top recurrent state r i will need to carry enough information to identify word i 's head , find all its dependents , exclude all its non-dependents , assign itself the correct label , and assign all its dependents their correct labels , as well as transfer any relevant information to the recurrent states of words before and after it .",0
12231,"Thus r i necessarily contains significantly more information than is needed to compute any individual score , and training on this superfluous information needlessly reduces parsing speed and increases the risk of overfitting .",0
12232,Reducing dimensionality and applying a nonlinearity ( 4 - 6 ) addresses both of these problems .,0
12233,"We call this a deep bilinear attention mechanism , as opposed to shallow bilinear attention , which uses the recurrent states directly .",0
12234,We apply MLPs to the recurrent states before using them in the label classifier as well .,0
12235,"As with other graph - based models , the predicted tree at training time is the one where each word is a dependent of its highest scoring head ( although at test time we ensure that the parse is a well - formed tree via the MST algorithm ) .",0
12236,"Aside from architectural differences between ours and the other graph - based parsers , we make a number of hyperparameter choices that allow us to outperform theirs , laid out in .",0
12237,We use 100 - dimensional uncased word vectors 2 and POS tag vectors ; three BiLSTM layers ( 400 dimensions in each direction ) ; and 500 - and 100 - dimensional ReLU MLP layers .,0
12238,"We also apply dropout at every stage of the model : we drop words and tags ( independently ) ; we drop nodes in the LSTM layers ( input and recurrent connections ) , applying the same dropout mask at every recurrent timestep ( cf. the Bayesian dropout of ) ; and we drop nodes in the MLP layers and classifiers , likewise applying the same dropout mask at every timestep .",0
12239,"We optimize the network with annealed Adam for about 50,000 steps , rounded up to the nearest epoch .",0
12240,HYPERPARAMETER CONFIGURATION,0
12241,EXPERIMENTS & RESULTS,0
12242,DATASETS,0
12243,"We show test results for the proposed model on the English Penn Treebank , converted into Stanford Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter ( PTB - SD 3.3.0 and PTB - SD 3.5.0 ) ; the Chinese Penn Treebank ; and the CoNLL 09 shared task dataset , 3 following standard practices for each dataset .",0
12244,We omit punctuation from evaluation only for the PTB - SD and CTB .,0
12245,"For the English PTB - SD datasets , we use POS tags generated from the Stanford POS tagger ; for the Chinese PTB dataset we use gold tags ; and for the CoNLL 09 dataset we use the provided predicted tags .",0
12246,"Our hyperparameter search was done with the PTB - SD 3.5.0 validation dataset in order to minimize overfitting to the more popular PTB - SD 3.3.0 benchmark , and in our hyperparameter analysis in the following section we report performance on the PTB - SD 3.5.0 test set , shown in Tables 2 and 3 .",0
12247,HYPERPARAMETER CHOICES,0
12248,ATTENTION MECHANISM,0
12249,We examined the effect of different classifier architectures on accuracy and performance .,0
12250,What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy .,0
12251,"The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings , but because the label classifier is much larger ( ( 801 c 801 ) as opposed to ( 101 c 101 ) ) , it runs much slower and overfits .",0
12252,"One way to decrease this overfitting is by increasing the MLP dropout , but that of course does n't change parsing speed ; another way is to decrease the recurrent size to 300 , but this hinders unlabeled accuracy without increasing parsing speedup to the same levels as our deeper model .",0
12253,We also implemented the MLP - based approach to attention and classification used in .,0
12254,We found this version to : Test Accuracy on PTB - SD 3.5.0 .,0
12255,Statistically significant differences are marked with an asterisk .,0
12256,likewise be somewhat slower and significantly underperform the deep biaffine approach in both labeled and unlabeled accuracy .,0
12257,NETWORK SIZE,0
12258,We also examine more closely how network size influences speed and accuracy .,0
12259,"In Kiperwasser & Goldberg 's 2016 model , the network uses 2 layers of 125 - dimensional bidirectional LSTMs ; in has one layer of 100 - dimensional bidirectional LSTMs dedicated to parsing ( two lower layers are also trained on other objectives ) ; and Cheng et al. 's 2016 model has one layer of 368 - dimensional GRU cells .",0
12260,"We find that using three or four layers gets significantly better performance than two layers , and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance .",0
12261,5,0
12262,RECURRENT CELL,0
12263,"GRU cells have been promoted as a faster and simpler alternative to LSTM cells , and are used in the approach of ; however , in our model they drastically underperformed LSTM cells .",0
12264,"We also implemented the coupled input - forget gate LSTM cells ( Cif - LSTM ) suggested by , 6 finding that while the resulting model still slightly underperforms the more popular LSTM cells , the difference between the two is much smaller .",0
12265,"Additionally , because the gate and candidate cell activations can be computed simultaneously with one matrix multiplication , the Cif - LSTM model is faster than the GRU version even though they have the same number of parameters .",0
12266,"We hypothesize that the output gate in the Cif - LSTM model allows it to maintain a sparse recurrent output state , which helps it adapt to the high levels of dropout needed to prevent overfitting in a way that GRU cells are unable to do .",0
12267,"Because we increase the parser 's power , we also have to increase its regularization .",0
12268,"In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in , we also regularize the input layer .",0
12269,"We drop 33 % of words and 33 % of tags during training : when one is dropped the other is scaled by a factor of two to compensate , and when both are dropped together , the model simply gets an input of zeros .",0
12270,"Models trained with only word or tag dropout but not both windup signficantly overfitting , hindering label accuracy and - in the latter case - attachment accuracy .",0
12271,"Interestingly , not using any tags at all actually results in better performance than using tags without dropout .",0
12272,OPTIMIZER,0
12273,"We choose to optimize with Adam , which ( among other things ) keeps a moving average of the L 2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average , ensuring that the magnitude of the gradients will on average be close to one .",1
12274,"However , we find that the value for ?",0
12275,2 recommended by Kingma & Bawhich controls the decay rate for this moving average - is too high for this task ( and we suspect more generally ) .,0
12276,"When this value is very large , the magnitude of the current update is heavily influenced by the larger magnitude of gradients very far in the past , with the effect that the optimizer ca n't adapt quickly to recent changes in the model .",0
12277,Thus we find that setting ? 2 to . 9 instead of . 999 makes a large positive impact on final performance .,0
12278,RESULTS,0
12279,"Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .",1
12280,"It is worth noting that the CoNLL 09 datasets contain many non-projective dependencies , which are difficult or impossible for transition - based - but not graph - based - parsers to predict .",0
12281,"This may account for some of the large , consistent difference between our model and Andor et al. 's 2016 transition - based model applied to these datasets .",0
12282,"Where our model appears to lag behind the SOTA model is in LAS , indicating one of a few possibilities .",0
12283,"Firstly , it maybe the result of inefficiencies or errors in the GloVe embeddings or POS tagger , in which case using alternative pretrained embeddings or a more accurate tagger might improve label classification .",0
12284,"Secondly , the SOTA model is specifically designed to capture phrasal compositionality ; so another possibility is that ours does n't capture this compositionality as effectively , and that this results in a worse label score .",0
12285,"Similarly , it maybe the result of a more general limitation of graph - based parsers , which have access to less explicit syntactic information than transition - based parsers when making decisions .",0
12286,Addressing these latter two limitations would require a more innovative architecture than the relatively simple one used in current neural graph - based parsers .,0
12287,CONCLUSION,0
12288,In this paper we proposed using a modified version of bilinear attention in a neural dependency parser that increases parsing speed without hurting performance .,0
12289,We showed that our larger but more regularized network outperforms other neural graph - based parsers and gets comparable performance to the current SOTA transition - based parser .,0
12290,We also provided empirical motivation for the proposed architecture and configuration over similar ones in the existing literature .,0
12291,Future work will involve exploring ways of bridging the gap between labeled and unlabeled accuracy and augment the parser with a smarter way of handling out - of - vocabulary tokens for morphologically richer languages .,0
12292,title,0
12293,Dense Face Alignment,1
12294,abstract,0
12295,Face alignment is a classic problem in the computer vision field .,1
12296,"Previous works mostly focus on sparse alignment with a limited number of facial landmark points , i.e. , facial landmark detection .",0
12297,"In this paper , for the first time , we aim at providing a very dense 3D alignment for largepose face images .",0
12298,"To achieve this , we train a CNN to estimate the 3D face shape , which not only aligns limited facial landmarks but also fits face contours and SIFT feature points .",0
12299,"Moreover , we also address the bottleneck of training CNN with multiple datasets , due to different landmark markups on different datasets , such as 5 , 34 , 68 .",0
12300,"Experimental results show our method not only provides highquality , dense 3 D face fitting but also outperforms the stateof - the - art facial landmark detection methods on the challenging datasets .",0
12301,Our model can run at real time during testing and it 's available at,0
12302,Introduction,0
12303,"Face alignment is a long - standing problem in the computer vision field , which is the process of aligning facial components , e.g. , eye , nose , mouth , and contour .",0
12304,"An accurate face alignment is an essential prerequisite for many face related tasks , such as face recognition , 3 D face reconstruction and face animation .",0
12305,"There are fruitful previous works on face alignment , which can be categorized as generative methods such as the early Active Shape Model and Active Appearance Model ( AAM ) based approaches , and discriminative methods such as regression - based approaches .",0
12306,"Most previous methods estimate a sparse set of landmarks , e.g. , 68 landmarks .",0
12307,"As this field is being developed , we believe that Dense Face Alignment ( DeFA ) becomes highly desired .",0
12308,"Here , DeFA denotes that it 's doable to map any face - region pixel to the pixel in other face images , which has the same anatomical position inhuman faces .",0
12309,"For example , given two face images from the same .",0
12310,"A pair of images with their dense 3D shapes obtained by imposing landmark fitting constraint , contour fitting constraint and sift pair constraint .",0
12311,"individual but with different poses , lightings or expressions , a perfect DeFA can even predict the mole ( i.e. darker pigment ) on two faces as the same position .",0
12312,"Moreover , DeFA should offer dense correspondence not only between two face images , but also between the face image and the canonical 3 D face model .",0
12313,This level of detailed geometry interpretation of a face image is invaluable to many conventional facial analysis problems mentioned above .,0
12314,"Since this interpretation has gone beyond the sparse set of landmarks , fitting a dense 3 D face model to the face image is a reasonable way to achieve DeFA .",0
12315,"In this work , we choose to develop the idea of fitting a dense 3 D face model to an image , where the model with thousands of vertexes makes it possible for face alignment to go very "" dense "" .",0
12316,3 D face model fitting is well studied in the seminal work of 3D Morphorbal Model ( 3 DMM ) .,0
12317,"We see a recent surge when it is applied to problems such as large - pose face alignment , 3D reconstruction , and face recognition , especially using the convolutional neural network ( CNN ) architecture .",0
12318,"However , most prior works on 3D - model - fitting - based face alignment only utilize the sparse landmarks as supervision .",0
12319,"There are two main challenges to be addressed in 3D face model fitting , in order to enable high - quality DeFA .",0
12320,"First of all , to the best of our knowledge , no public face dataset has dense face shape labeling .",0
12321,All of the in - the - wild face alignment datasets have no more than 68 landmarks in the labeling .,0
12322,"Apparently , to provide a high - quality alignment for face - region pixels , we need information more than just the landmark labeling .",0
12323,"Hence , the first challenge is to seek valuable information for additional supervision and in - tegrate them in the learning framework .",0
12324,"Secondly , similar to many other data - driven problems and solutions , it is preferred that multiple datasets can be involved for solving face alignment task since a single dataset has limited types of variations .",0
12325,"However , many face alignment methods can not leverage multiple datasets , because each dataset either is labeled differently .",0
12326,"For instance , AFLW dataset contains a significant variation of poses , but has a few number of visible landmarks .",0
12327,"In contrast , 300W dataset contains a large number of faces with 68 visible landmarks , but all faces are in a near - frontal view .",0
12328,"Therefore , the second challenge is to allow the proposed method to leverage multiple face datasets .",0
12329,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .",1
12330,"While the proposed method works for any face image , we mainly pay attention to faces with large poses .",0
12331,"Large - pose face alignment is a relatively new topic , and the performances in still have room to improve .",0
12332,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .",1
12333,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .",1
12334,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .",1
12335,"For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .",1
12336,"Regardless of the landmark labeling number in a particular dataset , we can always define the corresponding 3 D vertexes to guide the training .",0
12337,"Generally , our main contributions can be summarized as : 1 . We identify and define anew problem of dense face alignment , which seeks alignment of face - region pixels beyond the sparse set of landmarks .",0
12338,2 .,0
12339,"To achieve dense face alignment , we develop a novel 3 D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets .",0
12340,3 .,0
12341,"Our dense face alignment algorithm outperforms the SOTA on challenging large - pose face alignment , and achieves competitive results on near - frontal face alignment .",0
12342,The model runs at real time .,0
12343,Related Work,0
12344,"We review papers in three relevant areas : 3 D face alignment from a single image , using multiple constraints in face alignment , and using multiple datasets for face alignment .",0
12345,3D model fitting in face alignment,0
12346,"Recently , there are increasingly attentions in conducting face alignment by fitting the 3D face model to the single 2D image .",0
12347,"In , Blanz and Vetter proposed the 3 DMM to represent the shape and texture of a range of individuals .",0
12348,The analysis - by - synthesis based methods are utilized to fit the 3 DMM to the face image .,0
12349,Ina set of cascade CNN regressors with the extracted 3D features is utilized to estimate the parameters of 3 DMM and the projection matrix directly .,0
12350,"Liu et al. proposed to utilize two sets of regressors , for estimating update of 2D landmarks and the other set estimate update of dense 3 D shape by using the 2D landmarks update .",0
12351,They apply these two sets of regressors alternatively .,0
12352,"Compared to prior work , our method imposes additional constraints , which is the key to dense face alignment .",0
12353,Multiple constraints in face alignment,0
12354,"Other than landmarks , there are other features that are useful to describe the shape of a face , such as contours , pose and face attributes .",0
12355,"Unlike landmarks , those features are often not labeled in the datasets .",0
12356,"Hence , the most crucial step of leveraging those features is to find the correspondence between the features and the 3D shape .",0
12357,"In , multiple features constraints in the cost function is utilized to estimate the 3D shape and texture of a 3 D face .",0
12358,"2 D edge is detected by Canny detector , and the corresponding 3D edges ' vertices are matched by Iterative Closest Point ( ICP ) to use this information .",0
12359,"Furthermore , provides statistical analysis about the 2D face contours and the 3D face shape under different poses .",0
12360,There is a few work using constraints as separate side tasks to facilitate face alignment .,0
12361,"In , they set a pose classification task , predicting faces as left , right profile or frontal , in order to assist face alignment .",0
12362,"Even with such a rough pose estimation , this information boosts the alignment accuracy .",0
12363,"Zhang et al. jointly estimates 2 D landmarks update with the auxiliary attributes ( e.g. , gender , expression ) in order to improve alignment accuracy .",0
12364,"The "" mirrorability "" constraint is used in to force the estimated 2D landmarks update be consistent between the image and its mirror image .",0
12365,"In contrast , we integrate a set of constraints in an end - to - end trainable CNN to perform 3 D face alignment .",0
12366,"Multiple datasets in face alignment Despite the huge advantages ( e.g. , avoiding dataset bias ) , there are only a few face alignment works utilizing multiple datasets , owing to the difficulty of leveraging different types of face landmark labeling .",0
12367,Zhu et al.,0
12368,"propose a transductive supervised descent method to transfer face annotation from a source dataset to a target dataset , and use both datasets for training .",0
12369,"ensembles a non-parametric appearance model , shape model and graph matching to estimate the superset of the landmarks .",0
12370,"Even though achieving good results , it suffers from high computation cost .",0
12371,Zhang et al. propose a deep regression network for predicting the superset of landmarks .,0
12372,"For each training sample , the sparse shape regression is adopted to generate the different types of landmark annotations .",0
12373,"In general , most of the mentioned prior work learn to map landmarks between two datasets , while our method can readily handle an arbitrary number of datasets since the dense 3 D face model can bridge the discrepancy of landmark definitions in various datasets .",0
12374,Dense Face Alignment,1
12375,"In this section , we explain the details of the proposed dense face alignment method .",0
12376,We train a CNN for fitting the dense 3 D face shape to a single input face image .,0
12377,"We utilize the dense 3D shape representation to impose multiple constraints , e.g. , landmark fitting constraint , contour fitting constraint and SIFT pairing constraint , to train such CNN .",0
12378,3D,0
12379,Face Representation,0
12380,"We represent the dense 3D shape of the face as , S , which contains the 3D locations of Q vertices ,",0
12381,"To compute S fora face , we follow the 3 DMM to represent it by a set of 3D shape bases ,",0
12382,"where the face shape S is the summation of the mean shapeS and the weighted PCA shape bases Sid and S exp with corresponding weights of p id , p exp .",0
12383,"In our work , we use 199 shape bases Si id , i = { 1 , ... , 199 } for representing identification variances such as tall / short , light / heavy , and male / female , and 29 shape bases Si exp , i = { 1 , ... , 29 } for representing expression variances such as mouth - opening , smile , kiss and etc .",0
12384,"Each basis has Q = 53 , 215 vertices , which are corresponding to vertices overall the other bases .",0
12385,"The mean shapeS and the identification bases Sid are from Basel Face Model , and the expression bases S exp are from FaceWarehouse .",0
12386,"A subset of N vertices of the dense 3 D face U corresponds to the location of 2D landmarks on the image ,",0
12387,"By considering weak perspective projection , we can estimate the dense shape of a 2 D face based on the 3D face shape .",0
12388,"The projection matrix has 6 degrees of freedom and can model changes w.r.t. scale , rotation angles ( pitch ? , yaw ? , roll ? ) , and translations ( t x , t y ) .",0
12389,The transformed dense face shape A ?,0
12390,"R 3Q can be represented as ,",0
12391,"where A can be orthographically projected onto 2 D plane to achieve U. Hence , z - coordinate translation ( m 12 ) is out of our interest and assigned to be 0 .",0
12392,The orthographic projection can be denoted as matrix Pr = 1 0 0 0 1 0 .,0
12393,"Given the properties of projection matrix , the normalized third row of the projection matrix can be represented as the outer product of normalized first two rows , Therefore , the dense shape of an arbitrary 2 D face can be determined by the first two rows of the projection parameters m = [ m 1 , , m 8 ] ?",0
12394,R 8 and the shape basis,0
12395,"The learning of the dense 3D shape is turned into the learning of m and p , which is much more manageable in term of the dimensionality .",0
12396,CNN,0
12397,Architecture,0
12398,"Due to the success of deep learning in computer vision , we employ a convolutional neural network ( CNN ) to learn the nonlinear mapping function f ( ? ) from the input image Ito the corresponding projection parameters m and shape parameters p.",0
12399,The estimated parameters can then be utilized to construct the dense 3 D face shape .,0
12400,"Our CNN network has two branches , one for predicting m and another for p , shown in .",0
12401,Two branches share the first three convolutional blocks .,0
12402,"After the third block , we use two separate convolutional blocks to extract taskspecific features , and two fully connected layers to transfer the features to the final output .",0
12403,"Each convolutional block is a stack of two convolutional layers and one max pooling layer , and each conv / fc layer is followed by one batch normalization layer and one leaky ReLU layer .",0
12404,"In order to improve the CNN learning , we employ a loss function including multiple constraints :",0
12405,Parameter Constraint ( PC ) J pr minimizes the difference between the estimated parameters and the ground truth parameters ; Landmark Fitting Constraint ( LFC ),0
12406,J lm reduces the alignment error of 2D landmarks ; Contour Fitting Constraint ( CFC ) J c enforces the match between the contour of the estimated 3D shape and the contour pixels of the input image ; and SIFT Pairing Constraint ( SPC ) J s encourages that the SIFT feature point pairs of two face images to correspond to the same 3D vertices .,0
12407,"We define the overall loss function as ,",0
12408,"where the parameter constraint ( PC ) loss is defined as ,",0
12409,Landmark Fitting Constraint ( LFC ) aims to minimize the difference between the estimated 2D landmarks and the ground truth 2D landmark labeling U lm ?,0
12410,R 2 N .,0
12411,"Given 2 D face images with a particular landmark labeling , we first manually mark the indexes of the 3D face vertices that are anatomically corresponding to these landmarks .",0
12412,The collection of these indexes is denoted as i lm .,0
12413,"After the shape A is computed from Eqn. 4 with the estimatedm andp , the 3D landmarks can be extracted from A by A ( : , i lm ) .",0
12414,"With projection of A ( : , i lm ) to 2 D plain , the LFC loss is defined as ,",0
12415,( a ) ( b ) ( c ) .,0
12416,The CFC fitting process .,0
12417,Ac is computed from estimated 3 D face shape and Uc is computed from the off - the - shelf edge detector .,0
12418,"Contour correspondence is obtained via Closest Pair Algorithm , and loss Jc is calculated based on Eqn.",0
12419,10,0
12420,"where the subscript F represents the Frobenius Norm , and L is the number of pre-defined landmarks .",0
12421,Contour Fitting Constraint ( CFC ),0
12422,"Contour Fitting Constraint ( CFC ) aims to minimize the error between the projected outer contour ( i.e. , silhouette ) of the dense 3D shape and the corresponding contour pixels in the input face image .",0
12423,The outer contour can be viewed as the boundary between the background and the 3D face while rendering 3 D space onto a 2D plane .,0
12424,"On databases such as AFLW where there is alack of labeled landmarks on the silhouette due to self - occlusion , this constraint can be extremely helpful .",0
12425,"To utilize this contour fitting constraint , we need to follow these three steps :",0
12426,1 ) Detect the true contour in the 2 D face image ; 2 ) Describe the contour vertices on the estimated 3D shape A ; and 3 ),0
12427,"Determine the correspondence between true contour and the estimated one , and backpropagate the fitting error .",0
12428,"First of all , we adopt an off - the - shelf edge detector , HED , to detect the contour on the face image , U c ?",0
12429,R 2 L .,0
12430,The HED has a high accuracy at detecting significant edges such as face contour in our case .,0
12431,"Additionally , in certain datasets , such as 300 W and AFLW - LPFA , additional landmark labelings on the contours are available .",0
12432,"Thus we can further refine the detected edges by only retaining edges that are within a narrow band determined by those contour landmarks , shown ina .",0
12433,This preprocessing step is done offline before the training starts .,0
12434,"In the second step , the contour on the estimated 3D shape A can be described as the set of boundary vertices A ( : , i c ) ?",0
12435,R 3 L .,0
12436,A is computed from the estimatedm and p parameters .,0
12437,"By utilizing the Delaunay triangulation to represent shape A , one edge of a triangle is defined as the boundary if the adjacent faces have a sign change in the zvalues of the surface normals .",0
12438,This sign change indicates a change of visibility so that the edge can be considered as a boundary .,0
12439,"The vertices associated with this edge are defined as boundary vertices , and their collection is denoted as i c .",0
12440,This process is shown inb .,0
12441,"In the third step , the point - to - point correspondences between U c and A ( : , i c ) are needed in order to evaluate the constraint .",0
12442,"Given that we normally detect partial contour pixels on 2D images while the contour of 3D shape is typically complete , we match the contour pixel on the 2D images with closest point on 3D shape contour , and then calculate the minimun distance .",0
12443,"The sum of all minimum distances is the error of CFC , as shown in the Eqn. 10 .",0
12444,"To make CFC loss differentiable , we rewrite Eqn. 10 to compute the vertex index of the closest contour projection point , i.e. , k 0 = arg min k? ic PrA ( : , k) ? U c (: , j) 2 . Once k 0 is determined , the CFC loss will be differentiable , similar to Eqn .",0
12445,9 .,0
12446,SIFT Pairing Constraint ( SPC ),0
12447,"SIFT Pairing Constraint ( SPC ) regularizes the predictions of dense shape to be consistent on the significant facial points other than pre-defined landmarks , such as edges , wrinkles , and moles .",0
12448,"The Scale - invariant feature transform ( SIFT ) descriptor is a classic local representation that is invariant to image scaling , noise , and illumination .",0
12449,It is widely used in many regression - based face alignment methods to extract the local information .,0
12450,"In our work , the SIFT descriptors are used to detect and represent the significant points within the face pair .",0
12451,"The face pair can either come from the same people with different poses and expressions , or the same image with different augmentation , e.g. , cropping , rotation and 3D augmentation , shown in .",0
12452,"The more face pairs we have , the stronger this constraint is .",0
12453,"Given a pair of faces i and j , we first detect and match SIFT points on two face images .",0
12454,The matched SIFT points are denoted as U i sand U j s ?,0
12455,R 2 Lij .,0
12456,"With a perfect dense face alignment , the matched SIFT points would overlay with exactly the same vertex in the estimated 3 D face shapes , denoted as A i and A j .",0
12457,"In practices , to verify how likely this ideal world is true and leverage it as a constraint , we first find the 3D vertices ii s whose projections overlay with the 2D SIFT points , U i s .",0
12458,"Similarly , we find j j s based on U j s .",0
12459,Now we define the SPC loss function as,0
12460,"where A i is computed using {m i , pi }.",0
12461,"As shown in , we map SIFT points from one face to the other and compute their distances w.r.t. the matched SIFT points on the other face .",0
12462,"With the mapping from both images , we have two terms in the loss function of Eqn. 12 .",0
12463,Experimental Results,0
12464,Datasets,0
12465,"We evaluate our proposed method on four benchmark datasets : AFLW - LFPA , AFLW2000 - 3D , 300W and IJBA .",0
12466,All datasets used in our training and testing phases are listed in Tab .,0
12467,"1 . AFLW - LFPA : AFLW contains around 25 , 000 face images with yaw angles between 90 , and each image is labeled with up to 21 visible landmarks .",0
12468,"In , a subset of AFLW with a balanced distribution of the yaw angle is introduced as AFLW - LFPA .",0
12469,"It consists of 3 , 901 training images and 1 , 299 testing images .",0
12470,Each image is labeled with 13 additional landmarks .,0
12471,"AFLW2000 - 3D : Prepared by , this dataset contains 2 , 000 images with yaw angles between 90 of the AFLW dataset .",0
12472,Each image is labeled with 68 landmarks .,0
12473,Both this dataset and AFLW - LFPA are widely used for evaluating large - pose face alignment .,0
12474,"IJBA : IARPA Janus Benchmark A ( IJB - A ) is an inthe-wild dataset containing 500 subjects and 25 , 795 images with three landmark , two landmarks at eye centers and one on the nose .",0
12475,"While this dataset is mainly used for face recognition , the large dataset size and the challenging variations ( e.g. , 90 yaw and images resolution ) make it suitable for evaluating face alignment as well .",0
12476,"300W : 300W integrates multiple databases with standard 68 landmark labels , including AFW , LFPW , HELEN , and IBUG .",0
12477,This is the widely used database for evaluating near - frontal face alignment .,0
12478,COFW : This dataset includes near - frontal face images with occlusion .,0
12479,We use this dataset in training to make the model more robust to occlusion .,0
12480,"Caltech10k : It contains four labeled landmarks : two on eye centers , one on the top of the nose and one mouth center .",0
12481,We do not use the mouth center landmark since there is no corresponding vertex on the 3D shape existing for it .,0
12482,"LFW : Despite having no landmark labels , LFW can be used to evaluate how dense face alignment method performs via the corresponding SIFT points between two images of the same individual .",0
12483,Experimental setup,0
12484,Training sets and procedures :,0
12485,"While utilizing multiple datasets is beneficial for learning an effective model , it also poses challenges to the training procedure .",0
12486,"To make the training more manageable , we train our DeFA model in three stages , with the intention to gradually increase the datasets and employed constraints .",0
12487,"At stage 1 , we use 300W - LP to train our DeFA network with parameter constraint ( PL ) .",0
12488,"At stage 2 , we additionally include samples from the Caltech10K , and COFW to continue the training of our network with the additional landmark fitting constraint ( LFC ) .",0
12489,"At stage 3 , we fine - tune the model with SPC and CFC constraints .",0
12490,"For large - pose face alignment , we fine - tune the model with AFLW - LFPA training set .",0
12491,"For near - frontal face alignment , we fine - tune the model with 300 W training set .",0
12492,"All samples at the third stage are augmented 20 times with up to 20 random in - plain rotation and 15 % random noise on the center , width , and length of the initial bounding box .",0
12493,Tab. 2 shows the datasets and .,0
12494,"To train the network , we use 20 , 10 , and 10 epochs for stage 1 to 3 .",1
12495,"We set the initial global learning rate as 1 e ? 3 , and reduce the learning rate by a factor of 10 when the training error approaches a plateau .",1
12496,"The minibatch size is 32 , weight decay is 0.005 , and the leak factor for Leaky ReLU is 0.01 .",1
12497,"In stage 2 , the regularization weights ?",0
12498,pr for PC is 1 and ?,0
12499,"lm for LFC is 5 ; In stage 3 , the regularization weights ? lm , ? s , ?",0
12500,"c for LFC , SPC and CFC are set as 5 , 1 and 1 , respectively .",0
12501,Evaluation metrics :,0
12502,"For performance evaluation and comparison , we use two metrics for normalizing the MSE .",0
12503,"We follow the normalization method in for large - pose faces , which normalizes the MSE by using the bounding box size .",0
12504,"We term this metric as "" NME - lp "" .",0
12505,"For the nearfrontal view datasets such as 300 W , we use the inter-ocular distance for normalizing the MSE , termed as "" NME - nf "" .",0
12506,Experiments on Large - pose Datasets,0
12507,"To evaluate the algorithm on large - pose datasets , we use the AFLW - LFPA , AFLW2000 - 3D , and IJB - A datasets .",0
12508,"The results are presented in Tab. 3 , where the performance of the baseline methods is either reported from the published papers or by running the publicly available source code .",0
12509,"For AFLW - LFPA , our method outperforms the best methods with a large margin of 17.8 % improvement .",1
12510,"For AFLW2000 - 3D , our method also shows a large improvement .",1
12511,"Specifically , for images with yaw angle in [ 60 , 90 ] , our method improves the performance by 28 % ( from 7.93 to 5.68 ) .",1
12512,"For the IJB - A dataset , even though we are able to only compare the accuracy for the three labeled landmarks , our method still reaches a higher accuracy .",1
12513,"Note that the best performing baselines , 3DDFA and PAWF , share the similar overall approach in estimating m and p , and also aim for large - pose face alignment .",0
12514,The consistently superior performance of our DeFA indicates that we have advanced the state of the art in large - pose face alignment .,0
12515,Experiments on Near-frontal Datasets,0
12516,"Even though the proposed method can handle largepose alignment , to show its performance on the near- frontal datasets , we evaluate our method on the 300W dataset .",1
12517,The result of the state - of - the - art method on the both common and challenging sets are shown in Tab .,0
12518,"4 . To find the corresponding landmarks on the cheek , we apply the landmark marching algorithm to move contour landmarks from self - occluded location to the silhouette .",0
12519,Our method is the second best method on the challenging set .,1
12520,"In general , the performance of our method is comparable to other methods that are designed for near - frontal datasets , especially under the following consideration .",1
12521,"That is , most prior face alignment methods do not employ shape constraints such as 3 DMM , which could bean advantage for near - frontal face alignment , but might be a disadvantage for large - pose face alignment .",0
12522,"The only exception in Tab. 4 in 3DDFA , which attempted to overcome the shape constraint by using the additional SDM - based finetuning .",0
12523,"It is a strong testimony of our model in that DeFA , without further finetuning , outperforms both 3DDFA and its fine tuned version with SDM .",0
12524,Ablation Study,0
12525,"To analyze the effectiveness of the DeFA method , we design two studies to compare the influence of each part in the DeFA and the improvement by adding each dataset .",0
12526,Tab. 5 shows the consistent improvement achieved by utilizing more datasets in different stages and constraints according to Tab.,0
12527,2 on both testing datasets .,0
12528,It shows the advantage and the ability of our method in leveraging more datasets .,0
12529,The accuracy of our method on the AFLW2000 - 3D consistently improves by adding more datasets .,1
12530,"For the AFLW - PIFA dataset , our method achieves 9.5 % and 20 % relative improvement by utilizing the datasets in the stage 2 and stage 3 over the first stage , respectively .",1
12531,"If including the datasets from both the second and third stages , we can have 26 % relative improvement and achieve NME of 3.86 % .",1
12532,Comparing the second and third rows in Tab .,0
12533,5 shows that the effectiveness of CFC and SPC is more than LFC .,0
12534,This is due to the utilization of more facial matching in the CFC and SPC .,0
12535,The second study shows the performance improvement achieved by using the proposed constraints .,0
12536,We train models with different types of active constraints and test them on the AFLW - PIFA test set .,0
12537,"Due to the time constraint , for this experiment , we did not apply 20 times augmentation of the third stage 's dataset .",0
12538,We show the results in the left of .,0
12539,Comparing LFC + SPC and LFC + CFC performances shows that the CFC is more helpful than the SPC .,1
12540,The reason is that CFC is more helpful in correcting the pose of the face and leads to more landmark error reduction .,0
12541,Using all constraints achieves the best performance .,1
12542,"Finally , to evaluate the influence of using the SIFT pairing constraint ( SPC ) , we use all of the three stages datasets to train our method .",0
12543,"We select 5 , 000 pairs of images from the IJB - A dataset and compute the NME - lp of all matched SIFT points according to Eqn. 12 .",0
12544,"The right plot in illustrates the CED diagrams of NME - lp , for the trained models with and without the SIFT pairing constraint .",0
12545,This result shows that for the images with NME - lp between 5 % and 15 % the SPC is helpful .,1
12546,Part of the reason DeFA works well is that it receives .,0
12547,The estimated dense 3D shape and their landmarks with visibility labels for different datasets .,0
12548,"From top to bottom , the results on AFLW - LPFA , IJB - A and 300W datasets are shown in two rows each .",0
12549,The green landmark are visible and the red landmarks show the estimated locations for invisible landmarks .,0
12550,"Our model can fit to diverse poses , resolutions , and expressions .",0
12551,""" dense "" supervision .",0
12552,"To show that , we take all matched SIFT points in the 300W - LP dataset , find their corresponding vertices , and plot the log of the number of SIFT points on each of the 3D face vertex .",0
12553,"As shown in , SPC utilizes SIFT points to cover the whole 3D shape and the points in the highly textured areas are substantially used .",0
12554,We can expect that these SIFT constraints will act like anchors to guild the learning of the model fitting process .,0
12555,Conclusion,0
12556,We propose a large - pose face alignment method which estimates accurate 3 D face shapes by utilizing a deep neural network .,0
12557,"In addition to facial landmark fitting , we propose to align contours and the SIFT feature point pairs to extend the fitting beyond facial landmarks .",0
12558,Our method is able to leverage from utilizing multiple datasets with different land -.,0
12559,The log plot of the number of matched SIFT points in the 300W - LP training set .,0
12560,"It shows that the SIFT constraints cover the whole face , especially the highly textured area .",0
12561,mark markups and numbers of landmarks .,0
12562,We achieve the state - of - the - art performance on three challenging large pose datasets and competitive performance on hard medium pose datasets .,0
12563,title,0
12564,Deep Multi- Center Learning for Face Alignment,1
12565,abstract,0
12566,Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks .,0
12567,Most of the existing deep learning methods only use one fully - connected layer called shape prediction layer to estimate the locations of facial landmarks .,0
12568,"In this paper , we propose a novel deep learning framework named Multi - Center Learning with multiple shape prediction layers for face alignment .",0
12569,"In particular , each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively .",0
12570,"Challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",0
12571,"Moreover , to reduce the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",0
12572,Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real - time performance .,0
12573,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,1
12574,"Index Terms - Multi- Center Learning , Model Assembling , Face Alignment * Corresponding author .",0
12575,( a ),0
12576,Chin is occluded .,0
12577,( b ) Right contour is invisible .,0
12578,I. INTRODUCTION,0
12579,"Face alignment refers to detecting facial landmarks such as eye centers , nose tip , and mouth corners .",0
12580,"It is the preprocessor stage of many face analysis tasks like face animation , face beautification , and face recognition .",0
12581,"A robust and accurate face alignment is still challenging in unconstrained scenarios , owing to severe occlusions and large appearance variations .",0
12582,"Most conventional methods , , , only use low - level handcrafted features and are not based on the prevailing deep neural networks , which limits their capacity to represent highly complex faces .",0
12583,"Recently , several methods use deep networks to estimate shapes from input faces .",0
12584,"Sun et al. ,",0
12585,"Zhou et al. , and Zhang et al .",0
12586,employed cascaded deep networks to refine predicted shapes successively .,0
12587,"Due to the use of multiple networks , these methods have high model complexity with complicated training processes .",0
12588,"Taking this into account , Zhang et al. , proposed a Tasks - Constrained Deep Convolutional Network ( TCDCN ) , which uses only one deep network with excellent performance .",0
12589,"However , it needs extra labels of facial attributes for training samples , which limits its universality .",0
12590,Each facial landmark is not isolated but highly correlated with adjacent landmarks .,0
12591,"As shown in , facial landmarks along the chin are all occluded , and landmarks around the mouth are partially occluded .",0
12592,shows that landmarks on the right side of face are almost invisible .,0
12593,"Therefore , landmarks in the same local face region have similar properties including occlusion and visibility .",0
12594,It is observed that the nose can be localized roughly with the locations of eyes and mouth .,0
12595,There are also structural correlations among different facial parts .,0
12596,"Motivated by this fact , facial landmarks are divided into several clusters based on their semantic relevance .",0
12597,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .",1
12598,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .",1
12599,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",1
12600,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",1
12601,The entire framework reinforces the learning process of each landmark with a low model complexity .,1
12602,The main contributions of this study can be summarized as follows :,0
12603,We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks .,0
12604,We propose a model assembling method which ensures a low model complexity .,0
12605,Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real - time performance .,0
12606,The remainder of this paper is structured as below .,0
12607,We discuss related works in the next section .,0
12608,"In Section III , we illuminate the structure of our network and the learning algorithm .",0
12609,Extensive experiments are carried out in Section IV .,0
12610,Section V concludes this work .,0
12611,II .,0
12612,RELATED WORK,0
12613,"We review researches from three aspects related to our method : conventional face alignment , unconstrained face alignment , face alignment via deep learning .",0
12614,A. Conventional Face Alignment,0
12615,Conventional face alignment methods can be classified as two categories : template fitting and regression - based .,0
12616,Template fitting methods match faces by constructing shape templates .,0
12617,"Cootes et al. proposed atypical template fitting method named Active Appearance Model ( AAM ) , which minimizes the texture residual to estimate the shape .",0
12618,"Asthana et al. used regression techniques to learn functions from response maps to shapes , in which the response map has stronger robustness and generalization ability than texture based features of AAM .",0
12619,Pedersoli et al .,0
12620,"developed the mixture of trees of parts method by extending the mixtures from trees to graphs , and learned a deformable detector to align its parts to faces .",0
12621,"However , these templates are not complete enough to cover complex variations , which are difficult to be generalized to unseen faces .",0
12622,Regression - based methods predict the locations of facial landmarks by learning a regression function from face features to shapes .,0
12623,Cao et al. proposed an Explicit Shape Regression ( ESR ) method to predict the shape increment with pixeldifference features .,0
12624,"Xiong et al. proposed a Supervised Descent Method ( SDM ) to detect landmarks by solving the nonlinear least squares problem , with Scale - Invariant Feature Transform ( SIFT ) features and linear regressors being applied .",0
12625,Ren et al .,0
12626,"used a locality principle to extract a set of Local Binary Features ( LBF ) , in which a linear regression is utilized for localizing landmarks .",0
12627,Lee et al.,0
12628,employs Cascade Gaussian Process Regression Trees ( cGPRT ) with shape - indexed difference of Gaussian features to achieve face alignment .,0
12629,"It has a better generalization ability than cascade regression trees , and shows strong robustness against geometric variations of faces .",0
12630,"Most of these methods give an initial shape and refine the shape in an iterative manner , where the final solutions are prone to getting trapped in a local optimum with a poor initialization .",0
12631,"In contrast , our method uses a deep neural network to regress from raw face patches to the locations of landmarks .",0
12632,B. Unconstrained Face Alignment,0
12633,Large pose variations and severe occlusions are major challenges in unconstrained environments .,0
12634,Unconstrained face alignment methods are based on 3D models ordeal with occlusions explicitly .,0
12635,Many methods utilize 3D shape models to solve largepose face alignment .,0
12636,Nair et al .,0
12637,refined the fit of a 3D point distribution model to perform landmark detection .,0
12638,Yu et al . used a cascaded deformable shape model to detect landmarks of large - pose faces .,0
12639,Cao et al .,0
12640,employed a displaced dynamic expression regression to estimate the 3D face shape and 2D facial landmarks .,0
12641,The predicted 2D landmarks are used to adjust the model parameters to better fit the current user .,0
12642,"Jeni et al. proposed a 3D cascade regression method to implement 3 D face alignment , which can maintain the pose invariance of facial landmarks within the range of around 60 degrees .",0
12643,There are several occlusion - free face alignment methods .,0
12644,"Burgos - Artizzu et al. developed a Robust Cascaded Pose Regression ( RCPR ) method to detect occlusions explicitly , and uses shape - indexed features to regress the shape increment .",0
12645,"Yu et al. utilizes a Bayesian model to merge the estimation results from multiple regressors , in which each regressor is trained to localize facial landmarks with a specific pre-defined facial part being occluded .",0
12646,"Wu et al. proposed a Robust Facial Landmark Detection ( RFLD ) method , which uses a robust cascaded regressor to handle complex occlusions and large head poses .",0
12647,"To improve the performance of occlusion estimation , landmark visibility probabilities are estimated with an explicit occlusion constraint .",0
12648,"Different from these methods , our method is not based on 3D models and does not process occlusions explicitly .",0
12649,C. Face Alignment via Deep Learning,0
12650,Deep learning methods can be divided into two classes : single network based and multiple networks based .,0
12651,Sun et al .,0
12652,"estimated the locations of 5 facial landmarks using Cascaded Convolutional Neural Networks ( Cascaded CNN ) , in which each level computes averaged estimated shape and the shape is refined level by level .",0
12653,Zhou et al .,0
12654,used multi - level deep networks to detect facial landmarks from coarse to fine .,0
12655,"Similarly ,",0
12656,Zhang et al. proposed Coarse - to - Fine Auto - encoder Networks ( CFAN ) .,0
12657,These methods all use multi-stage deep networks to localize landmarks in a coarse - tofine manner .,0
12658,"Instead of using cascaded networks , Honari et al. proposed Recombinator Networks ( RecNet ) for learning coarse - to - fine feature aggregation with multi -scale input maps , where each branch extracts features based on current maps and the feature maps of coarser branches .",0
12659,A few methods employ a single network to solve the face alignment problem .,0
12660,"Shao et al. proposed a Coarse - to - Fine Training ( CFT ) method to learn the mapping from input face patches to estimated shapes , which searches the solutions smoothly by adjusting the relative weights between principal landmarks and elaborate landmarks .",0
12661,"Zhang et al. , used the TCDCN with auxiliary facial attribute recognition to predict correlative facial properties like expression and pose , which improves the performance of face alignment .",0
12662,"Xiao et al. proposed a Recurrent Attentive - Refinement ( RAR ) network for face alignment under unconstrained conditions , where shape - indexed deep features and temporal information are taken as inputs and shape predictions are recurrently revised .",0
12663,"Compared to these methods , our method uses only one network and is independent of additional facial attributes .",0
12664,III .,0
12665,MULTI - CENTER LEARNING FOR FACE ALIGNMENT,0
12666,A. Network Architecture,0
12667,The architecture of our network MCL is illustrated in .,0
12668,"MCL contains three max - pooling layers , each of which follows a stack of two convolutional layers proposed by VGGNet .",0
12669,"In the fourth stack of convolutional layers , we use a convolutional layer with D feature maps above two convolutional layers .",0
12670,We perform Batch Normalization ( BN ) and Rectified Linear Unit ( ReLU ) after each convolution to accelerate the convergence of our network .,0
12671,"Most of the existing deep learning methods such as TCDCN , use the fully - connected layer to extract features , which is apt to overfit and hamper the generalization ability of the network .",0
12672,"To sidestep these problems , we operate Global Average Pooling on the last convolutional layer to extract a high - level feature representation x , which computes the average of each feature map .",0
12673,"With this improvement , our MCL acquires a higher representation power with fewer parameters .",0
12674,"Face alignment can be regarded as a nonlinear regression problem , which transforms appearance to shape .",0
12675,"A transformation ? S ( ) is used for modeling this highly nonlinear function , which extracts the feature x from the input face image I , formulated as",0
12676,"corresponds to the bias , and ? S ( ) is a composite function of operations including convolution , BN , ReLU , and pooling .",0
12677,"Traditionally , only one shape prediction layer is used , which limits the performance .",0
12678,"In contrast , our MCL uses multiple shape prediction layers , each of which emphasizes on the detection of a certain cluster of landmarks .",0
12679,"The first several layers are shared by multiple shape prediction layers , which are called shared layers forming the composite function ? S ( ) .",0
12680,"For the i - th shape prediction layer , i = 1 , , m , a weight matrix W i = ( w i 1 , w i 2 , , w i 2 n ) ?",0
12681,"R ( D+1 ) 2n is used to connect the feature x , where m and n are the number of shape prediction layers and landmarks , respectively .",0
12682,"The reason why we train each shape prediction layer to predict n landmarks instead of one cluster of landmarks is that different facial parts have correlations , as shown in .",0
12683,"To decrease the model complexity , we use a model assembling function ? a ( ) to integrate multiple shape prediction layers into one shape prediction layer , which is formulated as",0
12684,"where W a = ( w a 1 , w a 2 , , w a 2 n ) ?",0
12685,R ( D+1 ) 2n is the assembled weight matrix .,0
12686,"Specifically , w a",0
12687,where ?,0
12688,2j?1 and ?,0
12689,2 j denote the predicted x - coordinate and y-coordinate of the j - th landmark respectively .,0
12690,"Compared to other typical convolutional networks like VG - GNet , GoogLe - Net , and ResNet , our network MCL is substantially smaller and shallower .",0
12691,We believe that such a concise structure is efficient for estimating the locations of facial landmarks .,0
12692,"Firstly , face alignment aims to regress coordinates of fewer than 100 facial landmarks generally , which demands much lower model complexity than visual recognition problems with more than 1 , 000 classes .",0
12693,"Secondly , a very deep network may fail to work well for landmark detection owing to the reduction of spatial information layer by layer .",0
12694,"Other visual localization tasks , like face detection , usually use multiple cascaded shallow networks rather than a single very deep network .",0
12695,"Finally , common face alignment benchmarks only contain thousands of training images .",0
12696,A simple network is not easy to overfit given a small amount of raw training data .,0
12697,B. Learning Algorithm,0
12698,The overview of our learning algorithm is shown in Algorithm,0
12699,1 . ? t and ?,0
12700,v are the training set and the validation Algorithm 1 Multi - Center Learning Algorithm .,0
12701,Output : ?.,0
12702,1 : Pre-train shared layers and one shape prediction layer until convergence ; 2 : Fix the parameters of the first six convolutional layers and fine - tune subsequent layers until convergence ; Fine - tune all the layers until convergence ; for i = 1 tom do Fix ?,0
12703,Sand fine - tune the i - th shape prediction layer until convergence ; 6 : end for,0
12704,set respectively . ?,0
12705,"is the set of parameters including weights and biases of our network MCL , which is updated using Mini - Batch Stochastic Gradient Descent ( SGD ) at each iteration .",0
12706,The face alignment loss is defined as,0
12707,"where u j is the weight of the j - th landmark , y 2 j?",0
12708,"1 and y 2 j denote the ground - truth x - coordinate and y-coordinate of the j - th landmark respectively , and dis the ground truth interocular distance between the eye centers .",0
12709,"Inter-ocular distance normalization provides fair comparisons among faces with different size , and reduces the magnitude of loss to speedup the learning process .",0
12710,"During training , a too high learning rate may cause the missing of optimum so far as to the divergence of network , and a too low learning rate may lead to falling into a local optimum .",0
12711,"We employ a low initial learning rate to avoid the divergence , and increase the learning rate when the loss is reduced significantly and continue the training procedure .",0
12712,"1 ) Pre-Training and Weighting Fine - Tuning : In Step 1 , a basic model ( BM ) with one shape prediction layer is pretrained to learn a good initial solution .",0
12713,"In Eq. 4 , u j = 1 for all j.",0
12714,The average alignment error of each landmark of BM on ?,0
12715,"v are b 1 , , b n respectively , which are averaged overall the images .",0
12716,The landmarks with larger errors than remaining landmarks are treated as challenging landmarks .,0
12717,"In Steps 2 and 3 , we focus on the detection of challenging landmarks by assigning them larger weights .",0
12718,The weight of the j - th landmark is proportional to its alignment error as,0
12719,"Instead of fine - tuning all the layers from BM directly , we use two steps to search the solution smoothly .",0
12720,Step 2 searches the solution without deviating from BM overly .,0
12721,Step 3 searches the solution within a larger range on the basis of the previous step .,0
12722,"This stage is named weighting fine - tuning , which learns a weighting model ( WM ) with higher localization accuracy of challenging landmarks .",0
12723,2 ) Multi- Center Fine - Tuning and Model Assembling :,0
12724,"The face is partitioned into seven parts according to its semantic structure : left eye , right eye , nose , mouth , left contour , chin , and right contour .",0
12725,"As shown in , different labeling patterns of 5 , 29 , and 68 facial landmarks are partitioned into 4 , 5 , and 7 clusters respectively .",0
12726,"For the i - th shape prediction layer , the i - th cluster of landmarks are treated as the optimized center , and the set of indexes of remaining landmarks is denoted as Q i .",0
12727,"From Steps 4 to 6 , the parameters of shared layers ?",0
12728,"S are fixed , and each shape prediction layer is initialized with the parameters of the shape prediction layer of WM .",0
12729,"When finetuning the i - th shape prediction layer , the weights of landmarks in P i and Q i are defined as",0
12730,where ?,0
12731,1 is a coefficient to make the i - th shape prediction layer emphasize on the detection of the i - th cluster of landmarks .,0
12732,The constraint between u P i and u Q i is formulated as,0
12733,where | | refers to the number of elements in a cluster .,0
12734,"With Eqs. 6 and 7 , the solved weights are formulated as",0
12735,The average alignment error of each landmark of WM on ?,0
12736,"v are w 1 , , w n respectively .",0
12737,"Similar to Eq. 5 , the weight of the j - th landmark is",0
12738,"Although the landmarks in P i are mainly optimized , remaining landmarks are still considered with very small weights rather than zero .",0
12739,This is beneficial for utilizing implicit structural correlations of different facial parts and searching the solutions smoothly .,0
12740,This stage is called multi-center finetuning which learns multiple shape prediction layers .,0
12741,"In Step 7 , multiple shape prediction layers are assembled into one shape prediction layer by Eq.",0
12742,2 .,0
12743,"With this model assembling stage , our method learns an assembling model ( AM ) .",0
12744,"There is no increase of model complexity in the assembling process , so AM has a low computational cost .",0
12745,It improves the detection precision of each facial landmark by integrating the advantage of each shape prediction layer .,0
12746,3 ) Analysis of Model Learning :,0
12747,"To investigate the influence from the weights of landmarks on learning procedure , we calculate the derivative of Eq. 4 with respect to ?",0
12748,k :,0
12749,"where k ? { 2 j ? 1 , 2 j} , j = 1 , , n. During the learning process , the assembled weight matrix W a in Eq. 3 is updated by SGD .",0
12750,"Specifically ,",0
12751,where ?,0
12752,is the learning rate .,0
12753,"If the j-th landmark is given a larger weight , its corresponding parameters will be updated with a larger step towards the optimal solution .",0
12754,"Therefore , weighting the loss of each landmark ensures that the landmarks with larger weights are mainly optimized .",0
12755,"Our method first uses the weighting fine - tuning stage to optimize challenging landmarks , and further uses the multi-center fine - tuning stage to optimize each cluster of landmarks respectively .",0
12756,IV .,0
12757,EXPERIMENTS A. Datasets and Settings,0
12758,1 ) Datasets :,0
12759,"There are three challenging benchmarks AFLW , COFW , and IBUG , which are used for evaluating face alignment with severe occlusion and large variations of pose , expression , and illumination .",0
12760,The provided face bounding boxes are employed to crop face patches during testing .,0
12761,"AFLW contains 25 , 993 faces under real - world conditions gathered from Flickr .",0
12762,"Compared with other datasets like MUCT and LFPW , AFLW exhibits larger pose variations and extreme partial occlusions .",0
12763,"Following the settings of , , 2 , 995 images are used for testing , and 10 , 000 images annotated with 5 landmarks are used for training , which includes 4 , 151 LFW images and 5 , 849 web images .",0
12764,"COFW is an occluded face dataset in the wild , in which the faces are designed with severe occlusions using accessories and interactions with objects .",0
12765,"It contains 1 , 007 images annotated with 29 landmarks .",0
12766,"The training set includes 845 LFPW faces and 500 COFW faces , and the testing set includes remaining 507 COFW faces .",0
12767,"IBUG contains 135 testing images which present large variations in pose , expression , illumination , and occlusion .",0
12768,"The training set consists of AFW , the training set of LFPW , and the training set of Helen , which are from 300 - W with 3 , 148 images labeled with 68 landmarks .",0
12769,2 ) Implementation Details :,0
12770,"We enhance the diversity of raw training data on account of their limited variation patterns , using five steps : rotation , uniform scaling , translation , horizontal flip , and JPEG compression .",0
12771,"In particular , for each training face , we firstly perform multiple rotations , and attain a tight face bounding box covering the ground truth locations of landmarks of each rotated result respectively .",0
12772,"Uniform scaling and translation with different extents on face bounding boxes are further conducted , in which each newly generated face bounding box is used to crop the face .",1
12773,Finally training samples are augmented through horizontal flip and JPEG compression .,1
12774,It is beneficial for avoiding overfitting and improving the robustness of learned models by covering various patterns .,0
12775,We train our MCL using an open source deep learning framework Caffe .,1
12776,"The input face patch is a 50 50 grayscale image , and each pixel value is normalized to [ ?1 , 1 ) by subtracting 128 and multiplying 0.0078125 .",1
12777,"A more complex model is needed for a labeling pattern with more facial landmarks , so Dis set to be 512/512/1 , 024 for 5/29/68 facial landmarks .",1
12778,"The type of solver is SGD with a mini-batch size of 64 , a momentum of 0.9 , and a weight decay of 0.0005 .",1
12779,"The maximum learning iterations of pre-training and each finetuning step are 1810 4 and 610 4 respectively , and the initial learning rates of pre-training and each fine - tuning step are 0.02 and 0.001 respectively .",1
12780,Note that the initial learning rate of fine - tuning should below to preserve some representational structures learned in the pre-training stage and avoid missing good intermediate solutions .,0
12781,"The learning rate is multiplied by a factor of 0.3 at every 3 10 4 iterations , and the remaining parameter ?",1
12782,is set to be 125 .,0
12783,3 ) Evaluation Metric :,0
12784,"Similar to previous methods , , , we report the inter-ocular distance normalized mean error , and treat the mean error larger than 10 % as a failure .",0
12785,"To conduct a more comprehensive comparison , the cumulative errors distribution ( CED ) curves are plotted .",0
12786,"To measure the time efficiency , the average running speed ( Frame per Second , FPS ) on a single core i5-6200U",0
12787,2.3 GHz CPU is also reported .,0
12788,A single image is fed into the model at a time when computing the speed .,0
12789,"In other words , we evaluate methods on four popular metrics : mean error ( % ) , failure rate ( % ) , CED curves , and average running speed .",0
12790,"In the next sections , % in all the results are omitted for simplicity .",0
12791,B. Comparison with State - of - the - Art Methods,0
12792,"We compare our work MCL against state - of - the - art methods including ESR , SDM , Cascaded CNN , RCPR , CFAN , LBF , c GPRT , CFSS , TCDCN , , ALR , CFT , RFLD , RecNet , RAR , and FLD + PDE .",1
12793,All the methods are evaluated on testing images using the face bounding boxes provided by benchmarks .,0
12794,"In addition to given training images , TCDCN uses outside training data labeled with facial attributes .",0
12795,"RAR augments training images with occlusions incurred by outside natural objects like sunglasses , phones , and hands .",0
12796,"FLD + PDE performs facial landmark detection , pose and deformation estimation simultaneously , in which the training data of pose and deformation estimation are used .",0
12797,Other methods including our MCL only utilize given training images from the benchmarks .,0
12798,reports the results of our method and previous works on three benchmarks .,0
12799,"Our method MCL outperforms most of the state - of - the - art methods , especially on AFLW dataset where a relative error reduction of 3.93 % is achieved compared to RecNet .",1
12800,Cascaded CNN estimates the location of each The result is acquired by running the code at https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceAlignment .,0
12801,We compare with other methods on several challenging images from AFLW and COFW respectively in .,1
12802,Our method MCL indicates higher accuracy in the details than previous works .,0
12803,More examples on challenging IBUG are presented in .,0
12804,"MCL demonstrates a superior capability of handling severe occlusions and complex variations of pose , expression , illumination .",1
12805,The CED curves of MCL and several state - of - the - art methods are shown in .,0
12806,It is observed that MCL achieves competitive performance on all three benchmarks .,1
12807,The average running speed of deep learning methods for detecting 68 facial landmarks are presented in .,1
12808,Except for the methods tested on the i5-6200U,0
12809,"2.3 GHz CPU , other methods are reported with the results in the original papers .",0
12810,"Since CFAN utilizes multiple networks , it costs more running time .",0
12811,"RAR achieves only 4 FPS on a Titan - Z GPU , which can not be applied to practical scenarios .",0
12812,"Both TCDCN and our method MCL are based on only one network , so they show higher speed .",0
12813,Our method only takes 17.5 ms per face on a single core i5-6200U,0
12814,2.3 GHz CPU .,0
12815,This profits from low model complexity and computational costs of our network .,0
12816,It can be concluded that our method is able to be extended to real - time facial landmark tracking in unconstrained environments .,0
12817,C. Ablation Study,0
12818,1 ) Global Average Pooling vs. Full Connection :,1
12819,"Based on the previous version of our work , the last maxpooling layer and the D-dimensional fully - connected layer are replaced with a convolutional layer and a Global Average Pooling layer .",0
12820,The results of the mean error of BM and the previous version ( pre - BM ) are shown in .,0
12821,It can be seen that BM performs better on IBUG and COFW but worse on AFLW than pre-BM .,1
12822,It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks .,1
12823,There are higher requirements for learned features when localizing more facial landmarks .,0
12824,"For simple problems especially for localizing 5 landmarks of AFLW , a plain network with full connection is more prone to being trained .",0
12825,The difference between pre -BM and BM is the structure of learning the feature x .,0
12826,"The number of parameters for this part of pre -BM and BM are ( 4 4 128 + 1 ) D = 2 , 049 D and ( 3 3 128 + 1 ) D + 2D + 2D = 1 , 157D respectively , where the three terms for BM correspond to the convolution , the expectation and variance of BN , and the scaling and shifting of BN .",0
12827,"Therefore , BM has a stronger feature learning ability with fewer parameters than pre -BM .",0
12828,2 ) Robustness of Weighting :,1
12829,"To verify the robustness of weighting , random perturbations are added to the weights of landmarks .",0
12830,"In particular , we plus a perturbation ?",0
12831,to the weight of each of random n / 2 landmarks and minus ?,0
12832,to the weight of each of remaining n ?,0
12833,"n / 2 landmarks , where refers to rounding down to the nearest integer .",0
12834,shows the variations of mean error of WM with the increase of ?.,0
12835,"When ? is 0.4 , WM can still achieves good performance .",1
12836,"Therefore , weighting the loss of each landmark is robust to random perturbations .",0
12837,"Even if different weights are obtained , the results will not be affected as long as the relative sizes of weights are identical .",0
12838,3 ) Analysis of Shape Prediction Layers :,1
12839,Our method learns each shape prediction layer respectively with a certain cluster of landmarks being emphasized .,0
12840,The results of WM and two shape prediction layers with respect to the left eye and the right eye on IBUG benchmark are shown in .,0
12841,"Compared to WM , the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters .",1
12842,"As a result , the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM .",1
12843,Note that the two models also improve the localization precision of other clusters .,1
12844,"Taking the left eye model as an example , it additionally reduces the errors of landmarks of right eye , mouth , and chin , which is due to the correlations among different facial parts .",0
12845,"Moreover , for the right eye cluster , the right eye model improves the accuracy more significantly than the left eye model .",0
12846,It can be concluded that each shape prediction layer emphasizes on the corresponding cluster respectively .,0
12847,4 ) Integration of Weighting Fine - Tuning and Multi - Center Fine - Tuning :,1
12848,Here we validate the effectiveness of weighting fine - tuning by removing the weighting fine - tuning stage to learn a Simplified AM from BM . presents the results of mean error of Simplified AM and AM respectively on COFW and IBUG .,0
12849,"Note that Simplified AM has already acquired good results , which verifies the effectiveness of the multicenter fine - tuning stage .",0
12850,"The accuracy of AM is superior to that of Simplified AM especially on challenging IBUG , which is attributed to the integration of two stages .",1
12851,"A Weighting Simplified AM from Simplified AM using the weighting finetuning stage is also learned , whose results are shown in .",0
12852,It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG .,1
12853,"Therefore , we choose to use the multi-center finetuning stage after the weighting fine - tuning stage .",0
12854,"summarizes the results of mean error and failure rate of BM , WM , and AM .",0
12855,It can be observed that AM has higher accuracy and stronger robustness than BM and WM .,1
12856,depicts the enhancement from WM to AM for several examples of COFW .,0
12857,The localization accuracy of facial landmarks from each cluster is improved in the details .,1
12858,It is because each shape prediction layer increases the detection precision of corresponding cluster respectively .,0
12859,D. MCL for Partially Occluded Faces,1
12860,The correlations among different facial parts are very useful for face alignment especially for partially occluded faces .,1
12861,"To investigate the influence of occlusions , we directly use trained WM and AM without any additional processing for partially occluded faces .",0
12862,"Randomly 30 % testing faces from COFW are processed with left eyes being occluded , where the tight bounding box covering landmarks of left eye is filled with gray color , as shown in .",0
12863,"shows the mean error results for the left eye cluster and other clusters of WM and AM on COFW benchmark , where "" with ( w / ) occlusion ( occlu. ) "" denotes that left eyes of the testing faces are processed with handcrafted occlusions as illustrated in , and "" without ( w / o ) occlu. "" denotes that the testing faces are kept unchanged .",0
12864,"Note that our method does not process occlusions explicitly , in which the training data is not performed handcrafted occlusions .",0
12865,"After processing testing faces with occlusions , the mean error results of both WM and AM increase .",1
12866,"Besides the results of landmarks from the left eye cluster , the results of remaining landmarks from other clusters become worse slightly .",1
12867,This is because different facial parts have correlations and the occlusions of the left eye influences results of other facial parts .,0
12868,"Note that WM and AM still perform well on occluded left eyes with the mean error of 6.60 and 6.50 respectively , due to the following reasons .",1
12869,"First , WM weights each landmark proportional to its alignment error , which exploits correlations among landmarks .",0
12870,"Second , AM uses an independent shape prediction layer focusing on a certain cluster of landmarks with small weights u j > 0 , j ?",0
12871,"Q i in Eq. 9 for remaining landmarks , respectively , where correlations among landmarks are further exploited .",0
12872,E. Weighting Fine - Tuning for State - of - the - Art Frameworks,1
12873,"Most recently , there area few well - designed and welltrained deep learning frameworks advancing the performance of face alignment , in which DAN is atypical work .",0
12874,"DAN uses cascaded deep neural networks to refine the localization accuracy of landmarks iteratively , where the entire face image and the landmark heatmap generated from the previous stage are used in each stage .",0
12875,"To evaluate the effectiveness of our method extended to state - of - the - art frameworks , we conduct experiments with our proposed weighting fine - tuning being applied to DAN .",0
12876,"In particular , each stage of DAN is first pre-trained and further weighting fine - tuned , where DAN with weighting fine - tuning is named DAN - WM .",0
12877,Note that the results of retrained DAN ( re - DAN ) using the published code are slightly worse than reported results of DAN .,0
12878,"For a fair comparison , the results of mean error of DAN , re - DAN , and DAN - WM on IBUG benchmark are all shown in .",0
12879,It can be seen that the mean error of re -DAN is reduced from 7.97 to 7.81 after using our proposed weighting fine - tuning .,1
12880,"Note that our method uses only a single neural network , which has a concise structure with low model complexity .",0
12881,"Our network can be replaced with a more powerful one such as cascaded deep neural networks , which could further improve the performance of face alignment .",0
12882,V .,0
12883,CONCLUSION,0
12884,"In this paper , we have developed a novel multi-center learning framework with multiple shape prediction layers for face alignment .",0
12885,The structure of multiple shape prediction layers is beneficial for reinforcing the learning process of each cluster of landmarks .,0
12886,"In addition , we have proposed the model assembling method to integrate multiple shape prediction layers into one shape prediction layer so as to ensure a low model complexity .",0
12887,Extensive experiments have demonstrated the effectiveness of our method including handling complex occlusions and appearance variations .,0
12888,"First , each component of our framework including Global Average Pooling , multiple shape prediction layers , weighting fine - tuning , and multicenter fine - tuning contributes to face alignment .",0
12889,"Second , our proposed neural network and model assembling method allow real - time performance .",0
12890,"Third , we have extended our method for detecting partially occluded faces and integrating with state - of - the - art frameworks , and have shown that our method exploits correlations among landmarks and can further improve the performance of state - of - the - art frameworks .",0
12891,The proposed framework is also promising to be applied for other face analysis tasks and multi-label problems .,0
12892,title,0
12893,Unsupervised Training for 3D Morphable Model Regression,1
12894,abstract,0
12895,We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs .,0
12896,"The training loss is based on features from a facial recognition network , computed onthe-fly by rendering the predicted faces with a differentiable renderer .",0
12897,"To make training from features feasible and avoid network fooling effects , we introduce three objectives : a batch distribution loss that encourages the output distribution to match the distribution of the morphable model , a loopback loss that ensures the network can correctly reinterpret its own output , and a multi-view identity loss that compares the features of the predicted 3 D face and the input photograph from multiple viewing angles .",0
12898,"We train a regression network using these objectives , a set of unlabeled photographs , and the morphable model itself , and demonstrate state - of - the - art results .",0
12899,Introduction,0
12900,"A 3D morphable face model ( 3 DMM ) provides a smooth , low - dimensional "" face space "" spanning the range of human appearance .",0
12901,"Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .",0
12902,"The conventional approach is to search the space through inverse rendering , which generates a face that matches the photograph by optimizing shape , texture , pose , and lighting parameters .",0
12903,"This approach requires a complex , nonlinear optimization that can be difficult to solve in practice .",0
12904,"Recent work has demonstrated fast , robust fitting by regressing from image pixels to morphable model coordinates using a neural network .",0
12905,The major issue with the regression approach is the lack of ground - truth 3 D face data for training .,0
12906,"Scans of face geometry and texture are difficult to acquire , both because of expense and privacy considerations .",0
12907,"Previous approaches have explored synthesizing training pairs of image and morphable model coordinates in a preprocess , or training an image -.",0
12908,Neutral 3D faces computed from input photographs using our regression network .,0
12909,We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .,0
12910,"to - image autoencoder with a fixed , morphable - model - based decoder and an image - based loss .",0
12911,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,1
12912,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",1
12913,"These features are robust to pose , expression , lighting , and even non-photorealistic inputs .",0
12914,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,1
12915,"The synthetic rendering need not have the same pose , expression , or lighting of the photograph , allowing our network to predict only shape and texture .",0
12916,"Simply optimizing for similarity between identity features , however , can teach the regression network to fool the recognition network by producing faces that match closely in feature space but look unnatural .",0
12917,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .",1
12918,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .",1
12919,"We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .",1
12920,Related Work,0
12921,Morphable 3D,0
12922,Face Models,0
12923,Blanz and Vetter introduced the 3D morphable face model as an extension of the 2D active appearance model .,0
12924,"They demonstrated face reconstruction from a single image by iteratively fitting a linear combination of registered scans and pose , camera , and lighting parameters .",0
12925,"They decomposed the geometry and texture of the face scans using PCA to produce separate , reduced - dimension geometry and texture spaces .",0
12926,Later work added more face scans and extended the model to include expressions as another separate space .,0
12927,We build directly off of this work by using the PCA weights as the output of our network .,0
12928,"Convergence of iterative fitting is sensitive to the initial conditions and the complexity of the scene ( i.e. , lighting , expression , and pose ) .",0
12929,"Subsequent work ( and others ) has applied a range of techniques to improve the accuracy and stability of the fitting , producing very accurate results under good conditions .",0
12930,"However , iterative approaches are still unreliable under general , in - the - wild , conditions , leading to the interest in regression - based approaches .",0
12931,Learning to Generate 3D Face Models,0
12932,Deep neural networks provide the ability to learn a regression from image pixels to 3D model parameters .,0
12933,The chief difficulty becomes how to collect enough training data to feed the network .,0
12934,One solution is to generate synthetic training data by drawing random samples from the morphable model and rendering the resulting faces .,0
12935,"However , a network trained on purely synthetic data may perform poorly when faced with occlusions , unusual lighting , or ethnicities that are not well - represented by the morphable model .",0
12936,"We include randomly generated , synthetic faces in each training batch to provide ground truth 3D coordinates , but train the network on real photographs at the same time .",0
12937,Tran et al .,0
12938,"address the lack of training data by using an iterative optimization to fit an expressionless model to a large number of photographs , and treat results where the optimization converged as ground truth .",0
12939,"To generalize to faces with expression , identity labels and at least one neutral image are required , so the potential size of the training dataset is restricted .",0
12940,"We also directly predict a neutral expression , but our unsupervised approach removes the need for an initial iterative fitting step .",0
12941,"An approach closely related to ours was recently proposed by Tewari , et al. , who train an autoencoder network on unlabeled photographs to predict shape , expression , texture , pose , and lighting simultaneously .",0
12942,"The encoder is a regression network from images to morphablemodel coordinates , and the decoder is a fixed , differentiable rendering layer that attempts to reproduce the input photograph .",0
12943,"Like ours , this approach does not require supervised training pairs .",0
12944,"However , since the training loss is based on individual image pixels , the network is vulnerable to confounding variation between related variables .",0
12945,"For example , it can not readily distinguish between dark skin tone and a dim lighting environment .",0
12946,"Our approach exploits a pretrained face recognition network , which distinguishes such related variables by extracting and comparing features across the entire image .",0
12947,"Other recent deep learning approaches predict depth maps or voxel grids , trading off a compact and interpretable output mesh for more faithful reproductions of the input image .",0
12948,"As for , identity and expression are confounded in the output mesh .",0
12949,"The result maybe suitable for image processing tasks , such as relighting , at the expense of animation tasks such as rigging .",0
12950,Facial Identity Features,0
12951,Current face recognition networks achieve high accuracy over millions of identities .,0
12952,"The networks operate by embedding images in a high - dimensional space , where images of the same person map to nearby points .",0
12953,"Recent work has shown that this mapping is somewhat reversible , meaning the features can be used to produce a likeness of the original person .",0
12954,"We build on this work and use FaceNet to both produce input features for our regression network , and to verify that the output of the regression resembles the input photograph ..",0
12955,End - to - end computation graph for unsupervised training of the 3 DMM regression network .,0
12956,Training batches consist of combinations of real ( blue ) and synthetic ( red ) face images .,0
12957,"Identity , loopback and batch distribution losses are applied to real images , while the 3 DMM parameter loss is applied to synthetic images .",0
12958,"The regression network ( yellow ) is shown in two places , but both correspond to the same instance during training .",0
12959,The identity encoder network is fixed during training .,0
12960,Model,0
12961,We employ an encoder - decoder architecture that permits end - to - end unsupervised learning of 3D geometry and texture morphable model parameters ) .,0
12962,"Our training framework utilizes a realistic , parameterized illumination model and differentiable renderer to form neutralexpression face images under varying pose and lighting conditions .",0
12963,We train our model on hybrid batches of real face images from VGG - Face and synthetic faces constructed from the Basel Face 3DMM .,0
12964,The main strength and novelty of our approach lies in isolating our loss function to identity .,0
12965,"By training the model to preserve identity through conditions of varying expression , pose , and illumination , we are able to avoid network fooling and achieve robust state - of - the - art recognizability in our predictions .",0
12966,Encoder,0
12967,"We use FaceNet for the network encoder , since its features have been shown to be effective for generating face images .",0
12968,"Other facial recognition networks such as VGG - Face , or even networks not focused on recognition , may work equally well .",0
12969,"The output of the encoder is the penultimate , 1024 - D avgpool layer of the "" NN2 "" FaceNet architecture .",0
12970,"We found the avgpool layer more effective than the final , 128 - D normalizing layer as input to the decoder , but use the normalizing layer for our identity loss ( Sec. 3.3.2 ) .",0
12971,Decoder,0
12972,"Given encoder outputs generated from a face image , our decoder generates parameters for the Basel Face Model 2017 3 DMM .",0
12973,The Basel 2017 model generates shape,0
12974,"Here , s , t ? R 199 and e ?",0
12975,"R 100 are shape , texture , and expression parameterization vectors with standard normal distributions ; S , T ?",0
12976,"R 3N are the average face shape and texture ; P SS , PT ? R 3N 199 and P SE ?",0
12977,"R 3N 100 are linear PCA bases ; and W SS , W T ? R 199199 and W SE ?",0
12978,R 100100 are diagonal matrices containing the square roots of the corresponding PCA eigenvalues .,0
12979,"The decoder is trained to predict the 398 parameters constituting the shape and texture vectors , sand t , fora face .",0
12980,The expression vector e is not currently predicted and is set to zero .,0
12981,The decoder network consists of two 1024 - unit fully connected + ReLU layers followed by a 398 - unit regression layer .,0
12982,The weights were regularized towards zero .,0
12983,"Deeper networks were considered , but they did not significantly improve performance and were prone to overfitting .",0
12984,Differentiable Renderer,0
12985,"In contrast to previous approaches that backpropagate loss through an image , we employ a general - purpose , differentiable rasterizer based on a deferred shading model .",0
12986,The rasterizer produces screen - space buffers containing triangle IDs and barycentric coordinates at each pixel .,0
12987,"After rasterization , per-vertex attributes such as colors and normals are interpolated at the pixels using the barycentric coordinates and IDs .",0
12988,"This approach allows rendering with full perspective and any lighting model that can be computed in screen - space , which prevents image quality from being a bottleneck to accurate training .",0
12989,The source code for the renderer is publicly available 1 .,0
12990,"The rasterization derivatives are computed for the barycentric coordinates , but not the triangle IDs .",0
12991,"We extend the definition of the derivative of barycentric coordinates with respect to vertex positions to include negative barycentric coordinates , which lie outside the border of a triangle .",0
12992,"Including negative barycentric coordinates and omitting triangle IDs effectively treats the shape as locally planar , which is an acceptable approximation away from occlusion boundaries .",0
12993,"Faces are largely smooth shapes with few occlusion boundaries , so this approximation is effective in our case , but it could pose problems if the primary source of loss is related to translation or occlusion .",0
12994,Illumination Model,0
12995,"Because our differentiable renderer uses deferred shading , illumination is computed independently per-pixel with a set of interpolated vertex attribute buffers computed for each image .",0
12996,We use the Phong reflection model for shading .,0
12997,"Because human faces exhibit specular highlights , Phong reflection allows for improved realism over purely diffuse approximations , such as those used in MoFA .",0
12998,It is both efficient to evaluate and differentiable .,0
12999,"To create appropriately even lighting , we randomly position two point light sources of varying intensity several meters from the face to be illuminated .",0
13000,"We select a random color temperature for each training image from approximations of common indoor and outdoor light sources , and perturb the color to avoid overfitting .",0
13001,"Finally , since the Basel Face model does not contain specular color information , we use a heuristic to define specular colors K s from the diffuse colors K d of the predicted model :",0
13002,Losses,0
13003,"We propose a novel loss function that focuses on facial identity , and ignores variations in facial expression , illumination , pose , occlusion , and resolution .",0
13004,This loss function is conceptually straightforward and enables unsupervised endto - end training of our network .,0
13005,It combines four terms :,0
13006,"Here , L param imposes 3D shape and texture similarity for the synthetic images , L id imposes identity preservation on 1 http://github.com/google/tf_mesh_renderer",0
13007,"the real images in a batch , L batchdistr regularizes the predicted parameter distributions within a batch to the distribution of the morphable model , and L loopback ensures the network can correctly interpret its own output .",0
13008,"The effects of removing the batch distribution , loopback , and limiting the identity loss to a single view are shown in .",0
13009,We use ? batch = 10.0 and ?,0
13010,loop = 0.07 for our results .,0
13011,Training proceeds in two stages .,0
13012,"First , the model is trained solely on batches of synthetic faces generated by randomly sampling for shape , texture , pose , and illumination parameters .",0
13013,"This stage performs only a partial training of the model : since shape and texture parameters are sampled independently in this stage , the model is restricted from learning correlations between them .",0
13014,"Second , the partiallytrained model is trained to convergence on batches consisting of a combination of real face images from the VGG - Face dataset and synthetic faces .",0
13015,"Synthetic faces are subject to only the L param loss , while real face images are subject to all losses except L param .",0
13016,Parameter Loss,0
13017,"For synthetic faces , the true shape and texture parameters are known , so we use independent Euclidean losses between the randomly generated true synthetic parameter vectors , s b andt b , and the predicted ones , s band tb , in a batch .",0
13018,where ?,0
13019,sand ?,0
13020,t control the relative contribution of the shape and texture losses .,0
13021,"Due to different units , we set ? s = 0.4 and ? t = 0.002 .",0
13022,Identity Loss,0
13023,Robust prediction of recognizable meshes can be facilitated with a loss that derives from a facial recognition network .,0
13024,"We used FaceNet , though the identity - preserving loss generalizes to other networks such as VGG - Face .",0
13025,"The final FaceNet normalizing layer is a 128 - D unit vector such that , regardless of expression , pose , or illumination , same - identity inputs map closer to each other on the hypersphere than different - identity ones .",0
13026,"For our identity loss L id , we define similarity of two faces as the cosine score of their respective output unit vectors , ? 1 and ?",0
13027,2 :,0
13028,"To use this loss in an unsupervised manner on real faces , we calculate the cosine score between a face image and the image resulting from passing the decoder outputs into the differentiable renderer with random pose and illumination .",0
13029,Identity prediction can be further enhanced by using multiple poses for each face .,0
13030,Multiple poses decrease the presence of occluded regions of the mesh .,0
13031,"Additionally , since each pose provides a backpropagation path to the mesh vertices , the model trains in a more robust manner than if only a single pose is used .",0
13032,We use three randomly determined poses for each real face .,0
13033,Batch Distribution,0
13034,Loss,0
13035,Applying the identity loss alone allows training to introduce biases into the decoder outputs that change their distribution from the zero- mean standard normal distribution assumption made by the Basel Face Model .,0
13036,These changes are likely due to domain transfer effects between the real images and those rendered from the decoder outputs .,0
13037,"Initially , we attempted to compensate for these effects by adding a shallow network to transform the model - rendered encoder outputs prior to calculating the identity loss .",0
13038,"While this approach did increase overall recognizability in the model , it also introduced unrealistic artifacts into the model outputs .",0
13039,"Instead , we opted to regularize each batch to directly constrain the lowest two moments of the shape and texture parameter distributions to match those of a zeromean standard normal distribution .",0
13040,"This loss , which is applied at a batch level , combines four terms :",0
13041,"Here , L Sand L T regularize the batch shape and texture parameters to have zero mean , and L ?",0
13042,Sand L ?,0
13043,T regularize them to have unit variance .,0
13044,Loopback Loss,0
13045,A limitation of using real face images for unsupervised training is that the true shape and texture parameters for the faces are unknown .,0
13046,"If they were known , then the more direct lower - level parameter loss in Sec. 3.3.1 could be directly imposed instead of the identity loss in Sec. 3 .",0
13047,3.2 .,0
13048,"A close approximation to this lower - level loss for real images can be achieved using a "" loopback "" loss ( .",0
13049,The nature of this loss lies in generalizing the model near the regions for which real face image data exists .,0
13050,Similar techniques have proven to be successful in generalizing model learning for image applications .,0
13051,"To compute the loopback loss at any training step , the current - state decoder outputs fora batch of real face images are extracted and used to generate synthetic faces rendered in random poses and illuminations .",0
13052,"The synthetic faces are then passed back through the encoder and decoder again , and the parameter loss in Sec. 3.3.1 is imposed between the resulting parameters and those first output by the decoder .",0
13053,"As shown in , two loopback loss backpropagation paths to the decoder exist .",0
13054,"The effects of each are complementary : the synthetic face parameter path generalizes the decoder in the region near that of real face parameters , and the real image channel regularizes the decoder away from generating unrealistic faces .",0
13055,"Additionally , the two paths encourage the regression network to match its responses for real and synthetic versions of the same face .",0
13056,Experiments,0
13057,We first show and discuss the qualitative improvements of our method compared with other morphable model regression approaches ( Sec. 4.1 ) .,0
13058,"We then evaluate our method quantitatively by comparing reconstruction error against scanned 3 D face geometry ( Sec. 4.2 ) and features produced by VGG - Face , which was not used for training ( Sec. 4.3 and 4.4 ) .",0
13059,We also show qualitative results on corrupted and non-photorealistic inputs ( Sec. 4.5 ) .,0
13060,Tran MoFA MoFA + Exp Sela .,0
13061,Results on the MoFA - Test dataset .,0
13062,"Our method shows improved likeness and color fidelity over competing methods , especially in the shape of the eyes , eyebrows , and nose .",0
13063,"Note that MoFA solves for pose , expression , lighting , and identity , so is shown both with ( row 5 ) and without ( row 4 ) expression .",0
13064,"The unstructured method of Sela , et al. produces only geometry , so is shown without color .",0
13065,Qualitative Comparison,0
13066,as part of MoFA .,0
13067,An extended evaluation is available in the supplemental material .,0
13068,"Our method improves on the likenesses of previous approaches , especially in features relevant to facial recognition such as the eyebrow texture and nose shape .",0
13069,Our method also predicts coloration and skin tone more faithfully .,0
13070,"This improvement is likely a consequence of our batch distribution loss , which allows individual faces to vary from the mean of the Basel model ( light skin tone ) , so long as the faces match the mean in aggregate .",0
13071,"Previous methods , by contrast , regularize each face towards the mean of the model 's distribution , tending to produce light skin tone overall .",0
13072,"The MoFA approach also sometimes confounds identity and expression ( , and skin tone and lighting .",0
13073,Our method and Tran et al. are more resistant to confounding variables .,0
13074,"The unstructured method of Sela et al. does not sepa-rate identity and expression , predicts only shape , and is less robust than the model - based methods .",0
13075,Neutral Pose Reconstruction on MICC,1
13076,We quantitatively evaluate the ground - truth accuracy of our models on the MICC Florence 3D Faces dataset ( MICC ) in .,0
13077,This dataset contains the ground truth scans of 53 Caucasian subjects in a neutral expression .,0
13078,"Accompanying the scans are three observation videos for each subject , in conditions of increasing difficulty : ' cooperative ' , ' indoor ' , and ' outdoor . '",0
13079,"We run the methods on each frame of the videos , and average the results over each video to get a single reconstruction .",0
13080,"The results of Tran et al. are averaged over the mesh , as in .",0
13081,We instead average our encoder embeddings before making a single reconstruction .,0
13082,"To evaluate our predictions , we crop the ground truth scan to 95 mm around the tip of the nose as in , and run ICP with isotropic scale to find an alignment .",0
13083,"We solve for isotropic scale because we do not assume the methods predict absolute scale , and a small misalignment in scale can have a large effect on error .",0
13084,"shows the symmetric point - to - plane distance in millimeters within the ICPdetermined region of intersection , rather than point - to - point distances , as the methods and ground truth have different vertex densities .",0
13085,"Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .",1
13086,"We are also more stable across changing environments , with similar results for all three test sets .",1
13087,Face Recognition Results,1
13088,"In order to quantitatively evaluate the likeness of our reconstructions , we use the VGG - Face recognition network 's activations as a measure of similarity .",0
13089,"VGG - Face was chosen because FaceNet appears in our training loss , making it unsuitable as an evaluation metric .",0
13090,"For each face in our evaluation datasets , we compute the cosine similarity of the ?( t ) layers of VGG - Face between the input image and a rendering of our output geometry , as described in .",0
13091,"The similarity distributions for Labeled Faces in the Wild ( LFW ) , MICC , and MoFA - Test are shown in .",0
13092,"The similarity between all pairs of photographs in the LFW dataset , separated into same - person and differentperson distributions , is shown for comparison in , top .",0
13093,Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .,1
13094,"By comparison , 22.7 % of pairs of photos of the same person in LFW have a score below 0.403 , and only 0.04 % of pairs of photos of different people have a score above 0.403 .",0
13095,"For additional validation , shows the Earth Mover 's distance between the all - pairs LFW distributions and the results of each method .",0
13096,"Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .",1
13097,"We conclude that ours is the first method that generates neutralpose , 3 D faces with recognizability approaching a photo .",0
13098,The scores of the ground - truth 3 D face scans from MICC .,0
13099,Earth mover 's distance between distributions of VGG - Face ?( t ) similarity and distributions of same and different identities on LFW .,0
13100,"A low distance for "" Same "" means the similarity scores between a photo and its associated 3 D rendering are close to the scores of same identity photos in LFW , while a low distance for "" Diff . "" means the scores are close to the scores of different identity photos .",0
13101,and their input photographs provide a ceiling for similarity scores .,0
13102,"Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .",1
13103,MoFA - Test,0
13104,LFW,0
13105,Method,0
13106,Top - 1 Top - 5 Top - 1 Top - 5 random 0.01 0.06 0.0002 0.001 MoFA 0.19 0.54 ? ?,0
13107,Tran et al . 0.25 0.62 0.001 0.002 ours 0.87 0.96 0.16 0.51 . Identity Clustering Recall using VGG - Face distances on MoFA - Test and LFW .,0
13108,"Given a rendered mesh , the task is to recover the unknown source identity by looking up the nearest neighbor photographs according to VGG - Face ?( t ) cosine similarity .",0
13109,"Top - 1 and Top - 5 show the fractions for which a photograph of the correct identity was recalled as the nearest neighbor , or in the nearest 5 , respectively .",0
13110,"Performance is higher for MoFA - Test because it contains 84 images and 78 identities , while the LFW set contains 12,993 images and 5,749 identities ..",0
13111,FERET dataset stress test .,0
13112,"The regression network is robust to changes in pose , lighting , expression , occlusion , and blur .",0
13113,See supplemental material for additional results .,0
13114,Face Clustering,0
13115,"To establish that our reconstructions are recognizable , we perform a clustering task to recover the identities of our generated meshes .",0
13116,"For each of LFW and MoFA - Test , we run our method on all faces in the dataset , and render the output geometry as shown in the figures in this paper .",0
13117,"For each rendering , we find the nearest neighbors according to the VGG - Face ?( t ) distance .",0
13118,"shows the fraction of meshes that recall a photo of the source identity as the nearest neighbor , and within the top 5 nearest neighbors .",0
13119,"On MoFA - Test , which has 84 images and 78 identities , we achieve a Top - 1 recall of 87 % , compared to 25 % for Tran et al. and 19 % for MoFA .",0
13120,"On the larger LFW dataset , which contains over 5,000 identities in 13,000 photographs , we still achieve a Top - 5 recall of 51 % .",0
13121,"We conclude our approach generates recognizable 3D morphable models , even in test sets with thousands of candidate identities .",0
13122,Reconstruction from Challenging Images,0
13123,"Our regression network uses a facial identity feature vector as input , yielding results robust to changes in pose , expression , lighting , occlusion , and resolution , while remaining sensitive to changes in identity .",0
13124,qualitatively demonstrates this robustness by varying conditions fora single subject and displaying consistent output ..,0
13125,Art from the BAM dataset .,0
13126,"Because the inputs to our regression network are high - level identity features , the results are robust to stylized details at the pixel level .",0
13127,"Additionally , shows that our network can reconstruct plausible likenesses from non-photorealistic artwork , in cases where a fitting approach based on inverse rendering would have difficulty .",0
13128,"This result is possible because of the invariance of the identity features to unrealistic pixel - level information , and because our unsupervised loss focuses on aspects of reconstruction that are important for recognition .",0
13129,Discussion and Future Work,0
13130,"We have shown it is possible to train a regression network from images to neutral , expressionless 3D morphable model coordinates using only unlabeled photographs and improve on the accuracy of supervised methods .",0
13131,Our results approach the face recognition similarity scores of real photographs and exceed the scores of other regression approaches by a large margin .,0
13132,"Because of the accuracy of the approach , the predicted face can be directly used for facetracking based on landmarks .",0
13133,"This paper focuses on learning an expressionless face , which is suitable for creating VR avatars or landmark - based tracking .",0
13134,"In future work , we hope to extend the approach to predict pose , expression , and lighting , similar to Tewari , et al . .",0
13135,Predicting these factors while avoiding their confounding effects should be possible by adding an inverse rendering stage to our decoder while maintaining the neutral - pose losses we currently apply .,0
13136,The method produces generally superior results for young adults and Caucasian ethnicities .,0
13137,"The differences could be due to limited representation in the scans used to produce the morphable model , bias in the features extracted from the face recognition network , or limited representation in the VGG - Face dataset we use for training .",0
13138,"In future work , we hope to improve the performance of the method on a diverse range of ages and ethnicities ..",0
13139,Occlusion Stress,0
13140,Test on subjects from the MICC and FERET dataset .,0
13141,We increase occlusion in the input image until our algorithm no longer predicts accurate features .,0
13142,Facial features smoothly degrade as the necessary information is no longer present in the input image .,0
13143,( bottom ) datasets .,0
13144,"Beginning with a frontal image of the subject , we apply a progressively larger gaussian blur kernel to examine the effect of lost detail in the input .",0
13145,"For the female subject , global shape begins to change subtly as the blur becomes extreme .",0
13146,"For both subjects , fine detail in the eyebrow shape and thickness is lost as the input is increasingly blurred .",0
13147,Input,0
13148,Ours,0
13149,Tran MoFA,0
13150,Input Ours Tran MoFA,0
13151,Input Ours,0
13152,Tran MoFA,0
13153,Input Ours Tran MoFA,0
13154,A.1 . Fitting Pose and Expression,0
13155,"Our system reconstructs shape and texture of faces , and ignores aspects such as pose , expression , and lighting .",0
13156,"Those components are needed to exactly match the reconstruction to the source image , and our neutral face output is an excellent starting point to find them .",0
13157,shows results of gradient descent that starts with our output and fits the pose and expression by minimizing the distances of landmarks on our mesh and the image ( we used the 68 landmark configuration from the Multi - PIE database ) .,0
13158,title,0
13159,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,1
13160,abstract,0
13161,"Recently , deep learning based facial landmark detection has achieved great success .",0
13162,"Despite this , we notice that the semantic ambiguity greatly degrades the detection performance .",0
13163,"Specifically , the semantic ambiguity means that some landmarks ( e.g. those evenly distributed along the face contour ) do not have clear and accurate definition , causing inconsistent annotations by annotators .",0
13164,"Accordingly , these inconsistent annotations , which are usually provided by public databases , commonly work as the groundtruth to supervise network training , leading to the degraded accuracy .",0
13165,"To our knowledge , little research has investigated this problem .",0
13166,"In this paper , we propose a novel probabilistic model which introduces a latent variable , i.e. the ' real ' ground - truth which is semantically consistent , to optimize .",0
13167,This framework couples two parts ( 1 ) training landmark detection CNN and ( 2 ) searching the ' real ' groundtruth .,0
13168,These two parts are alternatively optimized : the searched ' real ' ground - truth supervises the CNN training ; and the trained CNN assists the searching of ' real ' groundtruth .,0
13169,"In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .",0
13170,Extensive experiments on both image - based ( 300 W and AFLW ) and video - based ( 300 - VW ) databases demonstrate that our method effectively improves the landmark detection accuracy and achieves the state of the art performance .,0
13171,Introduction,0
13172,Deep learning methods have achieved great success on landmark detection due to the strong modeling capacity .,0
13173,"Despite this success , precise and credible landmark detection still has many challenges , one * equal contribution .",0
13174,Non semantic moving,0
13175,Semantic moving .,0
13176,The landmark updates in training after the model is roughly converged .,0
13177,"Due to ' semantic ambiguity ' , we can see that many optimization directions , which are random guided by random annotation noises along with the contour and ' non semantic ' .",0
13178,The others move to the semantically accurate positions .,0
13179,"Red and green dots denote the predicted and annotation landmarks , respectively .",0
13180,of which is the degraded performance caused by ' semantic ambiguity ' .,0
13181,This ambiguity results from the lack of clear definition on those weak semantic landmarks on the contours ( e.g. those on face contour and nose bridge ) .,0
13182,"In comparison , strong semantic landmarks on the corners ( e.g. eye corner ) suffer less from such ambiguity .",0
13183,"The ' semantic ambiguity ' can make human annotators confused about the positions of weak semantic points , and it is inevitable for annotators to introduce random noises during annotating .",0
13184,The inconsistent and imprecise annotations can mislead CNN training and cause degraded performance .,0
13185,"Specifically , when the deep model roughly converges to the ground - truth provided by public databases , the network training is misguided by random annotation noises caused by ' semantic ambiguity ' , shown in .",0
13186,"Clearly these noises can make the network training trapped into local minima , leading to degraded results .",0
13187,"In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .",1
13188,We assume that there exist ' real ' ground - truths which are semantically consistent and more accurate than human annotations provided by databases .,0
13189,"We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .",1
13190,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .",1
13191,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .",1
13192,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,1
13193,The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,1
13194,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",1
13195,"We conduct experiments on 300W , AFLW and 300 - VW databases and achieve the state of the art performance .",0
13196,Related work,0
13197,"In recent years , convolutional neural networks ( CNN ) achieves very impressive results on many computer vision tasks including face alignment .",0
13198,Sun et al proposes to cascade several DCNN to predict the shape stage by stage .,0
13199,"Zhang et al proposes a single CNN and jointly optimizes facial landmark detection together with facial attribute recognition , further enhancing the speed and performance .",0
13200,"The methods above use shallow CNN models to directly regress facial landmarks , which are difficult to cope the complex task with dense landmarks and large pose variations .",0
13201,"To further improve the performance , many popular semantic segmentation and human pose estimation frameworks are used for face alignment .",0
13202,"For each landmark , they predict a heatmap which contains the probability of the corresponding landmark .",0
13203,Yang et al .,0
13204,"uses a two parts network , i.e. , a supervised transformation to normalize faces and a stacked hourglass network to get prediction heatmaps .",0
13205,"Most recently , JMFA and FAN also achieve the state of the art accuracy by leveraging stacked hourglass network .",0
13206,"However , these methods do not consider the ' semantic ambiguity ' problem which potentially degrades the detection performance .",0
13207,"Two recent works , LAB and SBR , are related to this ' semantic ambiguity ' problem .",0
13208,"By introducing more information than pixel intensity only , they implicitly alle - viate the impact of the annotation noises and improve the performance .",0
13209,LAB trains a facial boundary heatmap estimator and incorporates it into the main landmark regression network .,0
13210,"LAB uses the well - defined facial boundaries which provide the facial geometric structure to reduce the ambiguities , leading to improved performance .",0
13211,"However , LAB is computational expensive .",0
13212,SBR proposes a registration loss which uses the coherency of optical flow from adjacent frames as its supervision .,0
13213,The additional information from local feature can mitigate the impact of random noises .,0
13214,"However , the optical flow is not always credible in unconstrained environment and SBR trains their model on the testing video before the test , limiting its applications .",0
13215,"To summarize , LAB and SBR do not intrinsically address the problem of ' semantic ambiguity ' because the degraded accuracy is actually derived from the inaccurate labels ( human annotations provided by databases ) .",0
13216,"In this work , we solve the ' semantic ambiguity ' problem in a more intrinsic way .",0
13217,"Specifically , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth without semantic ambiguity and train a hourglass landmark detector without using additional information .",0
13218,Semantic ambiguity,0
13219,The semantic ambiguity indicates that some landmarks do not have clear and accurate definition .,0
13220,"In this work , we find the semantic ambiguity can happen on any facial points , but mainly on those weak semantic facial points .",0
13221,"For example , the landmarks are defined to evenly distribute along the face contour without any clear definition of the exact positions .",0
13222,This ambiguity can potentially affect : ( 1 ) the accuracy of the annotations and ( 2 ) the convergence of deep model training .,0
13223,"For ( 1 ) , when annotating a database , annotators can introduce random errors to generate inconsistent ground - truths on those weak semantic points due to the lack of clear definitions .",0
13224,"For ( 2 ) , the inconsistent ground - truths generate inconsistent gradients for back - propagation , leading to the difficulty of model convergence .",0
13225,"In this section , we qualitatively analyze the influence of semantic ambiguity on landmark detection .",0
13226,"Before this analysis , we briefly introduce our heatmapbased landmark detection network .",0
13227,"Specifically , we use a four stage Hourglass ( HGs ) .",0
13228,"It can generate the heatmap which provides the probability of the corresponding landmark located at every pixel , and this probability can facilitate our analysis of semantic ambiguity .",0
13229,"Firstly , we find CNN provides a candidate region rather than a confirmed position fora weak semantic point .",0
13230,"In ( a ) , we can see that the heatmap of a strong semantic point is nearly Gaussian , while the 3D heatmap of a weak semantic point has a ' flat hat ' , meaning that the confidences in that area are very similar .",0
13231,Since the position with the highest confidence is chosen as the output .,0
13232,The landmark ( a ) The difference between the heatmap of the eye corner ( strong semantic ) points and the eye contour ( weak semantic ) points .,0
13233,Col 2 and 3 represent 2 D and 3D heatmaps respectively .,0
13234,"In the 3D Gaussian , the x , y axes are image coordinates and z axis is the prediction confidence .",0
13235,We can seethe 3D heatmap of a weak semantic point has a ' flat hat ' .,0
13236,( b ),0
13237,The predictions from a series of checkpoints after convergence .,0
13238,"When the model has roughly converged , we continue training and achieve the predictions from different iterations .",0
13239,"Red and green dots denote the predicted and annotation landmarks , respectively .",0
13240,We can seethe predicted landmarks from different checkpoints fluctuate in the neighborhood area of the annotated position ( green dots ) .,0
13241,"Secondly , we analyze the ' semantic ambiguity ' by visualizing how the model is optimized after convergence .",0
13242,"When the network has roughly converged , we continue training the network and save a series of checkpoints .",0
13243,"In , the eyebrow landmarks , from different checkpoints fluctuate along with the edge of eyebrow , which always generates considerable loss to optimize .",0
13244,"However , this loss is ineffectual since the predicted points from different checkpoints also fluctuate in the neighborhood area of the annotated position ( green dots in ) .",0
13245,"It can be concluded that the loss caused by random annotation noises dominate the back - propagated gradients after roughly convergence , making the network training trapped into local minima .",0
13246,Semantically consistent alignment,0
13247,"In this section , we detail our methodology .",0
13248,"In Section 4.1 , we model the landmark detection problem using a probabilistic model .",0
13249,"To deal with the semantic ambiguity caused by human annotation noise , we introduce a latent variabl y which represents the ' real ' ground - truth .",0
13250,"Then we model the prior model and likelihood in Section 4.2 and 4.3 , respectively .",0
13251,Section 4.4 proposes an alternative optimization strategy to search ?,0
13252,and train the landmark detector .,0
13253,"To recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) in Section 4.5 , which refines the predictions by considering the global face shape as a constraint , leading to a more robust model .",0
13254,A probabilistic model of landmark prediction,0
13255,"In the probabilistic view , training a CNN - based landmark detector can be formulated as a likelihood maximization problem :",0
13256,where o ?,0
13257,R 2N is the coordinates of the observation of landmarks ( e.g. the human annotations ) .,0
13258,"N is the number of landmarks , x is the input image and Wis the CNN parameters .",0
13259,"Under the probabilistic view of Eq. ( 1 ) , one pixel value on the heatmap works as the confidence of one particular landmark at that pixel .",0
13260,"Therefore , the whole heatmap works as the probability distribution over the image .",0
13261,"As analyzed in Section 3 , the annotations provided by public databases are usually not fully credible due to the ' semantic ambiguity ' .",0
13262,"As a result , the annotations , in particular those of weak semantic landmarks , contain random noises and are inconsistent among faces .",0
13263,"In this work , we assume that there exists a ' real ' ground - truth without semantic ambiguity and can better supervise the network training .",0
13264,"To achieve this , we introduce a latent variable ?",0
13265,as the ' real ' ground - truth which is optimized during learning .,0
13266,"Thus , Eq. ( 1 ) can be reformulated as :",0
13267,"where o is the observation of ? , for example , the annotation can be seen as an observation of ?",0
13268,from human annotator .,0
13269,P ( o|? ) is a prior of ?,0
13270,given the observation o and P ( ? |x ; W ) is the likelihood .,0
13271,Prior model of ' real ' ground - truth,0
13272,"To optimize Eq. , an accurate prior model is important to regularize ?",0
13273,and reduce searching space .,0
13274,We assume that the kth landmark ?,0
13275,"k is close to the o k , which is the observation of ?.",0
13276,"Thus , this prior is modeled as Gaussian similarity overall {o k , ? k } pairs : where ?",0
13277,1 can control the sensitivity to misalignment .,0
13278,"To explain o k , we should know in advance that our whole framework is iteratively optimized detailed in Section 4.4 .",0
13279,"o k is initialized as the human annotation in the iteration , and will be updated by better observation with iterations .",0
13280,Network likelihood model,0
13281,We now discuss the likelihood P ( ?|x ; W ) of Eq . .,0
13282,"The point - wise joint probability can be represented by the confidence map , which can be modelled by the heatmap of the deep model .",0
13283,Note that our hourglass architecture learns to predict heatmap consisting of a 2D Gaussian centered on the ground - truth ?,0
13284,k .,0
13285,"Thus , for any position y , the more the heatmap region around y follows a standard Gaussian , the more the pixel at y is likely to be ? k .",0
13286,"Therefore , the likelihood can be modeled as the distribution distance between the predicted heatmap ( predicted distribution ) and the standard Gaussian region ( expected distribution ) .",0
13287,"In this work , we use Pearson Chi-square test to evaluate the distance of these two distributions :",0
13288,"where E is a standard Gaussian heatmap ( distribution ) , which is a template representing the ideal response ; i is the pixel index ; ?",0
13289,is a cropped patch ( of the same size as Gaussian template ) from the predicted heatmap centered on y .,0
13290,"Finally , the joint probability can also be modeled as a product of Gaussian similarities maximized overall landmarks :",0
13291,"where k is the landmark index , ?",0
13292,2 is the bandwidth of likelihood .,0
13293,"To keep the likelihood credible , we first train a network with the human annotations .",0
13294,"Then in the likelihood , we can consider the trained network as a super annotator to guide the searching of the real ground - truth .",0
13295,"It results from the fact that a well trained network is able to capture the statistical law of annotation noise from the whole training set , so that it can generate predictions with better semantic consistency .",0
13296,Optimization,0
13297,"Combining Eq. , and and taking log of the likelihood , we have :",0
13298,Reduce Searching Space,0
13299,To optimize the latent semantically consistent ' real ' landmark ?,0
13300,"k , the prior Eq. indicates that the latent ' real ' landmark is close to the observed landmark o k .",0
13301,"Therefore , we reduce the search space of y k to a small patch centered on o k .",0
13302,"Then , the optimization problem of Eq. can be re-written as :",0
13303,where N ( o k ) represents a region centered on o k .,0
13304,Alternative Optimization,0
13305,"To optimize Eq. , an alternative optimization strategy is applied .",0
13306,"In each iteration , y is firstly searched with the network parameter W fixed .",0
13307,Then ?,0
13308,is fixed and Wis updated ( landmark prediction network training ) under the supervision of newly searched ?.,0
13309,Step 1 : When,0
13310,"Wis fixed , to search the latent variable ? , the optimization becomes a constrained discrete optimization problem for each landmark :",0
13311,where all the variables are known except ? k .,0
13312,We search ?,0
13313,k by going through all the pixels in N ( o k ) ( a neighborhood area of o k as shown in ) and the one with minimal loss in Eq. ( 8 ) is the solution .,0
13314,"Since the searching space N ( o k ) is very small , i.e. 17 17 in this work for 256256 heatmap , the optimization is very efficient .",0
13315,"Note that in the prior part of Eq. ( 8 ) , o k is the observation of ? k :",0
13316,"In the 1st iteration , o k is set to the human annotations which are the observation of human annotators ; From the 2nd iteration , o k is set to ?",0
13317,k t?1 ( where t is the iteration ) .,0
13318,Note that ?,0
13319,k t?1 is the estimated ' real ' ground - truth from the last iteration .,0
13320,"With the iterations , ?",0
13321,kt is converging to the ' real ' ground - truth because both the current observation o k ( i.e. ? k t?1 ) and CNN prediction iteratively become more credible .,0
13322,"Step 2 : When ? is fixed , the optimization becomes :",0
13323,The optimization becomes atypical network training process under the supervision of ?.,0
13324,Here ?,0
13325,is set to the estimate of the latent ' real ' ground - truth obtained in Step 1 .,0
13326,shows an example of the gradual convergence from the observation o ( ? of the last iteration ) to the estimate of real ground - truth ?.,0
13327,The optimization of ?,0
13328,"in our semantic alignment can easily converge to a stable position , which does not have hard convergence problem like the traditional landmark training as shown in .",0
13329,Global heatmap correction unit,0
13330,Traditional heatmap based methods predict each landmark as an individual task without considering global face shape .,0
13331,The prediction might fail when the model fits images of low - quality and occlusion as shown in .,0
13332,The outliers such as occlusions destroy the face shape and significantly reduce overall performance .,0
13333,Existing methods like local feature based CLM and deep learning based LGCN apply a 2D shape PCA as their post-processing step to remove the outliers .,0
13334,"However , PCA based method is weak to model out - of - plane rotation and very slow ( about 0.8 fps in LGCN ) .",0
13335,"In this work , we propose a Global Heatmap Correction Unit ( GHCU ) to recover the outliers efficiently .",0
13336,We view the predicted heatmaps as input and directly regress the searched / optimized ?,0
13337,through alight weight CNN as shown in Tab .,0
13338,"1 . The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks , as shown in .",0
13339,Our experiments demonstrate the GHCU completes fitting with the speed 8 times faster than PCA on the same hardware platform and achieves higher accuracy than PCA .,0
13340,Experiments,0
13341,Datesets .,0
13342,"We conduct evaluation on three challenging datasets including image based 300W , AFLW , and video based 300 - VW .",0
13343,"300 W is a collection of multiple face datasets , including the LFPW , HELEN , AFW and XM2 VTS which have 68 landmarks .",0
13344,"The training set contains 3148 training samples , 689 testing samples which are further divided into the common and challenging subsets .",0
13345,AFLW is a very challenging dataset which has a wide range of pose variations in yaw ( ?90 to 90 ) .,0
13346,"In this work , we follow the AFLW - Full protocol which ignores two landmarks of ears and use the remaining 19 landmarks .",0
13347,"300 - VW is a large dataset for video - based face alignment , which consists of 114 videos in various conditions .",0
13348,"Following , we utilized all images from 300 W and 50 sequences for training and the remaining 64 sequences for testing .",0
13349,"The test set consists of three categories : well - lit , mild unconstrained and challenging .",0
13350,Evaluation metric .,0
13351,"To compare with existing popular methods , we conduct different evaluation metrics on different datasets .",0
13352,"For 300 W dataset ,",0
13353,We follow the protocol in and use Normalized mean errors ( NME ) which normalizes the error by the inter-pupil distance .,0
13354,"For AFLW , we follow to use face size as the normalizing factor .",0
13355,"For 300 - VW dataset , we employed the standard normalized root mean squared error ( RMSE ) which normalizes the error by the outer eye corner distance .",0
13356,Implementation Details .,0
13357,"In our experiments , all the training and testing images are cropped and resized to 256256 according to the provided bounding boxes .",0
13358,"To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .",1
13359,We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .,1
13360,"As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .",0
13361,"Specifically , at each epoch , we first search the real ground - trut ?",0
13362,y and then use ?,0
13363,to supervise the network training .,0
13364,"When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .",1
13365,"When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .",1
13366,"During semantic alignment , we search the latent variable ?",0
13367,"from a 1717 region centered at the current observation point o , and we crop a no larger than 2525 patch from the predicted heatmap around current position for Pearson Chi - square test in Eq. ( 4 ) .",0
13368,We set batch size to 10 for network training .,1
13369,"For GHCU , the network architecture is shown in Tab .",0
13370,1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .,1
13371,Comparison experiment,0
13372,300 W .,1
13373,We compare our approach against the state - of the - art methods on 300W in Tab .,0
13374,2 . The baseline ( HGs in Tab .,0
13375,"2 ) uses the hourglass architecture with human annotations , which is actually the traditional landmark detector training .",0
13376,From Tab.,0
13377,"2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .",1
13378,"By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .",1
13379,"The improvement is not significant because the images of 300W are of high resolution , while GHCU works particularly well for images of low resolution and occlusions verified in the following evaluations .",0
13380,"Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .",1
13381,"In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .",1
13382,AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .,1
13383,"In comparison , AFLW has only 19 points , most of which are strong semantic landmarks .",0
13384,"Since our SA is particularly effective on weak semantic points , we conduct experiments on AFLW to verify whether SA generalizes well to the point set , most of which are strong semantic points .",0
13385,"For fair comparison , we do not compare methods using additional outside training data , e.g.",0
13386,LAB used additional boundary information from outside database .,0
13387,"As shown in Tab. 3 , HGs + SA outperforms",1
13388,"HGs , 1.62 % vs 1.95 % .",1
13389,"It means that even though corner points are easily to be recognized , there is still random error in annotation , which can be corrected by SA .",0
13390,It is also observed that HGs + SA + GHCU works better than HGs + SA .,1
13391,300 - VW .,1
13392,"Unlike the image - based databases 300 W and AFLW , 300 - VW is video - based database , which is more challenging because the frame is of low resolution and with strong occlusions .",0
13393,The subset Category 3 is the most challenging one .,0
13394,From Tab.,0
13395,"4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .",1
13396,"Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .",1
13397,Comparison with state of the art on AFLW dataset .,0
13398,The error ( NME ) is normalized by the face bounding box size .,0
13399,AFLW - Full ( % ) LBF 4.25 CFSS 3.92 CCL ( CVPR16 ) 2.72 TSR ( CVPR17 ) 2.17 DCFE ( ECCV18 ) 2.17 SBR ( CVPR18 ) 2.14 DSRN ( CVPR18 ) 1.86 Wing ( CVPR18 ) 1.65 HGs 1.95 HGs + SA 1.62 HGs + SA + GHCU 1.60,0
13400,"GHCU considers the global face shape as constraint , being robust to such challenging factors .",0
13401,Self evaluations,0
13402,"Balance of prior and likelihood As shown in Eq. ( 6 ) , the ' real ' ground - truth is optimized using two parts : prior and likelihood , where ? 1 and ?",0
13403,2 determine the importance of these two parts .,0
13404,"Thus , we can use one parameter ? 2 2 /?",0
13405,2 1 to estimate this importance weighting .,0
13406,We evaluate different values of ? 2 2 /? 2 1 in Tab .,0
13407,"5 . Clearly , the performance of ? 2 2 /? 2 1 = 0 ( removing Semantic Alignment and using human annotations only ) is worst , showing the importance of the proposed Semantic Alignment .",0
13408,"We find that ? 2 2 /? 2 1 = 0.1 achieves the best performance , meaning that the model relies much more ( 10 times ) on prior than likelihood to achieve the best trade - off .",0
13409,Template size .,0
13410,"As discussed in the Section 3 , fora position y , the similarity between the heatmap region around it and standard Gaussian template is closely related to the detection confidence .",0
13411,"Therefore , the size of the Gaussian template , which is used to measure the network confidence in Eq. ( 5 ) , can affect the final results .",0
13412,reports the results under different template sizes using the model HGs + SA .,0
13413,Too small size ( size = 1 ) means that the heatmap value is directly used to model the likelihood instead of Chi-square test .,0
13414,"Not surprisingly , the performance with size = 1 is not promising .",0
13415,"Large size ( size = 25 ) introduces more useless information , degrading the performance .",0
13416,"In our experiment , we find size = 15 for AFLW and size = 19 for 300 W can achieve the best result .",0
13417,Analysis of the training of semantic alignment .,0
13418,"To verify the effectiveness of Semantic Alignment , we train a baseline network using hourglass under the supervision of human annotation to converge .",0
13419,"Use this roughly converged baseline , we continue training using 3 strategies as shown in : baseline , SA w / o update ( always using human annotation as the observation , see Eq. ) and SA ( the observation is iteratively updated ) .",0
13420,"visualize the changes of training loss and NME on test set against the training epochs , respectively .",0
13421,We can see that the baseline curve in do not decrease because of the ' semantic ambiguity ' .,0
13422,"By introducing SA , the training loss and test NME steadily drop .",0
13423,"Obviously , SA reduces the random optimizing directions and helps the roughly converged network to further improve the detection accuracy .",0
13424,We also evaluate the condition that uses semantic alignment without updating the observation o ( ' SA w/ o update ' in .,0
13425,It means o is always set to the human annotations .,0
13426,"We can see that the curve of ' SA w/ o update ' can be further optimized but quickly trapped into local optima , leading to worse performance than SA .",0
13427,We assume that the immutable observation o reduces the capacity of searching ' real ' ground - truth ?. after each epoch .,0
13428,"To explore the effects of the number of epochs on model convergence , we train different models by stopping semantic alignment at different epochs .",0
13429,"In it is observed that the final performance keeps improving with the times of semantic alignment , which demonstrates that the improvement is highly positive related to the quality of the learned ?.",0
13430,"From our experiment , 10 epochs of semantic alignment are enough for our data sets .",0
13431,Quality of the searched ' real ' ground - truth .,0
13432,One important assumption of this work is that there exist ' real ' ground - truths which are better than the human annotations .,0
13433,"To verify this , we train two networks which are supervised by the human annotations provided by public database and the searched ' real ' ground - truth , respectively .",0
13434,These two detectors area Hourglass model ( HGs ) and a ResNet model which directly regresses the landmark coordinates as .,0
13435,As shown in Tab .,0
13436,"7 , we can see that on both models the ' real ' ground - truth ( SA ) outperforms the human annotations ( HA ) .",0
13437,"Clearly , our learned labels are better than the human annotations , verifying our assumption that the se-mantic alignment can find the semantic consistent groundtruths .",0
13438,Global heatmap correction unit .,0
13439,"The 2D shape PCA can well keep the face constraint and can be conducted as a post-processing step to enhance the performance of heatmap based methods , like CLM and most recently LGCN .",0
13440,We apply the powerful PCA refinement method in LGCN and compare it with our GHCU .,0
13441,We evaluate on 300 - VW where the occlusion and low - quality are particularly challenging .,0
13442,"As shown in Tab. 8 , our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency .",0
13443,Ablation study .,0
13444,"To verify the effectiveness of different components in our framework , we conduct this ablation study on 300 - VW .",1
13445,"For a fair comparison , all the experiments use the same parameter settings .",0
13446,As shown in Tab .,0
13447,"9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .",1
13448,"GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .",1
13449,Conclusion,0
13450,"In this paper , we first analyze the semantic ambiguity of facial landmarks and show that the potential random noises of landmark annotations can degrade the performance considerably .",0
13451,"To address this issue , we propose a a novel latent variable optimization strategy to find the semantically consistent annotations and alleviate random noises during training stage .",0
13452,Extensive experiments demonstrated that our method effectively improves the landmark detection accuracy on different data sets .,0
13453,title,0
13454,"3D Face Morphable Models "" In - the - Wild """,0
13455,abstract,0
13456,"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images .",1
13457,"With the advent of new 3D sensors , many 3 D facial datasets have been collected containing both neutral as well as expressive faces .",0
13458,"However , all datasets are captured under controlled conditions .",0
13459,"Thus , even though powerful 3 D facial shape models can be learnt from such data , it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions ( "" in - the - wild "" ) .",0
13460,"In this paper , we propose the first , to the best of our knowledge , "" in - the - wild "" 3 DMM by combining a powerful statistical model of facial shape , which describes both identity and expression , with an "" in - the -wild "" texture model .",0
13461,"We show that the employment of such an "" in - thewild "" texture model greatly simplifies the fitting procedure , because there is no need to optimize with regards to the illumination parameters .",0
13462,"Furthermore , we propose anew fast algorithm for fitting the 3 DMM in arbitrary images .",0
13463,"Finally , we have captured the first 3 D facial database with relatively unconstrained conditions and report quantitative evaluations with state - of - the - art performance .",0
13464,"Complementary qualitative reconstruction results are demonstrated on standard "" in - the - wild "" facial databases .",0
13465,An open source implementation of our technique is released as part of the Menpo Project [ 1 ] .,0
13466,Introduction,0
13467,"During the past few years , we have witnessed significant improvements in various face analysis tasks such as face detection and 2D facial landmark localization on static images .",0
13468,"This is primarily attributed to the fact that the community has made a considerable effort to collect and annotate facial images captured under unconstrained conditions ( commonly referred to as "" in - the -wild "" ) and to the discriminative methodologies that can capitalise on the availability of such large amount of data .",0
13469,"Nevertheless , discriminative techniques can not be applied for 3D facial shape estimation "" in - the -wild "" , due to lack of ground - truth data .",0
13470,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,1
13471,The two main lines of research are ( i ) fitting a 3D Morphable Model ( 3 DMM ) and ( ii ) applying Shape from Shading ( SfS ) techniques .,0
13472,The 3 DMM fitting proposed in the work of Blanz and Vetter was among the first model - based 3 D facial recovery approaches .,0
13473,The method requires the construction of a 3 DMM which is a statistical model of facial texture and shape in a space where there are explicit correspondences .,0
13474,The first 3 DMM was built using 200 faces captured in well - controlled conditions displaying only the neutral expression .,0
13475,"That is the reason why the method was only shown to work on real - world , but not "" in - the -wild "" , images .",0
13476,State - of - the - art,0
13477,SfS techniques capitalise on special multi-linear decompositions that find an approximate spherical harmonic decomposition of the illumination .,0
13478,"Furthermore , in order to benefit from the large availability of "" in - the - wild "" images , these methods jointly reconstruct large collections of images .",0
13479,"Nevertheless , even thought the results of are quite interesting , given that there is no prior of the facial surface , the methods only recover 2.5 D representations of the faces and particular smooth approximations of the facial normals .",0
13480,"3 D facial shape recovery from a single image under "" inthe - wild "" conditions is still an open and challenging problem in computer vision mainly due to the fact that :",0
13481,The general problem of extracting the 3D facial shape from a single image is an ill - posed problem which is notoriously difficult to be solved without the use of any statistical priors for the shape and texture of faces .,0
13482,"That is , without prior knowledge regarding the shape of the object at - hand there are inherent ambiguities present in the problem .",0
13483,"The pixel intensity at a location in an image is the result of a complex combination of the underlying shape of the object , the surface albedo and normal characteristics , camera parameters and the arrangement of scene lighting and other objects in the scene .",0
13484,"Hence , there are potentially infinite solutions to the problem .",0
13485,"Learning statistical priors of the 3D facial shape and texture for "" in - the - wild "" images is currently very difficult by using modern acquisition devices .",0
13486,"That is , even though there is a considerable improvement in 3D acquisition devices , they still can not operate in arbitrary conditions .",0
13487,"Hence , all the current 3 D facial databases have been captured in controlled conditions .",0
13488,"With the available 3 D facial data , it is feasible to learn a powerful statistical model of the facial shape that generalises well for both identity and expression .",0
13489,"However , it is not possible to construct a statistical model of the facial texture that generalises well for "" in - the - wild "" images and is , at the same time , in correspondence with the statistical shape model .",0
13490,That is the reason why current stateof - the - art 3 D face reconstruction methodologies rely solely on fitting a statistical 3 D facial shape prior on a sparse set of landmarks .,0
13491,"In this paper , we make a number of contributions that enable the use of 3 DMMs for "" in - the -wild "" face reconstruction ( ) .",0
13492,"In particular , our contributions are :",0
13493,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .",1
13494,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .",1
13495,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .",1
13496,"By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .",0
13497,"Furthermore , we make the implementation of our algorithm publicly available , which we believe can be of great benefit to the community , given the lack of robust open - source implementations for fitting 3 DMMs .",0
13498,"Due to lack of ground - truth data , the majority of the 3D face reconstruction papers report only qualitative results .",0
13499,"In this paper , in order to provide quantitative evaluations , we collected anew dataset of 3D facial surfaces , using Kinect Fusion , which has many "" in - the -wild "" characteristics , even though it is captured indoors .",0
13500,We release an open source implementation of our technique as part of the Menpo Project .,0
13501,The remainder of the paper is structured as follows .,0
13502,"In Section 2 we elaborate on the construction of our "" inthe -wild "" 3 DMM , whilst in Section 3 we outline the proposed optimization for fitting "" in - the - wild "" images with our model .",0
13503,"Section 4 describes our new dataset , the first of its kind , to provide images with a ground - truth 3 D facial shape that exhibit many "" in - the - wild "" characteristics .",0
13504,"We outline a series of quantitative and qualitative experiments in Section 5 , and end with conclusions in Section 6 .",0
13505,Model Training,0
13506,"A 3 DMM consists of three parametric models : the shape , camera and texture models .",0
13507,Shape Model,0
13508,Let us denote the 3D mesh ( shape ) of an object with N vertexes as a 3N 1 vector orthonormal basis after keeping the first n s principal components .,0
13509,This model can be used to generate novel 3 D shape instances using the function S :,0
13510,"where p = [ p 1 , . . . , p ns ]",0
13511,T are then s shape parameters .,0
13512,Camera Model,0
13513,The purpose of the camera model is to map ( project ) the object - centered Cartesian coordinates of a 3D mesh instance s into 2D Cartesian coordinates on an image plane .,0
13514,"In this work , we employ a pinhole camera model , which utilizes a perspective transformation .",0
13515,"However , an orthographic projection model can also be used in the same way .",0
13516,Perspective projection .,0
13517,"The projection of a 3D point x = [ x , y , z ] T into its 2 D location in the image plane x = [ x , y ]",0
13518,T involves two steps .,0
13519,"First , the 3D point is rotated and translated using a linear view transformation , under the assumption that the camera is still",0
13520,"T are the 3D rotation and translation components , respectively .",0
13521,"Then , a nonlinear perspective transformation is applied as",0
13522,where f is the focal length in pixel units ( we assume that the x and y components of the focal length are equal ) and,0
13523,"[ c x , c y ]",0
13524,T is the principal point that is set to the image center .,0
13525,Quaternions .,0
13526,We parametrize the 3D rotation with quaternions .,0
13527,"The quaternion uses four parameters q = [ q 0 , q 1 , q 2 , q 3 ] T in order to express a 3 D rotation as",0
13528,"( 5 ) Note that by enforcing a unit norm constraint on the quaternion vector , i.e. q T q = 1 , the rotation matrix constraints of orthogonality with unit determinant are withheld .",0
13529,"Given the unit norm property , the quaternion can be seen as a three - parameter vector [ q 1 , q 2 , q 3 ] T and a scalar",0
13530,"Most existing works on 3 DMM parametrize the rotation matrix R v using the three Euler angles that define the rotations around the horizontal , vertical and camera axes .",0
13531,"Even thought Euler angles are more naturally interpretable , they have strong disadvantages when employed within an optimization procedure , most notably the solution ambiguity and the gimbal lock effect .",0
13532,"Parametrization based on quaternions overcomes these disadvantages and further ensures computational efficiency , robustness and simpler differentiation .",0
13533,being the vector of camera parameters with length n c = 7 .,0
13534,"For abbreviation purposes , we represent the camera model of the 3 DMM with the function W : R ns ,nc ?",0
13535,R 2N as,0
13536,where S ( p ) is a 3 D mesh instance using Eq. 2 .,0
13537,""" In - the - Wild "" Feature - Based Texture Model",0
13538,"The generation of an "" in - the -wild "" texture model is a key component of the proposed 3 DMM .",0
13539,"To this end , we take advantage of the existing large facial "" in - the -wild "" databases that are annotated in terms of sparse landmarks .",0
13540,"Assume that fora set of M "" in - the - wild "" images { I i } M 1 , we have access to the associated camera and shape parameters {p i , c i }.",0
13541,Let us also define a dense feature extraction function,0
13542,where C is the number of channels of the feature - based image .,0
13543,"For each image , we first compute its feature - based representation as F i = F (I i ) and then use Eq. 7 to sample it at each vertex location to build back a vectorized texture",0
13544,Building an ITW texture model will be nonsensical for some regions mainly due to selfocclusions present in the mesh projected in the image space,0
13545,"To alleviate these issues , we cast a ray from the camera to each vertex and test for self - intersections with the triangulation of the mesh in order to learn a per-vertex occlusion mask mi ?",0
13546,RN for the projected sample .,0
13547,"Let us create the matrix X = [t 1 , . . . , t M ] ?",0
13548,R CN M by concatenating the M grossly corrupted feature - based texture vectors with missing entries that are represented by the masks mi .,0
13549,"To robustly build a texture model based on this heavily contaminated incomplete data , we need to recover a low - rank matrix L ? R CN M representing the clean facial texture and a sparse matrix E ?",0
13550,R CN M accounting for gross but sparse non-Gaussian noise such that X = L + E .,0
13551,"To simultaneously recover both Land E from incomplete and grossly corrupted observations , the Principal Component Pursuit with missing values is solved arg min",0
13552,"where * denotes the nuclear norm , 1 is the matrix 1 norm and ? >",0
13553,0 is a regularizer .,0
13554,?,0
13555,"represents the set of locations corresponding to the observed entries of X ( i.e. , ( i , j ) ? ?",0
13556,if mi = m j = 1 ) .,0
13557,"Then , P ? ( X ) is defined as the projection of the matrix X on the observed entries ? , namely P ?",0
13558,"( X ) ij = x ij if ( i , j ) ? ? and P ?",0
13559,( X ) ij = 0 otherwise .,0
13560,The unique solution of the convex optimization problem in Eq. 9 is found by employing an Alternating Direction Method of Multipliers - based algorithm .,0
13561,The final texture model is created by applying PCA on the set of reconstructed feature - based textures acquired from the previous procedure .,0
13562,"This results in {t , Ut } , wher ? t ?",0
13563,R CN is the mean texture vector and Ut ?,0
13564,R CN nt is the orthonormal basis after keeping the first n t principal components .,0
13565,This model can be used to generate novel 3D feature - based texture instances with the function T : R nt ?,0
13566,R CN as,0
13567,"where ? = [? 1 , . . . , ? nt ]",0
13568,T are then t texture parameters .,0
13569,"Finally , an iterative procedure is used in order to refine the texture .",0
13570,"That is , we started with the 3D fits provided by using only the 2D landmarks .",0
13571,"Then , a texture model is learned using the above procedure .",0
13572,The texture model was used with the proposed 3 DMM fitting algorithm on the same data and texture model was refined .,0
13573,Model Fitting,0
13574,We propose to fit the 3 DMM on an input image using Gauss - Newton iterative optimization .,0
13575,"To this end , herein , we first formulate the cost function and then present two optimization procedures .",0
13576,Cost Function,0
13577,"The overall cost function of the proposed 3 DMM formulation consists of a texture - based term , an optional error term based on sparse 2D landmarks and optional regularization terms on the parameters .",0
13578,Texture reconstruction cost .,0
13579,"The main term of the optimization problem is the one that aims to estimate the shape , texture and camera parameters that minimize the 2 2 norm of the difference between the image feature - based texture that corresponds to the projected 2D locations of the 3D shape instance and the texture instance of the 3 DMM .",0
13580,Let us denote by F = F ( I ) the feature - based representation with C channels of an input image I using Eq .,0
13581,8 .,0
13582,"Then , the texture reconstruction cost is expressed as",0
13583,"Note that F ( W ( p , c ) ) ?",0
13584,R CN denotes the operation of sampling the feature - based input image on the projected 2D locations of the 3D shape instance acquired by the camera model ( Eq. 7 ) .,0
13585,Regularization .,0
13586,"In order to avoid over - fitting effects , we augment the cost function with two optional regularization terms over the shape and texture parameters .",0
13587,Let us denote as ? s ?,0
13588,R nsns and ? t ?,0
13589,"R nt nt the diagonal matrices with the eigenvalues in their main diagonal for the shape and texture models , respectively .",0
13590,"Based on the PCA nature of the shape and texture models , it is assumed that their parameters follow normal prior distributions , i.e. p ? N ( 0 , ? s ) and ? ? N ( 0 , ? t ) .",0
13591,"We formulate the regularization terms as the 2 2 of the parameters ' vectors weighted with the corresponding inverse eigenvalues , i.e.",0
13592,arg min,0
13593,where c sand ct are constants that weight the contribution of the regularization terms in the cost function .,0
13594,2D landmarks cost .,0
13595,"In order to rapidly adapt the camera parameters in the cost of Eq. 11 , we further expand the where s l = [ x 1 , y 1 , . . . , x L , y L ]",0
13596,"T denotes a set of L sparse 2D landmark points ( L N ) defined on the image coordinate system and W l ( p , c ) returns the 2L 1 vector of 2D projected locations of these L sparse landmarks .",0
13597,"Intuitively , this term aims to drive the optimization procedure using the selected sparse landmarks as anchors for which we have the optimal locations s l .",0
13598,This optional landmarks - based cost is weighted with the constant cl .,0
13599,Overall cost function .,0
13600,"The overall 3 DMM cost function is formulated as the sum of the terms in Eqs. 11 , 12 , 13 , i.e. arg min",0
13601,The landmarks term as well as the regularization terms are optional and aim to facilitate the optimization procedure in order to converge faster and to a better minimum .,0
13602,"Note that thanks to the proposed "" in - the - wild "" feature - based texture model , the cost function does not include any parametric illumination model similar to the ones in the relative literature , which greatly simplifies the optimization .",0
13603,Gauss - Newton Optimization,0
13604,"Inspired by the extensive literature in Lucas - Kanade 2D image alignment , we formulate a Gauss - Newton optimization framework .",0
13605,"Specifically , given that the camera projection model is applied on the image part of Eq. 14 , the proposed optimization has a "" forward "" nature .",0
13606,Parameters update .,0
13607,"The shape , texture and camera parameters are updated in an additive manner , i.e. p ? p + ? p , ? ? ? + ?? , c ? c + ?c where ? p , ?? and ?c are their increments estimated at each fitting iteration .",0
13608,"Note that in the case of the quaternion used to parametrize the 3D rotation matrix , the update is performed as the multiplication q ?(? q ) q = ?q 0 ? q 1:3 q 0 q 1:3 = = ?q 0 q 0 ? ?q T 1:3 q 1:3 ?q 0 q 1:3 + q 0 ?q 1:3 + ? q 1:3 q 1:3",0
13609,"However , we will still denote it as an addition for simplicity .",0
13610,"Finally , we found that it is beneficial to keep the focal length constant inmost cases , due to its ambiguity with t z .",0
13611,Linearization .,0
13612,"By introducing the additive incremental updates on the parameters of Eq. 14 , the cost function is where J F , p = ? F ? W ?p p=p and J F , c = ? F ? W ?c",0
13613,"c=c are the image Jacobians with respect to the shape and camera parameters , respectively .",0
13614,"Note that most dense featureextraction functions F ( ) are non-differentiable , thus we simply compute the gradient of the multi -channel feature image ?",0
13615,"F . Similarly , the linearization on the sparse landmarks projection term gives",0
13616,"and J W l , c = ? W l ?c",0
13617,c=c are the camera Jacobians .,0
13618,Please refer to the supplementary material for more details on the computation of these derivatives .,0
13619,Simultaneous,0
13620,"Herein , we aim to simultaneously solve for all parameters ' increments .",0
13621,By substituting Eqs .,0
13622,"and e F = F ( W ( p , c ) ) ? T (? )",0
13623,are the residual terms .,0
13624,The computational complexity of the Simultaneous algorithm per iteration is dominated by the texture reconstruction term as O ( ( n s + n c + n t ),0
13625,"3 + CN ( n s + n c + n t ) 2 ) , which in practice is too slow .",0
13626,Project - Out,0
13627,We propose to use a Project - Out optimization approach that is much faster than the Simultaneous .,0
13628,The main idea is to optimize on the orthogonal complement of the texture subspace which will eliminate the need to solve for the texture parameters increment at each iteration .,0
13629,"By substituting Eqs. 18 and 19 into Eq. 17 and removing the incremental update on the texture parameters as well as the texture parameters regularization term , we end up with the problem",0
13630,"T is the orthogonal complement of the texture subspace that functions as the "" project - out "" operator with E denoting the CN CN unitary matrix .",0
13631,"Note that in order to derive Eq. 26 , we use the properties PT = P and PT P = P .",0
13632,"By differentiating Eq. 26 and equalizing to zero , we get the solution",0
13633,where,0
13634,are the Hessian matrices and,0
13635,are the residual terms .,0
13636,The texture parameters can be estimated at the end of the iterative procedure using Eq. 25 .,0
13637,"Note that the most expensive operation is J T F , p P. However , if we first do J T F , p Ut and then multiply this result with UT t , the total cost becomes O ( CN n tn s ) .",0
13638,"The same stands for J T F , c P .",0
13639,"Consequently , the cost per iteration is O ( ( n s + n c )",0
13640,3 + CN n t ( n s + n c ) + CN ( n s + n c ) 2 ) which is much faster than the Simultaneous algorithm .,0
13641,Residual masking .,0
13642,"In practice , we apply a mask on the texture reconstruction residual of the Gauss - Newton optimization , in order to speed - up the 3 DMM fitting .",0
13643,This mask is constructed by first acquiring the set of visible vertexes using z- buffering and then randomly selecting K of them .,0
13644,"By keeping the number of vertexes small ( K ? 5000 N ) , we manage to greatly speed - up the fitting process without any accuracy penalty .",0
13645,KF - ITW,0
13646,Dataset,0
13647,"For the evaluation of the 3 DMM , we have constructed KF - ITW , the first dataset of 3D faces captured under relatively unconstrained conditions .",0
13648,"The dataset consists of 17 different subjects recorded under various illumination conditions performing a range of expressions ( neutral , happy , surprise ) .",0
13649,We employed the KinectFusion framework to acquire a 3D representation of the subjects with a Kinect v 1 sensor .,0
13650,The fused mesh for each subject serves as a 3D face ground - truth in which we can evaluate our algorithm and compare it to other methods .,0
13651,A voxel grid of size 608 3 was utilized to get the detailed 3 D scans of the faces .,0
13652,"In order to accurately reconstruct the entire surface of the faces , a circular motion scanning pattern was carried out .",0
13653,Each subject was instructed to stay still in a fixed pose during the entire scanning process .,0
13654,The frame rate for every subject was constant to 8 frames per second .,0
13655,After getting the 3D scans from the KinectFusion framework we fit our shape model in a non-rigid manner to get a clear mesh with a distinct number of vertexes for the evaluation process .,0
13656,"Finally , each mesh was manually annotated with the iBUG 49 sparse landmark set .",0
13657,Experiments,0
13658,"To train our model , which we label as ITW , we use a variant of the Basel Face Model ( BFM ) that we trained to contain both identities drawn from the original BFM model along with expressions provided by .",0
13659,"We trained the "" in - the -wild "" texture model on the images of iBUG , LFPW & AFW datasets as described in Sec. 2.3 using the 3D shape fits provided by .",0
13660,"Additionally , we elect to use the project - out formulation for the throughout our experiments due its superior run-time performance and equivalent fitting performance to the simultaneous one .",0
13661,reports additional measures .,0
13662,3D Shape Recovery,1
13663,"Herein , we evaluate our "" in - the -wild "" 3 DMM ( ITW ) in terms of 3D shape estimation accuracy against two popular state - of - the - art alternative 3 DMM formulations .",0
13664,The first one is a classic 3 DMM with the original Basel laboratory texture model and full lighting equation which we term Classic .,0
13665,The second is the texture - less linear model proposed in which we refer to as Linear .,0
13666,For Linear code we use the Surrey Model with related blendshapes along with the implementation given in .,0
13667,"We use the ground - truth annotations provided in the KF - ITW dataset to initialize and fit all three techniques to the "" in - the -wild "" style images in the dataset .",0
13668,"The mean mesh of each model under testis landmarked with the same 49 point markup used in the dataset , and is registered against the ground truth mesh by performing a Procrustes alignment using the sparse annotations followed by Non-Rigid Iterative Closest Point ( N - ICP ) to iteratively deform the two surfaces until they are brought into correspondence .",0
13669,This provides a per-model ' ground - truth ' for the 3D shape recovery problem for each image under test .,0
13670,"Our error metric is the per-vertex dense error between the recovered shape and the model - specific corresponded ground - truth fit , normalized by the inter-ocular distance for the test mesh .",0
13671,shows the cumulative error distribution for this experiment for the three models under test .,0
13672,reports the corresponding Area,0
13673,Under the Curve ( AUC ) and failure rates .,0
13674,"The Classic model struggles to fit to the "" in - the -wild "" conditions present in the test set , and performs the worst .",1
13675,"The texture - free Linear model does better , but the ITW model is most able to recover the facial shapes due to its ideal feature basis for the "" in - the -wild "" conditions .",1
13676,"demonstrates qualitative results on a wide range of fits of "" in - the - wild "" images drawn from the Helen and 300 W datasets that qualitatively highlight the effectiveness of the proposed technique .",0
13677,"We note that in a wide variety of expression , identity , lighting and occlusion conditions our model is able to robustly reconstruct a realistic 3 D facial shape that stands up to scrutiny .",1
13678,Quantitative Normal Recovery,1
13679,"As a second evaluation , we use our technique to find per-pixel normals and compare against two well established Shape - from - Shading ( SfS ) techniques : PS - NL and IMM .",0
13680,For experimental evaluation we employ images of 100 subjects from the Photoface database .,0
13681,As a set of four illumination conditions are provided for each subject then we can generate ground - truth facial surface normals using calibrated 4 - source Photometric Stereo .,0
13682,In we show the cumulative error distribution in terms of the mean angular error .,0
13683,ITW slightly outperforms IMM even though both IMM and PS - NL use all four available images of each subject .,1
13684,Conclusion,0
13685,"We have presented a novel formulation of 3 DMMs reimagined for use in "" in - the -wild "" conditions .",0
13686,"We capitalise on the annotated "" in - the - wild "" facial databases to propose a methodology for learning an "" in - the -wild "" feature - based texture model suitable for 3 DMM fitting without having to optimise for illumination parameters .",0
13687,"Furthermore , we propose a novel optimisation procedure for 3 DMM fitting .",0
13688,We show that we are able to recover shapes with more detail than is possible using purely landmark - driven approaches .,0
13689,"Our newly introduced "" in - the -wild "" KinectFusion dataset allows for the first time a quantitative evaluation of 3D fa-cial reconstruction techniques in the wild , and on these evaluations we demonstrate that our in the wild formulation is state of the art , outperforming classical 3 DMM approaches by a considerable margin .",0
13690,title,0
13691,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,1
13692,abstract,0
13693,:,0
13694,Dense face alignment ( odd rows ) and 3D face reconstruction ( even rows ) results from our proposed method .,0
13695,"For alignment , only 68 key points are plotted for clear display ; for 3D reconstruction , reconstructed shapes are rendered with headlight for better view .",0
13696,"Our method offers strong robustness and good performance even in presence of large poses ( the 3th , 4th and 5th columns ) and occlusions ( the 6th , 7th and 8th columns ) .",0
13697,Best viewed in color .,0
13698,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,1
13699,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,0
13700,"However , the shortage of training data with 3D annotations considerably limits performance of those methods .",0
13701,"To alleviate this issue , we propose a novel 2D - assisted self - supervised learning ( 2DASL ) method that can effectively use "" in - the -wild "" 2 D face images with noisy landmark information to substantially improve 3 D face model learning .",0
13702,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .",0
13703,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .",0
13704,Experiments on multiple challenging datasets show that our method outperforms state - of - the - arts for both 3 D face reconstruction and dense face alignment by a large margin .,0
13705,narrative,0
13706,:,0
13707,Dense face alignment ( odd rows ) and 3D face reconstruction ( even rows ) results from our proposed method .,0
13708,"For alignment , only 68 key points are plotted for clear display ; for 3D reconstruction , reconstructed shapes are rendered with headlight for better view .",0
13709,"Our method offers strong robustness and good performance even in presence of large poses ( the 3th , 4th and 5th columns ) and occlusions ( the 6th , 7th and 8th columns ) .",0
13710,Best viewed in color .,0
13711,Abstract,0
13712,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,1
13713,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,0
13714,"However , the shortage of training data with 3D annotations considerably limits performance of those methods .",0
13715,"To alleviate this issue , we propose a novel 2D - assisted self - supervised learning ( 2DASL ) method that can effectively use "" in - the -wild "" 2 D face images with noisy landmark information to substantially improve 3 D face model learning .",0
13716,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .",0
13717,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .",0
13718,Experiments on multiple challenging datasets show that our method outperforms state - of - the - arts for both 3 D face reconstruction and dense face alignment by a large margin .,0
13719,Introduction,0
13720,3 D face reconstruction is an important task in the field of computer vision and graphics .,1
13721,"For instance , the recovery of 3D face geometry from a single image can help address many challenges ( e.g. , large pose and occlusion ) for 2 D face alignment through dense face alignment .",0
13722,"Traditional 3 D face reconstruction methods are mainly based on optimization algorithms , e.g. , iterative closest point , to obtain coefficients for the 3D Morphable Model ( 3 DMM ) model and render the corresponding 3 D faces from a single face image .",0
13723,"However , such methods are usually time - consuming due to the high optimization complexity and suffer from local optimal solution and bad initialization .",0
13724,Recent works thus propose to use CNNs to learn to regress the 3 DMM coefficients and significantly improve the reconstruction quality and efficiency .,0
13725,CNN - based methods have achieved remarkable success in 3D face reconstruction and dense face alignment .,0
13726,"However , obtaining an accurate 3 D face CNN regression model ( from input 2D images to 3 DMM coefficients ) requires a large amount of training faces with 3D annotations , which are expensive to collect and even not achievable in some cases .",0
13727,"Even some 3 D face datasets , like 300W - LP , are publicly available , they generally lack diversity in face appearance , expression , occlusions and environment conditions , limiting the generalization performance of resulted 3 D face regression models .",0
13728,A model trained on such datasets can not deal well with various potential cases in - the - wild that are not present in the training examples .,0
13729,"Although some recent works bypass the 3 DMM parameter regression and use image - to - volume or image - to - image strategy instead , the ground truths are all still needed and generated from 3 DMM using 300W - LP , still lacking diversity .",0
13730,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .",1
13731,"With the method , the trained 3 D face model can perform 3 D face reconstruction and dense face alignment well .",0
13732,"This is inspired by the observation that a large number of 2 D face datasets are available with obtainable 2D landmark annotations , that could provide valuable information for 3D model learning , without requiring new data with 3D annotations .",0
13733,"Since these 2D images do not have any 3D annotations , it is not straightforward to exploit them in 3 D face model learning .",0
13734,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,1
13735,"In particular , the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2 Dto - 2D and 3D - to - 3D self - mapping procedure as supervi-sion .",0
13736,The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D - to - 2D projection .,1
13737,"Meanwhile , the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same .",0
13738,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .",1
13739,"By leveraging these self - supervision derived from 2 D face images without 3D annotations , our method could substantially improve the quality of learned 3 D face regression model , even though there is lack of 3D samples and no 3D annotations for the 2D samples .",0
13740,"To facilitate the overall learning procedure , our method also exploits self - critic learning .",1
13741,"It takes as input both the latent representation and 3 DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3 DMM coefficients and the corresponding face image , offering another supervision for 3 D face model learning .",0
13742,"Our proposed method is principled , effective and fully exploits available data resources .",0
13743,"As shown in , our method can produce 3D reconstruction and dense face alignment results with strong robustness to large poses and occlusions .",0
13744,"Our code , models and online demos will be available upon acceptance .",0
13745,Our contributions are summarized as follows :,0
13746,"We propose anew scheme that aims to fully utilize the abundant "" in - the - wild "" 2 D face images to assist 3 D face model learning .",0
13747,This is new and different from most common practices that pursues to improve 3 D face model by collecting more data with 3D annotations for model training .,0
13748,We introduce anew method that is able to train 3 D face models with 2 D face images by self - supervised learning .,0
13749,The devised multiple forms of self - supervision are effective and data efficient .,0
13750,"We develop anew self - critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model , even though the 2D landmark annotations are noisy .",0
13751,Comparison on the AFLW2000 - 3D and AFLW - LFPA datasets shows that our method achieves excellent performance on both tasks of 3D face reconstruction and dense face alignment .,0
13752,Related work,0
13753,3D,0
13754,Face Reconstruction,0
13755,Various approaches have been proposed to tackle the inherently ill - posed problem of 3D face reconstruction from a single image .,0
13756,"In , Vetter and Blanz observe that both the geometric structure and the texture of human faces can be approximated by a linear combination of orthogonal basis vectors obtained by PCA over 100 male and 100 female identities .",0
13757,"Based on this , they propose the 3 DMM to represent the shape and texture of a 3 D face .",0
13758,"After that , large amount of efforts have been proposed to improve 3 DMM modeling mechanism .",0
13759,"Most of them devote to regressing the 3 DMM coefficients by solving the non-linear optimization problem to establish the correspondences of the points between a single face image and the canonical 3 D face model , including facial landmarks and local features .",0
13760,"Recently , various attempts have been made to estimate the 3 DMM coefficients from a single face image using CNN as a regressor , as opposed to non-linear optimization .",0
13761,"In , cascaded CNN structures are used to regress the 3 DMM coefficients , which are time - consuming due to multi-stage .",0
13762,"Besides , end - to - end approaches are also proposed to directly estimate the 3 DMM coefficients in a holistic manner .",0
13763,"More recently , works are proposed to use CNN directly obtain the reconstructed 3 D face bypassing the 3 DMM coefficients regression .",0
13764,"In , Jackson et al. propose to map the image pixels to a volumetric representation of the 3D facial geometry through CNN - based regression .",0
13765,"While their method is not restricted to the 3 DMM space anymore , it needs a complex network structure and a lot of time to predict the voxel information .",0
13766,"Ina later work , Feng et al .",0
13767,store the 3D facial geometry into UV position map and train an imageto - image CNN to directly regress the complete 3 D facial structure along with semantic information from a single image .,0
13768,Face Alignment Traditional 2 D face alignment methods aim at locating a sparse set of fiducial facial landmarks .,0
13769,Initial progresses have been made with the classic Active Appearance Model ( AAM ) and Constrained Local Model ( CLM ) .,0
13770,"Recently , CNN - based methods have achieved state - of - the - art performance on 2D landmark localization .",0
13771,"However , 2 D face alignment only regresses visible landmarks on faces , which are unable to address large pose or occlusion situations , where partial face regions are invisible .",0
13772,"With the development of this field , 3 D face alignment have been proposed , aiming to fit a 3 DMM or register a 3 D facial template to a 2 D face image , which makes it possible to deal with the invisible points .",0
13773,The original 3 DMM fitting method fits the 3D model by minimizing the pixel - wise difference between image and the rendered face model .,0
13774,"It is the first method that can address arbitrary poses , which , however , suffers from the one - minute - per-image computational cost .",0
13775,"After that , some methods estimate 3 DMM coefficients and then project the estimated 3D landmarks onto 2 D space , such methods could significantly improve the efficiency .",0
13776,"Recently , the task of dense face alignment starts to attract more and more research attention , aiming to achieve very dense 3D alignment for large pose face images ( including invisible parts ) .",0
13777,"In , Liu et al .",0
13778,"use multi-constraints to train a CNN model , jointly estimating the 3 DMM coefficient and provides very dense 3D alignment .",0
13779,"directly learn the correspondence between a 2 D face image and a 3D template via a deep CNN , while only visible face - region is considered .",0
13780,"Overall , CNN - based methods have achieved great success in both 3 D face reconstruction and dense face alignment .",0
13781,"However , they need a huge amount of 3D annotated images for training .",0
13782,"Unfortunately , currently face datasets with 3D annotations are very limited .",0
13783,"As far as we know , only the 300W - LP dataset has been widely used for training .",0
13784,"However , the 300W - LP is generated by profiling faces of 300 W into larger poses , which is not strictly unconstrained and can not coverall possible scenes in - thewild .",0
13785,Proposed method,0
13786,In this section we introduce the proposed 2D - Aided Selfsupervised Learning ( 2DASL ) method for simultaneous 3 D face reconstruction and dense face alignment .,0
13787,We first review the popular 3D morphable model that we adopt to render the 3D faces .,0
13788,"Then we explain our method in details , in particular the novel cycle - consistency based self - supervised learning and the self - critic learning .",0
13789,3D morphable model,0
13790,We adopt the 3D morphable model ( 3 DMM ) to recover the 3D facial geometry from a single face image .,0
13791,The 3 DMM renders 3 D face shape S ?,0
13792,R 3N that stores 3 D coordinates of N mesh vertices with linear combination over a set of PCA basis .,0
13793,"Following , we use 40 basis from the Basel Face Model ( BFM ) to generate the face shape component and 10 basis from the Face Warehouse dataset to generate the facial expression component .",0
13794,The rendering of a 3 D face shape is thus formulated as :,0
13795,where S ?,0
13796,"R 3N is the mean shape , As ?",0
13797,"R 3N 40 is the shape principle basis trained on the 3D face scans , ? s ?",0
13798,R 40 is the shape representation coefficient ; A exp ?,0
13799,R 3N 10 is the expression principle basis and ? exp ?,0
13800,R 10 denotes the corresponding expression coefficient .,0
13801,The target of singleimage based 3 D face modeling is to predict the coefficients ?,0
13802,expand ?,0
13803,s for 3 D face rendering from a single 2D image .,0
13804,"After obtaining the 3D face shape S , it can be projected onto the 2D image plane with the scale orthographic projection to generate a 2 D face from specified viewpoint :",0
13805,"where V stores the 2D coordinates of the 3D vertices projected onto the 2D plane , f is the scale factor , Pr is the orthographic projection matrix 1 0 0 0 1 0 , ? is the projection matrix consisting of 9 parameters , and t is the translation vector .",0
13806,"Putting them together , we have in total 62 pa-rameters ? = [ f , t , ? , ? s , ? exp ] to regress for the 3 D face regressor model .",0
13807,Model overview,0
13808,"As illustrated in , the proposed 2 DASL model contains 3 modules , i.e. , a CNN - based regressor that predicts 3 DMM coefficients from the input 2D image , an encoder that transforms the input image into a latent representation , and a self - critic that evaluates the input ( latent representation , 3 DMM coefficients ) pairs to be consistent or not .",0
13809,We use ResNet - 50 to implement the CNN regressor .,0
13810,"The encoder contains 6 convolutional layers , each followed by a ReLU and a max pooling layer .",0
13811,"The critic consists of 4 fully - connected layers with 512 , 1024 , 1024 and 1 neurons respectively , followed by a softmax layer to output a score on the consistency degree of the input pair .",0
13812,The CNN regressor takes a 4 - channel tensor as input that concatenates a 3 - channel RGB face image and a 1 - channel 2D Facial Landmark Map ( FLM ) .,0
13813,"The FLM is a binary - value image , where the locations corresponding to facial landmarks take the value of 1 and others take the value of ? 1 .",0
13814,"Our proposed 2DSAL method trains the model using two sets of images , i.e. , the images with 3 DMM ground truth annotations and the 2D face images with only 2 D facial landmark annotations provided by an off - the - shelf facial landmark detector .",0
13815,The model is trained by minimizing the following one conventional 3D - supervision and four selfsupervision losses .,0
13816,The first one is the weighted coefficient prediction loss L 3d over the 3D annotated images that measures how accurate the model can predict 3 DMM coefficients .,0
13817,The second one is the 2D landmark consistency loss L 2 d- con that measures how well the predicted 3 D face shapes can recover the 2D landmark locations for the input 2D images .,0
13818,The third one is the 3D landmark consistency loss L 3 d- con .,0
13819,The fourth one is the cycle consistency loss L cyc .,0
13820,"The last one is the self - critic loss L sc that estimates the realism of the predicted 3 DMM coefficients for 3 D face reconstruction , conditioned on the face latent representation .",0
13821,Thus the overall training loss is :,0
13822,where ?'s are the weighting coefficients for different losses .,0
13823,The details of these losses are described in the following sections one by one .,0
13824,Weighted 3 DMM coefficient supervision,0
13825,"Following , we deploy the ground truth 3 DMM coefficients to supervise the model training where the contribution of each 3 DMM coefficient is re-weighted according to their importance .",0
13826,It trains the model to predict closer coefficients ?,0
13827,to its 3 DMM ground truth ? * .,0
13828,"Instead of calculating the conventional 2 loss , we explicitly consider im - : Illustration on the self - supervision introduced by our 2DSAL for utilizing sparse 2D landmark information .",0
13829,The 2D landmark prediction can be viewed as a self - mapping : X 2 d ?,0
13830,Y 2d ( forward training ) constrained by L2d - con .,0
13831,"To further supervise the model training , we introduce the Lcyc by mapping back from Y 2 d ? X 2 d ( backward training ) .",0
13832,The L3d- con is employed to constrain landmarks matching in 3D space during the cycle training .,0
13833,Here i indexes the landmark .,0
13834,Best viewed in color .,0
13835,portance of each coefficient and re-weigh their contribution to the loss computation accordingly .,0
13836,Thus we obtain the weighted coefficient prediction loss as follows :,0
13837,"where , W = diag ( w 1 , . . . , w 62 ) ,",0
13838,"Here w i indicates importance of the i th coefficient , computed from how much error it introduces to locations of 2D landmarks after projection .",0
13839,"Here H ( ) is the sparse landmark projection from rendered 3 D shape , ? * is the ground truth and ?",0
13840,i is the coefficient whose i th element comes from the predicted parameter and the others come from ? * .,0
13841,"With such a reweighting scheme , during training , the CNN model would first focus on learning the coefficients with larger weight ( e.g. , the ones for rotation and translation ) .",0
13842,"After decreasing their error and consequently their weights , the model will change to optimize the other coefficients ( e.g. , the ones for shape and expression ) .",0
13843,2D assisted self - supervised learning,0
13844,"To leverage the 2D face images with only annotation of sparse 2D landmark points offered by detector , we develop the following self - supervision scheme that offers three different self - supervision losses , including the 2D landmark consistency loss L 2 d - con , the 3D landmark consistency loss L 3 d - con and the cycle - consistency loss L cyc . gives a systematic overview .",0
13845,"The intuition behind this scheme is : if the 3D face estimation model is trained well , it should present consistency in the following three aspects .",0
13846,"First , the 2D landmarks Y 2 d recovered from the predicted 3D landmarks X 3 d via 3D - 2D projection should have small difference with the input 2 D landmarks X 2 d .",0
13847,"Second , : Illustration of the weight mask used for computing L2d - con .",0
13848,"We assign the highest weight to the red points , the medium weight to the pinky points , the yellow points has the lowest weight .",0
13849,Best viewed in color .,0
13850,the predicted 3D landmarks X 3 d from the input 2 D landmarks X 2 d should be consistent with the 3D landmarks,0
13851,X 3 d recovered from the predicted 2D landmarks Y 2 d by passing it through the same 3D estimation model .,0
13852,"Third , the pro-jected X 2 d from X 3 d should be consistent with the original input X 2 d , i.e. , forming a consistent cycle .",0
13853,"Thus , we define following two landmark consistency losses in our model correspondingly .",0
13854,The L 3 d- con is formulated as :,0
13855,where x 3 d,0
13856,"i is the i th 3D landmark output from the forward pass ( see red arrow in ) , x",0
13857,3 d i is the i th landmark predicted from the backward pass ( see green arrow in .,0
13858,"For computing the L 2 d - con , we first create a weight mask V = {v 1 , v 2 , ... , v N } based the contribution of each point .",0
13859,"Since the contour landmarks of a 2 D face are inaccurate to represent the corresponding points of 3 D face , we discard them and sample 18 landmarks from the 68 2D facial landmarks .",0
13860,The weight mask is shown in .,0
13861,"Here , the mouth center landmark is the midpoint of two mouth corner points .",0
13862,The L 2 d- con is defined as :,0
13863,"where x 2 d i is the i th 2D landmark of the input face , y 2 d i is the i th 2D landmark inferred from the output LMP , and vi is its corresponding weight .",0
13864,The weight values are specified in .,0
13865,We use the following relative weights in our experiments : ( red points ) : ( pinky points ) : ( yellow points ) = 4:2:1 that are set empirically .,0
13866,"We model the 2D facial landmarks prediction as a selfmapping process , and denote F : X 2 d ?",0
13867,"Y 2 d as the forward mapping , Q : Y 2 d ?",0
13868,X 2 d as the backward mapping .,0
13869,"The backward mapping brings the output landmarks y i back to its original position xi , i.e. , x ? F ( x ) ?",0
13870,Q ( F ( X ) ) ? x.,0
13871,We constrain this mapping using the cycle consistency loss :,0
13872,"where x 2 d are the input 2 D facial landmarks , andx 2 d are the landmarks output from Q ( F ( X ) ) . :",0
13873,Qualitative results on AFLW2000 - 3D dataset .,0
13874,The predictions by 2 DASL show that our predictions are more accurate than ground truth in some cases ( only 68 points are plotted to show ) .,0
13875,Green : landmarks predicted by our 2DASL .,0
13876,Red : ground truth from .,0
13877,The thumbnails on the top right corner of each image are the dense alignment results .,0
13878,Best viewed in color .,0
13879,Self - critic learning,0
13880,"We further introduce a self - critic scheme to weakly supervise the model training with the "" in - the -wild "" 2 D face images .",0
13881,"Given a set of face images I = { I 1 , . . . ,",0
13882,"In } without any 3D annotations and a set of face images J = { ( J 1 , ? * 1 ) , . . . , ( J m , ? * m ) } with accurate 3 DMM annotations , the CNN regressor model R :",0
13883,I i ? ?,0
13884,i would output 62 coefficients for each image .,0
13885,"We use another model as the critic C ( ) to evaluate whether the predicted coefficients are consistent with the input images as the pairs of ( J i , ? * i ) .",0
13886,"Since each coefficient is closely related to its corresponding face image , the critic model would learn to distinguish the realism of the coefficients conditioned on the latent representation of the input face images .",0
13887,"To this end , we feed the input images to an encoder to obtain the latent representation z and then concatenate with their corresponding 3 DMM coefficients as the inputs to the critic C ( ) .",0
13888,The critic is trained in the same way as the adversarial learning by optimizing the following loss :,0
13889,"where z * is the latent representation of a 3D annotated image J , ? * is the 3 DMM ground truth , I is the input "" inthe - wild "" face image , and z is its latent representation .",0
13890,"The above self - critic loss encourages the model to output 3 D faces that lie on the manifold of human faces , and predict landmarks that have the same distribution with the true facial landmarks .",0
13891,Experiments,0
13892,We evaluate 2DASL qualitatively and quantitatively under various settings for 3 D face reconstruction and dense face alignment .,0
13893,Training details and datasets,0
13894,Our proposed 2 DASL is implemented with Pytorch .,1
13895,"We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 10 ?5 and decays exponentially , the discriminator uses the Adam as optimizer with the fixed learning rate 1 10 ?4 .",1
13896,"The batch size is set as 32 . ? 1 , ? 2 , ? 3 and ?",0
13897,"4 are set as 0.005 , 0.005 , 1 and 0.005 respectively .",0
13898,We use a two - stage strategy to train our model .,1
13899,"In the first stage , we train the model using the overall loss L.",1
13900,"In the second stage , we fine - tune our model using the Vertex Distance Cost , following .",1
13901,The dataset 300W - LP is used to train our model .,0
13902,This dataset contains more than 60 K face images with annotated 3 DMM coefficients .,0
13903,"The "" in - the -wild "" face images are all from the UMDFaces dataset that contains 367,888 still face images for 8,277 subjects .",0
13904,The 2D facial landmarks of all the face images are detected by an advanced 2 D facial landmarks detector .,0
13905,The input images are cropped to the size 120 120 .,0
13906,We use the test datasets below to evaluate our method : AFLW2000 - 3D is constructed by selecting the first 2000 images from AFLW .,0
13907,Each face is annotated with its corresponding 3 DMM coefficients and the 68 3D facial landmarks .,0
13908,We use this dataset to evaluate our method on both 3 D face reconstruction and dense face alignment .,0
13909,is another extension of AFLW .,0
13910,It is constructed by picking images from AFLW according to the poses .,0
13911,"It contains 1,299 test images with a balanced distribution of yaw angle .",0
13912,Each image is annotated with 34 facial landmarks .,0
13913,We use this dataset to evaluate performance for the dense face alignment task .,0
13914,The 34 landmarks are used as the ground truth to measure the accuracy of our results .,0
13915,AFLW - LFPA,0
13916,Dense face alignment,1
13917,We first compare the qualitative results from our method and corresponding ground truths in .,0
13918,"Although all the state - of - the - art methods of dense face alignment conduct evaluation on AFLW2000 - 3D , the ground truth of AFLW2000 - 3D is controversial , since its annotation pipeline is based on the Landmarks Marching method in .",0
13919,"As can be seen , our results are more accurate than the ground truth in some cases .",0
13920,"This is mainly because 2 DASL involves a number of the "" in - the -wild "" images for training , enabling the model to perform well in cases even unseen in the 3D annotated training data .",0
13921,"For fair comparison , we adopt the normalized mean error ( NME ) as the metric to evaluate the alignment performance .",0
13922,The NME is the mean square error normalized by face bounding box size .,0
13923,"Since some images in AFLW2000 - 3D contains more than 2 faces , and the face detector sometimes gives the wrong face for evaluation ( not the test face with ground truth ) , leading to high NME .",0
13924,"Therefore , we discard the worst 20 cases of each method and only 1,980 images from AFLW2000 - 3D are used for evaluation .",0
13925,"We evaluate our 2 DASL using a sparse set of 68 facial landmarks and also the dense points with both 2 D and 3D coordinates , and compare it with other state - of - the - arts .",0
13926,The 68 sparse facial landmarks can be viewed as sampling from the dense facial points .,0
13927,"Since PRNet and VRN - Guided are not 3 DMM based , and the point cloud of these two methods are not corresponding to 3 DMM , we only compare with them on the sparse 68 landmarks .",0
13928,"The results are shown in , where we can see our 2 DASL achieves the lowest NME ( % ) on the evaluation of both 2 D and 3D coordinates among all the methods .",1
13929,"For 3 DMM - based methods : 3 DDFA and DeFA , our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates .",1
13930,"To further investigate performance of our 2 DASL across poses and datasets , we report the NME of faces with small , medium and large yaw angles on AFLW2000 - 3D dataset and the mean NME on both AFLW2000 - 3 D and AFLW - LPFA datasets .",0
13931,The comparison results are shown in Tab .,0
13932,1 . Note that all the images from these two datasets are used for evaluation to keep consistent with prior works .,0
13933,The results of the compared method are directly from their published papers .,0
13934,"As can be observed , our method achieves the lowest mean NME on both of the two datasets , and the lowest NME across all poses on AFLW2000 - 3D .",1
13935,"Our 2DASL even performs better than PRNet , reducing NME by 0.09 and 0.08 on AFLW2000 - 3D and AFLW - LFPA , respectively .",1
13936,"Es - pecially on large poses ( from 60 to 90 ) , 2 DASL achieves 0.2 lower NME than PRNet .",1
13937,"We believe more "" in - the -wild "" face images used for training ensures better performance of 2DASL .",0
13938,3 D face reconstruction,1
13939,"In this section , we evaluate our 2 DASL on the task of 3D face reconstruction on AFLW2000 - 3D by comparing with 3 DDFA and DeFA .",0
13940,The VRN - Guided and PRNet are not compared because of the mis-match of point cloud between them and our method .,0
13941,"Following , we first employ the Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the reconstructed 3 D face and the ground truth point cloud .",0
13942,We then calculate the NME normalized by the face bounding box size .,0
13943,shows the comparison results on AFLW2000 - 3D .,0
13944,"As can be seen , the 3D reconstruction results of 2 DASL outperforms 3 DDFA by 0.39 , and 2.29 for DeFA , which are significant improvements .",1
13945,We show some visual results of our 2 DASL and compare with PRNet and VRN - Guided in .,0
13946,"As can be seen , the reconstructed shape of our 2 DASL are more smooth , however , both PRNet and VRN - Guided introduce some artifacts into the reconstructed results , which makes the reconstructed faces look unnaturally .",0
13947,Ablation study,0
13948,"In this section , we perform ablation study on AFLW2000 - 3D by evaluating several variants of our model : ( 1 ) 2DASL ( base ) , which only takes the RGB images as input without self - supervision and self - critic supervision ;",0
13949,"( 2 ) 2DASL ( cyc ) , which takes as input the combination of RGB face images and the corresponding 2D FLMs with self - supervison , however without self - critic supervision ; ( 3 ) 2DASL ( sc ) , which takes as input the RGB face images only using self - critic learning .",0
13950,"( 4 ) 2DASL ( cyc+sc ) , which contains both self - supervision and self - critic supervision .",0
13951,"For each variant , we use the L 2 d - con with ( w / ) or without ( w / o ) weight mask .",0
13952,"Therefore , there are in total 6 variants .",0
13953,The ablation study results are shown in Tab .,0
13954,"2 . Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages , respectively .",1
13955,Both self - critic and the self - supervision are effective to improve the performance .,0
13956,"If the self - critic learning is not used , the NME increases by 0.04/0.18 for with / without weight mask , respectively .",1
13957,"While the self - supervision scheme reduce NME by 0.1 when the weight mask is used , and 0.23 if the weight mask is removed , no significant improvement is observed .",1
13958,The best result is achieved when both these two modules are used .,0
13959,"Moreover , in our experiments , we found taking the FLMs as input can accelerate the convergence of training process .",1
13960,"Therefore , the first training stage just takes one or two epochs to reach a good model .",0
13961,"To explore how the performance is affected by the number of "" in - the -wild "" face images involved in training , we train our model using different numbers .",0
13962,"Since the UMD - Faces dataset divides the whole dataset into 3 batches , each contains 77,228 , 115,126 , and 175,534 images respectively .",0
13963,We use the 3 batches and also the whole dataset to train our model .,0
13964,"The results are reported in Tab . 3 , where we can seethe more data that used for aiding training , the lower NME is achieved by 2DASL . :",0
13965,"The results ( NME ( % ) ) of 2 DASL by training with different number of "" in - the -wild "" face images .",0
13966,""" Num. # ITW "" indicates the number of the "" in - the -wild "" face images used for training .",0
13967,The numbers in bold are the best results of each stage .,0
13968,Conclusion,0
13969,"In this paper , we propose a novel 2D - Assisted Selfsupervised Learning ( 2DASL ) method for 3 D face reconstruction and dense face alignment based on the 3D Morphable face Model .",0
13970,The sparse 2D facial landmarks are taken as input of CNN regressor and learn themselves via 3 DMM coefficients regression .,0
13971,"To supervise and facilitate the 3D face model learning , we introduce four selfsupervision losses , including the self - critic which is employed to weakly supervise the training samples that without 3D annotations .",0
13972,"Our 2 DASL make the abundant "" inthe - wild "" face images could be used to aid 3D face analysis without any 2D - to - 3D supervision .",0
13973,Experiments on two challenging face datasets illustrate the effectiveness of 2 DASL on both 3 D face reconstruction and dense face alignment by comparing with other state - of - the - arts .,0
13974,title,0
13975,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,1
13976,abstract,0
13977,"We present a new loss function , namely Wing loss , for robust facial landmark localisation with Convolutional Neural Networks ( CNNs ) .",0
13978,"We first compare and analyse different loss functions including L2 , L1 and smooth L1 .",0
13979,"The analysis of these loss functions suggests that , for the training of a CNN - based localisation model , more attention should be paid to small and medium range errors .",0
13980,"To this end , we design a piece - wise loss function .",0
13981,"The new loss amplifies the impact of errors from the interval ( - w , w) by switching from L1 loss to a modified logarithm function .",0
13982,"To address the problem of under-representation of samples with large out - of - plane head rotations in the training set , we propose a simple but effective boosting strategy , referred to as pose - based data balancing .",0
13983,"In particular , we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation , bounding box translation and other data augmentation approaches .",0
13984,"Last , the proposed approach is extended to create a two - stage framework for robust facial landmark localisation .",0
13985,"The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function , and prove the superiority of the proposed method over the state - of - the - art approaches .",0
13986,Introduction,0
13987,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",1
13988,"A facial landmark usually has specific semantic meaning , e.g. nose tip or eye centre , which provides rich geometric information for other face analysis tasks such as face recognition , emotion estimation and 3D face reconstruction .",0
13989,"Thanks to the successive developments in this area of research during the past decades , we are able to perform very accurate facial landmark localisation in constrained scenarios , even using traditional approaches such as Active Shape Model ( ASM ) , Active Appearance Model ( AAM ) and Constrained Local Model ( CLM ) .",0
13990,"The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations , e.g. in pose , expression , illumination , image blurring and occlusion .",0
13991,"To this end , cascaded - regression - based approaches have been widely used , in which a set of weak regressors are cascaded to form a strong regressor .",0
13992,"However , the capability of cascaded regression is nearly saturated due to its shallow structure .",0
13993,"After cascading more than four or five weak regressors , the performance of cascaded regression is hard to improve further .",0
13994,"More recently , deep neural networks have been put forward as a more powerful alternative in a wide range of computer vision and pattern recognition tasks , including facial landmark localisation .",0
13995,"To perform robust facial landmark localisation us - ing deep neural networks , different network types have been explored , such as the Convolutional Neural Network ( CNN ) , Auto - Encoder Network and Recurrent Neural Network ( RNN ) .",0
13996,"In addition , different network architectures have been extensively studied during the recent years along with the development of deep neural networks in other AI applications .",0
13997,"For example , the Fully Convolutional Network ( FCN ) and hourglass network with residual blocks have been found very effective .",0
13998,One crucial aspect of deep learning is to define a loss function leading to better - learnt representation from underlying data .,0
13999,"However , this aspect of the design seems to belittle investigated by the facial landmark localisation community .",0
14000,"To the best of our knowledge , most existing facial landmark localisation approaches using deep learning are based on the L2 loss .",0
14001,"However , the L2 loss function is sensitive to outliers , which has been noted in connection with the bounding box regression problem in the well - known Fast R - CNN algorithm .",0
14002,Rashid et al. also notice this issue and use the smooth L1 loss instead of L2 .,0
14003,"To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .",0
14004,The main contributions of our work include :,0
14005,"presenting a systematic analysis of different loss functions that could be used for regression - based facial landmark localisation with CNNs , which to our best knowledge is the first such study carried out in connection with the landmark localisation problem .",0
14006,"We empirically and theoretically compare L1 , L2 and smooth L1 loss functions and find that L1 and smooth L1 perform much better than the widely used L2 loss .",0
14007,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .",1
14008,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .",1
14009,a two - stage facial landmark localisation framework for performance boosting .,1
14010,The paper is organised as follows .,0
14011,Section 2 presents a brief review of the related literature .,0
14012,The regression - based facial landmarking problem with CNNs is formulated in Section 3 .,0
14013,The properties of common loss functions ( L1 and L2 ) are discussed in Section 4 which also motivate the introduction of the novel Wing loss function .,0
14014,The pose - based data balancing strategy is the subject of Section 5 .,0
14015,The twostage localisation framework is proposed in Section 6 .,0
14016,The advocated approach is validated experimentally in Section 7 and the paper is drawn to conclusion in Section 8 .,0
14017,Related work,0
14018,Network Architectures :,0
14019,Most deep - learning - based facial landmark localisation approaches are regression - based .,0
14020,"For such a task , the most straightforward way is to use a CNN model with regression output layers .",0
14021,The input fora regression CNN is usually an image patch enclosing the whole face region and the output is a vector consisting of the 2D coordinates of facial landmarks .,0
14022,"Besides the classical CNN architecture , newly developed CNN systems have also been used for facial landmark localisation and shown promising results , e.g. FCN and the hourglass network .",0
14023,"Different from traditional CNN - based approaches , FCN and hourglass network output a heat map for each landmark .",0
14024,These heat maps are of the same size as the input image .,0
14025,The value of a pixel in a heat map indicates the probability that its location is the predicted position of the corresponding landmark .,0
14026,"To reduce false alarms of a generated 2D sparse heat map , Wu et al. propose a distance - aware softmax function that facilitates the training of their dual - path network .",0
14027,"Thanks to the extensive studies of different deep neural networks and their use cases in unconstrained facial landmark localisation , the development of the area has been greatly promoted .",0
14028,"However , the current research lacks a systematic analysis on the use of different loss functions .",0
14029,"In this paper , we close this gap and design a new loss function for CNN - based facial landmark localisation .",0
14030,Dealing with Pose Variations :,0
14031,Extreme pose variations bring many difficulties to unconstrained facial landmark localisation .,0
14032,"To mitigate this issue , different strategies have been explored .",0
14033,The first one is to use multiview models .,0
14034,"There is along history of the use of multiview models in landmark localisation , from the earlier studies on ASM and AAM to recent work on cascaded - regression - based and deep - learningbased approaches .",0
14035,"For example , Feng et al. train multiview cascaded regression models using a fuzzy membership weighting strategy , which , interestingly , outperforms even some deep - learning - based approaches .",0
14036,"The second strategy , which has become very popular in recent years , is to use 3 D face models .",0
14037,"By recovering the 3D shape and estimating the pose of a given input 2 D face image , the issue of extreme pose variations can be alleviated to a great extent .",0
14038,"In addition , 3 D face models have also been widely used to synthesise additional 2 D face images with pose variations for the training of a pose-invariant system .",0
14039,"Last , multi-task learning has been adopted to address the difficulties posed by image degradation , including pose variations .",0
14040,"For example , face attribute estimation , pose estimation or 3D face reconstruction can jointly be trained with facial landmark localisation .",0
14041,The collaboration of different tasks in a multi-task learning framework can boost the performance of individual sub - tasks .,0
14042,"Different from these approaches , we treat the challenge as a training data imbalance problem and advocate a posebased data balancing strategy to address this issue .",0
14043,Cascaded,0
14044,Networks :,0
14045,"In the light of the coarse - to - fine cascaded regression framework , multiple networks can be stacked to form a stronger network to boost the performance .",0
14046,"To this end , shape - or landmark - related features should be used to satisfy the training of multiple networks in cascade .",0
14047,"However , a CNN using a global face image as input can not meet this requirement .",0
14048,"To address this issue , one solution is to extract CNN features from local patches around facial landmarks .",0
14049,"This idea is advocated , for example , by Trigeorgis et al. who use the Recurrent Neural Network ( RNN ) for end - to - end model training .",0
14050,"As an alternative , we can train a network based on the global image patch for rough facial landmark localisation .",0
14051,"Then , for each landmark or a composition of multiple landmarks in a specific region of the face , a network is trained to perform fine - grained landmark prediction .",0
14052,"For another example , Yu et al. propose to inject local deformations to the estimated facial landmarks of the first network using thin - plate spline transformations .",0
14053,"In this paper , we use a two - stage CNN - based landmark localisation framework .",0
14054,The first CNN is a very simple one that can perform rough facial landmark localisation very quickly .,0
14055,The aim of the first network is to mitigate the difficulties posed by inaccurate face detection and in - plane head rotations .,0
14056,Then the second CNN is used to perform finegrained landmark localisation .,0
14057,CNN - based facial landmark localisation,0
14058,The target of CNN - based facial landmark localisation is to find a nonlinear mapping :,0
14059,that outputs a shape vector s ?,0
14060,R 2 L fora given input colour image I ? R HW 3 .,0
14061,The input image is usually cropped using the bounding box output by a face detector .,0
14062,"The shape vector is in the form of s = [ x 1 , ... , x L , y 1 , ... , y L ] T , where L is the number of pre-defined 2 D facial landmarks and ( x l , y l ) are the coordinates of the lth landmark .",0
14063,"To obtain this mapping , first , we have to define the architecture of a multi - layer neural network with randomly initialised parameters .",0
14064,"In fact , the mapping ? = (?",0
14065,1 ...,0
14066,", the target of CNN training is to find a ?",0
14067,that minimises : where loss ( ) is a pre-defined loss function that measures the difference between a predicted shape vector and its ground truth .,0
14068,"In such a case , the CNN is used as a regression model learned in a supervised manner .",0
14069,"To optimise the above objective function , optimisation algorithms such as Stochastic Gradient Descent ( SGD ) can be used .",0
14070,"To empirically analyse different loss functions , we use a simple CNN architecture , in the following termed CNN - 6 , for facial landmark localisation , to achieve high speed in model training and testing .",0
14071,The input for this network is a 64643 colour image and the output is a vector of 2L real numbers for the 2D coordinates of L landmarks .,0
14072,"As shown in , our CNN - 6 has five 3 3 convolutional layers , a fully connected layer and an output layer .",0
14073,"After each convolutional and fully connected layer , a standard Relu layer is used for nonlinear activation .",0
14074,A Max pooling after each convolutional layer is used to downsize the feature map to half of the size .,0
14075,"To boost the performance , more powerful network architectures can be used , such as our two - stage landmark localisation framework presented in Section 6 and the recently proposed ResNet architecture .",0
14076,We will report the results of these advanced network architectures in Section 7 .,0
14077,"It should be highlighted that , to the best of our knowledge , this is the first time that such a deep residual network , i.e. ResNet - 50 , is used for facial landmark localisation .",0
14078,Wing loss,0
14079,The design of a proper loss function is crucial for CNNbased facial landmark localisation .,0
14080,"However , mainly the L2 loss has been used in existing deep - neural - network - based facial landmarking systems .",0
14081,"In this paper , to the best of our knowledge , we are the first to analyse different loss functions for CNN - based facial landmark localisation and demonstrate that the L1 and smooth L1 loss functions perform much better than the L2 loss .",0
14082,"Motivated by our analysis , we propose a new loss function , namely Wing loss , which further improves the accuracy of CNN - based facial landmark localisation systems .",0
14083,Analysis of different loss functions,0
14084,"Given a training image I and a network ? , we can predict the facial landmarks as a vector s = ? ( I ) .",0
14085,The loss is defined as :,0
14086,where sis the ground - truth shape vector of the facial landmarks .,0
14087,"For f ( x ) in the above equation , L1 loss uses L1 ( x ) = | x | and L2 loss uses L2 ( x ) = 1 2 x 2 .",0
14088,The smooth L1 loss function is piecewise - defined as :,0
14089,which is quadratic for small values of | x | and linear for large values .,0
14090,"More specifically , smooth L1 uses L2 ( x ) for x ? ( ? 1 , 1 ) and shifted L1 ( x ) elsewhere .",0
14091,depicts the plots of these loss functions .,0
14092,It should be noted that the smooth L1 loss is a special case of the Huber loss .,0
14093,The loss function that has widely been used in facial landmark localisation is the L2 loss function .,0
14094,"However , it is wellknown that the L2 loss is sensitive to outliers .",0
14095,"This is the main reason why , e.g. , Girshick and Rashid et al.",0
14096,use the smooth L1 loss function for their localisation tasks .,0
14097,"For evaluation , the AFLW - Full protocol has been used 1 .",0
14098,This protocol consists of 20 k training images and 4386 test images .,0
14099,Each image has 19 facial landmarks .,0
14100,We use three state - of - the - art algorithms as our baseline for comparison .,0
14101,"The first one is the Cascaded Compositional Learning algorithm ( CCL ) , which is a multi-view cascaded regression model based on random forests .",0
14102,The second one is the Two - stage Re-initialisation Deep Regression Network ( TR - DRN ) .,0
14103,"The last baseline algorithm is a multi-view approach based on cascaded shape regression , namely DAC - CSR .",0
14104,We train the CNN - 6 network on AFLW using three different loss functions and report the results in .,0
14105,"The L2 loss function , which has been widely used for facial landmark localisation , performs well .",0
14106,The result is better than CCL in terms accuracy but worse than DAC - CSR and TR - DRN .,0
14107,"Surprisingly , when we use L1 or smooth L1 for The AFLW dataset is introduced in Section 7.2.1 . 2.7210 ?2 DAC- CSR ( CVPR2017 ) 2.2710 ?2 TR - DRN ( CVPR 2017 ) 2",0
14108,"the CNN - 6 training , the performance in terms of accuracy improves significantly and outperforms all the state - of - theart baseline approaches , despite the CNN network 's simplicity .",0
14109,The proposed Wing loss,0
14110,We compare the results obtained on the AFLW dataset using the simple CNN - 6 network in by plotting the Cumulative Error Distribution ( CED ) curves .,0
14111,We can see that all the loss functions analysed in the last section perform well for large errors .,0
14112,This indicates that the training of a neural network should pay more attention to the samples with small or medium range errors .,0
14113,"To achieve this target , we propose a new loss function , namely Wing loss , for CNN - based facial landmark localisation .",0
14114,"In order to motivate the new loss function , we provide an intuitive analysis of the properties of the L1 and L2 loss functions .",0
14115,"The magnitude of the gradients of these two functions is 1 and | x | respectively , and the magnitude of the corresponding optimal step sizes should be | x | and 1 .",0
14116,Finding the minimum in either case is straightforward .,0
14117,"However , the situation becomes more complicated when we try to optimise simultaneously the location of multiple points , as in our problem of facial landmark localisation for - mulated in Eq. ( 3 ) .",0
14118,In both cases the update towards the solution will be dominated by larger errors .,0
14119,"In the case of L1 , the magnitude of the gradient is the same for all the points , but the step size is disproportionately influenced by larger errors .",0
14120,"For L2 , the step size is the same but the gradient will be dominated by large errors .",0
14121,Thus in both cases it is hard to correct relatively small displacements .,0
14122,"The influence of small errors can be enhanced by an alternative loss function , such as ln x .",0
14123,"It s gradient , given by 1 / x , increases as we approach zero error .",0
14124,The magnitude of the optimal step size is x 2 .,0
14125,"When compounding the contributions from multiple points , the gradient will be dominated by small errors , but the step size by larger errors .",0
14126,This restores the balance between the influence of errors of different sizes .,0
14127,"However , to prevent making large update steps in a potentially wrong direction , it is important not to overcompensate the influence of small localisation errors .",0
14128,This can be achieved by opting fora log function with a positive offset .,0
14129,This type of loss function shape is appropriate for dealing with relatively small localisation errors .,0
14130,"However , in facial landmark detection of in - the - wild faces we maybe dealing with extreme poses where initially the localisation errors can be very large .",0
14131,In such a regime the loss function should promote a fast recovery from these large errors .,0
14132,This suggests that the loss function should behave more like L1 or L2 .,0
14133,"As L2 is sensitive to outliers , we favour L1.",0
14134,"The above intuitive argument points to a loss function which for small errors should behave as a log function with an offset , and for larger errors as L1 .",0
14135,Such a composite loss function can be defined as :,0
14136,"where the non-negative w sets the range of the nonlinear part to ( ? w , w ) , limits the curvature of the nonlinear region and C = w ? w ln ( 1 + w/ ) is a constant that smoothly links the piecewise - defined linear and nonlinear parts .",0
14137,Note that we should not set to a very small value because it makes the training of a network very unstable and causes the exploding gradient problem for very small errors .,0
14138,"In fact , the nonlinear part of our Wing loss function just simply takes the curve of ln ( x ) between [ / w , 1 + / w ) and scales it along both the X- axis and Y-axis by a factor of w .",0
14139,"Also , we apply translation along the Y-axis to allow wing ( 0 ) = 0 and to impose continuity on the loss function .",0
14140,"From , we can see that our Wing loss outperforms L2 , L1 and smooth L1 in terms of accuracy .",0
14141,"The Wing loss further reduces the average normalised error from 2 10 ?2 to 1.88 10 ? 2 , which is 6 % lower than the best result obtained in the last section ) and 13 % lower than the best state - of - the - art deep - learning baseline approach , i.e.",0
14142,Pose - based data balancing,0
14143,Extreme pose variations are very challenging for robust facial landmark localisation in the wild .,0
14144,"To mitigate this issue , we propose a simple but very effective Pose - based Data Balancing ( PDB ) strategy .",0
14145,"We argue that the difficulty for accurately localising faces with large poses is mainly due to data imbalance , which is a well - known problem in many computer vision applications .",0
14146,"For example , given a training dataset , most samples in it are likely to be nearfrontal faces .",0
14147,The neural network trained on such a dataset is dominated by frontal faces .,0
14148,By over- fitting to the frontal pose it can not adapt well to faces with large poses .,0
14149,"In fact , the difficulty of training and testing on merely frontal faces should be similar to that on profile faces .",0
14150,This is the main reason why a view - based face analysis algorithm usually works well for pose - varying faces .,0
14151,"As an evidence , even the classical view - based Active Appearance Model can localise faces with large poses very well ( up to 90 in yaw ) .",0
14152,"To perform PDB , we first align all the training shapes to a reference shape using Procrustes Analysis , with the mean shape as the reference shape .",0
14153,Then we apply PCA to the aligned training shapes and project the original shapes to the one dimensional space defined by the shape eigenvector ( pose space ) controlling pose variations .,0
14154,"The distribution of projection coefficient of the training samples is represented by a histogram with K bins , plotted in .",0
14155,"With this histogram , we balance the training data by duplicating the .",0
14156,"A comparison of different loss functions using our PDB strategy and two - stage landmark localisation framework , measured in terms of the average normalised error ( 10 ?2 ) on AFLW .",0
14157,The method CNN - 6/7 indicates the proposed two - stage localisation framework using CNN - 6 as the first network and CNN - 7 as the second network ( Section 6 ) .,0
14158,"For CNN - 7 , the learning rate is reduced from 1 10 ? 6 to 1 10 ?8 for L2 , and from 1 10 samples falling into the bins of lower occupancy .",0
14159,"We modify each duplicated sample by performing random image rotation , bounding box perturbation and other data augmentation approaches introduced in Section 7.1 .",0
14160,"To deal with in - plane rotations , we use a two - stage facial landmark localisation framework that will be introduced in Section 6 .",0
14161,The results obtained by the CNN - 6 network with PDB are shown in .,0
14162,It should be noted that PDB improves the performance of CNN - 6 on the AFLW dataset for all different types of loss functions .,0
14163,Two - stage landmark localisation,0
14164,"Besides the out - of - plane head rotations , the accuracy of a facial landmark localisation algorithm can be degraded by other factors , such as in - plane head rotations and inaccurate bounding boxes output from a poor face detector .",0
14165,"To mitigate this issue , we advocate the use of a two - stage landmark localisation framework .",0
14166,"In the proposed two - stage localisation framework , we use a very simple network , i.e. the CNN - 6 network with 64 64 3 input images , as the first network .",0
14167,"The CNN - 6 network is very fast ( 400 fps on an NVIDIA GeForce GTX Titan X Pascal ) , hence it will not slowdown the speed of our facial landmark localisation algorithm too much .",0
14168,The landmarks output by the CNN - 6 network are used to refine the input image for the second network by removing the in - plane head rotation and correcting the bounding box .,0
14169,"Also , the input image resolution for the second network is increased for fine - grained landmark localisation from 64 64 3 to 128 128 3 , with the addition of one set of convolutional , Relu and Max pooling layers .",0
14170,"Hence , the term ' CNN - 7 ' is used to denote the second network .",0
14171,The CNN - 7 network has a similar architecture to the CNN - 6 network in .,0
14172,The difference is that CNN - 7 has 6 convolutional layers which resize the feature map from 1281283 to 2 2 512 .,0
14173,"In addition , for the first convolutional layer in CNN - 7 , we double the number of 3 3 kernels from 32 to 64 .",0
14174,We use the term ' CNN - 6 / 7 ' for our two - stage facial landmark localisation framework and compare it with the CNN - 6 network in .,0
14175,"As reported in the table , the use of our two - stage landmark localisation framework further improves the accuracy , regardless of the type of loss function used .",0
14176,Experimental results,0
14177,"In this section , we evaluate our method on the Annotated Facial Landmarks in the Wild ( AFLW ) dataset and the 300 Faces in the Wild ( 300W ) dataset .",0
14178,We first introduce our implementation details and experimental settings .,0
14179,Then we compare our algorithm with state - of - the - art approaches on AFLW and 300W .,0
14180,"Last , we analyse the performance of different networks in terms of both accuracy and speed .",0
14181,Implementation details,0
14182,"In our experiments , we used Matlab 2017a and the Mat - ConvNet toolbox 2 .",1
14183,"The training and testing of our networks were conducted on a server running Ubuntu 16.04 with 2 Intel Xeon E5-2667 v4 CPU , 256 GB RAM and 4 NVIDIA GeForce GTX Titan X ( Pascal ) cards .",1
14184,Note that we only use one GPU card for measuring the run time .,1
14185,"We set the weight decay to 5 10 ? 4 , momentum to 0.9 and batch size to 8 for network training .",1
14186,Each model was trained for 120 k iterations .,1
14187,"We did not use any other advanced techniques in our CNN - 6 and CNN - 7 networks , such as batch normalisation , dropout or residual blocks .",0
14188,"The standard ReLu function was used for nonlinear activation , and Max pooling with the stride of 2 was used to downsize feature maps .",1
14189,"For the convolutional layer , we used 3 3 kernels with the stride of 1 .",1
14190,"All our networks , except ResNet - 50 , were trained from scratch without any pre-training on any other dataset .",0
14191,"For the proposed PDB strategy , the number of bins K was set to 17 for AFLW and 9 for 300W .",1
14192,"For CNN - 6 , the input image size is 64 64 3 . We reduced the learning rate from 3 10 ? 6 to 3 10 ?8 for the L2 loss , and from 3 10 ?5 to 3 10 ? 7 for the other loss functions .",1
14193,"The parameters of the Wing loss were set tow = 10 and = 2 . For CNN - 7 , the input image size is 128 128 3 . We reduced the learning rate from 1 10 ? 6 to 1 10 ?8 for the L2 loss , and from 1 10 ? 5 to 1 10 ? 7 for the other loss functions .",1
14194,The parameters of the Wing loss were set tow = 15 and = 3 .,0
14195,"To perform data augmentation , we randomly rotated each training image between [ ? 30 , 30 ] degrees for CNN - 6 and between [ ? 10 , 10 ] degrees for CNN - 7 .",1
14196,"In addition , we randomly flipped each training image with the probability of 50 % .",0
14197,"For bounding box perturbation , we applied random translations to the upper-left and bottom - right corners of the face bounding box within 5 % of the bounding .",1
14198,A comparison of the CED curves on the AFLW dataset .,0
14199,"We compare our method with a set of state - of - the - art approaches , including SDM , ERT , RCPR , CFSS , LBF , GRF , CCL , DAC - CSR and TR - DRN. box size .",1
14200,"Last , we randomly injected Gaussian blur (? = 1 ) to each training image with the probability of 50 % .",1
14201,Evaluation Metric :,0
14202,"For evaluation of a facial landmark localisation algorithm , we adopted the widely used Normalised Mean Error ( NME ) .",0
14203,"For the AFLW dataset using the AFLW - Full protocol , the given face bounding box of a test sample is a square .",0
14204,"To calculate the NME of a test sample , the AFLW - Full protocol uses the width ( or height ) of the face bounding box as the normalisation term .",0
14205,"For the 300 W dataset , we followed the protocol used in .",0
14206,"This protocol uses the inter-pupil distance as the normalisation term , which is different from the standard 300 W protocol that uses the outer eye corner distance .",0
14207,Comparison with state of the art 7.2.1 AFLW,1
14208,"We first evaluated our algorithm on the AFLW dataset , using the AFLW - Full protocol .",0
14209,AFLW is a very challenging dataset that has been widely used for benchmarking facial landmark localisation algorithms .,0
14210,"The images in AFLW consist of a wide range of pose variations in yaw ( from ?90 to 90 ) , as shown in .",0
14211,"The AFLW - Full protocol contains 20,000 training and 4,386 test images , and each image has 19 manually annotated facial landmarks .",0
14212,We compare the proposed method with state - of - the - art approaches in terms of accuracy in using the Cumulative Error Distribution ( CED ) curve .,0
14213,"In our experiments , we used our two - stage facial landmark localisation framework by stacking the CNN - 6 and CNN - 7 networks ( denoted by CNN - 6 / 7 ) , as introduced in Section 6 .",0
14214,"In addition , the proposed Pose - based Data Balancing ( PDB ) strategy was adopted , as presented in Section 5 .",0
14215,We report the results of the proposed approach using four different loss functions ..,0
14216,A comparison of the proposed approach with the stateof - the - art approaches on the 300W dataset in terms of the NME averaged overall the test samples .,0
14217,We follow the protocol used in .,0
14218,"Note that the error is normalised by the inter-pupil distance , rather than the outer eye corner distance .",0
14219,"As shown in , our CNN - 6/7 network outperforms all the other approaches even when trained with the commonly used L2 loss function ( magenta solid line ) .",1
14220,This validates the effectiveness of the proposed two - stage localisation framework and the PDB strategy .,0
14221,"Second , by simply switching the loss function from L2 to L1 or smooth L1 , the performance of our method has been improved significantly ( red solid and black dashed lines ) .",1
14222,"Last , the use of our newly proposed Wing loss function further improves the accuracy ( black solid line ) .",1
14223,The proportion of test samples ( Y-axis ) associated with a small to medium normalised mean error ( X-axis ) is increased .,0
14224,300 W,1
14225,"The 300W dataset is a collection of multiple face datasets , including LFPW , HELEN , AFW and XM2VTS .",0
14226,The face images involved in 300W have been semi-automatically annotated by 68 facial landmarks .,0
14227,"To perform the evaluation on 300 W , we followed the protocol used in .",0
14228,"The protocol uses the full set of AFW and the training subsets of LFPW and HELEN as the training set , which contains 3148 training samples in total .",0
14229,"The test set of the protocol includes the test subsets of LFPW and HELEN , as well as 135 IBUG face images newly collected by the managers of the 300W dataset .",0
14230,The final size of the test set is 689 .,0
14231,"The test set is further divided into two subsets for evaluation , i.e. the common and challenging subsets .",0
14232,The common subset has 554 face images from the LFPW and HELEN test subsets and the challeng - ing subset constitutes the 135 IBUG face images .,0
14233,"Similar to the experiments conducted on the AFLW dataset , we used the two - stage localisation framework with our PDB strategy .",0
14234,The results obtained by our approach with different loss functions are reported in .,0
14235,"As shown in , our two - stage landmark localisation framework with the PDB strategy and the newly proposed Wing loss function outperforms all the other stateof - the - art algorithms on the 300 W dataset inaccuracy .",1
14236,The error has been reduced by almost 20 % as compared to the current best result reported by the RAR algorithm .,1
14237,Run time and network architectures,0
14238,"Facial landmark localisation has been widely used in many real - time practical applications , hence the speed together with accuracy of an algorithm is crucial for the deployment of the algorithm in commercial use cases .",0
14239,"To analyse the performance of our Wing loss on more advanced network architectures , we evaluated ResNet for the task of landmark localisation on AFLW and 300W .",0
14240,We used the ResNet - 50 model that was pre-trained on the Image Net ILSVRC classification problem 3 .,0
14241,We fine - tuned the model on the training sets of AFLW and 300W separately for landmark localisation .,0
14242,The input for ResNet is a 224 224 3 colour image .,0
14243,"It should be highlighted that , to our best knowledge , this is the first time that such a deep network has been used for facial landmark localisation .",0
14244,"For both AFLW and 300 W , by replacing the CNN - 6/7 network with ResNet - 50 , the performance has been further improved by around 10 % , as shown in .",0
14245,"However , this performance boosting comes at the cost of much slower training and inference of ResNet compared to CNN - 6/7 .",0
14246,"To validate the effectiveness of our Wing loss for large capacity networks , we also conducted experiments using ResNet - 50 with different loss functions on AFLW .",0
14247,The results are reported in 6 .,0
14248,"The results further demonstrate the superiority of the proposed Wing loss over other loss functions for large capacity networks , e.g. ResNet - 50 .",0
14249,"Last , we evaluated the speed of different networks on the 300W dataset with 68 landmarks for both GPU and CPU 3 http://www.vlfeat.org/matconvnet/pretrained/ devices .",0
14250,The results are reported in .,0
14251,"According to the table , our simple CNN - 6/7 network is roughly an order of magnitude faster than ResNet - 50 at the compromise of 10 % performance difference inaccuracy .",0
14252,"Also , our CNN - 6 / 7 model is much faster than most existing DNNbased facial landmark localisation approaches such as TR - DRN .",0
14253,The speed of TR - DRN is 83 fps on an NVIDIA GeForce GTX Titan X card .,0
14254,"Even with a powerful GPU card , it is hard to achieve video rate ( 60 fps ) with ResNet - 50 .",0
14255,"It should be noted that our CNN - 6 / 7 still outperforms the state - of - the - art approaches by a significant margin while running at 170 fps on a GPU card , as shown in .",0
14256,Conclusion,0
14257,"In this paper , we analysed different loss functions that can be used for the task of regression - based facial landmark localisation .",0
14258,We found that L1 and smooth L1 loss functions perform much better inaccuracy than the L2 loss function .,0
14259,"Motivated by our analysis of these loss functions , we proposed a new , Wing loss performance measure .",0
14260,The key idea of the Wing loss criterion is to increase the contribution of the samples with small and medium size errors to the training of the regression network .,0
14261,"To prove the effectiveness of the proposed Wing loss function , extensive experiments have been conducted using several CNN network architectures .",0
14262,"Furthermore , a pose - based data balancing strategy and a two - stage landmark localisation framework were advocated to improve the accuracy of CNN - based facial landmark localisation further .",0
14263,"By evaluating our algorithm on multiple well - known benchmarking datasets , we demonstrated the merits of the proposed approach .",0
14264,It should be emphasised that the proposed Wing loss is relevant to other regression - based computer vision tasks using convolutional neural networks .,0
14265,"However , being constrained by the space limitations , we leave the discussion of its extended use to future reports .",0
14266,title,0
14267,Nonlinear 3D Face Morphable Model,0
14268,abstract,0
14269,"As a classic statistical model of 3D facial shape and texture , 3D Morphable Model ( 3 DMM ) is widely used in facial analysis , e.g. , model fitting , image synthesis .",0
14270,"Conventional 3 DMM is learned from a set of well - controlled 2 D face images with associated 3 D face scans , and represented by two sets of PCA basis functions .",0
14271,"Due to the type and amount of training data , as well as the linear bases , the representation power of 3 DMM can be limited .",0
14272,"To address these problems , this paper proposes an innovative framework to learn a nonlinear 3 DMM model from a large set of unconstrained face images , without collecting 3 D face scans .",0
14273,"Specifically , given a face image as input , a network encoder estimates the projection , shape and texture parameters .",0
14274,"Two decoders serve as the nonlinear 3 DMM to map from the shape and texture parameters to the 3D shape and texture , respectively .",0
14275,"With the projection parameter , 3D shape , and texture , a novel analytically - differentiable rendering layer is designed to reconstruct the original input face .",0
14276,The entire network is end - to - end trainable with only weak supervision .,0
14277,"We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction .",1
14278,1,0
14279,Introduction,0
14280,3D Morphable Model ( 3DMM ) is a statistical model of 3 D facial shape and texture in a space where there are explicit correspondences .,0
14281,"The morphable model framework provides two key benefits : first , a point - to - point correspondence between the reconstruction and all other models , enabling morphing , and second , modeling underlying transformations between types of faces ( male to female , neutral to smile , etc . ) .",0
14282,"3 DMM has been widely applied in numerous areas , such as computer vision , graphics , human behavioral analysis and craniofacial surgery .",0
14283,"3 DMM is learnt through supervision by performing dimension reduction , normally Principal Component Anal - Project page : http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html shape / texture , which are trained with 3 D face scans and associated controlled 2D images .",0
14284,We propose a nonlinear 3 DMM to model shape / texture via deep neural networks ( DNNs ) .,0
14285,"It can be trained from in - the - wild face images without 3 D scans , and also better reconstructs the original images due to the inherent nonlinearity .",0
14286,"ysis ( PCA ) , on a training set of face images / scans .",0
14287,"To model highly variable 3 D face shapes , a large amount of high - quality 3 D face scans is required .",0
14288,"However , this requirement is expensive to fulfill .",0
14289,The first 3 DMM was built from scans of 200 subjects with a similar ethnicity / age group .,0
14290,"They were also captured in well - controlled conditions , with only neutral expressions .",0
14291,"Hence , it is fragile to large variances in the face identity .",0
14292,The widely used Basel Face Model ( BFM ) is also built with only 200 subjects in neutral expressions .,0
14293,Lack of expression can be compensated using expression bases from FaceWarehouse or BD - 3FE .,0
14294,"After more than a decade , almost all models useless than 300 training scans .",0
14295,Such a small training set is far from adequate to describe the full variability of human faces .,0
14296,"Only recently , Booth et al. spent a significant effort to build 3 DMM from scans of ? 10 , 000 subjects .",0
14297,"Second , the texture model of 3 DMM is normally built with a small number of 2 D face images co-captured with 3D scans , under well - controlled conditions .",0
14298,"Therefore , such a model is only learnt to represent the facial texture in similar conditions , rather than in - the - wild environments .",0
14299,This substantially limits the application scenarios of 3DMM .,0
14300,"Finally , the representation power of 3 DMM is limited by not only the size of training set but also its formulation .",0
14301,The facial variations are nonlinear in nature .,0
14302,"E.g. , the variations in different facial expressions or poses are nonlinear , which violates the linear assumption of PCA - based models .",0
14303,"Thus , a PCA model is unable to interpret facial variations well .",0
14304,"Given the barrier of 3 DMM in its data , supervision and linear bases , this paper aims to revolutionize the paradigm of learning 3 DMM by answering a fundamental question :",0
14305,"Whether and how can we learn a nonlinear 3D Morphable Model of face shape and texture from a set of unconstrained 2 D face images , without collecting 3 D face scans ?",0
14306,"If the answer were yes , this would be in sharp contrast to the conventional 3 DMM approach , and remedy all aforementioned limitations .",0
14307,"Fortunately , we have developed approaches that offer positive answers to this question .",0
14308,"Therefore , the core of this paper is regarding how to learn this new 3 DMM , what is the representation power of the model , and what is the benefit of the model to facial analysis .",0
14309,"As shown in , starting with an observation that the linear 3 DMM formulation is equivalent to a single layer network , using a deep network architecture naturally increases the model capacity .",0
14310,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .",1
14311,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",1
14312,Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,1
14313,These two decoders are essentially the nonlinear 3 DMM .,0
14314,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .",1
14315,"The encoder takes a 2 D face image as input and generates the shape and texture parameters , from which two decoders estimate the 3D face and texture .",1
14316,"The 3 D face and texture would perfectly reconstruct the input face , if the fitting algorithm and 3 DMM are well learnt .",1
14317,"Therefore , we design a differentiable rendering layer to generate a reconstructed face by fusing the 3D face , texture , and the camera projection parameters estimated by the encoder .",1
14318,"Finally , the endto - end learning scheme is constructed where the encoder and two decoders are learnt jointly to minimize the difference between the reconstructed face and the input face .",1
14319,Jointly learning the 3 DMM and the model fitting encoder allows us to leverage the large collection of unconstrained 2D images without relying on 3D scans .,1
14320,We show significantly improved shape and texture representation power over the linear 3 DMM .,1
14321,"Consequently , this also benefits other tasks such as 2 D face alignment and 3D reconstruction .",0
14322,"In this paper , we make the following contributions :",0
14323,1 ) We learn a nonlinear 3 DMM model that has greater representation power than its traditional linear counterpart .,0
14324,"2 ) We jointly learn the model and the model fitting algorithm via weak supervision , by leveraging a large collection of 2D images without 3D scans .",0
14325,The novel rendering layer enables the end - to - end training .,0
14326,3 ) The new 3 DMM further improves performance in related tasks : face alignment and face reconstruction .,0
14327,Prior Work,0
14328,Linear 3 DMM .,0
14329,"Since the original work by Blanz and Vetter , there has been a large amount of effort trying to improve 3 DMM modeling mechanism .",0
14330,Paysan et al. use a Nonrigid Iterative Closest Point to directly align 3D scans as an alternative to the UV space alignment method in .,0
14331,Vlasic et al.,0
14332,use a multilinear model to model the combined effect of identity and expression variation on the facial shape .,0
14333,"Later , Bolkart and Wuhrer show how such a multilinear model can be estimated directly from the 3D scans using a joint optimization over the model parameters and groupwise registration of 3D scans .",0
14334,Improving Linear 3 DMM .,0
14335,"With PCA bases , the statistical distribution underlying 3 DMM is Gaussian .",0
14336,Koppen et al. argue that single - mode Gaussian ca n't represent real - world distribution .,0
14337,"They introduce the Gaussian Mixture 3 DMM that models the global population as a mixture of Gaussian subpopulations , each with its own mean , but shared covariance .",0
14338,Booth el al.,0
14339,aim to improve texture of 3 DMM to go beyond controlled settings by learning inthe -wild feature - based texture model .,0
14340,"However , both works are still based on statistical PCA bases .",0
14341,Duong et al.,0
14342,address the problem of linearity in face modeling by using Deep Boltzmann Machines .,0
14343,"However , they only work with 2 D face and sparse landmarks ; and hence can not handle faces with large - pose variations or occlusion well .",0
14344,2D Face Alignment .,0
14345,2D,0
14346,Face Alignment can be cast as a regression problem where 2D landmark locations are regressed directly .,0
14347,"For large - pose or occluded faces , strong priors of 3 DMM face shape have been shown to be beneficial .",0
14348,"Hence , there is increasing attention in conducting face alignment by fitting a 3 D face model to a single 2D image .",0
14349,"Among the prior works , iterative approaches with cascades of regressors tend to be preferred .",0
14350,"At each cascade , it can be a single or even two regressors .",0
14351,"In contrast to aforementioned works that use a fixed 3 DMM model , our model and model fitting are learned jointly .",0
14352,"This results in a more powerful model : a single - pass encoder , which is learnt jointly with the model , achieves state - of - the - art face alignment performance on AFLW2000 benchmark dataset .",0
14353,3D Face Reconstruction .,0
14354,3 DMM also demonstrates its strength in face reconstruction .,0
14355,"Since with a single image , present information about the surface is limited ; 3 D face re - construction must rely on prior knowledge like 3 DMM .",0
14356,"Besides 3 DMM fitting methods , recently , Richardson et al. design a refinement network that adds facial details on top of the 3 DMM - based geometry .",0
14357,"However , this approach can only learn 2.5 D depth map , which loses the correspondence property of 3 DMM .",0
14358,The recent work of Tewari et al. reconstruct a 3 D face by an elegant encoder - decoder network .,0
14359,"While their ability to decompose lighting with reflectance is satisfactory , our work has a different objective of learning a nonlinear 3 DMM .",0
14360,Proposed Method,0
14361,Conventional Linear 3DMM,0
14362,"The 3D Morphable Model ( 3DMM ) and its 2 D counterpart , Active Appearance Model , provide parametric models for synthesizing faces , where faces are modeled using two components : shape and texture .",0
14363,"In , Blanz et al. propose to describe the 3 D face space with PCA :",0
14364,where S ?,0
14365,"R 3Q is a 3 D face with Q vertices , S ?",0
14366,"R 3Q is the mean shape , ? ?",0
14367,R l S is the shape parameter corresponding to a 3D shape bases A .,0
14368,"The shape bases can be further split into A = [ A id , A exp ] , where A id is trained from 3 D scans with neutral expression , and A exp is from the offsets between expression and neutral scans .",0
14369,The texture T ( l ) ?,0
14370,"R 3 Q of the face is defined within the mean shapeS , which describes the R , G , B colors of Q corresponding vertices .",0
14371,T ( l ) is also formulated as a linear combination of texture basis functions :,0
14372,"whereT ( l ) is the mean texture , B is the texture bases , and ? ?",0
14373,R l T is the texture parameter .,0
14374,The 3 DMM can be used to synthesize novel views of the face .,0
14375,"Firstly , a 3 D face is projected onto the image plane with the weak perspective projection model :",0
14376,"where g (? , m ) is the model construction and projection function leading to the 2D positions V of 3 D vertices , f is the scale factor , Pr = 1 0 0 0 1 0 is the orthographic projection matrix , R is the rotation matrix constructed from three rotation angles pitch , yaw , roll , and t 2d is the translation vector .",0
14377,"While the projection matrix M has dimensions 2 4 , it has six degrees of freedom , which is parameterized by a 6 - dim vector m .",0
14378,"Then , the 2D image is rendered using texture and an illumination model as described in .",0
14379,Nonlinear 3DMM,0
14380,"As mentioned in Sec. 1 , the linear 3 DMM has the problems such as requiring 3 D face scans for supervised learning , unable to leverage massive unconstrained face images for learning , and the limited representation power due to the linear bases .",0
14381,We propose to learn a nonlinear 3 DMM model using only large - scale in - the - wild 2 D face images .,0
14382,Problem Formulation,0
14383,"In linear 3 DMM , the factorization of each components ( texture , shape ) can be seen as a matrix multiplication between coefficients and bases .",0
14384,"From a neural network 's perspective , this can be viewed as a shallow network with only one fully connected layer and no activation function .",0
14385,"Naturally , to increase the model 's representative power , the shallow network can be extended to a deep architecture .",0
14386,"In this work , we design a novel learning scheme to learn a deep 3 DMM and its inference ( or fitting ) algorithm .",0
14387,"Specifically , as shown in , we use two deep networks to decode the shape , texture parameters into the 3D facial shape and texture respectively .",0
14388,"To make the framework end - to - end trainable , these parameters are estimated by an encoder network , which is essentially the fitting algorithm of our 3 DMM .",0
14389,"Three deep networks join forces for the ultimate goal of reconstructing the input face image , with the assistance of a geometry - based rendering layer .",0
14390,"Formally , given a set of 2 D face images { I i } N i = 1 , we aim to learn an encoder E : I ? m , f S , f T that estimates the projection parameter m , and shape and texture parameters",0
14391,"shape decoder D S : f S ?S that decodes the shape parameter to a 3D shape S , and a texture decoder D T : f T ?",0
14392,"T that decodes the texture parameter to a realistic texture T ? R U V , with the objective that the rendered image with m , S , and T can approximate the original image well .",0
14393,"Mathematically , the objective function is :",0
14394,"where R ( m , S , T ) is the rendering layer ( Sec. 3.2.3 ) .",0
14395,Shape & Texture,0
14396,Representation,0
14397,"Our shape representation is the same as that of the linear 3 DMM , i.e. , S ?",0
14398,"R 3Q is a set of Q vertices v S = ( x , y , z ) on the face surface .",0
14399,The shape decoder D S is a MLP whose input is the shape parameter f S from E. illustrates three possible texture representations .,0
14400,Texture is defined per vertex in the linear 3 DMM and recent work such as ) .,0
14401,There is a texture intensity value corresponding to each vertex in the face mesh .,0
14402,"Since 3 D vertices are not defined on a 2 D grid , this representation will be parameterized as a vector , which not only loses the spatial relation of vertices , but also prevents it from leveraging the convenience of deploying CNN on 2D imagery .",0
14403,"In contrast , given the rapid progress in image synthesis , it is desirable to choose a 2D image , e.g. , a frontal - view face image in , as a texture representation .",0
14404,"However , frontal faces contain little information of two sides , which would lose much texture information for side - view faces .",0
14405,"In light of these considerations , we use an unwrapped 2D texture as our texture representation ) .",0
14406,"Specifically , each 3 D vertex v S is projected onto the UV space using cylindrical unwarp .",0
14407,"Assuming that the face mesh has the top pointing up they axis , the projection of v S = ( x , y , z )",0
14408,"where ? 1 , ? 2 , ? 1 , ?",0
14409,2 are constant scale and translation scalars to place the unwrapped face into the image boundaries .,0
14410,"Also , the texture decoder D T is a CNN constructed by fractionally - strided convolution layers .",0
14411,In- Network Face Rendering,0
14412,"To reconstruct a face image from the texture T , shape S , and projection parameter m , we define a rendering layer R ( m , S , T ) .",0
14413,This is accomplished in three steps .,0
14414,"Firstly , the texture value of each vertex in S is determined by its predefined location in the 2D texture T. Usually , it involves sub-pixel sampling via a bilinear sampling kernel :",0
14415,"where v T = ( u , v ) is the UV space projection of v S via Eqn .",0
14416,"5 . Secondly , the 3D shape / mesh S is projected to the image plane via Eqn .",0
14417,"3 . Finally , the 3D mesh is then rendered using a Z - buffer renderer , where each pixel is associated with a single triangle of the mesh ,",0
14418,"where ?( g , m , n ) = {v",0
14419,"S } is an operation returning three vertices of the triangle that encloses the pixel ( m , n ) after projection g .",0
14420,"In order to handle occlusions , when a single pixel resides in more than one triangle , the triangle that is closest to the image plane is selected .",0
14421,The value of each pixel is determined by interpolating the intensity of the mesh vertices via barycentric coordinates {?,0
14422,( i ) } 3 i=1 .,0
14423,There are alternative designs to our rendering layer .,0
14424,"If the texture representation is defined per vertex , as in , one may warp the input image I i onto the vertex space of the 3D shape S , whose distance to the per-vertex texture representation can form a reconstruction loss .",0
14425,This design is adopted by the recent work of .,0
14426,"In comparison , our rendered image is defined on a 2 D grid while the alternative is on top of the 3D mesh .",0
14427,"As a result , our rendered image can enjoy the convenience of applying the adversarial loss , which is shown to be critical in improving the quality of synthetic texture .",0
14428,"Another design for rendering layer is image warping based on the spline interpolation , as in .",0
14429,"However , this warping is continuous : every pixel in the input will map to the output .",0
14430,Hence this warping operation fails in the occlusion part .,0
14431,"As a result , Cole et al. limit their scope to only synthesizing frontal faces by warping from normalized faces .",0
14432,2,0
14433,FC ( for m only ) 646 6,0
14434,Network Architecture,0
14435,"We design our E , D T network architecture as in Tab .",0
14436,"1 . Also , D S includes two fully connected layers with 1 , 000 - dim intermediate representation with eLU activation .",0
14437,"The entire network is end - to - end trained to reconstruct the input images , with the loss function :",0
14438,where the reconstruction loss L rec = N i=1 || i ?,0
14439,"I i || 1 enforces the rendered image i to be similar to the input I i , the adversarial loss L adv favors realistic rendering , and the landmark loss LL enforces geometry constraint .",0
14440,Adversarial Loss .,0
14441,"Based on the principal of Generative Adversarial Network ( GAN ) , the adversarial loss is widely used to synthesize photo-realistic images , where the generator and discriminator are trained alternatively .",0
14442,"In our case , networks that generate the rendered image i is the generator .",0
14443,"The discriminator includes a dedicated network DA , which aims to distinguish between the real face image I i and rendered image i .",0
14444,"During the training of the generator , the texture model D T will be updated with the objective that i is being classified as real faces by DA .",0
14445,"Since our face rendering already creates correct global structure of the face image , the global imagebased adversarial loss may not be effective in producing high - quality textures on local facial regions .",0
14446,"Therefore , we employ patchGAN in our discriminator .",0
14447,"Here , DA is a CNN consisting of four 3 3 conv layers with stride of 2 , and number of filters are 32 , 64 , 128 and 1 , respectively .",0
14448,"Finally , one of key reasons we are able to employ adversarial loss is that we are rendering in the 2D image space , rather than the 3D vertices space or unwrapped texture space .",0
14449,This shows the necessity and importance of our rendering layer .,0
14450,Semi-Supervised Pre-Training .,0
14451,"Fully unsupervised training using only the mentioned reconstruction and adversarial loss on the rendered image could lead to a degenerate solution , since the initial estimation is far from ideal to render meaningful images .",0
14452,"Hence , we introduce pre-training loss functions to guide the training in the early iterations .",0
14453,"With face profiling technique ,",0
14454,Zhu et al .,0
14455,"expands the 300W dataset into 122 , 450 images with the fitted 3 DMM shape S and projection parameters m. Given S and m , we create the pseudo groundtruth texture T by referring every pixel in the UV space back to the input image , i.e. , backward of our rendering layer .",0
14456,"With m , S , T , we define our pre-training loss by :",0
14457,where,0
14458,"Due to the pseudo groundtruth , using L 0 may run into the risk that our solution learns to mimic the linear model .",0
14459,"Thus , we switch to the loss of Eqn. 8 after L 0 converges .",0
14460,Sparse Landmark Alignment .,0
14461,"To help D T to better learn the facial shape , the landmark loss can bean auxiliary task .",0
14462,where U ?,0
14463,"R 268 is the manually labeled 2D landmark locations , dis a constant 68 - dim vector storing the indexes of 68 3D vertices corresponding to the labeled 2D landmarks .",0
14464,"Unlike the three losses above , these landmark annotations are "" golden "" groundtruth , and hence LL can be used during the entire training process .",0
14465,"Different from traditional face alignment work where the shape bases are fixed , our work jointly learns the bases functions ( i.e. , the shape decoder D S ) as well .",0
14466,"Minimizing the landmark loss when updating D S only moves a tiny subset of vertices , since our D S is a MLP consisting of fully connected layers .",0
14467,This could lead to unrealistic shapes .,0
14468,"Hence , when optimizing the landmark loss , we fix the decoder D S and only update the encoder .",0
14469,"Note that the estimated groundtruth in L 0 and the landmarks are the only supervision used in our training , due to this our learning is considered as weakly supervised .",0
14470,Experimental Results,0
14471,"The experiments study three aspects of the proposed nonlinear 3 DMM , in terms of its expressiveness , representation power , and applications to facial analysis .",0
14472,"Using facial mesh triangle definition by Basel Face Model ( BFM ) , we train our 3 DMM using 300W - LP dataset .",0
14473,"The model is optimized using Adam optimizer with an initial learning rate of 0.001 when minimizing L 0 , and 0.0002 when minimizing L.",1
14474,"We set the following parameters : Q = 53 , 215 , U = V = 128 , l S = l T = 160 . ? values are set to make losses to have similar magnitudes .",1
14475,Expressiveness,0
14476,Exploring feature space .,0
14477,We use the entire CelebA dataset with ? 200 k images to feed to our network to obtain the empirical distribution of our shape and texture parameters .,0
14478,"By varying the mean parameter along each dimension proportional to their standard deviations , we can get a sense how each element contributes to the final shape and texture .",0
14479,We sort elements in the shape parameter f S based on their differences to the mean 3 D shape .,0
14480,"shows four examples of shape changes , whose differences rank No.1 , 40 , 80 , and 120 among 160 elements .",0
14481,Most of top changes are expression related .,0
14482,"Similarly , in , we visualize different texture changes by adjusting only one element off T off the mean parameterf T .",0
14483,The elements with the same 4 ranks as the shape counterpart are selected .,0
14484,Attribute Embedding .,0
14485,"To better understand different shape and texture instances embedded in our two decoders , we dig into their attribute meaning .",0
14486,"For a given attribute , e.g. , male , we feed images with that attribute { I i } n i =1 into our encoder to obtain two sets of parameters {f i S } n i =1 and {f i T } n i =1 .",0
14487,These sets represent corresponding empirical distributions of the data in the low dimensional spaces .,0
14488,"By computing the mean parametersf S , f T , and feed into their respective decoders , we can reconstruct the mean shape and texture with that attribute .",0
14489,visualizes the reconstructed shape and texture related to some attributes .,0
14490,Dif - ferences among attributes present in both shape and texture .,0
14491,Representation Power,1
14492,Texture .,0
14493,"Given a face image , assuming we know the groundtruth shape and projection parameters , we can unwarp the texture into the UV space , as we generate "" pseudo groundtruth "" texture in the weakly supervised step .",0
14494,"With the groundtruth texture , by using gradient descent , we can estimate a texture parameter f T whose decoded texture matches with the groundtruth .",0
14495,"Alternatively , we can minimize the reconstruction error in the image space , through the rendering layer with the groundtruth S and m .",1
14496,"Empirically , the two methods give similar performances but we choose the first option as it involves only one warping step , instead of rendering in every optimization iteration .",0
14497,"For the linear model , we use the fitting results of Basel texture and Phong illumination model given by .",0
14498,"As in , our nonlinear texture is closer to the groundtruth than the linear model , especially for in - the - wild images ( the first two rows ) .",1
14499,This is expected since the linear model is trained with controlled images .,0
14500,"Quantitatively , our nonlinear model has significantly lower L 1 reconstruction error than the lin - We also compare the power of nonlinear and linear 3 DMM in representing real - world 3D scans .",1
14501,"We compare with BFM , the most commonly used 3 DMM at present .",0
14502,"We use ten 3D face scans provided by , which are not included in the training set of BFM .",0
14503,"As these face meshes are already registered using the same triangle definition with BFM , no registration is necessary .",0
14504,"Given the groundtruth shape , by using gradient descent , we can estimate a shape parameter whose decoded shape matches the groundtruth .",0
14505,We define matching criteria on both vertex distances and surface normal direction .,0
14506,This empirically improves fidelity of final results compared to only optimizing vertex distances .,0
14507,"Also , to emphasize the compactness of nonlinear models , we train different models with different latent space sizes .",0
14508,shows the visual quality of two models ' reconstructions .,0
14509,"As we can see , our reconstructions closely match the face shapes .",0
14510,Meanwhile the linear model struggles with face shapes outside its PCA span .,0
14511,"To quantify the difference , we use NME , averaged pervertex errors between the recovered and groundtruth shapes , normalized by inter-ocular distances .",0
14512,"Our nonlinear model has a significantly smaller reconstruction error than the linear model , 0.0196 vs. 0.0241 ( Tab. 3 ) .",1
14513,"Also , the non-linear models are more compact .",0
14514,They can achieve similar performances as linear models whose latent spaces sizes doubled .,0
14515,Applications,0
14516,"Having shown the capability of our nonlinear 3 DMM ( i.e. , two decoders ) , now we demonstrate the applications of our entire network , which has the additional encoder .",0
14517,Many applications of 3 DMM are centered on its ability to fit to 2 D face images .,0
14518,visualizes our 3 DMM fitting results on CelebA dataset .,0
14519,"Our encoder estimates the shape S , texture T as well as projection parameter m .",0
14520,We can recover personal facial characteristic in both shape and texture .,0
14521,"Our texture can have variety skin color or facial hair , which is normally hard to be recovered by linear 3 DMM .",0
14522,2D Face Alignment .,0
14523,Face alignment is a critical step for any facial analysis task such as face recognition .,0
14524,"With enhancement in the modeling , we hope to improve this task ) .",0
14525,"We compare face alignment performance with state - of - the - art methods , SDM and 3DDFA , on the AFLW2000 dataset .",0
14526,"The alignment accuracy is evaluated by the Normalized Mean Error ( NME ) , the average of visible landmark error normalized by the bounding box size .",0
14527,"Here , current state - of - the - art 3DDFA is a cascade Input Our Richardson16 Tewari17 : 3D reconstruction results comparison .",0
14528,We achieve comparable visual quality in 3D reconstruction . :,0
14529,Quantitative evaluation of 3D reconstruction .,1
14530,We obtain a low error that is comparable to optimization - based methods .,1
14531,"of CNNs that iteratively refines its estimation in multiple steps , meanwhile ours is a single - pass of E and D S .",0
14532,"However , by jointly learning model fitting with 3 DMM , our network can surpass 's performance , as in Tab .",0
14533,"4 . Another perspective is that in conventional 3 DMM fitting , the texture is used as the input to regress the shape parameter , while ours adopts an analysis - by - synthesis scheme and texture is the output of the synthesis .",0
14534,"Further , fora more fair comparison of nonlinear vs. linear models , we train an encoder with the same architecture as our E , whose output parameter will multiple with the linear shape bases A , and train with the landmark loss function ( Eqn. 13 ) .",0
14535,Again we observe the higher error from the linear model - based fitting .,0
14536,3D Face Reconstruction .,1
14537,We compare our approach to recent works : the CNN - based iterative supervised regressor of Richardson et al. and unsupervised regressor method of Tewari et al ..,0
14538,The work by Tewari et al .,0
14539,is relevant to us as they also learn to fit 3 DMM in an unsupervised fashion .,0
14540,"However , they are limited to linear 3 DMM bases , which of course are not jointly trained with the model .",0
14541,"Also , we only compare with the coarse network in as their refinement network use SfS , which leads to a 2.5 D representation and loses correspondence between different 3 D shapes .",0
14542,This is orthogonal to our approach .,0
14543,shows visual comparison .,0
14544,"Following the same setting in , we also quantitatively compare our method with prior works on 9 subjects of FaceWarehouse database ) .",0
14545,"We achieve on - par results with Garrido et al. , an offline optimization method , while surpassing all other regression methods ] .",1
14546,Ablation on Texture Learning,0
14547,"With great representation power , we would like to learn a realistic texture model from in - the - wild images .",0
14548,The rendering layer opens a possibility to apply adversarial loss in addition to global L 1 loss .,0
14549,Using a global image - based discriminator is redundant as the global structure is guaranteed by the rendering layer .,1
14550,"Also , we empirically find that using global image - based discriminator can cause severe artifacts in the resultant texture .",1
14551,visualizes outputs of our network with different options of adversarial loss .,0
14552,"Clearly , patchGAN offers higher realism and fewer artifacts .",1
14553,Conclusions,0
14554,"Since it s debut in 1999 , 3 DMM has became a cornerstone of facial analysis research with applications to many problems .",0
14555,"Despite its impact , it has drawbacks in requiring training data of 3D scans , learning from controlled 2D images , and limited representation power due to linear bases .",0
14556,"These drawbacks could be formidable when fitting 3 DMM to unconstrained faces , or learning 3 DMM for generic objects such as shoes .",0
14557,"This paper demonstrates that there exists an alternative approach to 3 DMM learning , where a nonlinear 3 DMM can be learned from a large set of uncon-strained face images without collecting 3 D face scans .",0
14558,"Further , the model fitting algorithm can be learnt jointly with 3 DMM , in an end - to - end fashion .",0
14559,"Our experiments cover a diverse aspects of our learnt model , some of which might need the subjective judgment of the readers .",0
14560,"We hope that both the judgment and quantitative results could be viewed under the context that , unlike linear 3 DMM , no genuine 3 D scans are used in our learning .",0
14561,"Finally , we believe that unsupervisedly learning 3D models from large - scale in - the - wild 2D images is one promising research direction .",0
14562,This work is one step along this direction .,0
14563,title,0
14564,Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised,0
14565,Style Translation,0
14566,abstract,0
14567,"Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .",1
14568,"In this paper , we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement .",0
14569,"Given that any face images can be factored into space of style that captures lighting , texture and image environment , and a style - invariant structure space , our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation .",0
14570,"With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .",0
14571,"Extensive experiments verify the effectiveness of our idea with state - of - the - art results on WFLW [ 69 ] , 300W [ 56 ] , COFW [ 7 ] , and AFLW [ 36 ] datasets .",0
14572,Our proposed structure is general and could be assembled into any face alignment frameworks .,0
14573,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,1
14574,Introduction,0
14575,"Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .",0
14576,Accurate facial landmark localization was intensively studied with impressive progress made in these years .,0
14577,"The main streams are learning a robust and discriminative model through effective network structure , usage of geometric information , and correction of loss functions .",0
14578,"It is common wisdom now that factors such as variation of expression , pose , shape , and occlusion could greatly affect performance of landmark localization .",0
14579,"Almost all prior work aims to alleviate these problems from the perspective of structural characteristics , such as disentangling 3 D pose to provide shape constraint , and utilizing dense bound - :",0
14580,Problem in a well - trained facial landmark detector .,0
14581,"It is biased towards unconstrained environment factors , including lighting , image quality , and occlusion .",0
14582,"We regard these degradations as "" style "" in our analysis. ary information .",0
14583,"The influence of "" environment "" still lacks principled discussion beyond structure .",0
14584,"Also , considering limited labeled data for this task , how to optimally utilize limited training samples remains unexplored .",0
14585,"About "" environment "" effect , distortion brought by explicit image style variance was observed recently .",0
14586,"We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .",0
14587,"In practice , image content refers to objects , semantics and sharp edge maps , whereas style could be color and texture .",0
14588,"Our idea is based on the purpose of facial landmark detection , which is to regress "" facial content "" - the principal component of facial geometry - by filtering unconstrained "" styles "" .",0
14589,"The fundamental difference to define "" style "" from that of is that we refer it to image background , lighting , quality , existence of glasses , and other factors that prevent detectors from recognizing facial geometry .",0
14590,We note every face image can be decomposed into its facial structure along with a distinctive attribute .,0
14591,It is a natural conjecture that face alignment could be more robust if we augment images only regarding their styles .,0
14592,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .",1
14593,"Instead of directly generating images , we first map face images into the space of structure and style .",1
14594,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .",1
14595,"By factoring these features , we perform visual style translation between existing facial geometry .",1
14596,"Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .",1
14597,Our main contribution is as follows .,0
14598,1 .,0
14599,We offer a new perspective for facial landmark localization by factoring style and structure .,0
14600,"Consequently , a face image is decomposed and rendered from distinctive image style and facial geometry .",0
14601,2 .,0
14602,A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .,0
14603,"By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .",0
14604,3 .,0
14605,We propose a new dataset based on AFLW with new 68 - point annotation .,0
14606,It provides challenging benchmark considering large pose variation .,0
14607,"With extensive experiments on popular benchmark datasets including WFLW , 300W , COFW and AFLW , our approach outperforms previous state - of the - arts by a large margin .",0
14608,It is general to be incorporated into various frameworks for further performance improvement .,0
14609,Our method also works well under limited training computation resource .,0
14610,Related Work,0
14611,"This work has close connection with the areas of facial landmark detection , disentangled representation and selfsupervised learning .",0
14612,Facial Landmark Detection,0
14613,This area has been extensively studied over past years .,0
14614,"Classic parameterized methods , such as active appearance models ( AAMs ) and constrained local models ( CLMs ) provide satisfying results .",0
14615,"SDM , cascaded regression , and their variants were also proposed .",0
14616,"Recently , with the power of deep neural networks , regression - based models are able to produce better results .",0
14617,They are mainly divided into two streams of direct coordinate regression and heatmap - based regression .,0
14618,"Meanwhile , in , auxiliary attributes were used to learn a discriminative representation .",0
14619,Recurrent modules were introduced then .,0
14620,"Lately , methods improved performance via semi-supervised learning .",0
14621,"Influence of style variance was also discussed in , where a style aggregated component provides a stationary environment for landmark detector .",0
14622,"Our solutions are distinct with definition of "" style "" , different from prior work .",0
14623,"Our solution does not rely on the aggregation architecture , and instead is based on a semi-supervised scheme .",0
14624,Disentangled Representation,0
14625,Our work is also related to disentangled representation learning .,0
14626,Disentanglement is necessary to control and further alter the latent information in generated images .,0
14627,"Under the unsupervised setting , InfoGAN and MINE learned disentangled representation by maximizing the mutual information between latent code and data observation .",0
14628,"Recently , imageto - image translation explored the disentanglement between style and content without supervision .",0
14629,"In structured tasks such as conditional image synthesis , keypoints and person mask were utilized as self - supervision signals to disentangle factors , such as foreground , background and pose information .",0
14630,"As our "" style "" is more complex while "" content "" is represented by facial geometry , traditional style transfer is inapplicable since it may suffer from structural distortion .",0
14631,"In our setting , by leveraging the structure information base on landmarks , our separation component extracts the style factor from each face image .",0
14632,Self - Supervised Learning,0
14633,Our method also connects to self - supervised learning .,0
14634,"The mainstream work , such as , directly uses image data to provide proxy supervision through multi-task feature learning .",0
14635,Another widelyadopted approach is to use video data .,0
14636,Visual invariance of the same instance could be captured in a consecutive sequence of video frames .,0
14637,"Also , there is work focusing on fixed characteristics of objects from data statistics , such as image patch level information .",0
14638,"These methods learn visual invariance , which could essentially provide a generalized feature of objects .",0
14639,Our landmark localization involves computing the visual invariance .,0
14640,But our approach is different from prior selfsupervised frameworks .,0
14641,"Our goal lies in extracting facial structure and keypoints considering different environment factors , including occlusion , lighting , makeup and soon .",0
14642,Eliminating the influence of style makes it possible to reliably alter or process face structure and accordingly recog -,0
14643,Proposed Framework,0
14644,Our framework consists of two parts .,0
14645,"One learns the disentangled representation of facial appearance and structure , while the other can be any facial landmark detectors .",0
14646,"As illustrated in , during the first phase , conditional variational auto - encoder is proposed for learning disentangled representation between style and structure .",0
14647,"In the second phase , after translating style from other faces , "" stylized "" images with their structures are available for boosting training performance and our style - invariant detectors .",0
14648,Learning Disentangled Style and Structure,0
14649,"Given an image x , and its corresponding structure y .",0
14650,Two essential descriptors of a face image are facial geometry and image style .,0
14651,"Facial geometry is represented by labeled landmarks , while style captures all environmental factors that are mostly implicit , as described above .",0
14652,"With this setting , if the latent space of style and shape is mostly uncorrelated , using Cartesian product of z and y latent space should capture all variation included in a face image .",0
14653,"Therefore , the generator that re-renders a face image based on style and structure can be modeled as p ( x |y , z ) .",0
14654,"To encode the style and structure information and compute the parametric distribution p ( x |y , z ) , a conditional variational auto - encoder based network , which introduces two encoders , is applied .",0
14655,"Our network consists of a structure estimator E struct to encode landmark heatmaps into structure latent space , a style encoder E style that learns the style embedding of images , and a decoder that re-renders the style and structure to image space .",0
14656,"As landmarks available in this task , the facial geometry is represented by stacking landmarks to heat maps .",0
14657,Our goal therefore becomes inferring disentangled style code z from a face image and its structure by maximizing the conditional likelihood of,0
14658,"In particular , the generator G full ?",0
14659,"contains two encoders and a decoder ( renderer ) , i.e. , E style ? , E struct and D render , where G full ?",0
14660,and E style ?,0
14661,"respectively estimate parameters of p ( x |y , z ) and q ( z |x , y ) .",0
14662,"Consequently , the full loss function on learning separating information of style and structure is written as",0
14663,( 2 ),0
14664,KL - Divergence Loss Kullback - Leiber ( KL ) divergence loss severs as a key component in our design to help the encoder to learn decent representation .,0
14665,"Basically , the KLdivergence measures the similarity between the variational posterior and prior distribution .",0
14666,"In our framework , it is taken as regularization that discourages E style to encode structure - related information .",0
14667,"As the prior distribution is commonly assumed to be a unit Gaussian distribution p ? N ( 0 , 1 ) , the learned style feature is regularized to suppress contained structure information through reconstruction .",0
14668,The KL - divergence loss limits the distribution range and capacity of the style feature .,0
14669,"By fusing inferred style code z with encoded structure representation , sufficient structure information can be obtained from prior through multi - level skip connection .",0
14670,"Extra structure encoded in z incurs penalty of the likelihood p ( x |y , z ) during training with no new information captured .",0
14671,"In this way , E style is discouraged from learning structure information that is provided by E struct during training .",0
14672,"To better reconstruct the original image , E style is enforced to learn structure - invariant style information .",0
14673,Reconstruction Loss,0
14674,The second term L rec in Eq. refers to the reconstruction loss in the auto - encoder frame - work .,0
14675,"As widely discussed , basic pixel - wise L 1 or L 2 loss can not model rich information within images well .",0
14676,We instead adopt perceptual loss to capture style information and better visual quality .,0
14677,L rec is formulated as,0
14678,where we use VGG - 19 network ?,0
14679,structure that measures perceptual quality .,0
14680,l indexes the layer of network ?.,0
14681,"Since the style definition could be complicated , E style here encodes semantics of the style signal that simulates different types of degradation .",0
14682,It does not have to maintain fine - grained visual details .,0
14683,"Besides , to reserve the strong prior on structure information encoded from landmarks y , skip connection between E struct and D render is established to avoid landmark inaccuracy through style translation .",0
14684,"In this design , the model is capable of learning complementary representation of facial geometry and image style .",0
14685,Augmenting Training via Style Translation,0
14686,Disentanglement of structure and style forms a solid foundation for diverse stylized face images under invariant structure prior .,0
14687,"Given a dataset X that contains n face images with landmarks annotation , each face image xi ( 1 ? i ? n ) within the dataset has its explicit structure denoted by landmark y i , as well as an implicit style code z i depicted and embedded by E style .",0
14688,"To perform style translation between two images x i and x j , we pass their latent style and structure code embedded by E style and E struct to D render .",0
14689,"To put the style of image x j on x i 's structure , the stylized synthetic image is denoted as",0
14690,"As illustrated in , the first stage of our framework is to train the disentangling components .",0
14691,"In the second phase , by augmenting and rendering a given sample x in the original dataset X with styles from random k other faces , we produce k n "" stylized "" synthetic face images with respective annotated landmarks .",0
14692,These samples are then fed into training of facial landmark detectors together with the original dataset .,0
14693,Visualization of style translation results is provided in .,0
14694,"The input facial geometry is maintained under severe style variation , indicating its potential at augmenting training of facial landmark detectors .",0
14695,"Albeit with cohesive structure , the decoder generally does not re-render perfect - quality images , since the complexity of plentiful style information has been diminished to a parametric Gaussian distribution , confined by its capacity .",0
14696,"Also , as discussed before , each face image xi has its own style .",0
14697,"Theoretically , the renderer could synthesize n 2 images by rendering each available landmark with any other images ' style .",0
14698,"To understand how the quantity of stylized synthetic samples helps improve the facial landmark detectors , we analyze the effect of our design in following experiments and ablation study .",0
14699,Experiments,0
14700,Datasets,0
14701,"WFLW dataset is a challenging one , which contains 7,500 faces for training and 2,500 faces for testing , based .",0
14702,"Following the widely - adopted protocol , the AFLW - full dataset has 20,000 images for training and 4,386 for testing .",0
14703,It is originally annotated with 19 sparse facial landmarks .,0
14704,"To provide a better benchmark for evaluating pose variation and allow cross - dataset evaluation , we re-annotate it with 68 facial landmarks , which follow the common standard in 300W .",0
14705,"Based on the new 68 - point annotation , we conduct more precise evaluation .",0
14706,Cross - dataset evaluation is also provided among existing datasets .,0
14707,"COFW dataset contains 1,345 images for training and 507 images for testing , focusing on occlusion .",0
14708,The whole dataset is originally annotated with 29 landmarks and has been re-annotated with 68 landmarks in to allow crossdataset evaluation .,0
14709,We utilize 68 annotated landmarks provided by to conduct comparison with other approaches .,0
14710,Experimental Setting,0
14711,Evaluation Metrics,0
14712,We evaluate performance of facial landmark detection using normalized landmarks mean error and Cumulative Errors Distribution ( CED ) curve .,0
14713,"For the 300 W dataset , we normalize the error using inter-pupil distance .",0
14714,"In , we also report the NME using inter-ocular distance to compare with algorithms of , which also use it as the normalizing factor .",0
14715,"For other datasets , we follow the protocol used in and apply inter-ocular distance for normalization .",0
14716,"Implementation Details Before training , all images are cropped and resized to 256 256 using provided bounding boxes .",1
14717,"For the detailed conditional variational autoencoder network structures , we use a two - branch encoderdecoder structure as shown in .",0
14718,"We use 6 residual encoder blocks for downsampling the input feature maps , where batch normalization is removed for better synthetic results .",1
14719,"The facial landmark detector backbone is substitutable and different detectors are usable to achieve improvement , which we will discuss later .",0
14720,"For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .",1
14721,"For training of detectors , we first augment each landmark map with k random styles sampled from other face images .",1
14722,The number is set to 8 if not specially mentioned in experiments .,0
14723,"For the detector architecture , a simple baseline network based on ResNet - 18 is chosen by changing the output dimension of the last FC layers to landmark 2 to demonstrate the increase brought by style translation .",1
14724,"To compare with state - of - thearts and further validate the effectiveness of our approach , we replace our baseline model with similar structures proposed in , with the same affine augmentation .",0
14725,Comparison with State - of - the - arts,0
14726,WFLW,1
14727,We evaluate our approach on WFLW dataset .,0
14728,WFLW is a recently proposed challenging dataset with images from in - the - wild environment .,0
14729,"We compare algorithm in terms of NME ( % ) , Failure Rate ( % ) and AUC ( @0.1 ) following protocols used in .",0
14730,The Res - 18 baseline receives strong enhancement using synthetic images .,0
14731,"To further verify the effectiveness and generality of using style information , we replace the network by two strong baselines and report the result in .",0
14732,The light - weight Res - 18 is improved by 13.8 % .,1
14733,"By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .",1
14734,"In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .",1
14735,"The elevation is also determined by the model capacity . , we report different facial landmark detector performance ( in terms of normalized mean error ) on 300 W dataset .",0
14736,The baseline network follows Res - 18 structure .,0
14737,"With additional "" style- augmented "" synthetic training samples , our model based on a simple backbone outperforms previous state - of - the - art methods .",1
14738,"We also report results of models that are trained on original data , which reflect the performance gain brought by our approach .",0
14739,300W,1
14740,In,0
14741,"Similarly , we replace the baseline model with a state - of the - art method .",0
14742,"Following the same setting , this baseline is also much elevated .",0
14743,Note that the 4 - stack LAB and SAN are open - source frameworks .,0
14744,"We train the models from scratch , which perform less well than those reported in their original papers .",0
14745,"However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the "" style - augmented "" strategy .",1
14746,Method,0
14747,Common Cross - dataset Evaluation on COFW,1
14748,"To comprehensively evaluate the robustness of our method towards occlusion , COFW - 68 is also utilized for cross - dataset evaluation .",0
14749,We perform comparison against several state - of - theart methods in .,0
14750,"Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .",1
14751,AFLW,1
14752,We further evaluate our algorithm on the AFLW dataset following the AFLW Full protocol .,0
14753,AFLW is also challenging for its large pose variation .,0
14754,"It is originally annotated with 19 facial landmarks , which are relatively sparse .",0
14755,"To make it more useful , we richen the dataset by re-annotating it with 68 - point facial landmarks .",0
14756,This new set of data is also publicly available .,0
14757,"We compare our approach with several models in Table 3 , by re-implementing their algorithms on the new dataset along with our style - augmented samples .",0
14758,"Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .",1
14759,"Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .",1
14760,The visual comparison in shows hidden face part is better modeled with our strategy .,1
14761,Ablation Study,0
14762,Improvement on Limited Data,0
14763,Disentanglement of style and structure is the key that influences quality of style - augmented samples .,1
14764,We evaluate the completeness of disentanglement especially when the training samples are limited .,0
14765,To evaluate the performance and relative gain of our approach when training data is limited .,0
14766,The training set is split into 10 subsets and respectively we evaluate our model on different portions of training data .,0
14767,"Note that for different portions , we train the model from scratch with no extra data used .",0
14768,The quantitative result is reported in .,0
14769,"In : Normalized mean error ( % ) on 300W common and WFLW datasets when using different percentages of the training set , with the same protocol as in on a stronger baseline .",0
14770,The baseline network here follows SAN structure .,0
14771,show the relative improvement on different training samples .,0
14772,"Style - augmented synthetic images improve detectors ' performance by a large margin , while the improvement is even larger when the number of training images is quite small .",1
14773,"In , a stronger baseline SAN is chosen .",0
14774,"Surprisingly , the baseline easily reaches state - of - theart performance using only 50 % labeled images , compared to former methods provided in .",0
14775,"Besides , provides an intuitive visualization of the resulting generated faces when part of the data is used .",0
14776,"Each column contains output that is rendered from the input structure and given style , when using a portion of face image data .",0
14777,"It shows when the data is limited , our separation component tends to capture weak style information , such as color and lighting .",0
14778,"Given more data as examples , the style becomes complex and captures detailed texture and degradation , like occlusion .",0
14779,"The results verify that even using limited labeled images , our design is capable of disentangling style information and keeps improve those baseline methods that are already very strong .",0
14780,Estimating the Upper-bound,0
14781,"As discussed before , our method conceptually and empirically augments training with n 2 synthetic samples .",0
14782,By aug - : Results of style translation using different numbers of data .,0
14783,"The left 2 images are the input , with 2 different reference styles .",0
14784,The percentage refers to how much data is used to train the disentangle module .,0
14785,"menting each face image with k random styles , the training set could be very large and slows down convergence .",0
14786,"In this section , we experiment with choosing the style augmenting factor k and test the upper bound of style translation .",0
14787,We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet - 50 baseline .,1
14788,The result is reported in .,0
14789,"By adding a number of augmented styles , the model continue gaining improvement .",1
14790,"However , when k ? 8 , the performance grow slows down .",0
14791,It begins to decrease if k reaches 32 .,0
14792,"The reason is that due to the quantity imbalance between real and synthetic faces , a very large k makes the model overfit to synthetic image texture when the generated image quantity is large .",0
14793,Conclusion and Future Work,0
14794,"In this paper , we have analyzed the well - studied facial landmark detection problem from a new perspective of implicit style and environmental factor separation and utilization .",0
14795,"Our approach exploits the disentanglement representation of facial geometry and unconstrained style to provide synthetic faces via style translation , further boosting quality of facial landmarks .",0
14796,Extensive experimental results manifest its effectiveness and superiority .,0
14797,"We also note that utilizing synthetic data for more highlevel vision tasks still remains an open problem , mainly due to the large domain gap between generated and real images .",0
14798,"In our future work , we plan to model style in a more realistic way by taking into account the detailed degradation types and visual quality .",0
14799,We also plan to generalize our structure to other vision tasks .,0
14800,S1 . More Ablation Studies,0
14801,"In this section , we provide additional analysis about each design in our framework to facilitate understanding of our structure .",0
14802,Two key loss terms in our framework are studied to give insights into their respective roles .,0
14803,Qualitative visualization and quantitative results are reported for a comprehensive comparison .,0
14804,"KL divergence loss and perceptual loss , are incorporated into our framework during the disentangled learning procedure .",0
14805,shows their respective effect on style translation via visual comparisons of several incomplete variants .,0
14806,"Through visual observations , their roles could be inferred intuitively .",0
14807,"The perceptual loss , as discussed , is designed to capture better style information and visual quality .",0
14808,"Thus , removing this term leads to "" over-smoothness "" and poor diversity on synthetic images .",0
14809,"Removing KL divergence term shows severe structure distortion on translated results , which indicates that KL divergence loss plays a key role on disentangling structure and style information .",0
14810,Quantitative results of each variants are also reported in 7 .,0
14811,The normalized mean error ( NME ) is evaluated on WFLW test set when the model is trained on style augmented dataset using each variant .,0
14812,We observe that NME will increase if any loss function is removed .,0
14813,"In particular , the detector performance drops significantly lower than the baseline if L KL is removed .",0
14814,"Both the qualitative and quantitative result interprets the role of each component , indicating their essentialness in our framework .",0
14815,S2 . Additional Discussion,0
14816,"In this section , we provide more discussion on our approach along with our analysis towards some existing alternatives .",0
14817,S2.1 Comparison with GAN - based approaches,0
14818,"Generative adversarial network ( GAN ) and its applications are widely studied these days , using GAN - synthetic data to aid training , has also been explored along this line .",0
14819,Some works have utilized GANs to perform data augmentation .,0
14820,"However , its effect still remains questionable especially on high - level vision challenges .",0
14821,"For instance , in our task , face images need to be labeled with accurate landmarks .",0
14822,"Existing generative models are incapable of handling these tasks with fine - grained annotations , e.g. semantic segmentation , constrained by its limited generalizability .",0
14823,"We choose to escape the difficulties of GAN training , starting from a new perspective of internal representation .",0
14824,"With decent representation of separating style and structure , different interactions within a face image can be simulated by re-rendering from existing style and structure code .",0
14825,"In other words , our choice depends upon fully exploiting available information by mixing them , instead of creating new information and visually perfect results via adversarial learning procedure .",0
14826,"However , if two codes of structure and style are factored well , advances on high fidelity images synthesis could theoretically bring more gains based on our framework .",0
14827,S2.2 Comparison with Style Transfer,0
14828,Our method is motivated by advances in style transfer .,0
14829,A common doubt could be why not directly conducting style transfer as a augmentation or how basic style transfer could help training .,0
14830,"As discussed , our definition of style includes environments and degradation that prevent the model from recognizing while content refers to facial geometry .",0
14831,"Applying "" vanilla "" style transfer would leads to structural distortion on stylized images , as illustrated in .",0
14832,"Our definition of "" style "" helps preserve structure on synthetic images .",0
14833,"Besides , synthetic images using style transfer have a large domain gap with real - world face images .",0
14834,Simply augmenting training with these samples would instead hurt model 's localization ability on real images ..,0
14835,"Our results are more realistic than stylized images , with better structure coherence .",0
14836,Database,0
14837,Environment Number Multi- PIE,0
14838,Controlled 750000 XM2VTS 2360 LFPW 1035 HELEN In - the-wild 2330 AFW 468 IBUG 135 COFW - 68 507 AFLW - 68 ( Ours ) 25993,0
14839,S3 . Details of AFLW 68 - point dataset,0
14840,"We propose a new facial landmark dataset based on AFLW , to facilitate benchmarking on large pose performance .",0
14841,"To allow a more precise evaluation and crossdataset comparison , we follow the widely - used Multi -",0
14842,title,0
14843,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,1
14844,abstract,0
14845,We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment .,0
14846,"To achieve this , we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space , then train a simple Convolutional Neural Network to regress it from a single 2D image .",0
14847,We also integrate a weight mask into the loss function during training to improve the performance of the network .,0
14848,"Our method does not rely on any prior face model , and can reconstruct full facial geometry along with semantic meaning .",0
14849,"Meanwhile , our network is very light - weighted and spends only 9.8 ms to process an image , which is extremely faster than previous works .",0
14850,Experiments on multiple challenging datasets show that our method surpasses other state - of - the - art methods on both reconstruction and alignment tasks by a large margin .,0
14851,Code is available at https://github.com/YadiraF/PRNet.,1
14852,Introduction,0
14853,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,1
14854,"In the last decades , researches in these two fields benefit each other .",0
14855,"In the beginning , face alignment that aims at detecting a special 2D fiducial points is commonly used as a prerequisite for other facial tasks such as face recognition and assists 3 D face reconstruction to a great extent .",0
14856,"However , researchers find that 2D alignment has difficulties in dealing with problems of large poses or occlusions .",0
14857,"With the development of deep learning , many computer vision problems have been well solved by utilizing Convolution Neural Networks ( CNNs ) .",0
14858,"Thus , some works start to use CNNs to estimate the 3D Morphable Model ( 3 DMM ) coefficients or 3D model warping functions to restore the corresponding 3D information from a single 2D facial image , which provides both dense face alignment and 3D face reconstruction results .",0
14859,"However , the performance of these methods is restricted due to the limitation of the 3D space defined by face model basis or arXiv : 1803.07835v1 [ cs . CV ] 21 Mar 2018 templates .",0
14860,The required operations including perspective projection or 3D Thin Plate Spline ( TPS ) transformation also add complexity to the overall process .,0
14861,"Recently , two end - to - end works , which bypass the limitation of model space , achieve the state - of - the - art performances on their respective tasks .",0
14862,"trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .",0
14863,"Besides , dense alignment is not provided by this method .",0
14864,develops a volumetric representation of 3 D face and uses a network to regress it from a 2D image .,0
14865,"However , this representation discards the semantic meaning of points , thus the network needs to regress the whole volume in order to restore the facial shape , which is only part of the volume .",0
14866,"So this representation limits the resolution of the recovered shape , and need a complex network to regress it .",0
14867,"We find that these limitations do not exist in previous model - based methods , it motivates us to find anew approach to obtaining the 3D reconstruction and dense alignment simultaneously in a model - free manner .",0
14868,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .",1
14869,Our method surpasses all other previous works on both 3 D face alignment and reconstruction on multiple datasets .,0
14870,"Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .",0
14871,All of these are achieved by the elaborate design of the 2D representation of 3 D facial structure and the corresponding loss function .,0
14872,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .",1
14873,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,1
14874,"Figure1 shows our method is robust to poses , illuminations and occlusions .",0
14875,"In summary , our main contributions are :",0
14876,"- For the first time , we solve the problems of face alignment and 3D face reconstruction together in an end - to - end fashion without the restriction of low - dimensional solution space .",0
14877,"- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .",0
14878,"- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .",0
14879,We show that this design helps improving the performance of our network .,0
14880,- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .,0
14881,- Comparison on the AFLW2000 - 3D and Florence datasets shows that our method achieves more than 25 % relative improvements over other stateof - the - art methods on both tasks of 3 D face reconstruction and dense face alignment .,0
14882,Related Works,0
14883,"As described above , the main problems our method can solve are 3 D face reconstruction and face alignment .",0
14884,We will talk about closely related works on these tasks in the following subsections .,0
14885,3D,0
14886,Face Reconstruction,0
14887,"In this part , we evaluate our method on 3D face reconstruction task and compare with 3 DDFA , DeFA and VRN - Guided on AFLW2000 - 3D and Florence datasets .",0
14888,We use the same set of points as in evaluating dense alignment and changes the metric so as to keep consistency with other 3 D face reconstruction evaluation methods .,0
14889,"We first use Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the network output and ground truth point cloud , then calculate Mean Squared Error ( MSE ) normalized by outer interocular distance of 3D coordinates .",0
14890,The result is shown in .,0
14891,our method greatly exceeds the performance of other two state - of - the - art methods .,0
14892,"Since AFLW2000 -3D dataset is labeled with results from 3 DMM fitting , we further evaluate the performance of our method on Florence dataset , where ground truth 3 D point cloud is obtained from structured - light 3D scanning system .",0
14893,"Here we compare our method with 3 DDFA and VRN - Guided , using experimental settings in .",0
14894,"The evaluation images are the renderings with different poses from Florence database , we calculate the bounding box from the ground truth point cloud and using the cropped image as network input .",0
14895,"Although our method output more complete face point clouds than VRN , we only choose the common face region to compare the performance , 19 K points are used for the evaluation .",0
14896,"shows that our method achieves 28.7 % relative higher performance compared to VRN - Guided on Florence dataset , which is a significant improvement .",0
14897,"To better evaluate the reconstruction performance of our method across different poses , we calculated the NME for different yaw angle range .",0
14898,"As shown in figure 9 , all the methods perform well in near frontal view , however , 3 DDFA and VRN - Guided fail to keep low error as pose becomes large , while our method keeps relatively stable performance in all pose ranges .",0
14899,"We also illustrate the qualitative comparison in , our restored point cloud covers a larger region than in VRN - Guided , which ignores the lateral facial parts .",0
14900,"Besides , due to the limitation on resolution of VRN , our method provides finer details of face , especially on the nose and mouth region .",0
14901,"We also provide additional qualitative results on 300 VW and Multi - PIE datasets , please refer to supplementary material for full details . :",0
14902,Left : CED curves on Florence dataset with different yaw angles .,0
14903,Right : the qualitative comparison with VRN - Guided .,0
14904,"The first column is the input images from Florence dataset and the Internet , the second column is the reconstructed face from our method , the third column is the results from VRN - Guided .",0
14905,Face Alignment,0
14906,"In the field of computer vision , face alignment is a long - standing problem which attracts a lot of attention .",0
14907,"In the beginning , there area number of 2D facial alignment approaches which aim at locating a set of fiducial 2 D facial landmarks , such as classic Active Appearance Model ( AMM ) and Constrained Local Models ( CLM ) .",0
14908,Then cascaded regression and CNNbased methods are largely used to achieve state - of - the - art performance in 2D landmarks location .,0
14909,"However , 2D landmarks location only regresses visible points on faces , which leads to difficulties and is limited to describe face shape when the pose of faces is large .",0
14910,"Recent works then research the 3D facial alignment , which begins with fitting a 3 DMM or registering a 3 D facial template with a 2 D facial image .",0
14911,"Obviously , 3D reconstruction methods based on model can easily complete the task of 3D face alignment by selecting x ,y coordinates of landmarks vertices in reconstructed geometry .",0
14912,"Actually , are specially designated methods to achieve 3 D face alignment by means of 3 DMM fitting .",0
14913,Recently use a deep network to directly predict the heat map to obtain the 3D facial landmarks and achieves state - of - the - art performance .,0
14914,"Thus , as sparse face alignment tasks are highly completed by aforementioned methods , the task of dense face alignment begins to develop .",0
14915,"Notice that , the dense face alignment means the methods should offer the correspondence between two face images as well as between a 2 D facial image and a 3 D facial reference geometry .",0
14916,use multi-constraints to train a CNN which estimates the 3 DMM parameters and then provides a very dense 3D alignment .,0
14917,"directly learn the correspondence between a 2D input image and a 3D template via a deep network , while those predicted correspondence is not complete , only visible face - region is considered .",0
14918,"Compared to prior works , our method can directly establish the dense correspondence between all region in faces and 3D template once the position map is regressed .",0
14919,"No intermediate parameters such as 3 DMM coefficients and TPS warping parameters are needed in our method , which means our network can run very fast bypassing complex operations including perspective projection and TPS transformation .",0
14920,Proposed Method,0
14921,This section describes the framework and the details of our proposed method .,0
14922,"Firstly , we introduce the characteristics of the position map for our 3D face representation .",0
14923,Then we elaborate the CNN architecture and the loss function designed specially for learning the mapping from unconstrained RGB image to its 3D structure .,0
14924,The implementation details of our method are shown in the last subsection .,0
14925,3D,0
14926,Face Representation,0
14927,Our goal is to regress the 3D facial geometry and its dense correspondence information from a single 2D image .,0
14928,Thus we need a proper representation which can be directly predicted via a deep network .,0
14929,One simple and commonly used idea is to concatenate the coordinates of all points in 3 D face as a vector and use a network to predict it .,0
14930,"However , this transformation increases the difficulties in training since projection from 3D space into 1 D vector discards the spatial adjacency information among points .",0
14931,"While it 's natural to think that spatially adjacent points could share weights in predicting their positions , which can be easily achieved by using convolutional layers .",0
14932,"The coordinates as a 1 D vector needs a fully connected layer to predict each point with much more parameters , which increases the network size and is hard to train .",0
14933,proposed a point set generation network to directly predict the point cloud of 3 D object as a vector from a single image .,0
14934,"However , the max number of points is only 1024 , far from enough to represent an accurate 3 D face .",0
14935,"So model - based methods ] regress a few model parameters rather than the coordinates of points , which usually needs special care in training such as using Mahalanobis distance and inevitably limits the estimated face geometry to the their model space .",0
14936,proposed 3D binary volume as the representation of 3D structure and uses Volumetric Regression Network ( VRN ) to output a 192 192 200 volume as the discretized version of point cloud .,0
14937,"By using this representation , VRN can be built with full convolutional layers .",0
14938,"However , discretization limits the resolution of point cloud , and most part of the network output correspond to non-surface points which are of less usage .",0
14939,"To address the problems in previous works , we propose UV position map as the presentation of full 3 D facial structure .",0
14940,"UV position map or position map for short , is a 2D image recording 3D positions of all points in UV space .",0
14941,"In the past years , UV space or UV coordinates , which is a 2D image plane parameterized from the 3D space , has been utilized as away to express information including the texture of faces ( texture map ) , 2.5D geometry ( height map ) and the correspondences between 3 D facial meshes .",0
14942,"Different from previous works , we use UV space to store the 3D coordinates of points from 3 D face model .",0
14943,"As shown in , we define the 3D face point cloud in Left - handed Cartesian Coordinate system .",0
14944,"The origin of the 3D space overlaps with the upper-left of the input image , with the positive x - axis pointing to the right of the image .",0
14945,The ground truth 3 D face point cloud exactly matches the face in the 2D image when projected to the x -y plane .,0
14946,"Thus our position map can be easily comprehended as replacing the r , g , b value in texture map by x , y , z coordinates .",0
14947,"In order to keep the semantic meaning of points within position map , we create our UV coordinates based on 3 DMM .",0
14948,"Since we want to regress the 3D full structure directly , the unconstrained 2 D facial images and their corresponding 3D shapes are needed for end - to - end training .",0
14949,"300W - LP is a large dataset that contains more than 60K unconstrained images with fitted 3 DMM parameters , which is suitable to form our training pairs .",0
14950,"Besides , the 3 DMM parameters of this dataset are based on the Basel Face Model ( BFM ) .",0
14951,"Thus , in order to make full use of this dataset , we conduct the UV coordinates corresponding to BFM .",0
14952,"To be specific , we use the parameterized UV coordinates from which computes a Tutte embedding with conformal Laplacian weight and then maps the mesh boundary to a square .",0
14953,"Since the number of vertices in BFM is more than 50 K , we choose 256 as the position map size , which get a high precision point cloud with negligible re-sample error .",0
14954,"Thus our position map records a dense set of points from 3 D face with its semantic meaning , we are able to simultaneously obtain the 3D facial structure and dense alignment result by using a CNN to regress the position map directly from unconstrained 2D images .",0
14955,The network architecture in our method could be greatly simplified due to this convenience .,0
14956,"Notice that the position map contains the information of the whole face , which makes it different from other 2D representations such as Projected Normalized Coordinate Code ( PNCC ) , an ordinary depth image or quantized UV coordinates , which only reserve the information of visible face region in the input image .",0
14957,"Our proposed position map also infers the invisible parts of face , thus our method can predict a complete 3 D face .",0
14958,"Since our network transfers the input RGB image into position map image , we employ an encoder - decoder structure to learn the transfer function .",0
14959,"The encoder part of our network begins with one convolution layer followed by 10 residual blocks which reduce the 256 256 3 input image into 8 8 512 feature maps , the decoder part contains 17 transposed convolution layers to generate the predicted 256 256 3 position map .",0
14960,"We use kernel size of 4 for all convolution or transposed convolution layers , and use ReLU layer for activation .",0
14961,"Given that the position map contains both the full 3D information and dense alignment result , we do n't need extra network module for multi -task during training or inferring .",0
14962,The architecture of our network is shown in .,0
14963,Network Architecture and Loss Function,0
14964,"In order to learn the parameters of the network , we build a novel loss function to measure the difference between ground truth position map and the network output .",0
14965,"Mean square error ( MSE ) is a commonly used loss for such learning task , such as in .",0
14966,"However , MSE treats all points equally , so it is not entirely appropriate for learning the position map .",0
14967,"Since central region of face has more discriminative features than other regions , we employ a weight mask to form our loss function .",0
14968,"As shown in , the weight mask is a gray image recording the weight of each point on position map .",0
14969,It has the same size and pixel - to - pixel correspondence to position map .,0
14970,"According to our objective , we separate points into four categories , each has its own weights in the loss function .",0
14971,"The position of 68 facial keypoints has the highest weight , so that to ensure the network to learn accurate locations of these points .",0
14972,"The neck region usually attracts less attention , and is often occluded by hairs or clothes in unconstrained images .",0
14973,"Since learning the 3D shape of neck or clothes is beyond our interests , we assign 0 weight to points in neck region to reduce disturbance to the training process .",0
14974,"Thus , we denote the predicted position map as P ( x , y) for x , y representing each pixel coordinate .",0
14975,"Given the ground truth position mapP ( x , y) and weight",0
14976,Training Details,0
14977,"Because our training process needs datasets containing both 2 D face images and its corresponding 3 D point clouds with semantic meaning , we choose e.g. , 300W - LP to form our training sets , since it contains face images across different angles with the annotation of estimated 3 DMM coefficients , from which the 3D point cloud could be easily recovered .",0
14978,"Specifically , we crop the images according the ground truth bounding box and rescale them to size 256 256 .",0
14979,"Then utilize their annotated 3 DMM parameters to generate the corresponding 3D position , and render them into UV space to obtain the ground truth position map , the map size in our training is also 256 256 , which means a precision of more than 45 K point cloud to regress .",0
14980,"Notice that , although our training data is generated from 3 DMM , our network 's output , the position map is not restricted to any face template or linear space of 3 DMM .",0
14981,We perturb the training set by randomly rotating and translating the target face in 2D image plane .,0
14982,"To be specific , the rotation is from - 45 to 45 degree angles , translation changes is random from 10 percent of input size , and scale is from 0.9 to 1.2 .",0
14983,"Like , we also augment our training data by scaling color channels with scale range from 0.6 to 1.4 .",0
14984,"In order to handle images with occlusions , we synthesize occlusions by adding noise texture into raw images , which is similar to the work of .",0
14985,"With all above augmentation operations , our training data covers all the difficult cases .",0
14986,We use the network described 3 to train our transfer model .,0
14987,"For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .",1
14988,The batch size is set as 16 .,1
14989,All of our training codes are implemented with TensorFlow .,1
14990,Experimental Results,0
14991,"In this part , we evaluate the performance of our proposed method on the tasks of 3D face alignment and 3D face reconstruction .",0
14992,We first introduce the test datasets used in our experiments in section 4.1 .,0
14993,Then in section 4.2 and 4.3 we compare our results with other methods in both quantitative and qualitative way .,0
14994,We then compare our method 's runtime with other methods in section 4.4 .,0
14995,"In the end , the ablation study is conducted in section 4.5 to evaluate the effect of weight mask in our method .",0
14996,Test Dataset,0
14997,"To evaluate our performance on the task of dense alignment and 3D face reconstruction , multiple test datasets listed below are used in our experiments :",0
14998,AFLW2000 - 3D is constructed by to evaluate 3 D face alignment on challenging unconstrained images .,0
14999,This database contains the first 2000 images from AFLW and expands its annotations with fitted 3 DMM parameters and 68 3D landmarks .,0
15000,We use this database to evaluate the performance of our method on both face reconstruction and face alignment tasks .,0
15001,AFLW - LFPA is another extension of AFLW dataset constructed by .,0
15002,"By picking images from AFLW according to the poses , the authors construct this dataset which contains 1299 test images with a balanced distribution of yaw angle .",0
15003,"Besides , each image is annotated with 13 additional landmarks as a expansion to only 21 visible landmarks in AFLW .",0
15004,This database is evaluated on the task of 3D face alignment .,0
15005,We use 34 visible landmarks as the ground truth to measure the accuracy of our results .,0
15006,Florence is a 3 D face dataset that contains 53 subjects with its ground truth 3 D mesh acquired from a structured - light scanning system .,0
15007,"On experiments , each subject generates renderings with different poses as the same with : a pitch of - 15 , 20 and 25 degrees and spaced rotations between - 80 and 80 .",0
15008,We compare the performance of our method on face reconstruction against other very recent state - of - the - art methods VRN - Guided and 3DDFA on this dataset .,0
15009,3D Face Alignment,1
15010,To evaluate the face alignment performance .,0
15011,"We employ the Normalized Mean Error ( NME ) to be the evaluation metric , bounding box size is used as the normalization factor .",0
15012,"Firstly , we evaluate our method on a sparse set of 68 facial landmarks , and compare our result with 3 DDFA , DeFA and 3D - FAN on dataset AFLW2000 - 3D .",0
15013,"As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .",1
15014,"When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .",1
15015,"Notice that , the 3D - FAN needs another network to predict the z coordinate of landmarks , while the depth value can be obtained directly in our method .",0
15016,"To further investigate the performance of our method across poses and datasets , we also report the NME of faces with small , medium and large yaw angles on AFLW2000 - 3D dataset and the mean NME on both AFLW2000 - 3 D and AFLW - LPFA datasets .",0
15017,"shows the comparison with other methods , note that the numerical values are recorded from their published papers except the ones of 3D - FAN .",0
15018,"Follow the work , we also randomly select 696 faces from AFLW2000 to balance the distribution .",0
15019,The result shows that our method is robust to the change of pose and datasets .,1
15020,"Although all the state - of - the - art methods of face alignment conduct evaluation on AFLW2000 - 3D dataset , the ground truth of this dataset is still controversial due to its annotation pipeline which is based on Landmarks Marching method in .",0
15021,"Thus , we visualize some results in that have NME larger than 6.5 % and we find our results are more accurate than the ground truth in some cases . :",0
15022,Performance comparison on two large - pose face alignment datasets AFLW2000 - 3D ( 68 landmarks ) and AFLW - LFPA ( 34 visible landmarks ) .,0
15023,The NME ( % ) for faces with different yaw angles are reported .,0
15024,"The first best result in each category is highlighted in bold , the lower is the better . :",0
15025,Examples from AFLW2000 - 3 D dataset show that our predictions are more accurate than ground truth in some cases .,1
15026,Green : predicted landmarks by our method .,0
15027,Red : ground truth from .,0
15028,We also compare our dense alignment result against other state - of - the - art methods including 3DDFA and DeFA on the only test dataset AFLW2000 - 3D .,0
15029,"In order to compare different methods with the same set of points , we select the points from the largest common face region provided by all methods , and finally around 45 K points were used for the evaluation .",0
15030,"As shown in figure 7 , our method outperforms the best methods with a large margin of more than 27 % on both 2 D and 3D coordinates . : CED curves on AFLW2000 - 3D .",1
15031,Evaluation is performed on all points with both the 2D ( left ) and 3D ( right ) coordinates .,0
15032,Overall 2000 images from AFLW2000 - 3D dataset are used here .,0
15033,The mean NME % of each method is also showed in the legend .,0
15034,Runtime,0
15035,"Surpassing the performance of all other state - of - the - art methods on 3D face alignment and reconstruction , our method is surprisingly more light - weighted and faster .",0
15036,"Since our network uses basic encoder - decoder structure , our model size is only 160 MB compared to 1.5 GB in VRN .",0
15037,We also compare the runtime between our method and other state - of - the - art methods .,0
15038,shows the result .,0
15039,The results of 3DDFA and 3 DSTN are directly recorded from their published papers and others are recorded by running their publicly available source codes .,0
15040,"Notice that ,",0
15041,"We measure the run time of the process which is defined from inputing the cropped face image until recovering the 3D geometry ( point cloud , mesh or voxel data ) for 3D reconstruction methods or obtaining the 3D landmarks for alignment methods .",0
15042,The harware used for evaluation is an NVIDIA GeForce GTX 1080 GPU and an Intel ( R ) Xeon ( R ) CPU E5-2640 v 4 @ 2.40 GHz .,0
15043,"Specifically , DeFA needs 11.8 ms ( GPU ) to predict 3 DMM parameters and another 23.6 ms ( CPU ) to generate mesh data from predicted parameters , 3DFAN needs 29.1 ms ( GPU ) to estimate 2D coordinates first and 25.6 ms ( GPU ) to obtain depth value , VRN - Guided detects 68 2D landmarks with 28.4 m s ( GPU ) , then regress the voxel data with 40.6 ms ( GPU ) , our method provides both 3D reconstruction and dense alignment result from cropped image in one pass in 9.8 ms ( GPU ) .",0
15044,Ablation Study,0
15045,"In this section , we conduct several experiments to evaluate the influence of our weight mask on training and provide both sparse and dense alignment CED on AFLW2000 to evaluate different settings .",0
15046,"shows three options for our weight mask , we could see that weight ratio 1 corresponds to the situation when no weight mask is used , weight ratio 2 and 3 are slightly different on the emphasis in loss function .",0
15047,The results are shown in .,0
15048,Network trained without using weight mask has worst performance compared with other two settings .,1
15049,"By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .",1
15050,Conclusion,0
15051,"In this paper , we propose an end - to - end method , which well solves the problems of 3 D face alignment and 3D face reconstruction simultaneously .",0
15052,"By learning the position map , we directly regress the complete 3D structure along with semantic meaning from a single image .",0
15053,"Quantitative and qualitative results demonstrate our method is robust to poses , illuminations and occlusions .",0
15054,Experiments on three test datasets show that our method achieves significant improvements over others .,0
15055,We further show that our method runs faster than other methods and is suitable for real time usage .,0
15056,title,0
15057,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,1
15058,abstract,0
15059,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",1
15060,"State - of - the - art face alignment methods either consist in end - to - end regression , or in refining the shape in a cascaded manner , starting from an initial guess .",0
15061,"In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .",0
15062,DeCaFA uses fully - convolutional stages to keep full spatial resolution throughout the cascade .,0
15063,"Between each cascade stage , DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark - wise attention maps for each of several landmark alignment tasks .",0
15064,"Weighted intermediate supervision , as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end - to - end manner .",0
15065,"We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .",0
15066,"In addition , we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data .",0
15067,Introduction,0
15068,"Face alignment consists in localizing landmarks ( e.g. lips and eyes corners , pupils , nose tip ) on an face image .",0
15069,"It is an important computer vision field , as it is an essential preprocess for face recognition , tracking , expression analysis , and face synthesis .",0
15070,"Most recent face alignment approaches either belongs to cascaded regression methods , or to deep end - to - end regression methods .",0
15071,"On the one 's hand , cascaded regression consists in learning a sequence of updates , starting from an initial guess , to refine the landmark localization in a coarse - to - fine manner .",0
15072,"This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .",0
15073,"On the other hand , many deep approaches aim at regressing the landmark position from the original image directly .",0
15074,"However , because annotating several landmarks on a face image is a tedious task , data is rather scarce and the nature of the annotations usually vary a lot between the databases .",0
15075,"Because of the scarcity of the data , end - to - end approaches usually rely on learning an intermediate representation , such as edges detection to drive the alignment process .",0
15076,"However , these representations are usually ad hoc and do not guarantee to be optimal to address landmark localization tasks .",0
15077,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .",1
15078,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .",1
15079,shows attention maps extracted by the subsequent DeCaFA stages ( horizontally ) and for three different markups ( vertically ) .,0
15080,"It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .",1
15081,The contributions of this paper are tree - fold :,0
15082,"We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .",0
15083,"We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .",0
15084,"Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .",0
15085,"We show experimentally that DeCaFA significantly outperforms existing approaches on multiple datasets , inluding the recent WFLW database .",0
15086,"Additionally , we highlight how coarsely annotated data helps the network to learn fine landmark alignment even with very few annotated images .",0
15087,Related work,0
15088,"Popular examples of cascaded regression methods include SDM : in their pioneering work ,",0
15089,Xiong et al show that using simple linear regressors upon SIFT features in a cascaded manner already provides satisfying alignment results .,0
15090,LBF [ 14 ] is a refinement that employs randomized decision trees to dramatically speedup feature extraction .,0
15091,DAN uses deep networks to learn each cascade stage .,0
15092,"However , one downside of these approaches is that the update regressors are not learned jointly in a end - to - end fashion , thus there is no guarantee that the learned feature point alignment sequences might be optimal .",0
15093,MDM improves the feature extraction process by sharing the convolutional layer among all steps of the cascade that are performed through a recurrent neural network .,0
15094,This results in memory footprint reduction as well as better representation learning and a more optimized landmark trajectory throughout the cascade .,0
15095,TCDCN was perhaps the first end - to - end framework that could compete with cascaded regression approaches .,0
15096,It relies on supervised pretraining on a wide database of facial attributes .,0
15097,"More recently , PCD - CNN uses head pose information to drive the training process .",0
15098,CPM + SBR employs landmark registration to regularize training .,0
15099,"SAN uses adversarial networks to convert images from different styles to an aggregated style , upon which regression is performed .",0
15100,This aggregated style space thus serve as an intermediate representation that is more convenient for training .,0
15101,"In the authors propose to use edge map estimation as an intermediate representation to drive the landmark prediction task , as well as to provide a unified representation when images are annotated in terms of different markups , that correspond to different alignment tasks .",0
15102,"Finally , DSRN relies on Fourier Embedding and low - rank learning to produce such representation .",0
15103,"However , the use of such intermediate representation is usually ad hoc and it is hard to know which one would be all - around better for face alignment .",0
15104,"Recently , AAN proposes to use intermediate feature maps as attentional masks to select relevant spatial regions .",0
15105,It also uses intermediate supervision to constrain those maps to correspond to attention maps relatively to landmark localization .,0
15106,"However , there is no guarantee that the network will learn to align landmarks in a cascaded , coarse - to - fine manner .",0
15107,"Furthermore , annotating images in term of several face landmarks is a time - consuming task .",0
15108,"As a result , data is rather scarce and annotated in terms of varying number of landmarks .",0
15109,"For instance , 300W database contains approximately 3000 images labelled with 68 landmarks for train , whereas WFLW database contains 7500 images with 98 landmarks .",0
15110,"Thus , one can wonder if we can use all those images within the same framework to learn more robust landmark predictions .",0
15111,In the authors address this problem by using a classical multi-task formulation .,0
15112,"However , this essentially ignores the intrinsic relationship between the structure of different landmark alignment tasks .",0
15113,"Likewise , if we can predict the position of 68 landmarks , we can also easily deduce the position of landmarks fora coarser markup , such as eye / mouth corners and nose tip .",0
15114,DeCaFA overview,0
15115,"In this Section , we introduce our Deep convolutional Cascade for Face Alignment ( DeCaFA ) model , as illustrated on .",0
15116,"DeCaFA consists of S stages , each of which contains a fully - convolutional U - net backbone that preserves the full spatial resolution , as well as an attention map generation sub-network .",0
15117,Section 3.1 shows how we derive landmarkwise attention maps for one landmark prediction task .,0
15118,"Section 3.2 explains how several transfer layers can be chained to produce such attention maps , relatively to K landmark prediction tasks .",0
15119,"the input of the next stage is obtained by applying a feature fusion algorithm that involves the attention maps , as explained in Section 3.3 .",0
15120,In Section 3.4 we describe how DeCaFA is trained in an end - to - end manner with weighted intermediate supervision .,0
15121,"Finally , in Section 3.5 we provide implementation details to facilitate reproducibility of the results .",0
15122,Landmark - wise attention maps,0
15123,The U - net at stage i takes an input I i and gives rise to an embedding H i with parameters ?,0
15124,i .,0
15125,"In order to produce a suitable embedding from H i for predicting L landmarks , we apply a 1 1 convolutional layer with L filters with : DeCaFA architecture overview .",0
15126,"Several stages with fully - convolutional U-nets are stacked , multiple transfer layers are chained and intermediate supervision with increasing weights is applied to produce landmark estimates for heterogeneous alignment tasks .",0
15127,Landmark - wise attention maps are fused with the input image and the embeddings of the previous stage U - net to enable end - to - end cascaded alignment .,0
15128,parameters ?,0
15129,i .,0
15130,We denote the embeddings outputted by this transfer layer as T Li .,0
15131,In order to highlight its dominant mode we apply a spatial softmax operator .,0
15132,"Formally , fora pixel with coordinates ( x , y) and a landmark l:",0
15133,An estimation ?,0
15134,Li of the landmark coordinates can be obtained by computing the first order moments of ?,0
15135,Li :,0
15136,Where ?,0
15137,"L i , x and ?",0
15138,"L i , y are two vectors of size L containing the x and y landmark coordinates ?",0
15139,Li .,0
15140,The soft - argmax operator is inspired by the work in in the frame of human pose estimation and provides differentiable landmark coordinates estimate from the attention map ?,0
15141,Li .,0
15142,Chaining landmark localization tasks,0
15143,"As it will be explained in Section 4.1 , existing datasets for face alignment usually have heterogeneous annotations and varying numbers of annotated landmarks .",0
15144,"In order to deal with these heterogeneous annotations , we integrate K tasks that consist in predicting various numbers of landmarks",0
15145,we chain the landmark - wise attention maps in an decreasing order of the number of landmarks to predict ) .,0
15146,"To do so , we apply K transfer layers T L1 i , ... ,",0
15147,", at it is depicted on ( a ) .",0
15148,We have :,0
15149,The advantages of stacking the landmarks prediction pipelines in a descending order of the number of landmarks to be localized are two - fold :,0
15150,"First , from a semantic perspective , who can do more can do less , meaning that it shall be easier for the network to learn the sequence of transfer layers in that order ( i.e. if we can precisely localize a 68 - points markup it will be easy to also localize the nose tip , as well as mouth / eyes corners ) .",0
15151,"Second , labelling images with large amounts of landmarks is a tedious task , thus generally the more annotated landmarks in a database , the less images we have at our disposal .",0
15152,Using such architecture ensures that the former ( harder ) tasks benefit from all the images annotated with the latter ( easier ) task .,0
15153,"This can be seen as weakly supervised learning , where images labelled in terms of coarse markups can help to learn finer alignment tasks .",0
15154,"Also note that as these 1 1 convolutional layers have very few parameters , thus a lot of gradient can be backpropagated down to the U - net backbone and benefit the K prediction tasks .",0
15155,"Finally , as illustrated on , we use attention maps ?",0
15156,L k 0 i from markup k 0 to provide richer embeddings for the subsequent stages by applying feature fusion .,0
15157,Feature fusion,0
15158,"Ina standard feedforward deep network with S stacked stages , the i + 1 th stage takes an input I i = F 1 that corresponds to the embeddings H i outputted by the previous stage ( with the convention I 0 = I the original image ) .",0
15159,"By contrast , in cascade - based approaches , each stage shall learn an update to bring the feature points closer to the ground truth localizations , by using information sampled around current feature point localizations .",0
15160,"Within an end - to - end fully - convolutional deep network , an analogous statement would be that the i + 1 th stage shall use a local embedding F 2 that is calculated using information from the original image I highlighted by landmark - wise attention maps ?",0
15161,"In our method , we aggregate these maps by summing all the landmark - wise attention maps",0
15162,.,0
15163,"Thus , we can write the feature fusion model for the basic deep approach as :",0
15164,and the cascade - like approach as :,0
15165,Where denotes the Hadamard product .,0
15166,"This fusion scheme between the input image and the mask only preserves local information , for which the values of M i are high .",0
15167,"Alternatively , we can reinject the original image I inside each stage so that it can use global information in case where the mask M i is not precise enough or contains localizations errors ( as it is the case early in the cascade ) :",0
15168,With || the channel - wise concatenation operation .,0
15169,"Furthermore we can also fuse the relevant parts ( as highlighted by mask M i ) of the embedding H i of the previous stage U - net to provide the subsequent stages a richer , more semantically abstract information to estimate the landmarks coordinates :",0
15170,"Finally , we can aso use global information from not only the image I , but also from the embeddings H i :",0
15171,"This fusion model is more efficient and is used in De - CaFA ) , as it allows using both global and local information around the estimated landmarks so as to learn cascade - like alignment in an end - to - end fashion .",0
15172,Learning DeCaFA model,0
15173,DeCaFA models can be trained end - to - end by optimizing the following loss function w.r.t. parameters of the U-nets ?,0
15174,i and ?,0
15175,With z L k * the ground truth landmark position fora L klandmarks markup .,0
15176,"In practice , the summation in equation have less terms since usually each example is annotated with only one markup .",0
15177,"With this configuration , however , if the whole network is deep enough , few gradient will ever pass through the firsts attention maps .",0
15178,"Even worse , there is no guarantee that these feature maps will correspond to landmark - wise attention maps in the early stages , which is key to ensure cascade - like behavior of DeCaFA .",0
15179,"To ensure this , we add a differentiable soft - argmax layer after each spatial softmax and a supervised cost at stage i:",0
15180,"In practice , we use a L 1 loss function , as it has been shown to overfit lesson very bad examples and lead to more precise results for face alignment .",0
15181,"However , we need to make sure that the ( relatively ) shallow sub-networks does not overfit on these losses , which would result in very narrow heat maps with very localized dominant modes early in the cascade , and thus an overall lower accuracy .",0
15182,This is ensured by applying increasing ?,0
15183,i weights in ( 10 ) .,0
15184,Implementation details,0
15185,"The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .",1
15186,The input images are resized to 128 128 grayscale images prior to being processed by the network .,1
15187,Each convolution is followed by a batch normalization layer with ReLU activation .,1
15188,In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .,1
15189,The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .,1
15190,"We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .",1
15191,Experiments,0
15192,Datasets,0
15193,"The 300W database , introduced in , is considered as the benchmark dataset for training and testing face alignment models , with moderate variations in head pose , facial expressions and illuminations .",0
15194,"It consists in four databases : LFPW ( 811 images for train / 224 images for test ) , HELEN ( 2000 images for train / 330 images for test ) , AFW ( 337 images for train ) and IBUG ( 135 images for test ) , fora total of 3148 images annotated with 68 landmarks for training the models .",0
15195,"For comparison with state - of - the art methods , we refer to LFPW and HELEN test sets as the common subset and I - BUG as the challenging subset of 300W .",0
15196,The Celeb,0
15197,"A database is a large - scale face attribute database which contains 202599 celebrity images coming from 10177 identities , each annotated with 40 binary attributes and the localization of 5 landmarks ( nose , left and right pupils , mouth corners ) .",0
15198,"In our experiments , we train our models using the train partition that contains 162770 images from 8 k identities .",0
15199,The test partition contains 19962 instances from 1 k identities that are not seen in the train set .,0
15200,The Wider Facial Landmarks in the Wild or WFLW database contains 10000 faces ( 7500 for training and 2500 for testing ) with 98 annotated landmarks .,0
15201,"This database also features rich attribute annotations in terms of occlusion , head pose , make - up , illumination , blur and expressions .",0
15202,"In what follows , we train our models using the train partition of 300W , WFLW and CelebA , and evaluate of the test partition of these datasets .",0
15203,"As in we measure the average point - to - point distance between feature points ( ME ) , normalized by the inter-ocular distance ( distance between outer eye corners on ground truth markup ) .",0
15204,"As there is no consensus on how to measure the error we also report AUC and failure rates fora maximum error of 0.1 , as well as cumulative error distribution ( CED ) curves .",0
15205,Ablation study,0
15206,"In this section , we validate the architecture and hyperparameters of our model : the number of stages S , the number of landmark prediction tasks K , the fusion and task ordering scheme as well as the intermediate supervision weights .",0
15207,"shows CED curves for models with S = 1 , 2 , 3 and 4 cascade stages .",0
15208,"The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .",1
15209,"On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .",0
15210,"shows the interest of chaining multiple tasks , most notably on LFPW , that contains low - resolution images , and IBUG , which contains strong head pose variations as well as occlusions .",0
15211,"Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .",1
15212,This will be discussed more thoroughly in Section 4.4 .,0
15213,"shows a comparison between multiple fusion , task ordering and intermediate supervision weighting schemes .",0
15214,"We test our model on 300W ( full and challenging ) , WFLW ( All and challenging , i.e. pose subset ) as well as CelebA and report the average accuracy on those 5 subsets .",0
15215,"First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .",1
15216,F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .,1
15217,"Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .",1
15218,"Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .",1
15219,"Last but not least , using increasing intermediate supervision weights in Equation ( 10 ) ( i.e. ? 1 = 1 /8 , ? 2 = 1 /4 , ? 3 = 1 /2 , ? 4 = 1 ) is better than both using constant weights ( ? 1 = ? 2 = ? 3 = ? 4 = 1 ) and decreasing weights (? 1 = 1 , ? 2 = 1 /2 , ? 3 = 1 / 4 , ? 4 = 1 / 8 ) , as it enables proper cascade - like training of the network , with the first stage outputting coarser attention maps that can be refined in the latter stages of the network .",0
15220,shows a comparison between DeCaFA and recent state - of - the - art approaches on 300W database .,0
15221,"Our approach performs better than most existing approaches on the common subset , and performs very close to its best contenders on the challenging subset .",0
15222,"Note that DeCaFA trained only on 300 W trainset has a ME of 3.69 % and is already very competitive with recent approaches , thanks to its end - to - end cascade architecture .",0
15223,"DeCaFA is competitive with the best approaches , LAB and DAN - MENPO as well as JMFA - MENPO , which also use external data .",0
15224,shows a comparison between our method and LAB on WFLW database .,0
15225,"As in we report the aver-age point - to - point error on WFLW test partition , normalized by the outer eye corners .",0
15226,"We also report the error on multiple test subsets containing variations in head pose , facial expressions , illumination , make - up as well as partial occlusions and occasional blur .",0
15227,DeCaFA performs better than LAB and Wing by a significant margin on every subset .,0
15228,"Also , note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set , which is still better that these two methods .",0
15229,"Lastly , there is room for improvement on this benchmark as we do not excplicitely handle any of the factors of variation such as pose or occlusions .",0
15230,Comparisons with state - of - the - art methods,0
15231,"Finally , shows a comparison of our method and state - of - the - art approaches on Celeb A .",1
15232,"As in we report the average point - to - point error on the test partition , normalized by the distance between the two eye centers .",0
15233,Our 49.87 5.08 Densereg + MDM 52 . 3.67 JMFA 54.9 1.00 JMFA- MENPO 60.7 0.33 LAB 58.9 0.83 DeCaFA 0.661 0.15 : Comparison with state - of - the - art approaches on CelebA database ( lower is better ) .,0
15234,"Method Mean error ( % ) SDM 4.35 CFSS 3,95 DSRN 3.08 AAN 2.99 DeCaFA 2.10 approach is the best by a significant margin .",0
15235,"Noteworthy , even though we use auxiliary data from 300 W and WFLW , we do not use data from the val partition of CelebA , contrary to , thus there is significant room for improvement .",0
15236,"Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .",1
15237,Also notice that it embraces few parameters ( ?,0
15238,"10M ) compared to state - of - theart approaches , and is also modular : attest time , DeCaFA allows to find a good trade - of between speed and accuracy ( by evaluating only a fraction of the cascade ) , as well as to predict various numbers of landmarks .",0
15239,Weakly supervised learning,0
15240,"In this Section , we study the capability of DeCaFA to learn with few examples annotated with 68 and 98 - landmarks .",0
15241,"To do so , we train DeCaFA using only a small , randomly sampled fraction of 300W ( 100 and 500 images , 3 % and 15 % of trainset ) and WFLW ( 100 and 500 images , 1 % and 6 % of trainset ) and the whole CelebA trainset , and report results on 300 W and WFLW testsets on .",0
15242,Using coarsely annotated data from Celeb,0
15243,"A allows to substantially improve the landmark localization on both datasets , most notably when the number of training images is very low .",0
15244,"For instance , DeCaFA trained with 3 % of 300 W trainset and 1 % of WFLW trainset already outputs decent fine - grained landmark estimations , as it is better than CFSS and DVLN ( , see ) on WFLW .",0
15245,"DeCaFA trained with 15 % of 300 W trainset and 6 % of WFLW trainset is on par with SAN on 300W ( , see ) , and is substantially better than DVLN on WFLW .",0
15246,This indicates that weakly,0
15247,Qualitative results,0
15248,"Image 7 shows vizualisations of aligned facial landmarks on WFLW , I - BUG and CelebA , as well as visualizations of attention maps after the cascade stages 1 and 4 .",0
15249,"Notice how these attention maps are coarse after stage 1 and get refined after stage 4 , better highlighting the individual landmarks .",0
15250,"Also notice that the predicted landmarks are close to the corresponding ground truth , even in the presence of rotations and occlusions ( WFLW ) or facial expressions ( CelebA ) .",0
15251,Conclusion,0
15252,"In this paper , we introduced DeCaFA for face alignment .",0
15253,DeCaFA unifies cascaded regression approaches and an endto - end trainable deep methods .,0
15254,"It s fully - convolutional U - net backbone ensures to keep full spatial resolution throughout the network , and the intermediate supervisions between the cascade stages with increasing weights ensures that the network learns cascaded alignment .",0
15255,"Furthermore , by chaining multiple transfer layers to produce attention maps that correspond to multiple alignment tasks , DeCaFA can benefit from heterogeneous data .",0
15256,"We empirically show that DeCaFA significantly outperforms state - of - the - art approaches on 300W , CelebA and WFLW databases .",0
15257,"In addition , DeCaFA architecture is very modular and is suited for weakly supervised learning using coarsely annotated data with few landmarks .",0
15258,"Future work will consist in integrating other sources of data , or possibly other representations and tasks , such as head pose estimation , partial occlusion handling , as well as facial expressions , Action Unit and / or attributes ( such as age or gender estimation ) recognition within DeCaFA framework .",0
15259,"Furthemore , we will study the application of DeCaFA to closely related fields , such as human pose estimation . :",0
15260,"From left to right : images , attention maps outputted by stages 1 and 4 , alignment results , and ground truth for images from 300W ( I -bug , 68 landmarks ) and WFLW ( 98 landmarks ) .",0
15261,"Notice how the summed attention maps are iteratively refined , and how closely the predicted landmarks usually matches the ground truth , even under difficult illumination , non-frontal head poses , make - up , or occlusions .",0
15262,title,0
15263,Face Alignment Across Large Poses : A 3D Solution,1
15264,abstract,0
15265,"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community .",1
15266,"However , most algorithms are designed for faces in small to medium poses ( below 45 ) , lacking the ability to align faces in large poses up to 90 .",0
15267,The challenges are three - fold :,0
15268,"Firstly , the commonly used landmark - based face model assumes that all the landmarks are visible and is therefore not suitable for profile views .",0
15269,"Secondly , the face appearance varies more dramatically across large poses , ranging from frontal view to profile view .",0
15270,"Thirdly , labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed .",0
15271,"In this paper , we propose a solution to the three problems in an new alignment framework , called 3D Dense Face Alignment ( 3DDFA ) , in which a dense 3 D face model is fitted to the image via convolutional neutral network ( CNN ) .",0
15272,We also propose a method to synthesize large - scale training samples in profile views to solve the third problem of data labelling .,0
15273,Experiments on the challenging AFLW database show that our approach achieves significant improvements over state - of - the - art methods .,0
15274,Introduction,0
15275,"Traditional face alignment aims to locate face fiducial points like "" eye corner "" , "" nose tip "" and "" chin center "" , based on which the face image can be normalized .",0
15276,"It is an essential preprocessing step for many face analysis tasks , e.g. , face recognition , expression recognition and inverse rendering .",0
15277,The researches in face alignment can be divided into two categories : the analysis - by - synthesis based and regression based .,0
15278,The former simulates the process of image generation and achieves alignment by minimizing the difference between model appearance and input image .,0
15279,The latter extracts features around key points and regresses it to the ground truth landmarks .,0
15280,"With the development in the last decade , face alignment across medium poses , where the yaw angle is less than 45 and all the landmarks are visible , has been well addressed .",0
15281,"However , face alignment across large poses ( 90 ) is still a challenging problem without much attention and achievements .",0
15282,There are three main challenges : .,0
15283,Fitting results of 3DDFA .,0
15284,"For each pair of the four results , on the left is the rendering of the fitted 3D shape with the mean texture , which is made transparent to demonstrate the fitting accuracy .",0
15285,"On the right is the landmarks overlayed on the 3D face model , in which the blue / red ones indicate visible / invisible landmarks .",0
15286,The visibility is directly computed from the fitted dense model by .,0
15287,More results are demonstrated in supplemental material .,0
15288,Modelling :,0
15289,Landmark shape model implicitly assumes that each landmark can be robustly detected based on its distinctive visual patterns .,0
15290,"However , when faces deviate from the frontal view , some landmarks become invisible due to self - occlusion .",0
15291,"In medium poses , this problem can be addressed by changing the semantic positions of face contour landmarks to the silhouette , which is termed landmark marching .",0
15292,"However , in large poses where half of face is occluded , some landmarks are inevitably invisible and have no image data .",0
15293,"As a result , the landmark shape model no longer works well .",0
15294,Fitting : Face alignment across large poses is more challenging than medium poses due to the dramatic appearance variations when close to the profile views .,0
15295,The cascaded linear regression or traditional nonlinear models are not sophisticated enough to cover such complicated patterns in a unified way .,0
15296,"The view - based framework , which adopts different landmark configurations and fitting models for each view category , may significantly increase computation cost since every view has to be tested .",0
15297,Data Labelling :,0
15298,The most serious problem comes from the data .,0
15299,Manual labelling landmarks on large - pose faces is a very tedious task .,0
15300,"Firstly , no algorithm can provide a good initialization to reduce the workload .",0
15301,"Secondly , the occluded landmarks have to be "" guessed "" which is impossible for most of people .",0
15302,"As a result , almost all public face alignment databases such as AFW , LFPW , HELEN and IBUG are collected in medium poses .",0
15303,"Existing large - pose databases such as AFLW only contains visible landmarks , which could be ambiguous in invisible landmarks and hard to train a unified face alignment model .",0
15304,"In this paper , we address all the three challenges with the goal of improving the face alignment performance across large poses .",0
15305,To address the problem of invisible landmarks in large,0
15306,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .",1
15307,"By incorporating 3D information , the appearance variations and self - occlusion caused by 3D transformations can be inherently addressed .",0
15308,We call this method 3D Dense Face Alignment ( 3DDFA ) .,1
15309,Some results are shown in .,0
15310,2 .,0
15311,"To resolve the fitting process in 3 DDFA , we propose a cascaded convolutional neutral network ( CNN ) based regression method .",1
15312,CNN has been proved of excellent capability to extract useful information from images with large variations in object detection and image classification .,0
15313,"In this work , we adopt CNN to fit the 3D face model with a specifically designed feature , namely Projected Normalized Coordinate Code ( PNCC ) .",1
15314,"Besides , Weighted Parameter Distance Cost ( WPDC ) is proposed as the cost function .",1
15315,"To the best of our knowledge , this is the first attempt to solve the 3D face alignment with CNN .",1
15316,3 .,0
15317,"To enable the training of the 3DDFA , we construct a face database containing pairs of 2D face images and 3D face models .",1
15318,We further propose a face profiling algorithm to synthesize 60 k + training samples across large poses .,1
15319,The synthesized samples well simulate the face appearances in large poses and boost the performance of both prior and our proposed face alignment algorithms .,1
15320,"The database , face profiling code and 3 DDFA code are released at http://www.cbsr.ia.ac.cn/users / xiangyuzhu/.",1
15321,Related Works,0
15322,Generic Face Alignment :,0
15323,Face alignment in 2D aims at locating a sparse set of fiducial facial landmarks .,0
15324,A number of achievements have been made including the classic Active Appearance Model ( AAM ) and Constrained Local Model ( CLM ) .,0
15325,"Recently , the regression based method , which maps the discriminative features around landmarks to the desired landmark positions , has been proposed .",0
15326,"By utilizing the feedback characteristic that the the output ( landmark positions ) of the regression has an influence on the input ( features at landmarks ) , the cascaded regression cascades a list of weak regressors to reduce the alignment error progressively and reaches the state of the art .",0
15327,"Besides traditional models , convolutional neutral network ( CNN ) has also been employed in face alignment recently .",0
15328,Sun et al.,0
15329,firstly use CNN to regress landmark locations with the raw face image .,0
15330,Liang et al .,0
15331,improve the flexibility by estimating the landmark response map .,0
15332,Zhang et al. further combine face alignment with attribute analysis through multi - task CNN to boost the performance of both tasks .,0
15333,"Although with considerable achievements , most CNN methods only detect a sparse set of landmarks ( 5 points in ) with limited descriptive power of face shape .",0
15334,Large Pose Face Alignment :,0
15335,"Despite the great attentions on face alignment , literature on large - pose scenario is rather limited .",0
15336,"The most common method is the multiview framework , which uses different landmark configurations for different views .",0
15337,"For example , TSPM and CDM employ DPM - like method to align faces with different shape models , among which the highest possibility is chosen as the final result .",0
15338,"However , since every view has to be tested , the computation cost of multi-view method is always high .",0
15339,"Besides 2D methods , 3 D face alignment , which aims to fit a 3D morphable model ( 3 DMM ) from a 2D image , also has the potential to deal with large poses .",0
15340,It models the 3D face shape with a linear subspace ( PCA or Tensor ) and achieves fitting by minimizing the difference between image and model appearance .,0
15341,3 DMM can cover arbitrary poses but suffers from the oneminute - per-image computation cost .,0
15342,"Recently , regression based 3 DMM fitting , which estimates the model parameters by regressing the features at landmark positions , has been proposed to improve the efficiency .",0
15343,"However , since the features at landmarks maybe self - occluded as in 2D methods , the fitting algorithm is no longer poseinvariant and suffers from the three problems in Section 1 .",0
15344,"A relevant but different problem is the 3D face reconstruction , which recovers a 3 D face from given 2D landmarks .",0
15345,"Interestingly , based on that 2D / 3D face alignment results can be mutually transformed , where 3 D to 2D is made by selecting x , y coordinates of landmark vertexes and 2 D to 3D is made by 3 D face reconstruction .",0
15346,3D,0
15347,Dense Face Alignment ( 3DDFA ),0
15348,In this section we introduce the 3D Dense Face Alignment ( 3DDFA ) which fits 3D morphable model with cascaded CNN .,0
15349,3D,0
15350,Morphable Model,0
15351,Blanz et al . propose the 3D morphable model ( 3 DMM ) which describes the 3D face space with PCA :,0
15352,"where S is a 3 D face , S is the mean shape , A id is the principle axes trained on the 3D face scans with neutral expression and ?",0
15353,"id is the shape parameter , A exp is the principle axes trained on the offsets between expression scans and neutral scans and ?",0
15354,exp is the expression parameter .,0
15355,"In this work , the A id and A exp come from BFM and Face - Warehouse respectively .",0
15356,The 3 D face is then projected onto the image plane with Weak Perspective Projection :,0
15357,"where V ( p ) is the model construction and projection function , leading to the 2D positions of model vertexes , f is the scale factor , Pr is the orthographic projection matrix 1 0 0 0 1 0 , R is the rotation matrix constructed from rotation angles pitch , yaw , roll and t 2d is the translation vector .",0
15358,"The collection of all the model parameters is p = [ f , pitch , yaw , roll , t 2 d , ? id , ? exp ] T .",0
15359,Network Structure,0
15360,"The purpose of 3D face alignment is estimating p from a single face image I. Unlike existing CNN methods which apply different networks for different fitting stages , 3 DDFA employ a unified network structure across the cascade .",0
15361,"In general , at iteration k ( k = 0 , 1 , ... , K ) , given an initial parameter pk , we construct a specially designed feature PNCC with pk and train a convolutional neutral network",0
15362,Net k to predict the parameter update ?p k :,0
15363,"Afterwards , a better medium parameter p k+1 = pk + ? p",0
15364,k becomes the input of the next network,0
15365,Net k +1 which has the same structure as Net k .,0
15366,shows the network structure .,0
15367,The input is the 100 100 3 color image stacked by PNCC .,0
15368,"The network contains four convolution layers , three pooling layers and two fully connected layers .",0
15369,The first two convolution layers share weights to extract low - level features .,0
15370,"The last two convolution layers do not share weights to extract location sensitive features , which is further regressed to a 256 - dimensional feature vector .",0
15371,"The output is a 234 - dimensional parameter update including 6 - dimensional pose parameters [ f , pitch , yaw , roll , t 2 dx , t 2 dy ] , 199 - dimensional shape parameters ?",0
15372,id and 29 - dimensional expression parameters ? exp .,0
15373,Projected Normalized Coordinate Code,0
15374,The special structure of the cascaded CNN has three requirements of its input feature :,0
15375,"Firstly , the feedback property requires that the input feature should depend on the CNN output to enable the cascade manner .",0
15376,"Secondly , the convergence property requires that the input feature should reflect the fitting accuracy to make the cascade converge after some iterations .",0
15377,"Finally , the convolvable property requires that the convolution on the input feature should make sense .",0
15378,"Based on the three properties , we design our features as follows :",0
15379,"Firstly , the 3D mean face is normalized to 0 ?",0
15380,"1 in x , y , z axis as Equ .",0
15381,"4 . The unique 3D coordinate of each vertex is called its Normalized Coordinate Code ( NCC ) , see ( a ) .",0
15382,where the S is the mean shape of 3 DMM in Equ .,0
15383,"1 . Since NCC has three channels as RGB , we also show the mean face with NCC as its texture .",0
15384,"Secondly , with a model parameter p , we adopt the Z - Buffer to render the projected 3 D face colored by NCC as in Equ. 5 , which is called the Projected Normalized Coordinate Code ( PNCC ) , see",0
15385,"where Z- Buffer ( ? , ? ) renders an image from the 3D mesh ?",0
15386,colored by ? and V 3 d ( p ) is the current 3 D face .,0
15387,"Afterwards ,",0
15388,Cost Function,0
15389,"The performance of CNN can be greatly impacted by the cost function , which is difficult to design in 3DDFA since each dimension of the CNN output ( model parameter ) has different influence on the 3DDFA result ( fitted 3D face ) .",0
15390,"In this work , we discuss two baselines and propose a novel cost function .",0
15391,"Since the parameter range varies significantly , we conduct z- score normalization before training .",0
15392,Parameter Distance Cost ( PDC ),0
15393,Take the first iteration as an example .,0
15394,The purpose of CNN is predicting the parameter update ?p to move the initial parameter p 0 closer to the ground truth pg .,0
15395,"Intuitively , we can minimize the distance between the ground truth and the current parameter with the Parameter Distance Cost ( PDC ) :",0
15396,"Even though PDC has been used in 3 D face alignment , there is a problem that each dimension in p has different influence on the resultant 3 D face .",0
15397,"For example , with the same deviation , the yaw angle will bring a larger alignment error than a shape PCA coefficient , while PDC optimizes them equally .",0
15398,Vertex Distance Cost ( VDC ),0
15399,"Since 3DDFA aims to morph the 3 DMM to the ground truth 3 D face , we can optimize ?p by minimizing the vertex distances between the fitted and the ground truth 3 D face :",0
15400,where V ( ) is the face construction and weak perspective projection as Equ .,0
15401,2 . This cost is called the Vertex Distance Cost ( VDC ) and the derivative is provided in supplemental material .,0
15402,"Compared with PDC , VDC better models the fitting error by explicitly considering the semantics of each parameter .",0
15403,"However , we observe that VDC exhibits pathological curvature .",0
15404,The directions of pose parameters always exhibit much higher curvatures than the PCA coefficients .,0
15405,"As a result , optimizing VDC with gradient descend converges very slowly due to the "" zig - zagging "" problem .",0
15406,Second - order optimizations are preferred but they are expensive and hard to be implemented on GPU .,0
15407,Weighted Parameter Distance Cost ( WPDC ),0
15408,"In this work , we propose a simple but effective cost function Weighted Parameter Distance Cost ( WPDC ) .",0
15409,The basic idea is explicitly modeling the importance of each parameter :,0
15410,where,0
15411,"Wis the importance matrix whose diagonal is the weight of each parameter , pd ( i ) is the i-deteriorated parameter whose ith component comes from the predicted parameter ( p 0 + ?p ) and the others come from the ground truth parameter pg , V ( p d ( i ) ) ? V ( p g ) models the alignment error brought by miss-predicting the ith model parameter , which is indicative of its importance .",0
15412,"For simplicity , Wis considered as a constant when computing the derivative .",0
15413,"In the training process , CNN firstly concentrate on the parameters with larger V ( p d ( i ) ) ? V ( p g ) such as scale , rotation and translation .",0
15414,"As pd ( i ) is closer top g , the weights of these parameters begin to shrink and CNN will optimize less important parameters but at the same time keep the high - priority parameters sufficiently good .",0
15415,"Compared with VDC , the WPDC remedies the pathological curvature issue and is easier to implement without the derivative of V ( ) .",0
15416,Face Profiling,0
15417,"All the discriminative models rely on the training data , especially for CNN which has thousands of parameters to train .",0
15418,"Therefore , massive labelled faces across large poses are crucial for 3DDFA .",0
15419,"However , few of released face alignment database contains large - pose samples since labelling standardized landmarks on profile is very challenging .",0
15420,"In this section , we demonstrate that labelled profile faces can be well simulated from existing training samples with the help of 3D information .",0
15421,"Inspired by the recent breakthrough in face frontalization which generates the frontal view of faces , we propose to invert this process to generate the profile view of faces from mediumpose samples , which is called face profiling .",0
15422,The basic idea is predicting the depth of face image and generating the profile views with 3D rotation .,0
15423,3D,0
15424,Image Meshing,0
15425,"The depth estimation of a face image can be conducted on the face region and external region respectively , with different requirements of accuracy .",0
15426,"On the face region , we fit a 3 DMM through the Multi - Features Framework ( MFF ) , see .",0
15427,"With the ground truth landmarks as a solid constraint throughout the fitting process , the MFF can always converge to a very good result .",0
15428,Few failed samples can be easily adjusted manually .,0
15429,"On the external region , we follow the 3D meshing method proposed by Zhu et al. to mark some anchors beyond the face region and estimate their depth , see .",0
15430,"Afterwards the whole image is tuned into a 3D object through triangulation , see",0
15431,3D Image Rotation,0
15432,"When the depth information is estimated , the face image can be rotated in 3D space to generate the appearances in larger poses ) .",0
15433,It can be seen that the external face region is necessary fora realistic profile image .,0
15434,"Different from face frontalization , with larger rotation angles the self - occluded region can only be expanded .",0
15435,"As a result , we avoid the troubling invisible region filling which may produce large artifacts .",0
15436,"In this work , we enlarge the yaw of the depth image at the step of 5 until 90 .",0
15437,"Through face profiling , we not only obtain the face appearances in large poses and but also augment the dataset to a large scale , which means the CNN can be well trained even given a small database .",0
15438,Implementation Details,0
15439,Initialization Regeneration,0
15440,"With a huge number of parameters , CNN tends to overfit the training set and the networks at deeper cascade might receive training samples with almost zero errors .",0
15441,Therefore we can not directly adopt the cascade framework as in 2 D face alignment .,0
15442,Asthana et al. demonstrates that the initializations at each iteration can be well simulated with statistics .,0
15443,"In this paper , we also regenerate the pk but with a more sophisticated method .",0
15444,We observe that the fitting error highly depends on the ground truth face posture ( FP ) .,0
15445,"For example , the error of a profile face is mostly caused by a small yaw angle and the error of an open - mouth face is always caused by a close - mouth expression parameter .",0
15446,"As a result , it is reasonable to model the perturbation of a training sample with a set of similar - FP samples .",0
15447,"In this paper , we define the face posture as the ground truth 2 D landmarks without scale and translation : FP = Pr * R g * ( S + A id ? g id + A exp ? g exp ) landmark , ( 9 ) where R g , ? g id , ?",0
15448,"g exp represent the ground truth pose , shape and expression respectively and the subscript landmark means only landmark points are selected .",0
15449,"Before training , we select two folds of samples as the validation set .",0
15450,"For each training sample , we construct a validation subset {v 1 , ... , v m } whose members share similar FP with the training sample .",0
15451,"At iteration k , we regenerate the initial parameter by :",0
15452,"where pk and pg are the initial and ground truth parameter of a training sample , pk viand pg vi come from a validation sample vi which is randomly chosen from the corresponding validation subset .",0
15453,Note that vi is never used in training .,0
15454,Landmark Refinement,0
15455,Dense face alignment method fits all the vertexes of the face model by estimating the model parameters .,0
15456,"If we are only interested in a sparse set of points such as landmarks , the error can be further reduced by relaxing the PCA constraint .",0
15457,"In the 2D face alignment task , after 3DDFA we extract HOG features at landmarks and train a linear regressor to refine the landmark locations .",0
15458,"In fact , 3DDFA can team with any 2 D face alignment methods .",0
15459,"In the experiment , we also report the results refined by SDM .",0
15460,Experiments,0
15461,"In this section , we evaluate the performance of 3DDFA in three common face alignment tasks in the wild , i.e. , medium - pose face alignment , large - pose face alignment and 3D face alignment .",0
15462,"Due to the space constraint , qualitative alignment results are shown in supplemental material .",0
15463,Datasets,0
15464,"Evaluations are conducted with three databases , 300W , AFLW and a specifically constructed AFLW2000 - 3D database .",0
15465,"300 W - LP : 300W standardises multiple alignment databases with 68 landmarks , including AFW , LFPW , HELEN , IBUG and XM2 VTS .",0
15466,"With 300 W , we adopt the proposed face profiling to generate 61,225 samples across large poses ( 1,786 from IBUG , 5,207 from AFW , 16,556 from LFPW and 37,676 from HELEN , XM2 VTS is not used ) , which is further expanded to 122,450 samples with flipping .",0
15467,"We call the database as the 300W across Large Poses ( 300W - LP ) AFLW : AFLW contains 21,080 in - the - wild faces with large - pose variations ( yaw from ?90 to 90 ) .",0
15468,Each image is annotated with up to 21 visible landmarks .,0
15469,The dataset is very suitable for evaluating face alignment performance across large poses .,0
15470,AFLW2000-3D :,0
15471,Evaluating 3 D face alignment in the wild is difficult due to the lack of pairs of 2D image and 3D model in unconstrained environment .,0
15472,"Considering the recent achievements in 3D face reconstruction which can construct a 3 D face from 2D landmarks , we assume that a 3 D model can be accurately fitted if sufficient 2D landmarks are provided .",0
15473,Therefore 3D evaluation can be degraded to 2D evaluation which also makes it possible to compare 3DDFA with other 2 D face alignment methods .,0
15474,"However , AFLW is not suitable for evaluating this task since only visible landmarks lead to serious ambiguity in 3D shape , as reflected by the fake good alignment phenomenon in .",0
15475,"In this work , we construct a database called AFLW2000 - 3 D for 3 D face alignment evaluation , which contains the ground truth 3 D faces and the corresponding 68 landmarks of the first 2,000 AFLW samples .",0
15476,Construction details are provided in supplemental material ..,0
15477,Fake good alignment in AFLW .,0
15478,"For each sample , the first shows the visible 21 landmarks and the second shows all the 68 landmarks .",0
15479,The Normalized Mean Error ( NME ) reflects their accuracy .,0
15480,It can be seen that only evaluating visible landmarks can not well reflect the fitting accuracy .,0
15481,Performance Analysis,0
15482,Error Reduction in Cascade :,1
15483,To analyze the error reduction process in cascade and evaluate the effect of initialization regeneration .,0
15484,"We divide 300W - LP into 97,967 samples for training and 24,483 samples for testing , without identity overlapping .",0
15485,"shows the training and testing errors at each iteration , with and without initialization regeneration .",0
15486,"As observed , the testing error is reduced due to initialization regeneration .",1
15487,In the generic cascade process the training and testing errors converge fast after 2 iterations .,1
15488,"While with initialization regeneration , the training error is updated at the beginning of each iteration and the testing error continues to descend .",1
15489,"During testing , 3 DDFA takes 25.24ms for each iteration , 17.49 ms for PNCC construction on 3.40 GHZ CPU and 7.75 ms for CNN on GTX TITAN Black GPU .",0
15490,Note that the computing time of PNCC can be greatly reduced if Z - Buffer is conducted on GPU .,0
15491,Considering both effectiveness and efficiency we choose 3 iterations in 3DDFA .,0
15492,Performance with Different Costs :,1
15493,"In this experiment , we demonstrate the performance with different costs including PDC , VDC and WPDC .",0
15494,demonstrates the testing errors at each iteration .,0
15495,All the networks are trained until convergence .,0
15496,It is shown that PDC can not well model the fitting error and converges to an unsatisfied result .,1
15497,"VDC is better than PDC , but the pathological curvature problem makes it only concentrate on a small set of parameters , which limits its performance .",0
15498,"WPDC explicitly models the priority of each parameter and adaptively optimizes them with the parameter weights , leading to the best result .",1
15499,Comparison Experiments,0
15500,"In this paper , we test the performance of 3DDFA on three different tasks , including the large - pose face alignment on AFLW , 3 D face alignment on AFLW2000 - 3D and mediumpose face alignment on 300W .",0
15501,Large Pose Face Alignment in AFLW Protocol :,1
15502,"In this experiment , we regard 300 W and 300W - LP as the training set respectively and the whole AFLW as the testing set .",0
15503,The bounding boxes provided by AFLW are used for initialization ( which are not the ground truth ) .,0
15504,"During training , for 2D methods we use the projected 3D landmarks as the ground truth and for 3DDFA we directly regress the 3 DMM parameters .",0
15505,"During testing , we divide the testing set into 3 subsets according to their absolute yaw angles : [ 0 , 30 ] , [ 30 , 60 ] , and [ 60 , 90 ] with 11,596 , 5,457 and 4,027 samples respectively .",0
15506,"The alignment accuracy is evaluated by the Normalized Mean Error ( NME ) , which is the average of visible landmark error normalised by the bounding box size .",0
15507,Note that the metric only considers visible landmarks and is normalized by the bounding box size instead of the common inter-pupil distance .,0
15508,"Besides , we also report the standard deviation across testing subsets which is a good measure of pose robustness .",0
15509,"Methods : Since little experiment has been conducted on AFLW , we choose some baseline methods with released codes , including CDM , RCPR , ESR and SDM .",0
15510,Among them ESR and SDM are popular face alignment methods in recent years .,0
15511,CDM is the first one claimed to perform pose - free face alignment .,0
15512,RCPR is a occlusion - robust method with the potential to deal with selfocclusion and we train it with landmark visibility labels computed by .,0
15513,1 demonstrates the comparison results and shows the corresponding CED curves .,0
15514,Each method is trained on 300 W and 300W - LP respectively to demonstrate the boost from face profiling .,0
15515,"If a trained model is provided in the code , we also demonstrate its performance .",0
15516,"Since CDM only contains testing code , we just report its performance with the provided alignment model .",0
15517,"For 3DDFA which depends on large scales of data , we only report its performance trained on 300W - LP , with the network structure in .",0
15518,Results :,0
15519,"Firstly , the results indicate that all the methods benefits substantially from face profiling when dealing with large poses .",1
15520,"The improvements in [ 60 , 90 ] are 44.06 % for RCPR , 40.36 % for ESR and 42.10 % for SDM .",0
15521,This is especially impressive since the alignment models are trained on the synthesized data and tested on real samples .,0
15522,Thus the fidelity of the face profiling method can be well demonstrated .,0
15523,"Secondly , 3DDFA reaches the state of the art above all the 2D methods especially beyond medium poses .",1
15524,The minimum standard deviation of 3DDFA also demonstrates its robustness to pose variations .,1
15525,"Finally , the performance of 3DDFA can be further improved with the SDM landmark refinement in Section 5.2 .",1
15526,3D Face Alignment in AFLW2000-3D,1
15527,"As described in Section 6.1 , 3 D face alignment evaluation can be degraded to all - landmark evaluation considering both visible and invisible ones .",0
15528,Using AFLW2000-3D as 1 and the CED curves are plot in .,0
15529,We do not report the performance of provided CDM and RCPR models since they do not detect invisible landmarks .,0
15530,"Compared with the results in AFLW , we can seethe defect of barely evaluating visible landmarks .",0
15531,"For all the methods , despite with ground truth bounding boxes the performance in [ 60 , 90 ] and the standard deviation are obviously reduced when considering all the landmarks .",1
15532,"We think for 3 D face alignment which depends on both visible and invisible landmarks , evaluating all the landmarks are necessary .",0
15533,Medium Pose Face Alignment,0
15534,"Even though not aimed at advancing face alignment in medium poses , we are also interested in the performance of 3DDFA in this popular task .",0
15535,"The experiments are conducted on 300 W following the common protocol in , where we use the training part of LFPW , HELEN and the whole AFW for training images and 50,521 after augmentation ) , and perform testing on three parts : the test samples from LFPW and HELEN as the common subset , the 135 - image IBUG as the challenging subset , and the union of them as the full set ( 689 images in total ) .",0
15536,The alignment accuracy are evaluated by standard landmark mean error normalised by the inter-pupil distance ( NME ) .,0
15537,It can be seen in .,0
15538,"The NME ( % ) of face alignment results on 300W , with the first and the second best results highlighted .",0
15539,Method,0
15540,Common Challenging Full TSPM,1
15541,"8 that even as a generic face alignment algorithm , 3 DDFA still demonstrates competitive performance on the common set and state - of - the - art performance on the challenging set .",1
15542,Conclusions,0
15543,"In this paper , we propose a novel method , 3D Dense Face Alignment ( 3DDFA ) , which well solves the problem of face alignment across large poses .",0
15544,"Different from the traditional landmark detection framework , 3DDFA fits a dense 3D morphable model with cascaded CNN to solve the selfocclusion in modelling and the high nonlinearity in fitting in large poses .",0
15545,"We also propose a face profiling algorithm to synthesize face appearances in profile view , which can provide abundant samples for training .",0
15546,"Experiments show the state - of - the - art performance in AFLW , AFLW2000 - 3 D and 300W .",0
15547,"In future work , we believe that 3 DDFA can be further improved with more complicated network architecture , like larger input size and deeper network .",0
15548,title,0
15549,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,1
15550,abstract,0
15551,Facial alignment involves finding a set of landmark points on an image with a known semantic meaning .,0
15552,"However , this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes .",0
15553,"In order to extract consistent alignment points across large poses , the 3D structure of the face must be considered in the alignment step .",0
15554,"However , extracting a 3D structure from a single 2D image usually requires alignment in the first place .",0
15555,We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network ( 3DSTN ) to model both the camera projection matrix and the warping parameters of a 3D model .,0
15556,"By utilizing a generic 3D model and a Thin Plate Spline ( TPS ) warping function , we are able to generate subject specific 3 D shapes without the need fora large 3D shape basis .",0
15557,"In addition , our proposed network can be trained in an end - to - end framework on entirely synthetic data from the 300W - LP dataset .",0
15558,"Unlike other 3D methods , our approach only requires one pass through the network resulting in a faster than realtime alignment .",0
15559,Evaluations of our model on the Annotated Facial Landmarks in the Wild ( AFLW ) and AFLW2000 - 3D datasets show our method achieves state - of - the - art performance over other 3D approaches to alignment .,0
15560,Introduction,0
15561,Robust face recognition and analysis are contingent upon accurate localization of facial features .,1
15562,"When modeling faces , the landmark points of interest consist of points that lie along the shape boundaries of facial features , e.g. eyes , lips , mouth , etc .",0
15563,"When dealing with face images collected in the wild conditions , facial occlusion of landmarks becomes a common problem for off - angle faces .",0
15564,Predicting the occlusion state of each landmarking points is one of the .,0
15565,A subject from the CMU Multi - PIE dataset landmarked and frontalized by our method at various poses .,0
15566,Landmarks found by our model are overlaid in green if they are determined to be a visible landmark and blue if self - occluded .,0
15567,The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape .,0
15568,Best viewed in color .,0
15569,"challenges due to variations of objects in faces , e.g. beards and mustaches , sunglasses and other noisy objects .",0
15570,"Additionally , face images of interest nowadays usually contain off - angle poses , illumination variations , low resolutions , and partial occlusions .",0
15571,Many complex factors could affect the appearance of a face image in real - world scenarios and providing tolerance to these factors is the main challenge for researchers .,0
15572,"Among these factors , pose is often the most important factor to be dealt with .",0
15573,"It is known that as facial pose deviates from a frontal view , most face recognition systems have difficulty in performing robustly .",0
15574,"In order to handle a wide range of pose changes , it becomes necessary to utilize 3D structural information of faces .",0
15575,"However , many of the existing 3 D face modeling schemes have many drawbacks , such as computation time and complexity .",0
15576,"Though these can be mitigated by using depth sensors or by tracking results from frame to frame in video , this can cause difficulty when they have to be applied in real - world large scale unconstrained face recognition scenarios where video and depth information is not available .",0
15577,The 3D generic elastic model ( 3D - GEM ) approach was proposed as an efficient and reliable 3D modeling method from a single 2D image .,0
15578,Heo et al.,0
15579,claim that the depth information of a face is not extremely discriminative when factoring out the 2D spatial location of facial features .,0
15580,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .",1
15581,"We take the approach of using a simple mean shape and using a parametric , non-linear warping of that shape through alignment on the image to be able to model any unseen example .",0
15582,A key flaw in many approaches that rely on a 3D Morphable Model ( 3 DMM ) is that it needs enough examples of the data to be able to model unseen samples .,0
15583,"However , in the case of 3D faces , most datasets are very small .",0
15584,Our Contributions in this Work,0
15585,( 2 ) Our approach is efficiently implemented in an endto - end deep learning framework allowing for the alignment and 3D modeling tasks to be codependent .,0
15586,This ensures that alignment points are semantically consistent across changing poses of the object which also allows for more consistent 3 D model generation and frontalization on images in the wild as shown in Figs. 1 and 2 .,0
15587,( 3 ) Our method only requires a single pass through the network allowing us to achieve faster than real - time processing of images with state - of - the - art performance over other 2D and 3D approaches to alignment .,0
15588,Related Work,0
15589,There have been numerous studies related to face alignment since the first work of Active Shape Models ( ASM ) in 1995 .,0
15590,A comprehensive literature review in face alignment is beyond the scope of this work .,0
15591,"In this paper , we mainly focus on recent Convolutional Neural Network ( CNN ) approaches to solve the face alignment problem .",0
15592,Especially those methods aimed at using 3D approaches to achieve robust alignment results .,0
15593,Face Alignment,0
15594,Methods,0
15595,"While Principal Component Analysis ( PCA ) and its variants were successfully used to model the facial shapes and appearances , there have since been many advances in facial alignment .",0
15596,Landmark locations can be directly predicted by a regression from a learned feature space .,0
15597,Xiong et al. presented the Global Supervised Descent Method ( GSDM ) method to solve the problem of 2 D face alignment .,0
15598,The objective function in GSDM is divided into multiple regions of similar gradient directions .,0
15599,It then constructs a separate cascaded shape regressor for each region .,0
15600,Yu et al . incorporated 3 D pose landmarking models with group sparsity to indicate the best landmarks .,0
15601,These kind of methods shows an increase of performance on landmark localization .,0
15602,"However , these methods all rely on hand - crafted features .",0
15603,"Recently , CNN - based methods have achieved good results in facial alignment .",0
15604,3DDFA fits a dense 3 D face model to the image via CNN and DDN proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects .,0
15605,"Recently , shape regression has been used in numerous facial landmarking methods .",0
15606,"There are several recent works studying the human head rotations , nonlinear statistical models ( ) and 3D shape models .",0
15607,Nonlinear statistical model approaches are impractical in real - time applications .,0
15608,Viewbased methods employ a separate model for each viewpoint mode .,0
15609,"Traditionally , the modes are specified as part of the algorithm design , and problems can arise at midpoints between models .",0
15610,CNNs for 3D Object Modeling,0
15611,"While estimating a 3D model from images is not anew problem , the challenging task of modeling objects from a single image has always posed a challenge .",0
15612,"This is , of course , due to the ambiguous nature of images where depth information is removed .",0
15613,"With the recent success of deep learning and especially CNNs in extracting salient information from images , there have been many explorations into how to best use CNNs for modeling objects in 3 dimensions .",0
15614,Many of these approaches are aimed creating a depth estimation for natural images .,0
15615,"While the results on uncontrolled images are impressive , the fact that these models are very general means they tend to suffer when applied to specific objects , such as faces .",0
15616,"In fact , many times , the depth estimate for faces in the scene tend to be fairly flat .",0
15617,"By limiting the scope of the method , the resulting estimated 3 D model can be made much more accurate .",0
15618,Hassner et al .,0
15619,use a 3 D model of the face to be able to frontalize faces in unseen images with the end goal of improving face recognition by limiting the variations the matcher has to learn .,0
15620,"However , this approach requires landmarks on the input face in the same fashion as other methods .",0
15621,A 2D approach to landmarking inevitably suffers from the problem of visibility and self - occlusion .,0
15622,As Zhu et al.,0
15623,"show , the problem of landmark marching , where landmarks tend to move to the visible boundary , can cause issues when estimating 3 D models from purely 2D alignment .",0
15624,"However , this problem can be alleviated by using a 3 D model of the face in the alignment step itself as done in .",0
15625,Both of these methods make use of an underlying 3D Morphable Model ( 3 DMM ) and try to fit the model to the input image in order to find the 2D landmarks .,0
15626,This of course requires a basis to use and the Basel Face Model ( BFM ) is a very popular model to use .,0
15627,"However , the BFM is only created from a set of 100 male and 100 female scans .",0
15628,"As any basis can only recreate combinations of the underlying samples , this can severely limit the capability of these models to fit outlier faces or expressions not seen before .",0
15629,"Although there has been recent efforts to generate more accurate 3 DMMs , neither the data nor the model is available to researchers in the field of biometrics .",0
15630,"Therefore , we propose to use a smooth warping function , Thin Plate Splines ( TPS ) , to warp mean shapes to fit the input image and generate new 3D shapes .",0
15631,"In this fashion , any new face can be modeled , even if it s shape can not be reconstructed by the BFM .",0
15632,3D Spatial Transformer Networks,0
15633,"In order to model how a face truly changes from viewpoint to viewpoint , it is necessary to have both the true 3 D model of the subject in the image and the properties of the camera used to capture the image , usually in the form of the camera projection matrix .",0
15634,"However , knowledge of the true 3D model and the camera projection matrix are almost always not available .",0
15635,"Jaderberg et al. , in their work on Spatial Transformer Networks , use a deep network to estimate the parameters of either an affine transformation or a 2D Thin Plate Spline ( TPS ) transformation .",0
15636,These parameters are then used to generate anew sampling grid which can then be used to generate the transformed image .,0
15637,We approach finding the unknown camera projection matrix parameters and the parameters needed to generate the 3D model of the head in a similar fashion .,0
15638,"Both the camera projection parameters and the warping parameters , a TPS warp in this case , can be estimated from deep features generated from the image using any architecture .",0
15639,The TPS parameters can be used to warp a model of the face to match what the network estimates the true 3D shape is and the camera projection parameters can be used to texture the 3D coordinates from the 2D image .,0
15640,"Additionally , the pose of the face can be determined from the camera parameters allowing fora visibility map to be generated for the 3D model .",0
15641,This allows us to only texture vertexes that are visible in the image as opposed to vertexes that are occluded by the face itself .,0
15642,The architecture of our model is shown in .,0
15643,"Sections 3.1,3.2 , and 3.3 detail how to create differentiable modules to utilize the camera projection and TPS parameters that are estimated by the deep network to warp and project a 3 D model to a 2D image plane for texture sampling .",0
15644,Camera Projection Transformers,0
15645,"In order to be able to perform end - to - end training of a network designed to model 3D transformations of the face , a differentiable module that performs a camera projection must be created .",0
15646,This will be part of the grid generator portion of the Spatial Transformer .,0
15647,Modeling how a 3D point will map to the camera coordinates is expressed by the well known camera projection equation,0
15648,"where p c is the homogeneous 2 D point in the camera coordinate system , p w is the homogeneous 3 D point in the world coordinate system , and M is the 3x4 camera projection matrix .",0
15649,"This relationship is only defined up to scale due to the ambiguity of scale present in projective geometry , hence the = instead of a hard equality .",0
15650,The camera projection matrix has only 11 degrees of freedom since it is only defined up to scale as well .,0
15651,"Therefore , this module takes in the 11 parameters estimated by a previous layer as the input in the form of a length 11 vector , a.",0
15652,"In order to perform backpropogation on the new grid generator , the derivative of the generated grid with respect to a must be computed .",0
15653,Since Eqn.,0
15654,"1 is only defined up to scale , the final output of this module will have to divide out the scale factor .",0
15655,By first rewriting the camera projection matrix as M = ? ? a 1 a 2 a 3 a 4 a 5 a 6 a 7 a 8 a 9 a 10 a 11 1,0
15656,"where a i is the i th element of a , the final output of the camera projection module can be written as",0
15657,The gradient with respect to each of the rows of M can be shown to be,0
15658,"Using the chain rule , the gradient of the loss of the network with respect to the input can be found as",0
15659,Since,0
15660,"M is only defined up to scale , the last element of M can be defined to be a constant which means that only the first 11 elements of this gradient are used to actually perform the backpropogation on a .",0
15661,Since,0
15662,"M relates many pairs of 2D and 3D points , the gradient is computed for every pair and added together to give the final gradient that is used for updating a .",0
15663,3D,0
15664,Thin Plate Spline Transformers,0
15665,"When modeling the 3D structure of a face , a generic model can not represent the variety of shapes that might be seen in an image .",0
15666,"Therefore , some method of warping a model must be used to allow the method to handle unseen shapes .",0
15667,Thin Plate Spline ( TPS ) warping has been used by many applications to great effect .,0
15668,"TPS warps have the very dersirable features of providing a closed form of a smooth , parameterized warping given a set of control points and desired destination points .",0
15669,Jaderberg et al .,0
15670,showed how 2D TPS Spatial Transformers could lead to good normalization of nonlinearly transformed input images .,0
15671,Applying a TPS to a 3 D set of points follows a very similar process .,0
15672,"As in , the TPS parameters would be estimated from a deep network of some sort and passed as input to a 3 D grid generator module .",0
15673,A 3D TPS function is of the form,0
15674,", and w jx are the parameters of the function , c j is the j th control point used in determining the function parameters , and U ( r ) = r 2 log r.",0
15675,"This function is normally learned by setting up a system of linear equations using the known control points , c j and the corresponding points in the warped 3D object .",0
15676,"The function finds the change in a single coordinate , the change in the x - coordinate in the case of Eqn.",0
15677,"6 . Similarly , one such function is created for each dimension , i.e. f ?x ( x , y , z ) , f ?y ( x , y , z ) , and f ? z ( x , y , z ) .",0
15678,The 3D TPS module would then take in the parameters for all three of these functions as input and output the newly transformed points on a 3D structure as,0
15679,This means that the 3D TPS module must have all of the 3D vertices of the generic model and the control points on the generic model as fixed parameters specified from the start .,0
15680,This will allow the module to warp the specified model by the warps specified by the TPS parameters .,0
15681,"As in 3.1 , the gradient of the loss with respect to the input parameters must be computed in order to perform backpropogation on this module .",0
15682,"As usual , the chain rule can be used to find this by computing the gradient of the output with respect to the input parameters .",0
15683,"Since each 3 D vertex in the generic model will give one 3 D vertex as an output , it is easier to compute the gradient on one of these points , pi = ( x i , y i , z i ) , first .",0
15684,This can be shown to be,0
15685,where ? ?x are the parameters off ?x .,0
15686,"Similarly , the gradients for ? ?y and ? ? z are the same with only the non-zeros values in either the second or third row respectively .",0
15687,The final gradient of the loss with respect to the parameters can be computed as,0
15688,"Since this is only fora single point , once again the gradient can be computed for every point and added for each set of parameters to get the final gradient for each set of parameters that can be used to update previous layers of the network .",0
15689,Warped Camera Projection Transformers,0
15690,"In order to make use of the TPS warped 3D points in the camera projection module of the transformer network , the module must take in as input the warped coordinates .",0
15691,This means that such a module would also have to do backpropogation on the 3D coordinates as well as the camera projection parameters .,0
15692,"Since 3.1 already specified how to compute the gradient of the loss with respect to the camera projection parameters , all that is left to do is compute the gradient of the loss with respect to the 3D coordinates in this module .",0
15693,"Taking the derivative of the output in Eqn. 3 with respect to the 3D point , p w results in",0
15694,"However , since p w is in homogeneous coordinates and only the gradient with respect to the x , y , and z coordinates are needed , the actual gradient becomes",0
15695,where,0
15696,and m ij is the j th element of mi .,0
15697,This gradient is computed for every 3 D point independently and used in the chain rule to compute,0
15698,which can then be used to perform backpropogation on each p w .,0
15699,2D Landmark Regression,0
15700,"In order to further improve the landmark accuracy , we extend our network with a landmark refinement stage .",0
15701,This stage treats the projected 2D coordinates from the previous stage as initial points and estimates the offsets for each point .,0
15702,"To extract the feature vector for each point , a 3 3 convolution layer is attached on top of the last convolution layer in the base model , followed by a 1 1 convolution layer for more nonlinearity , resulting in a feature map with D channels .",0
15703,Then each initial point is projected onto this feature map and its D-dimensional feature vector is extracted along the channel direction .,0
15704,Notice that the initial points are often not aligned with the grids on the feature map .,0
15705,"Therefore , their feature vectors are sampled with bilinear interpolation .",0
15706,"Given the feature vector for each landmark , it goes through a fully - connected ( FC ) layer to output the offsets , i.e. ? x and ? y .",0
15707,Then the offsets are added to the coordinates of the initial location .,0
15708,For each landmark we use an independent FC layer .,0
15709,We do n't share the FC layer for all landmarks because each landmark should have a unique behavior of offsets .,0
15710,"For example , the center of the eye may move left after regression whereas the corner of the eye may move right .",0
15711,"Also , sometimes two initial landmarks maybe projected to the same location due to a certain pose .",0
15712,We want them to move to different locations even when they have the same feature vector .,0
15713,3D Model Regression From 2D Landmarks,0
15714,"Once the 2D regression is performed , the mapping between the 3D model and the 2D landmarks is broken .",0
15715,"While this is not necessarily a problem in the case of sparse facial alignment , if a denser scheme is needed , the entire model would have to be retrained .",0
15716,"In order to avoid this , we create anew 3 D model that does map to these 2D landmarks by finding anew set of 3D coordinates that project to the new 2D landmarks and warping the 3D model to fit these new points .",0
15717,"To find the new 3D coordinates , we need to backproject rays through each of the 2D landmarks through 3D space using the camera projection matrix we have estimated .",0
15718,"The equation for the ray of points associated with a given homogeneous 2 D point , pi 2 D , is defined as",0
15719,where A and bare the first three and the last column of the estimated camera projection matrix respectively .,0
15720,These rays represent all possible points in 3D that could project to the determined locations in the image .,0
15721,"We then find the closest point , pi 3 D , on the ray to the original 3D coordinate , pi 3 D , to use as the new 3 D point as shown in .",0
15722,These new correspondences are used to perform a TPS warping of the model .,0
15723,"After this warping , the landmark points on the model will project to exactly the regressed 2D landmarks , recovering the mapping between the 3D model and the 2D image .",0
15724,This new model can then be projected onto the image to generate a much more accurate texturing of the 3D model .,0
15725,This same style of warping can be used to move the 3D coordinates anywhere we choose .,0
15726,"This means neutralizing out expressions , especially smiles , is very easy to do by using the texture from the regressed 3D shape .",0
15727,"While the non-smiling shape will not be as accurate due to the fact that a non-smiling image was not seen , it still gives convincing qualitative results , as seen in , which indicate it maybe a worthwhile avenue of exploration for future work , especially in face recognition .",0
15728,Experiments,0
15729,Datasets,0
15730,300 W - LP :,0
15731,"The 300W - LP dataset contains 122,450 synthetically generated views of faces from the AFW , LFPW , HELEN , and IBUG datasets .",0
15732,"These images not only contain rotated faces but also attempt to move the background in a convincing fashion , making it a The resulting regressed 3D model ( green box ) maintains the smile and is very similar to the input image while the same texture applied to the original shape ( red box ) suffers a small degradation in shape but allows fora non-smiling rendering of the input image .",0
15733,very useful dataset for training 3 D approaches to work on real world images .,0
15734,AFLW : The Annotated Facial Landmarks in the Wild ( AFLW ) dataset is a relatively large dataset for evaluating facial alignment on wild images .,0
15735,"It contains approximately 25,000 faces annotated with 21 landmarks with visibility labels .",0
15736,"The dataset provides pose estimates so results are grouped into three different pose ranges , [ 0 , 30 ] , ( 30 , 60 ] , and ( 60 , 90 ] .",0
15737,"Due to the inconsistency in the bounding boxes in the AFLW dataset , we adopt the use of a face detector first to normalize the scale of the faces .",0
15738,The Multiple Scale Faster Region - based CNN approach has shown good results and at a fast speed .,0
15739,"We use the recent extension to this work , the Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) approach to perform the face detection in any experiment where face detection is needed .",0
15740,"The CMS - RCNN approach detects 98.8 % ( 13,865 ) , 95.9 % ( 5,710 ) , and 86.5 % of the faces in the [ 0 , 30 ] , ( 30 , 60 ] , and ( 60 , 90 ] pose ranges respectively .",0
15741,AFLW2000 -3D :,0
15742,Zhu et al .,0
15743,accurately pointed out how merely evaluating an alignment scheme on the visible landmarks in a dataset can result in artificially low errors .,0
15744,"Therefore , a true evaluation of any 3D alignment method must also evaluate alignment on the non-visible landmarks as well .",0
15745,The AFLW2000 - 3D dataset contains the first 2000 images of the AFLW dataset but with all 68 points defined by the scheme in the CMU MPIE dataset .,0
15746,These points were found by aligning the Basel Face Model to the images .,0
15747,"While this is a synthetic dataset , meaning the true location of the non-visible landmarks is not known , it is the best one can do when dealing with real images .",0
15748,"As these images are from the AFLW dataset , they are also grouped into the same pose ranges .",0
15749,Implementation Details,0
15750,Our network is implemented in the Caffe framework .,1
15751,"A new layer is created consisting of the 3D TPS transformation module , the camera projection module and the bilinear sampler module .",1
15752,All modules are differentiable so that the whole network can be trained end - to - end .,0
15753,"We adopt two architectures , AlexNet and VGG - 16 , as the pre-trained models for our shared feature extraction networks in , i.e. we use the convolution layers from the pre-trained models to initialize ours .",1
15754,"Since these networks already extract informative low - level features and we do not want to lose this information , we freeze some of the earlier convolution layers and finetune the rest .",0
15755,"For the AlexNet architecture , we freeze the first layer while for the VGG - 16 architecture , the first 4 layers are frozen .",1
15756,The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer .,1
15757,"With N landmarks to regress , we need NFC layers to compute the offsets for each individual landmark .",0
15758,"While it 's possible to setup N individual FC layers , here we implement this by adding one Scaling layer followed by a Reduction layer and Bias layer .",0
15759,During training only the new layers are updated and all previous layers are frozen .,0
15760,Training on 300W - LP,0
15761,"When training our model , we train on the AFW , HE - LEN , and LFPW subsets of the 300W - LP dataset and use the IBUG portion as a validation set .",0
15762,All sets are normalized using the bounding boxes from the CMS - RCNN detector by reshaping the detected faces to 250 x 250 pixels .,0
15763,"For the AlexNet architecture , we train for 100,000 iterations with a batch size of 50 .",0
15764,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 50,000 iterations .",0
15765,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 40,000 iterations .",0
15766,"For the VGG - 16 architecture , we train for 200,000 iterations with a batch size of 25 .",0
15767,"The initial learning rate is set to 0.001 and drops by a factor of 2 after 100,000 iterations .",0
15768,"When training the landmark regression , the initial learning rate is 0.01 and drops by a factor of 10 every 70,000 iterations .",0
15769,The momentum for all experiments is set to 0.9 .,0
15770,"Euclidean loss is applied to 3D vertexes , 2D projected landmarks and 2D",0
15771,Ablation Experiments,0
15772,"To investigate the effect of each component in our network , we conduct two ablation studies .",0
15773,All the models in these experiments are trained on the same 300W LP dataset and tested on the detected images in AFLW .,0
15774,We first test the effect of the different pre-trained models .,0
15775,We fine - tune our network from the AlexNet and VGG - 16 models pretrained on the ImageNet dataset and evaluate the landmark accuracy before the regression step .,0
15776,The VGG - 16 model outperforms the AlexNet model in all three pose ranges on the AFLW detected set as shown in .,0
15777,This seems to indicate that a good base model is important for the parameter estimation portion of the network .,0
15778,"Second , we evaluate the effect of landmark regression stage .",0
15779,We compare the errors between the regressed and projected landmarks .,0
15780,shows that the landmark regression step greatly helps to improve the accuracy .,0
15781,Comparison Experiments,0
15782,"AFLW : Since the CMS - RCNN approach may only detect the easier to landmark faces , we use the provided bounding box anytime the face is not detected by the detector .",0
15783,"Due to the inconsistency between the two bounding box schemes , faces are not always normalized properly .",0
15784,"However , we feel this is the only way to get a fair comparison to other methods without artificially making the dataset easier by only evaluating on detected faces .",0
15785,"We compare against baseline methods used by on the same dataset , namely Cascaded Deformable Shape Models ( CDM ) , Robust Cascaded Pose Regression ( RCPR ) , Explicit Shape Regression ( ESR ) , SDM and 3DDFA .",0
15786,All methods except for CDM were retrained on the 300W - LP dataset .,0
15787,The Normalized Mean Error ( NME ) is computed by averaging the error of the visible landmarks and normalizing it by the square root of the bounding box size ( h x w) provided in the dataset .,0
15788,"clearly shows that our model using the VGG - 16 architecture has achieved better accuracy in all pose ranges , especially the ( 60 , 90 ] category , and has achieved a smaller standard deviation in the error .",0
15789,"This means that not only are the landmarks more accurate , they are more consistent than the other methods ..",0
15790,CED curves for both the AlexNet ( red ) and VGG - 16 ( green ) architectures on both the AFLW ( left ) and AFLW2000 - 3D ( right ) dataset .,0
15791,"To balance the distributions , we randomly sample 13,209 faces from AFLW and 915 faces from AFLW2000 - 3 D , split evenly among the 3 categories , and compute the CED curve .",0
15792,This is done 10 times and the average of the resulting CED curves are reported .,0
15793,The mean NME % for each architecture from is also reported in the legend .,0
15794,AFLW2000 - 3D :,0
15795,The baseline methods were evaluated using the bounding box of the 68 landmarks so we retrained our models using the same bounding box on the training data .,0
15796,Generating these is trivial due to the 3D models .,0
15797,The NME is computed using the bounding box size .,0
15798,"Here we see that though 3DDFA + SDM performs well , the VGG - 16 architecture of our model still performs best in both the [ 0 , 30 ] and ( 60 , 90 ] ranges .",0
15799,"While the VGG - 16 model is only second best in the ( 30 , 60 ] range by a small amount , the improvement in ( 60 , 90 ] means that , once again , our method generates more accurate and more consistent landmarks , even in a 3D sense .",0
15800,Cumulative Error Distribution ( CED ) curves are reported for both architectures on both datasets in .,0
15801,Running Speed,0
15802,"In order to evaluate the speed of our method , we evaluate the models on a random subset of 1200 faces from the AFLW subset split evenly into the [ 0 , 30 ] , ( 30 , 60 ] , and ( 60 , 90 ] pose ranges .",0
15803,The images are processed one at a time to avoid any benefit from batch processing .,0
15804,The models are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and an NVIDIA GeForce GTX TITAN X GPU .,0
15805,Our AlexNet trained model takes a total of 7.064 seconds to landmark the 1200 faces for an average of 0.0059 seconds per image or approximately 170 faces per second .,0
15806,The deeper and more accurate VGG - 16 model landmarks the 1200 faces in 22.765 seconds for an average of 0.0190 seconds or approximately 52 faces per second .,0
15807,"In comparison , the 3DDFA approach takes 75.72 ms ( 3 iterations at 25.24 ms per iteration as specified in ) with 2 / 3 of the time being used to process data on the CPU .",0
15808,Conclusions,0
15809,In this paper we propose a method using 3D Spatial Transformer Networks with TPS warping to generate both a 3 D model of the face and accurate 2D landmarks across large pose variation .,0
15810,The limited data used in the generation of a 3 DMM can mean that unseen face shapes can not be modeled .,0
15811,"By using a TPS warp , any potential face can be modeled through a regression of 2D landmarks , of which there is much more data available .",0
15812,We have shown how this approach leads to more accurate and consistent landmarks over other 2D and 3D methods .,0
15813,title,0
15814,Deep Alignment Network : A convolutional neural network for robust face alignment,1
15815,abstract,0
15816,"In this paper , we propose Deep Alignment Network ( DAN ) , a robust face alignment method based on a deep neural network architecture .",0
15817,"DAN consists of multiple stages , where each stage improves the locations of the facial landmarks estimated by the previous stage .",0
15818,"Our method uses entire face images at all stages , contrary to the recently proposed face alignment methods that rely on local patches .",0
15819,This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm .,0
15820,The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations .,0
15821,An extensive evaluation on two publicly available datasets shows that DAN reduces the state - of - theart failure rate by up to 70 % .,0
15822,Our method has also been submitted for evaluation as part of the Menpo challenge .,0
15823,Introduction,0
15824,"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face .",1
15825,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",0
15826,Most of the face alignment methods introduced in the recent years are based on shape indexed features .,0
15827,"In these approaches image features , such as SIFT or learned features , are extracted from image patches extracted around each of the landmarks .",0
15828,The features are then used to iteratively refine the estimates of landmark locations .,0
15829,"While those approaches can be successfully applied to face alignment in many photos , their performance on the most challenging datasets leaves room for improvement .",0
15830,We believe that this is due to the fact that for the most difficult images the features extracted at disjoint patches do not provide enough information and can lead the method into a local minimum .,0
15831,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",1
15832,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .",1
15833,The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,1
15834,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",1
15835,A landmark heatmap is an image with high intensity values around landmark locations where intensity decreases with the distance from the nearest landmark .,0
15836,The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them .,0
15837,An example of a landmark heatmap can be seen in which shows an outline of our method .,0
15838,"By using landmark heatmaps , our DAN algorithm is able to reduce the failure rate on the 300W public test set by a large margin of 72 % with respect to the state of the art .",0
15839,"To summarize , the three main contributions of this work are the following :",0
15840,1 .,0
15841,We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method .,0
15842,"This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .",0
15843,2 .,0
15844,The resulting robust face alignment method we propose in this paper reduces the failure rate by 60 % on .,0
15845,A diagram showing an outline of the proposed method .,0
15846,"Each stage of the neural network refines the landmark location estimates produced by the previous stage , starting with an initial estimate S0 .",0
15847,The connection layers form a link between the consecutive stages of the network by producing the landmark heatmaps,0
15848,"Ht , feature images",0
15849,Ft and a transform,0
15850,Tt which is used to warp the input image to a canonical pose .,0
15851,"By introducing landmark heatmaps and feature images we can transmit crucial information , including the landmark location estimates , between the stages of our method .",0
15852,the 300W private test set and 72 % on the 300 - W public test set compared to the state of the art .,0
15853,3 .,0
15854,"Finally , we publish both the source code of our implementation of the proposed method and the models used in the experiments .",0
15855,The remainder of the paper is organized in the following manner .,0
15856,In section 2 we give an overview of the related work .,0
15857,In section 3 we provide a detailed description of the proposed method .,0
15858,"Finally , in section 4 we perform an evaluation of DAN and compare it to the state of the art .",0
15859,Related work,0
15860,"Face alignment has along history , starting with the early Active Appearance Models , moving to Constrained Local Models and recently shifting to methods based on Cascaded Shape Regression ( CSR ) and deep learning .",0
15861,"In CSR based methods , the face alignment begins with an initial estimate of the landmark locations which is then refined in an iterative manner .",0
15862,The initial shape S 0 is typically an average face shape placed in the bounding box returned by the face detector .,0
15863,Each CSR iteration is characterized by the following equation :,0
15864,where,0
15865,"St is the estimate of landmark locations at iteration t , rt is a regression function which returns the update to St given a feature ?",0
15866,extracted from image I at the landmark locations .,0
15867,The main differences between the variety of CSR based methods introduced in the literature lie in the choice of the feature extraction method ?,0
15868,and the regression method rt .,0
15869,"For instance , Supervised Descent Method ( SDM ) uses SIFT features and a simple linear regressor .",0
15870,LBF takes advantage of sparse features generated from binary trees and intensity differences of individual pixels .,0
15871,"LBF uses Support Vector Regression for regression which , combined with the sparse features , leads to a very efficient method running at up to 3000 fps .",0
15872,"Coarse to Fine Shape Searching ( CFSS ) , similarly to SDM , uses SIFT features extracted at landmark locations .",0
15873,However the regression step of CSR is replaced with a search over the space of possible face shapes which goes from coarse to fine over several iterations .,0
15874,This reduces the probability of falling into a local minimum and thus improves convergence .,0
15875,"MIX also uses SIFT for feature extraction , while regression is performed using a mixture of experts , where each expert is specialized in a certain part of the space of face shapes .",0
15876,"Moreover MIX , warps the input image before each iteration so that the current estimate of the face shape matches a predefined canonical face shape .",0
15877,Mnemonic Descent Method ( MDM ) fuses the feature extraction and regression steps of CSR into a single Recurrent Neural Network that is trained end - to - end .,0
15878,MDM also introduces memory into the process which allows information to be passed between CSR iterations .,0
15879,"While all of the above mentioned methods perform face alignment based only on local patches , there are some methods that estimate initial landmark positions using the entire face image and use local patches for refinement .",0
15880,"In contrast , DAN localizes the landmarks based on the entire face image at all of its stages .",0
15881,The use of heatmaps for face alignment related tasks precedes the proposed method .,0
15882,"One method that uses heatmaps is , where a neural network outputs predictions in the form of a heatmap .",0
15883,"In contrast , the proposed method uses heatmaps solely as a means for transferring information between stages .",0
15884,The development of novel methods contributes greatly in advancing face alignment .,0
15885,However it can not be overlooked that the publication of several large scale datasets of annotated face images also had a crucial role in both improving the state of the art and the comparability of face alignment methods .,0
15886,Deep Alignment Network,0
15887,"In this section , we describe our method , which we call the Deep Alignment Network ( DAN ) .",0
15888,"DAN is inspired by the Cascade Shape Regression ( CSR ) framework , just like CSR our method starts with an initial estimate of the face shape S 0 which is refined over several iterations .",0
15889,"However , in DAN we substitute each CSR iteration with a single stage of a deep neural network which performs both feature extraction and regression .",0
15890,The major difference between DAN and approaches based on CSR is that DAN extracts features from the entire face image rather than the patches around landmark locations .,0
15891,"This is achieved by introducing additional input to each stage , namely a landmark heatmap which indicates the current estimates of the landmark positions within the global face image and transmits this information between the stages of our algorithm .",0
15892,An outline of the proposed method is shown in .,0
15893,"Therefore , each stage of DAN takes three inputs : the input image I which has been warped so that the current landmark estimates are aligned with the canonical shape S 0 , a landmark heatmap H t and a feature image F t which is generated from a dense layer connected to the penultimate layer of the previous stage t ?",0
15894,1 . The first stage only takes the input image as the initial landmarks are always assumed to be the average face shape S 0 located in the middle of the,0
15895,image .,0
15896,A single stage of DAN consists of a feed - forward neural network which performs landmark location estimation and connection layers that generate the input for the next stage .,0
15897,The details of the feed - forward network are described in subsection 3.1 .,0
15898,"The connection layers consist of the Transform Estimation layer , the Image Transform layer , Landmark Transform layer , Heatmap Generation layer and Feature Generation layer .",0
15899,The structure of the connection layers is shown in .,0
15900,"The transform estimation layer generates the transform T t + 1 , where t is the number of the stage .",0
15901,The transformation is used to warp the input image I and the current landmark estimates,0
15902,St so that St is close to the canonical shape S 0 .,0
15903,The transformed landmarks T t+1 ( S t ) are passed to the heatmap generation layer .,0
15904,The inverse transform T ?1 t+ 1 is used to map the output landmarks of the consecutive stage back into the original coordinate system .,0
15905,"The details of the Transform Estimation , Image Transform and Landmark Transforms layer are described in subsection 3.2 .",0
15906,"The Heatmap Generation and Feature Image layers are described in sections 3.3 , 3.4 .",0
15907,Section 3.5 details,0
15908,Name,0
15909,Shape the training procedure .,0
15910,Feed - forward neural network,0
15911,The structure of the feed - forward part of each stage is shown in .,0
15912,"With the exception of max pooling layers and the output layer , every layer takes advantage of batch normalization and uses Rectified Linear Units ( ReLU ) for activations .",0
15913,A dropout layer is added before the first fully connected layer .,0
15914,The last layer outputs the update ?,0
15915,S t to the current estimate of the landmark positions .,0
15916,The overall shape of the feed - forwad network was inspired by the network used in for the Image Net ILSVRC 2014 competition .,0
15917,Normalization to canonical shape,0
15918,In DAN the input image I is transformed for each stage so that the current estimates of the landmarks are aligned with the canonical face shape S 0 .,0
15919,This normalization step allows the further stages of DAN to be invariant to a given family of transforms .,0
15920,This in turn simplifies the alignment task and improves accuracy .,0
15921,The Transform Estimation layer of our network is responsible for estimating the parameters of transform T t + 1 at the output of stage t.,0
15922,As input the layer takes the output of the current stage St . Once T t + 1 is estimated the Image Transform and the Landmark Transform layers transform the image I and landmarks,0
15923,St to the canonical pose .,0
15924,The image is transformed using bilinear interpolation .,0
15925,"Note that for the first stage of DAN the normalization step is not necessary since the input shape is always the average face shape S 0 , which is also the canonical face shape .",0
15926,"Since the input image is transformed , the output of every stage has to be transformed back to match the original image , the output of any DAN stage is thus :",0
15927,where ?,0
15928,S t is the output of the last layer of stage t and T ? 1 t is the inverse of transform T t A similar normalization step has been previously proposed in with the use of affine transforms .,0
15929,In our implementation we chose to use similarity transforms as they do not cause non-uniform scaling and skewing of the output image .,0
15930,shows examples of images before and after the transformation .,0
15931,Landmark heatmap,0
15932,The landmark heatmap is an image where the intensity is highest in the locations of landmarks and it decreases with the distance to the closest landmark .,0
15933,Thanks to the use of landmark heatmaps the Convolutional Neural Network can infer the landmark locations estimated by the previous stage .,0
15934,In consequence DAN can perform face alignment based on entire facial images .,0
15935,At the input to a DAN stage the landmark heatmap is created based on the landmark estimates produced by the previous stage and transformed to the canonical pose : T t ( S t?1 ) .,0
15936,The heatmap is generated using the following equation :,0
15937,where H is the heatmap image and s i is the i - th landmark of T t ( S t?1 ) .,0
15938,In our implementation the heatmap values are only calculated in a circle of radius 16 around each landmark to improve performance .,0
15939,"Note that similarly to normalization , this step is not necessary at the input of the first stage , since the input shape is always assumed to be S 0 , which would result in an identical heatmap for any input .",0
15940,An example of a face image and a corresponding landmark heatmap is shown in .,0
15941,Feature image layer,0
15942,The feature image layer F t is an image created from a dense layer connected to the fc1 layer ( see ) of the previous stage t ?,0
15943,1 . Such a connection allows any information learned by the preceding stage to be transferred to the consecutive stage .,0
15944,This naturally complements the heatmap which transfers the knowledge about landmark locations learned by the previous stage .,0
15945,The feature image layer is a dense layer which has 3136 units with ReLU activations .,0
15946,"The output of this dense layer is reshaped to a 5656 2D layer and upscaled to 112112 , which is the input shape of DAN stages .",0
15947,"We use the smaller 5656 image rather than 112112 since it showed similar results in our experiments , with considerably less parameters .",0
15948,shows an example of a feature image .,0
15949,Training procedure,0
15950,The stages of DAN are trained sequentially .,0
15951,The first stage is trained by itself until the validation error stops improving .,0
15952,Subsequently the connection layers and the second stage are added and trained .,0
15953,This procedure is repeated until further stages stop reducing the validation error .,0
15954,"While many face alignment methods learn a model that minimizes the Sum of Squared Errors of the landmark locations , DAN minimizes the landmark location error normalized by the distance between the pupils :",0
15955,"where S * is a vector of ground truth landmark locations , T t is the transform that normalizes the input image and shape for stage t and d ipd is the distance between the pupils of S * .",0
15956,The use of this error is motivated by the fact that it is afar more common benchmark for face alignment methods than the Sum of Squared Errors .,0
15957,Thanks to the fact that all of the layers used in DAN are differentiable DAN can also be trained end - to - end .,0
15958,In order to evaluate end - to - end training in DAN we have experimented with several approaches .,0
15959,Pre-training the first stage for several epochs followed by training of the entire network yielded similar accuracy to the proposed approach but the training was significantly longer .,0
15960,Training the entire network from scratch yielded results significantly inferior to the proposed approach .,0
15961,While we did not manage to obtain improved results with end - to - end training we believe that it is possible with a better training strategy .,0
15962,We leave the creation of such a strategy for future work .,0
15963,Experiments,0
15964,In this section we perform an extensive evaluation of the proposed method on several public datasets as well as the test set of the Menpo challenge to which we have submitted our method .,0
15965,"The following paragraphs detail the datasets , error measures and implementation .",0
15966,"Section 4.1 compares our method with the state of the art , while section 4.2 shows our results in the Menpo challenge .",0
15967,Section 4.3 discusses the influence of the number of stages on the performance of DAN .,0
15968,Datasets,0
15969,In order to evaluate our method we perform experiments on the data released for the 300W competition and the recently introduced Menpo challenge dataset .,0
15970,"The 300W competition data is a compilation of images from five datasets : LFPW , HELEN , AFW , IBUG and 300W private test set .",0
15971,"The last dataset was originally used for evaluating competition entries and at that time was private to the organizers of the competition , hence the name .",0
15972,Each image in the dataset is annotated with 68 landmarks and accompanied by a bounding box generated by a face detector .,0
15973,We follow the most established approach and divide the 300 - W competition data into training and testing parts .,0
15974,"The training part consists of the AFW dataset as well as training subsets of LFPW and HELEN , which results in a total of 3148 images .",0
15975,"The test data consists of the remaining datasets : IBUG , 300W private test set , test sets of LFPW , HELEN .",0
15976,In order to facilitate comparison with previous methods we split this test data into four subsets :,0
15977,"the common subset which consists of the test subsets of LFPW and HELEN ( 554 images ) ,",0
15978,"the challenging subset which consists of the IBUG dataset ( 135 images ) ,",0
15979,"the 300W public test set which consists of the test subsets of LFPW and HELEN as well as the IBUG dataset ( 689 images ) ,",0
15980,the 300W private test set ( 600 images ) .,0
15981,The annotation for the images in the 300W public test set were originally published for the 300W competition as part of its training set .,0
15982,We use them for testing as it became a common practice to do so in the recent years .,0
15983,The Menpo challenge dataset consists of semi-frontal and profile face image datasets .,0
15984,In our experiments we only use the semi-frontal dataset .,0
15985,The dataset consists of training and testing subsets containing 6679 and 5335 images respectively .,0
15986,The training subset consists of images from the FDDB and AFLW datasets .,0
15987,The image were annotated with the same set of 68 landmarks as the 300W competition data but no face detector bounding boxes .,0
15988,The annotations of the test subset have not been released .,0
15989,Error measures,0
15990,Several measures of face alignment error for an individual face image have been recently introduced :,0
15991,"the mean distance between the localized landmarks and the ground truth landmarks divided by the interocular distance ( the distance between the outer eye corners ) ,",0
15992,"the mean distance between the localized landmarks and the ground truth landmarks divided by the interpupil distance ( the distance between the eye centers ) ,",0
15993,the mean distance between the localized landmarks and the ground truth landmarks divided by the diagonal of the bounding box .,0
15994,"In our work , we report our results using all of the above measures .",0
15995,"For evaluating our method on the test datasets we use three metrics : the mean error , the area under the cumulative error distribution curve ( AUC ? ) and the failure rate .",0
15996,"Similarly to , we calculate AUC ?",0
15997,"as the area under the cumulative distribution curve calculated up to a threshold ? , then divided by that threshold .",0
15998,As a result the range of the AUC ?,0
15999,values is always 0 to 1 .,0
16000,"Following , we consider each image with an inter-ocular normalized error of 0.08 or greater as failure and use the same threshold for AUC 0.08 .",0
16001,In all the experiments we test on the full set of 68 landmarks .,0
16002,Implementation,0
16003,"We train two models , DAN which is trained on the training subset of the 300W competition data and DAN - Menpo which is trained on both the above mentioned dataset and the Menpo challenge training set .",0
16004,"Data augmentation is performed by mirroring around the Y axis as well as random translation , rotation and scaling , all sampled from normal distributions .",0
16005,During data augmentation a total of 10 images are created from each input image in the training set .,1
16006,Both models ( DAN and DAN - Menpo ) consist of two stages .,0
16007,Training is performed using Theano 0.9.0 and Lasagne 0.2 .,1
16008,For optimization we use Adam stochastic optimization with an initial step size of 0.001 and mini batch size of 64 .,1
16009,For validation we use a random subset of 100 images from the training set .,1
16010,The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU .,1
16011,"We believe that the processing speed can be further improved by optimizing the implementation of some of our custom layers , most notably the Image Transform layer .",0
16012,"To enable reproducible research , we release the source code of our implementation as well as the models used in the experiments",0
16013,1 .,0
16014,The published implementation also contains an example of face tracking with the proposed method .,0
16015,Comparison with state - of - the - art,0
16016,We compare the DAN model with state - of - the - art methods on all of the test sets of the 300W competition data .,0
16017,We also show results for the DAN - Menpo model but do not perform comparison since at the moment there are no published methods that use this dataset for training .,0
16018,For each test set we initialize our method using the face detector bounding boxes provided with the datasets .,0
16019,"show the mean error , AUC 0.08 and the failure rate of the proposed method and other methods on the 300 W public test set .",0
16020,"shows the mean error , the AUC 0.08 and failure rate on the 300 W private test set .",0
16021,"All of the experiments performed on the two most difficult test subsets ( the challenging subset and the 300 W private test set ) show state - of - the - art results , including :",0
16022,"a failure rate reduction of 60 % on the 300 W private test set ,",1
16023,"a failure rate reduction of 72 % on the 300W public test set ,",1
16024,a 9 % improvement of the mean error on the challenging subset .,1
16025,This shows that the proposed DAN is particularly suited for handling difficult face images with a high degree of occlusion and variation in pose and illumination .,0
16026,Results on the Menpo challenge test set,0
16027,In order to evaluate the proposed method on the Menpo challenge test dataset we have submitted our results to the challenge and received the error scores from the challenge organizers .,0
16028,The Menpo test data differs from the other datasets we used in that it does not include any bounding boxes which could be used to initialize face alignment .,0
16029,"For that reason we have decided to use a two step face alignment procedure , where the first step serves as an initialization for the second step .",0
16030,The first step performs face alignment using a square initialization bounding box placed in the middle of the image with a size set to a percentage of image height .,0
16031,"The second step takes the result of the first step , transforms the landmarks and the image to the canonical face shape and Method AUC 0.08 Failure ( % ) inter-ocular normalization ESR 43 . 10.45 SDM 42.94 10.89 CFSS 49.87 5.08 MDM",0
16032,52 . In order to determine the optimal size of the bounding boxes in the first step we ran DAN on a small subset of the Menpo test set for several bounding box sizes .,0
16033,The optimal size was determined using a method that would estimate the face alignment error of a given set of landmarks and an image .,0
16034,Said method extracts HOG [? ] features at each of the landmarks and uses a linear model to estimate the error .,0
16035,The method was trained on the 300 W training set using ridge regression .,0
16036,The chosen bounding box size was 46 % of the image height .,0
16037,"and show the CED curve , mean error , AU C 0.03 and failure rate for the DAN - Menpo model on the Menpo test set .",0
16038,"In all cases the errors are calculated using the diagonal of the bounding box normalization , used by the challenge organizers .",0
16039,For the AUC and the failure rate we have chosen a threshold of 0.03 of the bounding box diagonal as it is approximately equivalent to 0.08 of the interocular distance used in the previous chapter .,0
16040,shows examples of images from the Menpo test set and corresponding results produced by our method .,0
16041,Note that even though DAN was trained primarily on semifrontal images it can handle fully profile images as well .,0
16042,Further evaluation,0
16043,In this subsection we evaluate several DAN models with a varying number of stages on the 300 W private test set .,0
16044,All of the models were trained identically to the DAN model from section 4.1 .,0
16045,shows the results of our eval - uation .,0
16046,The addition of the second stage increases the AUC 0.08 by 20 % while the mean error and failure rate are reduced by 14 % and 56 % respectively .,0
16047,The addition of a third stage does not bring significant benefit in any of the metrics .,0
16048,Conclusions,0
16049,"In this paper , we introduced the Deep Alignment Network -a robust face alignment method based on convo - lutional neural networks .",0
16050,"Contrary to the recently proposed face alignment methods , DAN performs face alignment based on entire face images , which makes it highly robust to large variations in both initialization and head pose .",0
16051,Using entire face images instead of local patches extracted around landmarks is possible thanks to the use of novel landmark heatmaps which transmit the information about landmark locations between DAN stages .,0
16052,"Extensive evaluation performed on two challenging , publicly available datasets shows that the proposed method improves the stateof - the - art failure rate by a significant margin of over 70 % .",0
16053,Future research includes investigation of new strategies for training DAN in an end - to - end manner .,0
16054,We also plan to introduce learning into the estimation of the transform T t that normalizes the shapes and images between stages .,0
16055,Acknowledgements,0
16056,title,0
16057,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,1
16058,abstract,0
16059,We present a novel boundary - aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation .,0
16060,"Unlike the conventional heatmap based method and regression based method , our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition .",0
16061,Three questions are explored and answered by this work :,0
16062,1 . Why using boundary ?,0
16063,2 . How to use boundary ?,0
16064,3 . What is the relationship between boundary estimation and landmarks localisation ?,0
16065,"Our boundaryaware face alignment algorithm achieves 3.49 % mean error on 300 - W Fullset , which outperforms state - of - the - art methods by a large margin .",0
16066,Our method can also easily integrate information from other datasets .,0
16067,"By utilising boundary information of 300 - W dataset , our method achieves 3.92 % mean error with 0.39 % failure rate on COFW dataset , and 1.25 % mean error on AFLW - Full dataset .",0
16068,"Moreover , we propose anew dataset WFLW to unify training and testing across different factors , including poses , expressions , illuminations , makeups , occlusions , and blurriness .",0
16069,Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html,1
16070,Introduction,0
16071,"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation .",1
16072,The objective of this paper is to devise an effective face alignment algorithm to handle faces with unconstrained pose variation and occlusion across multiple datasets and annotation protocols . *,0
16073,This work was done during an internship at SenseTime Research .,0
16074,datasets with different number of landmarks .,0
16075,The second column illustrates the universally defined facial boundaries estimated by our methods .,0
16076,"With the help of boundary information , our approach achieves high accuracy localisation results across multiple datasets and annotation protocols , as shown in the third column .",0
16077,"Different to face detection and recognition , face alignment identifies geometry structure of human face which can be viewed as modeling highly structured output .",0
16078,"Each facial landmark is strongly associated with a well - defined facial boundary , e.g. , eyelid and nose bridge .",0
16079,"However , compared to boundaries , facial landmarks are not so well - defined .",0
16080,Facial landmarks other than corners can hardly remain the same semantical locations with large pose variation and occlusion .,0
16081,"Besides , different annotation schemes of existing datasets lead to a different number of landmarks ( 19/29/68/194 points ) and annotation scheme of future face alignment datasets can hardly be determined .",0
16082,We believe the reasoning of a unique facial structure is the key to localise facial landmarks since human face does not include ambiguities .,0
16083,"To this end , we use well - defined facial boundaries to represent the geometric structure of the human face .",1
16084,It is easier to identify facial boundaries comparing to facial landmarks under large pose and occlusion .,0
16085,"In this work , we represent facial structure using 13 boundary lines .",1
16086,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .",0
16087,Our boundary - aware face alignment algorithm contains two stages .,1
16088,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,1
16089,"As noticed in , facial landmarks of different annotation schemes can be derived from boundary heatmaps with the same definition .",0
16090,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .",1
16091,"Experiments have shown that the better quality estimated boundaries have , the more accurate landmarks will be .",0
16092,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .",1
16093,We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,1
16094,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .",0
16095,The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor .,1
16096,We observe that a model guided by ground truth boundary heatmaps can achieve 76. 26 % AUC on 300 W test while the state - of - the - art method can only achieve 54.85 % .,0
16097,This suggests the richness of information contained in boundary heatmaps .,0
16098,"To fully utilise the structure information , we apply boundary heatmaps at multiple stages in the landmark regression network .",0
16099,"Our experiment shows that the more stages boundary heatmaps are used in feature learning , the better landmark prediction results we will get .",0
16100,"We evaluate the proposed method on three popular face alignment benchmarks including 300W , COFW , and AFLW .",0
16101,Our approach significantly outperforms previous state - of - the - art methods by a large margin .,0
16102,"3.49 % mean error on 300 - W Fullset , 3.92 % mean error with 0.39 % failure rate on COFW and 1.25 % mean error on AFLW - Full dataset respectively .",0
16103,"To unify the evaluation , we propose anew large dataset named Wider Facial Landmarks in - the - wild ( WFLW ) which contain 10 , 000 images .",0
16104,"Our new dataset introduces large pose , expression , and occlusion variance .",0
16105,Each image is annotated with 98 landmarks and 6 attributes .,0
16106,Comprehensive ablation study demonstrates the effectiveness of each component .,0
16107,Related Work,0
16108,"In the literature of face alignment , besides classic methods ( ASMs , AAMs , CLMs and Cascaded Regression Models ) , recently , state - of - the - art performance has been achieved with Deep Convolutional Neural Networks ( DC - NNs ) .",0
16109,"These methods mainly fall into two categories , i.e. , coordinate regression model and heatmap regression model .",0
16110,Coordinate regression models directly learn the mapping from the input image to the landmark coordinates vector .,0
16111,"Zhang et al. frames the problem as a multi-task learning problem , learns landmark coordinates and predicts facial attributes at the same time .",0
16112,MDM is the first end - to - end recurrent convolutional system for face alignment from coarse to fine .,0
16113,TSR splits face into several parts to ease the parts variations and regresses the coordinates of different parts respectively .,0
16114,Even though coordinate regression models have the advantage of explicit inference of landmark coordinates without any post-processing .,0
16115,"Nevertheless , they are not performing as well as heatmap regression models .",0
16116,"Heatmap regression models , which generate likelihood heatmaps for each landmark respectively , have recently achieved state - of - the - art performance in face alignment .",0
16117,CALE is a two - stage convolutional aggregation model to aggregate score maps predicted by detection stage along with early CNN features for final heatmap regression .,0
16118,Yang et al .,0
16119,"uses a two parts network , i.e. , a supervised transformation to normalise faces and a stacked hourglass network to get prediction heatmaps .",0
16120,"Most recently , JMFA achieves state - of - the - art accuracy by leveraging stacked hourglass network for multi-view face alignment and demonstrates better than the best three entries of the last Menpo Challenge .",0
16121,Since boundary detection was set as one of the most fundamental problems in computer vision and there have emerged a large number of materials .,0
16122,It has been proved efficient in vision tasks as segmentation and object detection .,0
16123,"In face alignment , boundary information demonstrates especial importance because almost all of the landmarks are defined lying on the facial boundaries .",0
16124,"However , as far as we know , in face alignment task , no work before has investigated the use of boundary information from an explicit perspective .",0
16125,The recent advance inhuman pose estimation partially inspires our method of boundary heatmaps estimation .,0
16126,"Stacked hourglass network achieves compelling accuracy with a bottom - up , top - down design which endows the network with capabilities of obtaining multi-scale information .",0
16127,Message passing has shown great power in structure modeling of human joints .,0
16128,"Recently , adversarial learning ] is adopted to further improve the accuracy of estimated human pose under heavy occlusion .",0
16129,network is used to estimate boundary heatmaps .,0
16130,Message passing layers are introduced to handle occlusion .,0
16131,( b ) Boundary - aware landmarks regressor is used to generate the final prediction of landmarks .,0
16132,Boundary heatmap fusion scheme is introduced to incorporate boundary information into the feature learning of regressor .,0
16133,"( c ) Boundary effectiveness discriminator , which distinguishes "" real "" boundary heatmaps from "" fake "" , is used to further improve the quality of the estimated boundary heatmaps .",0
16134,Boundary - Aware Face Alignment,0
16135,"As mentioned in the introduction , landmarks have difficulty in presenting accurate and universal geometric structure of face images .",0
16136,We propose facial boundary as geometric structure representation and help landmarks regression problem in the end .,0
16137,"Boundaries are detailed and welldefined structure descriptions , which are consistent across head poses and datasets .",0
16138,They are also closely related to landmarks since most of the landmarks are located along boundary lines .,0
16139,Other choices are also available for geometric structure representations .,0
16140,Recent works ] has adopted facial parts to aid face alignment tasks .,0
16141,"However , facial parts are too coarse thus not as powerful as boundary lines .",0
16142,Another choice would be face parsing results .,0
16143,Face parsing leads to disjoint facial components which needs the boundaries of each component form a closed loop .,0
16144,"However , some facial organs such as nose are naturally blended into the whole face thus are inaccurate to be defined as separate parts .",0
16145,"On the contrary , boundary lines are not necessary to form a closed loop , which is more flexible in representing geometric structure .",0
16146,Experiments in Sec 4.2 have shown that boundary lines are the best choice to aid landmark coordinates regression .,0
16147,The detailed configuration of our proposed Boundary - Aware Face Alignment framework is illustrated in .,0
16148,It is composed of three closely related components :,0
16149,"Boundary - Aware Landmark Regressor , Boundary Heatmap Estimator and Landmark - Based Boundary Effectiveness Discriminator .",0
16150,Boundary - Aware Landmark Regressor incorporates boundary information in a multi-stage manner to predict landmark coordinates .,0
16151,Boundary Heatmap Estimator produces boundary heatmaps as face geometric structure .,0
16152,"Since boundary information is used heavily , the quality of boundary heatmaps is crucial for final landmark regression .",0
16153,"We introduce adversarial learning idea by proposing Landmark - Based Boundary Effectiveness Dis-criminator , which is paired with the Boundary Heatmap Estimator .",0
16154,This discriminator can further improve the quality of boundary heatmaps and lead to better landmark coordinates prediction .,0
16155,Boundary - aware landmarks regressor,0
16156,"In order to fuse boundary line into feature learning , we transform landmarks to boundary heatmaps to aid the learning of feature .",0
16157,The responses of each pixel in boundary heatmap are decided by its distance to the corresponding boundary line .,0
16158,"As shown in , the details of boundary heatmap are defined as follows .",0
16159,"Given a face image I , denote its ground truth annotation by L landmarks as S = {s l } L l=1 .",0
16160,K subsets,0
16161,Si ?,0
16162,"S are defined to represent landmarks belongs to K boundaries respectively , such as upper left eyelid and nose bridge .",0
16163,"For each boundary ,",0
16164,Si is interpolated to get a dense boundary line .,0
16165,"Then a binary boundary map Bi , the same size as I , is formed by setting only points on the boundary line to be 1 , others",0
16166,"0 . Finally , a distance transform is performed based on each Bi to get distance map Di .",0
16167,We use a gaussian expression with standard deviation ? to transform the distance map to ground - truth boundary heatmap M i .,0
16168,3 ?,0
16169,is used to threshold,0
16170,Di to make boundary heatmaps focus more on boundary areas .,0
16171,"In practice , the length of the ground - truth boundary heatmap side is set to a quarter of the size of I for computation efficiency .",0
16172,"In order to fully utilise the rich information contained in boundary heatmaps , we propose a multi-stage boundary heatmap fusion scheme .",0
16173,"As illustrated in , A fourstage res - 18 network is adopted as our baseline network .",0
16174,Boundary heatmap fusion is conducted at the input and every stage of the network .,0
16175,"Comprehensive results in Sec. have shown that the more fusion we conducted to the baseline network , the better performance we can get .",0
16176,Input image fusion .,0
16177,"To fuse boundary heatmap M with input image I , the fused input H is defined as :",0
16178,where ?,0
16179,represents the element - wise dot product operation and ?,0
16180,represents channel - wise concatenation .,0
16181,The above design makes fused input focus only on detailed texture around boundaries .,0
16182,Thus most background and texture - less face regions are ignored which greatly enhance the effectiveness of input .,0
16183,The original input is also concatenated to the fused ones to keep other valuable information in the original image .,0
16184,Feature map fusion .,0
16185,"Similar to above , to fuse boundary heatmap M with feature map F , the fused feature map H is defined as :",0
16186,"Since the number of channels of M equals to the number of pre-defined boundaries , which is constant .",0
16187,A transform function T is necessary to convert M to have the same channels with F .,0
16188,We choose hourglass structure subnet as T to keep feature map size .,0
16189,Down - sampling and upsampling are performed symmetrically .,0
16190,Skip connections,0
16191,Input Face Image Baseline Hourglass Baseline + Message Passing,0
16192,Baseline + Message Passing + Adversarial Learning Since boundary heatmaps are used heavily in landmarks coordinates regression .,0
16193,The quality of boundary heatmaps is essential to the prediction accuracy .,0
16194,"By fusing ground truth boundary heatmaps , our method can achieve 76. 26 % AUC on 300 - W test , comparing to the state - of - art result 54.85 % .",0
16195,"Based on this experiment , in the following sections , several methods will be introduced to improve the quality of generated boundary heatmaps .",0
16196,Experiment in ablation study also shows the consistent performance gain with better heatmap quality .,0
16197,Boundary heatmap estimator,0
16198,"Following previous work in face alignment and human pose , we use stacked hourglass as the baseline of boundary heatmap estimator .",0
16199,Mean square error ( MSE ) between generated and groundtruth boundary heatmaps is optimized .,0
16200,"However , as demonstrated in , when heavy occlusions happen , the generated heatmaps always suffer from the noisy and multi-mode response , which has also been mentioned in .",0
16201,"In order to relieve the problem caused by occlusion , we introduce message passing layers to pass information between boundaries .",0
16202,This process is visualised in .,0
16203,"During occlusion , visible boundaries can provide help to occluded ones according to face structure .",0
16204,Intra - level message passing is used at the end of each stack to pass information between different boundary heatmaps .,0
16205,"Thus , information can be passed from visible boundaries to occluded ones .",0
16206,"Moreover , since different stacks of hourglass focus on different aspects of face information .",0
16207,Inter - level message passing is adopted to pass message from lower stacks to the higher stacks to keep the quality of boundary heatmaps when stacking more hourglass subnets .,0
16208,We implemented message passing following .,0
16209,"In this implementation , the feature map at the end of each stack needs be divided into K branches , where K is the number",0
16210,Intra - level Message Passing,0
16211,Inter - level Message Passing :,0
16212,An illustration of message pass scheme .,0
16213,A bi-direction tree structure is used for intra-level message passing .,0
16214,Inter - level message is passed between adjacent stacks from lower to higher .,0
16215,"of boundaries , each represents a type of boundary feature map .",0
16216,This requirement demonstrates the advantage of our boundary heatmaps compared with landmark heatmaps for the small and constant number K of them .,0
16217,"Thus , the computational and parameter cost of message passing layers within boundaries is small while it is not practical for message passing within 68 or even 194 landmarks .",0
16218,Boundary effectiveness discriminator,0
16219,"In structured boundary heatmap estimator , mean squared error ( MSE ) is used as the loss function .",0
16220,"However , minimizing MSE sometimes makes the prediction look blurry and implausible .",0
16221,This regression - to - the - mean problem is a well - known fact in the literature of super- resolution .,0
16222,It damages the learning of regression network when bad boundary heatmaps are generated .,0
16223,"However , in our framework , the hard - to - define term "" quality "" of heatmaps has a very clear evaluation metric .",0
16224,"If helping to produce accurate landmark coordinates , the boundary heatmap has a good quality .",0
16225,"According to this , we propose a landmark based boundary effectiveness discriminator to decide the effectiveness of the generated boundary heatmaps .",0
16226,"For a generated boundary heatmap M ( all index i such as M i is omitted for the simplicity of notation ) , denote its corresponding generated landmark coordinates set as ? , the ground - truth distance matric map as Dist .",0
16227,The ground truth d fake of discriminator D that determines whether the generated boundary heatmap is fake can be defined as,0
16228,Where ?,0
16229,is the distance threshold to ground truth boundary and ?,0
16230,is the probability threshold .,0
16231,This discriminator predicts whether most generated corresponding landmarks would be close to the ground truth boundary .,0
16232,"Following , we introduce the idea of adversarial learning by pairing the boundary effectiveness discriminator D and the boundary heatmaps estimator G.",0
16233,The loss of D can be expressed as :,0
16234,( 5 ) Where M is the ground truth boundary heatmap .,0
16235,The discriminator learns to predict ground truth boundary heatmap as one while predict generated boundary heatmap according to d fake .,0
16236,"With effectiveness discriminator , the adversarial loss can be expressed as :",0
16237,"Thus , the estimator is optimised to fool D by giving more plausible and high - confidence maps that will benefit the learning of regression network .",0
16238,The following pseudo - code shows the training process of the whole methods .,0
16239,Forward D byd fake = D (M ) and optimize D by minimizing the second term of L D defined in Eq.5 ;,0
16240,5 :,0
16241,"Forward R by ? = R ( I , M ) and optimize R by minimizing ? ? S 2 2 ; 6 : end while",0
16242,Cross - Dataset Face Alignment,0
16243,"Recently , together with impressive progress of algorithms for face alignment , various benchmarks have also been released , e.g. , LFPW , AFLW and 300 - W .",0
16244,"However , because of the gap between annotation schemes , these datasets can hardly be jointly used .",0
16245,Models trained on one specific dataset perform poorly on recent in - the - wild test sets .,0
16246,"However , introduction of an annotation transfer component will bring new problems .",0
16247,"From anew perspective , we take facial boundaries as an all - purpose middle - level face geometry representation .",0
16248,Facial boundaries naturally unify different landmark definitions with enough landmarks .,0
16249,And it can also be applied to help training landmarks regressor with any specific landmarks definition .,0
16250,The cross - dataset capacity is an important by - product of our methods .,0
16251,It s effectiveness is evaluated in Sec. 4.1 .,0
16252,Experiments,0
16253,Datesets .,0
16254,"We conduct evaluation on four challenging datasets including 300W , COFW , AFLW and WFLW which is annotated by ourself .",0
16255,300 W dataset : 300W is currently the most widelyused benchmark dataset .,0
16256,We regard all the training samples ( 3148 images ) as the training set and perform testing on ( i ) full set and ( ii ) test set .,0
16257,( i ) Full set contains 689 images and is split into common subset ( 554 images ) and challenging subsets ( 135 images ) .,0
16258,( ii ) Test set is the private test - set used for the 300W competition which contains 600 images .,0
16259,COFW dataset consists of 1345 images for training and 507 faces for testing which are all occluded to different degrees .,0
16260,Each COFW face originally has 29 manually annotated landmarks .,0
16261,We also use the test set which has been re-annotated by with 68 landmarks annotation scheme to allow easy comparison to previous methods .,0
16262,AFLW dataset : AFLW contains 24386 in - the - wild faces with large head pose up to 120 for yaw and 90 for pitch and roll .,0
16263,We follow to adopt three settings on our experiments : ( i ) AFLW - Full : 20000 and 4386 images are used for training and testing respectively .,0
16264,( ii ) AFLW - Frontal : 1314 images are selected from 4386 testing images for evaluation on frontal faces .,0
16265,WFLW dataset :,0
16266,"In order to facilitate future research of face alignment , we introduce anew facial dataset base on WIDER Face named Wider Facial Landmarks inthe-wild ( WFLW ) , which contains 10000 faces ( 7500 for training and 2500 for testing ) with 98 fully manual annotated landmarks .",0
16267,"Apart from landmark annotation , out new dataset includes rich attribute annotations , i.e. , occlusion , pose , make - up , illumination , blur and expression for comprehensive analysis of existing algorithms .",0
16268,"Compare to previous dataset , faces in the proposed dataset introduce large variations in expression , pose and occlusion .",0
16269,"We can simply evaluate the robustness of pose , occlusion , and expression on proposed dataset instead of switching between multiple evaluation protocols in different datasets .",0
16270,The comparison of WFLW with popular benchmarks is illustrated in the supplementary material .,0
16271,Evaluation metric .,0
16272,We evaluate our algorithm using standard normalised landmarks mean error and Cumulative Errors Distribution ( CED ) curve .,0
16273,"In addition , two further statistics i.e. the area-under - the - curve ( AUC ) and the failure rate fora maximum error of 0.1 are reported .",0
16274,"Because of various profile face on AFLW dataset , we follow to use face size as the normalising factor .",0
16275,"For other dataset , we follow MDM and to use outer - eye - corner distance as the "" inter-ocular "" normalising factor .",0
16276,"Specially , to compare with the results that reported to be normalised by "" inter-pupil "" ( eye-centre - distance ) distance , we report our results with both two normalising factors on and resized to 256 256 according to provided bounding boxes .",0
16277,The estimator is stacked four times if not specially indicated in our experiment .,0
16278,"For ablation study , the estimator is stacked two times due to the consideration of time and computation cost .",0
16279,All our models are trained with Caffe [ 24 ] on 4 Titan X GPUs .,1
16280,Note that all testing images are cropped and resized according to provided bounding boxes without any spatial transformation for fair comparison with other methods .,0
16281,"For the limited space of paper , we report all of the training details and experiment settings in our supplementary material .",0
16282,Comparison with existing approaches 4.1.1 Evaluation on 300W,1
16283,We compare our approach against the state - of - the - art methods on 300W Fullset .,0
16284,The results are shown in error on the Challenging subset which reflects the effectiveness of handling large head rotation and exaggerated expressions .,0
16285,"Apart from 300W Fullset , we also show our results on 300W Testset in .",0
16286,Our method performs best among all of the state - of - the - art methods .,1
16287,"To verify the effectiveness and potential of boundary maps , we use ground truth boundary in the proposed method and report results named "" LAB + oracle "" which significantly outperform all the methods .",0
16288,The results demonstrate the effectiveness of boundary information and show great potential performance gain if the boundary information can be well captured .,0
16289,Evaluation on WFLW,1
16290,"For comprehensively evaluating the robustness of our method , we report mean error , failure rate and AUC on the Testset and six typical subsets of WFLW on 3 .",0
16291,These six subsets were split from Testset by the provided attribute annotations .,0
16292,"Though reasonable performance is obtained , there is illustrated to be still a lot of room for improvement for the extreme diversity of samples on WFLW , e.g. , large pose , exaggerated expressions and heavy occlusion .",1
16293,Cross - dataset evaluation on COFW and AFLW,1
16294,COFW - 68 is produced by re-annotating COFW dataset with 68 landmarks annotation scheme to perform cross - dataset experiments by .,0
16295,shows the CED curves of our method against state - of - the - art methods on the COFW - 68 dataset .,0
16296,Our model outperforms previous results with a large margin .,1
16297,We achieve 4.62 % mean error with 2.17 % failure rate .,1
16298,"The failure rate is significantly reduced by 3.75 % , which indicates the robustness of our method to handle occlusions .",1
16299,"In order to verify the capacity of handling cross - dataset face alignment of our method , we use boundary heatmaps estimator trained on 300W Fullset which has no overlap with COFW and AFLW dataset and compare the performance with and without using boundary information fusion ( "" LAB w/o boundary "" ) .",0
16300,The results are reported in .,0
16301,The performance of previous methods without using 300 - W datasets is also attached as a reference .,0
16302,There is a clear boost between our method without and with using boundary information .,0
16303,"Thanks to the generalization of facial boundaries , the estimator learned on 300 W can be conveniently used to supply boundary information for coordinate regression on COFW - 29 and AFLW dataset , even though these datasets have different annotation protocols .",0
16304,"Moreover , our method uses boundary information achieves 29 % , 32 % and 29 % relative performance improve- ment over the baseline method ( "" LAB without boundary "" ) on COFW - 29 , AFLW - Full and AFLW - Frontal respectively .",1
16305,"Since COFW covers different level of occlusion and AFLW has significant view changes and challenging shape variations , the results emphasise the robustness brought by boundary information to occlusion , pose and shape variations .",0
16306,More qualitative results are demonstrated in our supplementary material .,0
16307,Ablation study,0
16308,"Our framework consists of several pivotal components , i.e. , boundary information fusion , message passing and adversarial learning .",0
16309,"In this section , we validate their effectiveness within our framework on the 300W Challenging Set and WFLW Dataset .",0
16310,"Based on the baseline res - 18 network ( BL ) , we analyse each proposed component , i.e. , with the baseline hourglass boundary estimator ( "" HBL "" ) , message passing ( "" MP "" ) , and adversarial learning ( "" AL "" ) , by comparing their mean error and failure rate .",0
16311,The overall results are shown in .,0
16312,Boundary information is chosen as geometric structure representation in our work .,0
16313,"We verify the potential of other structure information as well , i.e. , facial parts gaussian ( "" FPG "" ) and face parsing results ( "" FP "" ) .",0
16314,We report the landmarks accuracy with oracle results in using different structure information .,0
16315,"It can be observed easily that boundary map ( "" BM "" ) is the most effective one .",1
16316,Boundary information fusion is one of the key steps in our algorithm .,1
16317,We can fuse boundary information at different levels for the regression network .,0
16318,"As indicated in , our final model that fuses boundary information in all four levels improves mean error from 7.12 % to 6.13 % .",1
16319,"To evaluate the relationship between the quantity of boundary information fusion and the final prediction accuracy , we vary the number of fusion levels from 1 to 4 and report the mean error results in .",0
16320,It can be observed that performance is improved consistently by fusing boundary heatmaps at more levels .,1
16321,Method,0
16322,"BL BL+ FPG BL + FP BL + BM Mean Error 7 . 5.25 4.16 3.28 To verify the effectiveness of the fusion scheme shown in , we report the results of mean error on several settings in , i.e. , the baseline res - 18 network ( "" BL "" ) , hourglass module without boundary feature ( "" HG / B "" ) , hourglass module with boundary feature ( "" HG "" ) and consecutive convolutional layers with boundary feature ( "" CL "" ) .",0
16323,"The comparison between "" BL + HG "" and "" BL + HG/ B "" indicates the effectiveness of boundary information fusion rather than network structure changes .",1
16324,"The comparison between "" BL + HG "" and "" BL + CL "" indicates the effectiveness of the using hourglass structure design .",1
16325,Message passing plays a vital role for heatmap quality improvement when severe occlusions happen .,1
16326,"As illustrated in on Occlusion Subset of WFLW , message passing , which combines information from visible boundaries and occluded ones , reduce the mean error over 11 % relatively .",0
16327,Adversarial learning further improves the quality and effectiveness of boundary heatmaps .,1
16328,"As illustrated in , heatmaps can be observed to be more focused and salience when adversarial loss is added .",0
16329,"To verify the effectiveness of our landmark based boundary effectiveness discriminator , a baseline method using traditionally defined discriminator is tested on 300W Challenging Set .",0
16330,The failure rate is reduced from 5.19 % to 3.70 % .,0
16331,Relationship between boundary estimator and landmarks regressor is evaluated by analyzing the quality of estimated heatmap and final prediction accuracy .,0
16332,We report the MSE of estimated heatmaps and corresponding landmarks accuracy in .,0
16333,"We observe that with message passing ( "" HBL + MP "" ) and adversarial learning ( "" HBL + AL "" ) , the errors of estimated heatmaps are reduced together with landmarks accuracy .",0
16334,Conculsion,0
16335,Unconstrained face alignment is an emerging topic .,0
16336,"In this paper , we present a novel use of facial boundary to derive facial landmarks .",0
16337,"We believe the reasoning of a unique facial structure is the key to localise facial landmarks , since human face does not include ambiguities .",0
16338,"By estimating facial boundary , our method is capable of handling arbitrary head poses as well as large shape , appearance , and occlusion variations .",0
16339,Our experiment shows the great potential of modeling facial boundary .,0
16340,The runtime of our algorithm is 60 ms on TITAN X GPU .,0
16341,title,0
16342,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,1
16343,abstract,0
16344,Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks .,0
16345,"However , the loss function for heatmap regression is rarely studied .",0
16346,"In this paper , we analyze the ideal loss function properties for heatmap regression in face alignment problems .",0
16347,"Then we propose a novel loss function , named Adaptive Wing loss , that is able to adapt its shape to different types of ground truth heatmap pixels .",0
16348,This adaptability penalizes loss more on foreground pixels while lesson background pixels .,0
16349,"To address the imbalance between foreground and background pixels , we also propose Weighted Loss Map , which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization .",0
16350,"To further improve face alignment accuracy , we introduce boundary prediction and CoordConv with boundary coordinates .",0
16351,"Extensive experiments on different benchmarks , including COFW , 300 W and WFLW , show our approach outperforms the state - of - the - art by a significant margin on various evaluation metrics .",0
16352,"Besides , the Adaptive Wing loss also helps other heatmap regression tasks .",0
16353,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,1
16354,Introduction,0
16355,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .",1
16356,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",0
16357,"In recent years , Convolutional Neural Network ( CNN ) based heatmap regression has become one of the mainstream approaches for face alignment problems and achieved considerable performance on frontal faces .",0
16358,"However , landmarks on faces with large pose , occlusion and significant blur are still challenging to localize .",0
16359,"Heatmap regression , which regresses a heatmap generated from landmark coordinates , is widely used for face alignment .",0
16360,"In heatmap regression , the ground truth heatmap is generated by plotting a Gaussian distribution centered at each landmark on each channel .",0
16361,The model regresses against the ground truth heatmap at pixel level and then use the predicted heatmaps to infer landmark locations .,0
16362,"Prediction accuracy on foreground pixels ( pixels with positive values ) , especially the ones near the mode of each Gaussian distribution ( , is essential to accurately localize landmarks , even small prediction errors on these pixels can cause the prediction to shift from the correct modes .",0
16363,"On the contrary , accurately predicting the values of background pixels ( pixels with zero values ) is less important , since small errors on these pixels will not affect landmark prediction inmost cases .",0
16364,"However , prediction accuracy on difficult background pixels ( background pixels near foreground pixels ) are also important since they are often incorrectly regressed as foreground pixels and could cause inaccurate predictions .",0
16365,"From this discussion , we locate two issues of the widely used Mean Square Error ( MSE ) loss in heatmap regression : i ) MSE is not sensitive to small errors , which hurts the capability to correctly locate the mode of the Gaussian dis-tribution ; ii ) During training all pixels have the same loss function and equal weights , however , background pixels absolutely dominates foreground pixels on a heatmap .",0
16366,"As a result of i ) and ii ) , models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth ( .",0
16367,This low quality heatmap could cause wrong estimation of facial landmarks .,0
16368,"Wing loss is shown to be effective to improve coordinate regression , however , according to our experiment , it is not applicable for heatmap regression .",0
16369,Small errors on background pixels will accumulate significant gradients and thus cause the training process to diverge .,0
16370,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .",1
16371,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",1
16372,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .",1
16373,The encoded coordinate information further improves the performance of our approach .,0
16374,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .",1
16375,"In summary , our main contributions include :",0
16376,"Propose a novel loss function for heatmap regression named Adaptive Wing loss , that is able to adapt its curvature to ground truth pixel values .",0
16377,"This adaptive property reduces small errors on foreground pixels for accurate landmark localization , while tolerates small errors on background pixels fora better convergence rate .",0
16378,With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .,0
16379,"Encode coordinate information , including coordinates on boundary , into the face alignment algorithm using CoordConv .",0
16380,"Our approach outperforms the state - of - the - art algorithms by a significant margin on mainstream face alignment datasets including 300W , COFW and WFLW .",0
16381,We also show the validity of the Adaptive Wing loss in the human pose estimation task which also utilizes heatmap regression .,0
16382,Related Work,0
16383,CNN based heatmap regression models leverage CNN to perform heatmap regression .,0
16384,"In recent work , joint bottom - up and top - down architectures such as stacked HG were able to achieve the state - of - the - art performance .",0
16385,Bulat et al .,0
16386,"proposed a hierarchical , parallel and multi-scale block as a replacement for the original ResNet block to further improve the localization accuracy of HG .",0
16387,Tang et al. was able to achieve current state - of - the - art with quantized densely connected U - Nets with fewer parameters than stacked HG models .,0
16388,Other architectures are also able to achieve excellent performance .,0
16389,Merget et al .,0
16390,proposed a fully convolutional neural network ( FCN ) that combines global and local context information fora refined prediction .,0
16391,Valle et al .,0
16392,combined CNN with ensemble of regression trees in a coarse - to - fine fashion to achieve the state - of - the art accuracy .,0
16393,"Another focus of this area is the 3D face alignment , that aims to provide 3D dense alignment based on 2D images .",0
16394,Loss functions for heatmap regression were rarely studied in previous work .,0
16395,"GoDP used a distance - aware softmax loss to assign large penalty on incorrectly classified positive samples , while gradually reducing penalty on missclassified negative samples as the distance from nearby positive samples decrease .",0
16396,The Wing loss is a modified log loss for direct regression of landmark coordinates .,0
16397,"Compared with MSE , it amplifies the influence of small errors .",0
16398,"Although the Wing loss is able to achieve the state - of - theart performance in coordinate regression , it is not applicable to heatmap regression due to its high sensitivity to small errors on background pixels and the discontinuity of gradient at zero .",0
16399,"Our proposed Adaptive Wing loss is novel since it is able to adapt its curvature to different ground truth pixel values , such that it can be sensitive to small errors on foreground pixels yet be able to tolerance small errors on background pixels .",0
16400,"Hence , our loss can be applied to heatmap regression while the original Wing loss can not be .",0
16401,Boundary information was first introduced into face alignment by Wu et al ..,0
16402,"LAB proposed a two - stage network with a stacked HG model to generate a facial boundary map , and then regress facial landmark coordinates directly with the help of boundary map .",0
16403,We believe including boundary information is beneficial to the heatmap regression and utilized a modified version to our model .,0
16404,Coordinate Encoding .,0
16405,Translation invariance is intrinsic to the convolution operation .,0
16406,"Although CNN greatly benefited from this parameter sharing scheme , Liu et al .",0
16407,"showed the inability of the convolution operation to handle simple coordinate transforms , and proposed a new operation called CoordConv , which encodes coordinate information as additional channels before convolution operation .",0
16408,Coord,0
16409,Conv was shown to improve vision tasks such as object detection and generative modeling .,0
16410,"For face alignment , the input images are always generated from a face detector with small variance on location and scale .",0
16411,These properties inspire us to include Coord Conv to help CNN :,0
16412,An overview of our model .,0
16413,"The stacked HG takes a face image cropped with the ground truth bounding box and output one predicted heatmap for each landmark , respectively .",0
16414,An additional channel is used to predict facial boundaries .,0
16415,"Due to limited space , we omitted the detailed structure of the stacked HG architecture , please refer for details .",0
16416,Our Model,0
16417,Our model is based on the stacked HG architecture from Bulat et al .,0
16418,which improved over the original convolution block design from Newell et al ..,0
16419,"For each HG , the output heatmap is supervised with the ground truth heatmap .",0
16420,We also added a sub-task of boundary prediction as an additional channel of the heatmap .,0
16421,Coordinate encoding is added before the first convolution layer of our network and before the first convolution block of each HG module .,0
16422,An overview of our model is shown in,0
16423,Adaptive Wing Loss for Face Alignment,0
16424,Loss function rationale,0
16425,"Before starting our analysis , we would like to introduce a concept from robust statistics .",0
16426,Influence is a heuristic tool used in robust statistics to investigate the properties of an estimator .,0
16427,"In the context of our paper , the influence function is proportional to the gradient of our loss function .",0
16428,So if the gradient magnitude is large at point y??,0
16429,"( indicting the error ) , then we say the loss function has a large influence at pointy ??.",0
16430,"If the gradient magnitude is close to zero at this point , then we say the loss function has a small influence at pointy ??.",0
16431,"Theoretically , for heatmap regression , training is converged only if :",0
16432,"where N is the total number of training samples , H , W and C are the height , width and channels of heatmap , respectively .",0
16433,Loss n is the loss of n ?,0
16434,"th sample , y i , j , k and y i , j, k are ground truth pixel intensity and predicted pixel intensity respectively .",0
16435,"At convergence , the influence of all errors must balance each other .",0
16436,"Hence , a positive error on a pixel with large gradient magnitude ( hence large influence ) would need to be balanced by negative errors on many pixels with smaller influence .",0
16437,Errors with large gradient magnitude will also be more focused on during training compare to errors with small gradient magnitude .,0
16438,The essence of heatmap regression is to output a Gaussian distribution centered at each ground truth landmark .,0
16439,Thus the accuracy of estimating pixel intensity at the mode of the Gaussian plays a vital role on correctly localizing landmarks .,0
16440,The two issues we illustrated in Sec. 1 result in an inaccurate estimation on the position of landmarks due to lacking of focus during training on foreground pixels .,0
16441,"In this section and Sec. 4.2 , we will discuss the causes of the first issue and how our proposed Adaptive Wing loss is able to remedy it .",0
16442,The second issue will be discussed in Sec. 4.3 .,0
16443,The first issue is due to the commonly used MSE loss function for Heatmap regression .,0
16444,"The gradient of the MSE loss is linear , so pixels with small errors have small influence , as shown in .",0
16445,This property could cause training to converge while many pixels still have small errors .,0
16446,"As a result , models trained with MSE loss tend to predict a blurry and dilated heatmap .",0
16447,"Even worse , the predicted heatmap often has low intensity on foreground pixels around difficult landmarks , e.g. occluded landmarks or faces with unusual illumination conditions .",0
16448,Accurately localizing landmarks from these low intensity pixels can be difficult .,0
16449,A good example can be found in .,0
16450,L1 loss has constant gradient so that pixels with small errors have the same influence as pixels with large errors .,0
16451,"However , the gradient of L1 loss is not continuous at point zero , which means for convergence , the amount of pixels with positive errors has to be exactly equal to the amount that has negative errors .",0
16452,The difficulty of achieving such delicate balance could cause training process to be unstable and oscillating .,0
16453,Feng et al .,0
16454,is able to improve the above loss functions by proposing,0
16455,"Wing loss that has constant gradient when error is large , and large gradient when the error is small .",0
16456,Thus pixels with small errors will be amplified .,0
16457,The Wing loss is defined as follows :,0
16458,where y and ?,0
16459,"are the pixel values on ground truth heatmap and the predicted heatmap respectively , C = ? ? ? ln ( 1 + ?/ ) is used to make function continuous at |y ??| = ?.",0
16460,"The Wing loss is , however , still not be able to overcome the discontinuity of its gradient at y ?? = 0 , with its large gradient magnitude around this point , training is even more difficult to converge compared with L1 loss .",0
16461,"This property makes the Wing loss not applicable for heatmap regression , since with the Wing loss calculated on all background pixels , small errors on background pixels are having out - ofproportion influence .",0
16462,Training a neural network that outputs zero or small gradient on these pixels is very difficult .,0
16463,"According to our experiment , the training of a heatmap regression network with the Wing loss is never able to converge .",0
16464,The above analysis leads us to define the desired properties of an ideal loss function for heatmap regression .,0
16465,"We expect our loss function to have a constant influence when error is large , so that it will be robust to inaccurate annotations and occlusions .",0
16466,"As the training process continues and errors getting smaller , there will be two scenarios : i) For foreground pixels , the influence ( as well as the gradient ) should start to increase so that the training is able to focus on reducing these errors .",0
16467,"The influence should then decrease rapidly as the errors go very close to zero , so that these "" good enough "" pixels will no longer be focused on .",0
16468,"The reduced influence of correct estimations helps the network to stay converged , instead of oscillating like the L1 and the Wing loss .",0
16469,"i i ) For background pixels , the gradient should behaves more similar to the MSE loss , that is , it will gradually decrease to zero as the training error decreases , so that the influence will be relatively small when the errors are small .",0
16470,"This property reduces the focus of the training on background pixels , stabilizing the training process .",0
16471,A fixed loss function can not achieve both properties simultaneously .,0
16472,"Thus , the loss function should be able to adapt to different pixel intensities on the ground truth heatmaps .",0
16473,"As the ground truth pixels close to the mode ( have intensities that are close to 1 ) , the influence of small errors should increase .",0
16474,"With ground truth pixel intensities close to 0 , the loss function should behave more similar to the MSE loss .",0
16475,"Since pixel values on the ground truth heatmap range from 0 to 1 , we also expect our loss function to have a smooth transition according to different pixel values .",0
16476,The Adaptive Wing Loss,0
16477,"Following intuitions above , we propose our Adaptive Wing ( AWing ) loss , defined as follows :",0
16478,where y and ?,0
16479,"are the pixel values on the ground truth heatmap and the predicted heatmap respectively , ? , ? , and ?",0
16480,"are positive values ,",0
16481,) are used to make loss function continuous and smooth at |y ??| = ?.,0
16482,Unlike Wing loss which uses ?,0
16483,"as the threshold , we introduce a new variable ?",0
16484,as a threshold to switch between linear and nonlinear part .,0
16485,"For heatmap regression , we often regress a value between 0 and 1 , so we expect our threshold lies in this range .",0
16486,"When |y ??| < ? , we consider the error to be small and need stronger influence .",0
16487,"More importantly , we adopt an exponential term ??y , which is used to adapt the shape of the loss function toy and makes loss function smooth at point zero .",0
16488,Note ?,0
16489,"has to be slightly larger than 2 to maintain the ideal properties we discussed in Sec. 4.1 , this is due to the normalization of yin the range of [ 0 , 1 ] .",0
16490,"For pixels on y with values close to 1 ( the landmarks we want to localize ) , the power term ? ?",0
16491,"y will be slightly larger than 1 , and the nonlinear part will behave like",0
16492,"Wing loss , which has large influence on smaller errors .",0
16493,"But different from Wing loss , the influence will decrease to zero rapidly as errors are very close to zero ( see ) .",0
16494,"As y decreases , the loss function will shift to a MSE - like loss function , which allows the training not to focus on the pixels that still have errors but small influence .",0
16495,shows how the power term ? ?,0
16496,"y facilities the smooth transition across different values of y , so that the influence of small errors will gradually increase as the value of y increases .",0
16497,Larger ?,0
16498,"and smaller values will increase the influence on small errors and vice versa , large ?",0
16499,values are shown to be effective according to our experiment .,0
16500,The nonlinear part of our Adaptive Wing loss function behaves similarly to Lorentzian ( aka. Cauchy ) loss in a more generalized fashion .,0
16501,"But different from robust loss functions such as Lorentzian and Geman - McClure , we do not need the gradient to decrease to zero as error increases .",0
16502,This is due to the nature of heatmap regression .,0
16503,"In robust regression , the learner learns to ignore noisy outliers with large error .",0
16504,"In the context of face alignment , all facial landmarks are annotated with relatively small noises , so we do not have noisy outliers to ignore .",0
16505,"A linear loss is sufficient for the training to converge to a location where predictions will be fairly close to the ground truth heatmap , and after that the loss function will switch to its nonlinear part to refine the prediction with increased influence on small errors .",0
16506,"In practice , we found the linear form when errors are :",0
16507,The nonlinear part of the Adaptive Wing loss is able to adapt its shape according to different values of y .,0
16508,"As y increases , the shape is more similar to the Wing loss , and the influence of small errors ( near - side of they axis ) will remain strong .",0
16509,"As y decreases , the influence on these errors will decrease and the loss function will behave more like MSE .",0
16510,"large to achieve better performance , compared with keep using the nonlinear form when the error is large .",0
16511,We empirically used ? = 2.1 in our model .,0
16512,"In our experiments , we found ? = 14 , = 1 , ? = 0.5 to be most effective , detailed ablation studies on parameter settings are shown at Sec. 7.6.1 .",0
16513,Weighted loss map,0
16514,In this section we will discuss the second issue in Sec. 4.1 .,0
16515,"Ina typical setting for facial landmark localization with a 64 64 heatmap , and the size of Gaussian of 7 7 , foreground pixels only constitute 1.2 % of all the pixels .",0
16516,Assigning equal weight for such an unbalanced data could make the training process slow to converge and result in an inferior performance .,0
16517,"To further establish the network 's ability to focus on foreground pixels and difficult background pixels ( background pixels that are close to foreground pixels ) , we introduce the Weighted Loss Map to balance the loss from different types of pixels .",0
16518,We first define our loss map mask to be :,0
16519,where H dis generated from ground truth heatmap H by a 3 3 gray dilation .,0
16520,"The loss map mask M assigns foreground pixels and difficult background pixels 1 , and other pixels 0 .",0
16521,"With the loss map mask M ,",0
16522,We define our Weighted Loss Map as follows :,0
16523,where ?,0
16524,"is element - wise production ,",0
16525,Wis a scalar hyperparameter to control how much weight to be added .,0
16526,See fora visualization of weight map generation .,0
16527,In our experiments we use W = 10 .,0
16528,The intuition is to assign pixels on heatmap with different weights .,0
16529,"Foreground pixels have to be focused on during training , since these pixels are the most useful for localizing the mode of the Gaussian distribution .",0
16530,"Difficult background pixels should also be focused on since these pixels are relatively difficult to regress , accurately regressing them could help narrow down the area of foreground pixels to improve localization accuracy . :",0
16531,"Important pixels are generated by dilating H from with 3x3 dilation , and then binarizing to with a threshold of 0.2 .",0
16532,"For visualization purposes , all channels are max - pooled into one channel .",0
16533,Boundary Information,0
16534,"Inspired by , we introduce boundary prediction into our network as a sub - task , but in a different manner .",0
16535,"Instead of breaking boundaries into different parts , we use only one additional channel as the boundary channel that combines all boundary lines to our heatmap .",0
16536,We believe this will efficiently capture the global information on a human face .,0
16537,"The boundary information then will be aggregated into the network naturally via convolution operations in a forward pass , and will also be used in Section 6 to generate the boundary coordinate map , which can further improve localization accuracy according to our ablation study in Sec. 7.6.1 .",0
16538,Coordinate aggregation,0
16539,We integrate CoordConv into our model to improve the capability of traditional convolutional neural network to capture coordinate information .,0
16540,"In addition to X , Y and radius coordinate encoding in , we also leverage our boundary prediction to generate X and Y coordinates only at boundary .",0
16541,"More specifically , we define X coordinate encoding to be C x , the boundary prediction from previous HG is B , the boundary coordinate encoding B x is defined as :",0
16542,B y is generated in the similar fashion from C y .,0
16543,The coordinate channels are generated at runtime and then concatenated with the original input to perform regular convolution .,0
16544,Experiments,0
16545,Datasets,0
16546,"We tested our approach on the COFW , 300W , 300W private test dataset and the WFLW dataset .",0
16547,The WFLW dataset is the most difficult dataset of them all .,0
16548,"For more details on theses datasets , please refer to supplementary materials .",0
16549,Evaluation Metrics,0
16550,Normalized Mean Error ( NME ) is commonly used to evaluate the quality of face alignment algorithms .,0
16551,The NME for each image is defined as :,0
16552,where P and,0
16553,"P are the ground truth and the predicted landmark coordinates for each image respectively , M is the number of landmarks of each image , p i is the i - th predicted landmark coordinates in P and pi is the i - th ground truth landmark coordinates inP , dis the normalization factor .",0
16554,"For the COFW dataset , we use inter-pupil ( distance of eye centers ) as the normalization factor .",0
16555,"For the 300 W dataset , we provide both inter-ocular distance ( distance of outer eye corners ) used as the original evaluation protocol in , and inter-pupil distance used in .",0
16556,"For the WFLW dataset , we use the inter-ocular distance described in .",0
16557,Failure Rate ( FR ) is another metric to evaluate localization quality .,0
16558,"For one image , if NME is larger than a threshold , then it is considered a failed prediction .",0
16559,"For the 300 W private test dataset , we use 8 % and 10 % respectively to compare with different approaches .",0
16560,"For the WFLW dataset , we follow and use 10 % as the threshold .",0
16561,Cumulative Error Distribution ( CED ) curve shows the NME to the proportion of total test samples .,0
16562,"The curve is usually plotted from zero up to the NME failure rate threshold ( e.g. 10 % , 8 % ) .",0
16563,Area Under Curve ( AUC ) is calculated based on the CED curve .,0
16564,Larger AUC reflects that larger portion of the test dataset is well predicted .,0
16565,Implementation details,0
16566,"During training and testing , we use provided bounding boxes from dataset ( with the longer side as the length of a square ) to crop faces from images , except for the 300 W private test dataset since no official bounding boxes are provided .",0
16567,"For the WFLW dataset , the provided bounding boxes are not very accurate , to ensure all landmarks are preserved from cropping , we enlarge the bounding boxes by 10 % on both dimensions .",0
16568,"For the 300 W private test dataset , we use ground truth landmarks to crop faces .",0
16569,"The input of the network is 256 256 , the output of each stacked HG is 64 64 .",0
16570,"We use four stacks of HG , same with other baselines .",0
16571,"During training , we use RM - SProp with an initial learning rate of 1 10 ?4 .",1
16572,We set the momentum to be 0 ( adopted from ) and the weight decay to be 1 10 ?5 .,1
16573,"We train for 240 epoches , and the learning rate is reduced to 1 10 ?5 and 1 10 ? 6 after 80 and 160 epoches .",1
16574,"Data augmentation is performed with random rotation ( 50 ) , translation ( 25 px ) , flipping ( 50 % ) , and rescaling ( 15 % ) .",1
16575,"Random Gaussian blur , noise and occlusion are also used .",1
16576,All models are trained from scratch .,0
16577,"During inference , we adopt the same strategy used in Newell et al. , the location on the pixel with the highest response is shifted a quarter pixel to the second highest nearby pixel .",0
16578,"The boundary line is generated from landmarks via distance transform similar to , different boundary lines are merged into one channel by selecting maximum values on each pixel across all channels .",0
16579,Method NME AUC 10 % FR 10 % Human 5.60 - 0.00 TCDCN ECCV,0
16580,"14 . Our approach outperforms previous state - of - the - art by a significant margin , especially on the failure rate .",0
16581,We are able to reduce the failure rate measured at 10 % NME from 3.73 % to 0.99 % .,0
16582,"As for NME , our method perform much better than human ( 5.60 % ) .",0
16583,Our performance on the COFW shows the robustness of our approach against faces with large pose and heavy occlusion .,0
16584,Evaluation on 300W,1
16585,"Our method is able to achieve the state - of - the - art performance on the 300W testing dataset , see .",1
16586,"For the challenge subset ( iBug dataset ) , we are able to outperform",1
16587,"Wing by a significant margin , which also proves the robustness of our approach against occlusion and large pose variation .",1
16588,"Furthermore , on the 300 W private test dataset ) , we again outperform the previous state - of - theart on variant metrics including NME , AUC and FR measured with either 8 % NME and 10 % NME .",1
16589,"Note that we more than halved the failure rate of the next best baseline to 0.83 % , which means only 5 faces out of 600 have an NME that is larger than 8 % .",0
16590,Evaluation on WFLW,1
16591,"Our method again achieves the best results on the WFLW dataset in , which is significantly more difficult than COFW and 300W ( see for visualizations ) .",1
16592,On every subset we outperform the previous state - of - the - art ap - :,1
16593,Evaluation on the 300W testset proaches by a significant margin .,0
16594,"Note that the baseline Wing is using ResNet50 as the backbone architecture , which already performs better than the CNN6 / 7 architecture they used in COFW and 300W .",0
16595,We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly .,0
16596,"All in all , our approach fails on only 2.84 % of all images , more than a two times improvement compared with 7.6 .",1
16597,Ablation study,0
16598,.1,0
16599,Evaluation on different loss function parameters,0
16600,"To find the optimal parameter settings for the Adaptive Wing loss for heatmap regression , we examined different parameter combinations and evaluated on the WFLW dataset with faces cropped from ground truth landmarks .",0
16601,"However , the search space is too large and we only have limited resources .",0
16602,"To reduce the search space , we set our initial ? to 0.5 , since the pixel value of the ground truth heatmap is from 0 to 1 , we believe focusing on errors that are smaller than 0.5 is more than enough .",0
16603,shows NMEs on different combinations of ? and .,0
16604,"As a result , we picked ? = 14 and = 1 .",0
16605,The experiments also show our Adaptive Wing loss is not very sensitive to ?,0
16606,"and , since the difference of NMEs are not significant within a certain range of different settings .",0
16607,Then we fixed ?,0
16608,"and , and examine different ? , the results are shown in .",0
16609,Evaluation of different modules,0
16610,Evaluation on the effectiveness of different modules is shown in .,0
16611,The dataset used for ablation study is WFLW .,0
16612,"During training and testing , faces are cropped from ground truth landmarks .",0
16613,Note the baseline model ( model trained with MSE ) underperforms the state - of - theart .,0
16614,"To compare with a naive weight mask without focus on hard negative pixels , we introduced a baseline weight map W M base =? W + 1 , where W = 10 . The major contribution comes from Adaptive Wing loss , which improves the benchmark by 0.74 % .",0
16615,"All other modules contributed incrementally to the localization performance , our Weighted Loss Map improves 0.25 % , boundary prediction and coordinates encoding are able to contribute another 0.09 % .",0
16616,Our Weighted Loss Map also outperforms,0
16617,"W M base by a considerable margin , thanks to its ability to focus on hard background pixels .",0
16618,Evaluation on human pose estimation,0
16619,"Although this paper mainly deals with face alignment , we have also performed experiments to prove the ability of the proposed Adaptive Wing loss in another heatmap regression task , human pose estimation .",0
16620,We choose LSP ( using person - centric ( PC ) annotations ) as evaluation dataset .,0
16621,"LSP dataset consists of 11,000 training images and 1,000 testing images .",0
16622,Each image is labeled with 14 keypoints .,0
16623,"The goal of this experiment is to examine the capability of the proposed Adaptive Wing loss to handle the pose estimation task compared with baseline MSE loss , rather than achieving the state - of - the - art inhuman pose estimation .",0
16624,"Some other works obtain better results by adding MPII into training or as pre-training , or use re-annotated labels with high resolution images in .",0
16625,"Besides the MSE loss baseline , we also reported baselines from methods that trained solely on the LSP dataset .",0
16626,We trained our model from scratch with original labeling and low resolution images to see how well our Adaptive Wing loss could handle labeling noise and low quality images .,0
16627,Percentage Correct Keypoints ( PCK ) is used as the evaluation metric with torso dimension as the normalization factor .,0
16628,Please refer to the supplemental materials for more implementation details .,0
16629,Results are shown in .,0
16630,"Our proposed Adaptive Wing loss significantly boosts performance compared with MSE , which proves the general applicability of the proposed Adaptive Wing loss on more heatmap regression tasks .",0
16631,Conclusion,0
16632,"In this paper , we located two issues in the MSE loss function in heatmap regression .",0
16633,"To resolve these issues , we proposed the Adaptive Wing loss and Weighted Loss Map for accurate localization of facial landmarks .",0
16634,"To further improve localization results , we also introduced boundary prediction and CoordConv with boundary coordinates into our model .",0
16635,"Experiments show that our approach is able to outperform the state - of - the - art on multiple datasets by a significant margin , using various evaluation metrics , especially on failure rate and AUC , which indicates our approach is more robust to difficult scenarios .",0
16636,Acknowledgement,0
16637,This paper is partially supported by the National Science Foundation under award 1751402 .,0
16638,Datasets Used in Our Experiments,0
16639,"The COFW [ 8 ] dataset includes 1,345 training images and 507 testing images annotated with 29 landmarks .",0
16640,This dataset is aimed to test the effectiveness of face alignment algorithms on faces with large pose and heavy occlusion .,0
16641,Various types of occlusions are introduced and result in a 23 % occlusion on facial parts in average .,0
16642,The 300W is widely used as a 2 D face alignment benchmark with 68 annotated landmarks .,0
16643,"300 W consists of the following subsets : LFPW , HELEN , AFW , XM2 VTS and an additional dataset with 135 images with large pose , occlusion and expressions called iBUG .",0
16644,"To compare with other approaches , we adopt the widely used protocol described in to train and evaluate our approach .",0
16645,"More specifically , we use the training dataset of LFPW , HELEN , and the full AFW dataset as training dataset , and the test dataset of LFPW , HELEN and the full iBUG dataset as full test dataset .",0
16646,"The full test dataset is then further split into two subsets , the test dataset of LFPW and HELEN is called the common test dataset , and iBUG is called the challenge test dataset .",0
16647,"There is also a 300 W private test dataset for the 300 W contest , which contains 300 indoor and 300 outdoor faces .",0
16648,We also evaluated our approach on this dataset .,0
16649,"The WFLW is a newly introduced dataset with 98 manually annotated landmarks that constitutes of 7,500 training images and 2,500 testing images .",0
16650,"In addition to denser annotations , it also provides attribute annotations including pose , expression , illumination , make - up , occlusion and blur .",0
16651,The six different subsets can be used for analyzing algorithm performance on subsets with different properties separately .,0
16652,"The WFLW is considered more difficult than commonly used datasets such as AFLW and 300W due to its more densely annotated landmarks and difficult faces with occlusion , blur , large pose , makeup , expression and illumination .",0
16653,"For the LSP dataset , we used original label from author 's official website .",0
16654,"Although images with original resolutions are also provided , we choose not to use them .",0
16655,"Also , we did not use re-annotated labels on LSP extended 10,000 training images from .",0
16656,Note that occluded keypoints are annotated in LSP original dataset but not in LSP extended training dataset .,0
16657,"During training , we did not calculate loss on occluded keypoints for LSP extended training dataset .",0
16658,"During training and testing , we did not follow [? ] to crop single person from images with multiple persons to retain the difficulties of this dataset .",0
16659,Data augmentations is performed similarly to training with face alignment datasets .,0
16660,Evaluation on AFLW,0
16661,"The AFLW dataset contains 24,368 faces with large poses .",0
16662,"All faces are annotated by up to 21 landmarks per image , while the occluded landmarks were not labeled .",0
16663,"For fair comparison with other methods we adopt the protocol from , which provides revised annotations with 19 landmarks .",0
16664,"The training dataset contains 20,000 images , the full testing dataset contains 4,368 iamges .",0
16665,"A subset of 1,314 frontal faces ( no landmarks are occluded ) are selected from the full test dataset as the frontal test set .",0
16666,Method,0
16667,Full ( % ) Frontal ( % ) RCPR CVPR 13 3.73 2.87 ERT CVPR 14 4.35 2.75 LBF CVPR 14 4.25 2.74 CFSS CVPR 15 3.92 2.68 CCL CVPR 16 2.72 2.17 TSR CVPR 17 2.17 - DAC - OSR CVPR 17 2.27 1.81 DCFE ECCV 18 2.17 - CPM + SBR CVPR 18 2.14 - SAN CVPR 18 1.91 1.85 DSRN CVPR 18 1.86 - LAB CVPR 18 1.85 1.62 Wing CVPR 18 1.65 - RCN + ( L+ELT +A ) CVPR,0
16668,Experiment on different number of HG stacks,0
16669,We compare the performance of different number of stacks of HG module ( see details in ) .,0
16670,"With reduced number of HGs , the performance of our approach remains outstanding .",0
16671,"Even with only one HG block , our approach still outperforms previous state - of - the - arts in all datasets except the common subset and the full dataset of 300W .",0
16672,Note that the one HG model is able to run at 120 FPS with Nvidia GTX 1080 Ti graphics card .,0
16673,The result reflects the effectiveness of our approach on limited computation resources .,0
16674,Result Visualization,0
16675,"For visualization purpose , some localization results are shown in and : NME ( % ) on different number of stacks .",0
16676,"The NMEs of 300W are normalized by inter-pupil / inter - ocular distance , the NMEs of COFW are normalized by inter-pupil distance , and the NMEs of 300W Private and WFLW are normlaized by inter-ocular distance .",0
16677,"NMEs in the "" Previous Best "" row are selected from to 4 in our main paper .",0
16678,Runtime is evaluated on Nvidia GTX 1080 Ti graphics card with batch size of 1 .,0
16679,title,0
16680,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,1
16681,abstract,0
16682,"Cascaded Regression ( CR ) based methods have been proposed to solve facial landmarks detection problem , which learn a series of descent directions by multiple cascaded regressors separately trained in coarse and fine stages .",0
16683,They outperform the traditional gradient descent based methods in both accuracy and running speed .,0
16684,"However , cascaded regression is not robust enough because each regressor 's training data comes from the output of previous regressor .",0
16685,"Moreover , training multiple regressors requires lots of computing resources , especially for deep learning based methods .",0
16686,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to improve the model efficiency .",0
16687,"Only one self - iterative regressor is trained to learn the descent directions for samples from coarse stages to fine stages , and parameters are iteratively updated by the same regressor .",0
16688,"Specifically , we proposed Landmarks - Attention Network ( LAN ) as our regressor , which concurrently learns features around each landmark and obtains the holistic location increment .",0
16689,"By doing so , not only the rest of regressors are removed to simplify the training process , but the number of model parameters is significantly decreased .",0
16690,"The experiments demonstrate that with only 3.72 M model parameters , our proposed method achieves the stateof - the - art performance .",0
16691,Introduction,0
16692,"Facial landmarks detection is one of the most important techniques in face analysis , such as face recognition , facial animation and 3D face reconstruction .",0
16693,"It aims to detect the facial landmarks such as eyes , nose and mouth , namely predicting the location parameters of landmarks .",0
16694,Researchers usually regard this task as atypical non -linear least squares problem .,0
16695,"The Newton 's method and its variants are the traditional gradient based solution , whose convergence rate is quadratic and is guaranteed to converge , provided that the initial estimate is sufficiently close to the minimum .",0
16696,"However , when the objective function is not differentiable ( e.g. SIFT ) or the Hessian matrix is not positive definite , the method wo n't works well .",0
16697,"In recent years , cascaded regression based methods Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",0
16698,All rights reserved .,0
16699,( a ) Cascaded Regression .,0
16700,( b ) Self - Iterative Regression . :,0
16701,Facial landmarks detection process of Cascaded Regression ( a ) and Self - Iterative Regression ( b ) .,0
16702,"To predict the landmarks ' location parameters , the CR based methods require multiple regressors , while SIR just need one regressor and updates parameters iteratively .",0
16703,have been proposed and applied to solve the non-linear least squares problem .,0
16704,"They usually train multiple regressors to predict the parameters ' increment sequentially , which outperform the traditional gradient descent based methods in both accuracy and running speed .",0
16705,"Moreover , deep learning based cascaded regression methods ) are widely leveraged for this task because of the powerful ability to extract the discriminative feature .",0
16706,"However , when applying cascaded regression system , three main problems arise :",0
16707,"( 1 ) Each regressor just works well in its local data space , when previous regressor predicts the false descent direction , the final results are very likely to drift away ; ( 2 ) In general , higher accuracy can be obtained by adding more cascaded regressors , while it will increase model storage memory and computing resources ;",0
16708,"( 3 ) Subsequent regressors usually can not be activated for training until previous regressors finished their training process , which increases the system complexity .",0
16709,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .",1
16710,"By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .",1
16711,"The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .",0
16712,The testing process is illustrated in ( b ) .,0
16713,"The experimental results show that for deep learning based method , one regressor achieves comparable performance to state - of the - art multiple cascaded regressors and significantly reduce the training complexity .",0
16714,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .",1
16715,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .",1
16716,The contributions of this paper are summarized as follows :,0
16717,"1 . We propose a novel regression framework called SIR to solve the non-linear least squares problem , which simplifies the cascaded regression framework and obtains stateof - the - art performance in facial landmarks detection task .",0
16718,"2 . The Landmarks - Attention Network ( LAN ) is developed to independently learn discriminative features around each landmarks , which significantly reduces the dimension of feature layer and the number of model parameters .",0
16719,3 . Experimental results on several publicly available benchmarks demonstrate the effectiveness of the proposed method .,0
16720,Related Work,0
16721,"In this section , we will review related works in solving nonlinear least squares problems , especially facial landmarks detection problem. , which consists of multiple Stacked Auto - encoder Networks ( SANs ) .",0
16722,"The first SAN quickly predicts the preliminary location of landmarks by a low - resolution image , and the subsequent SANs then refine the location with higher and higher resolution .",0
16723,"Trigeorgis et al. proposed the Mnemonic Descent Method ( MDM ) , which regards the nonlinear least squares optimization as a dynamic process .",0
16724,The Recurrent Neural Network ( RNN ) is introduced to maintain an internal memory unit that accumulates the history information so as to relate the cascaded refinement process .,0
16725,Joo et al. proposed a iterative error feedback ( Carreira et al. 2016 ) method to solve the human pose extimation problems .,0
16726,"Same with MDM , their training data is generated by previous stages , while ours is obtained by random sampling in coarse stages and fine stages , which simplifies the training process .",0
16727,Xiao et al.,0
16728,"propose a Long Short Term Memory ( LSTM ) based recurrent attentive - refinement network , which also follows the pipeline of cascaded regressions .",0
16729,"Instead of updating all landmarks location together , it first extracts reliable landmarks by a CNN and then infers locations of the rest noisy landmarks , resulting in improved accuracy .",0
16730,"However , these deep cascaded regression methods usually require more computing resources and also suffer from the same drawbacks as discussed above .",0
16731,Cascaded Regression,0
16732,"Before introducing our method , we begin with the cascaded regression framework in brief for better understanding .",0
16733,"As illustrated in ( a ) , in the training process of cascaded regression , K regressors ( R 1 , R 2 , , R K ) are trained sequentially .",0
16734,Each regressor R k is computed by minimizing the expected loss between the predicted and the optimal parameters 's increment .,0
16735,It is formulated as,0
16736,"( 1 ) where x k, i is i th example ink th regression process , ?? * k , i = ? * i ? ?",0
16737,"k , i is the corresponding target increment , i.e. , the difference between ground truth parameter ? * i and present parameter ?",0
16738,"k , i .",0
16739,"After obtaining R k , the target parameter is updated by Equ. ,",0
16740,"( 2 ) Then , new training dataset will be generated according to the updated parameter for the next regression .",0
16741,"In the testing process , parameter will be sequentially refined by these cascaded regressors in Equ 2 .",0
16742,Self - Iterative Regression,0
16743,"In this section , we will describe our facial landmarks detection method including the Gaussian random sampling and the Landmarks - Attention Network in detail .",0
16744,The overall procedure is presented in .,0
16745,Gaussian random,0
16746,Sampling,0
16747,Generating training data is key important process in our method .,0
16748,"Cascaded regression generates training data according to previous regressor , while our method obtains it ( c ) Iterative predicting and updating process .",0
16749,"The training process consists of ( a ) and ( b ) , while the testing process consists of ( b ) and ( c ) .",0
16750,"In the figure , one of the dimension of facial Landmarks Model parameter S is showed , and ?",0
16751,is landmarks ' location parameter .,0
16752,"by random sampling , which includes most possible landmarks distribution from coarse stages to fine stages .",0
16753,"Let ( x j , y j ) be the j th landmark 's position coordinates and ? = ( x 1 , y 1 , x 2 , y 2 , ... , x M , y M ) be all landmarks ' location parameters , where M is the total number of facial landmarks .",0
16754,It is not a good choice to directly sample in location parameter ?,0
16755,since it s dimension is so high that the training process will be hard to converge and is very likely to generate unreasonable face shape .,0
16756,"To improve the effectiveness of sampling process , we indirectly obtain sampling location ?",0
16757,according to anew facial landmarks model that is similar to 3D Morphable Model ( 3 DMM ) .,0
16758,Facial landmarks distribution will be represented by pose and shape parameter .,0
16759,Facial Landmarks Model .,0
16760,"We obtain intrinsic face shape parameter by Principal Component Analysis ( PCA ) and pose parameter ( including 2D translation , in - plane rotation and scale ) by geometry transformation .",0
16761,"The shape , translation , rotation angle and scale coefficient are represented by ? , t 2 d , ? , f respectively .",0
16762,"Finally , facial landmarks model parameters can be represented by S = [? , t 2 d , ? , f ] .",0
16763,Sand ?,0
16764,are two kinds of representation for facial landmarks .,0
16765,Scan be converted to ?,0
16766,by,0
16767,"where S 0 is the mean shape , A is the PCA shape matrix and",0
16768,Random sampling in facial landmarks model,0
16769,Sand then converting to location parameter ?,0
16770,makes the sampling process easier to control and generates more reasonable landmarks ' distribution .,0
16771,Sampling space .,0
16772,"For each face I , let Sgt represent its ground truth facial landmarks model parameters .",0
16773,We random select values in each dimension of S obeying distribution D which is a union set of two Gaussian distribution .,0
16774,The sampling space of each face is represented by,0
16775,"where N ( , ) represents Gaussian distribution , and ?",0
16776,is its standard deviation .,0
16777,"We adopt this sampling distribution because training regressor around mean location and ground truth location affects the performance in coarse and fine stages , respectively , and the final location error usually obeys Gaussian distribution .",0
16778,The value of standard deviation ?,0
16779,affects the final performance .,0
16780,System with larger ?,0
16781,"will contain more training space which makes the system more robust , while the final accuracy may decrease because sampling probability around ground truth will decrease and vise versa .",0
16782,The effect of ? will be discussed in the Experiments section .,0
16783,"For i - th image in the t - th sampling period , sampling parameter S t , i is obtained by random selecting a value in Equ . ( 4 ) .",0
16784,We then calculate location parameters ?,0
16785,"t , i by Equ ( 3 ) and extract patches P t , i in location ?",0
16786,"t , i .",0
16787,"Finally , we set P t , i as the training input data and set ?? t , i = ? * i ? ?",0
16788,"t , i as regressor 's corresponding target increment .",0
16789,"The process is also summarized in Algorithm 1 , and the training data is represented as",0
16790,"where T is the number of sampling period , N is the number of images in raw dataset .",0
16791,The sampling process is illustrated in .,0
16792,"By the sampling process , we obtained nearly unlimited training data and the training space contains most possible landmarks ' distribution from coarse stages to fine stages .",0
16793,The sampled training data is online generated to save the system memory .,0
16794,Algorithm,0
16795,1 Sampling process of SIR Input : Raw face landmarks dataset :,0
16796,"for i = 1 to N do Set P t , i as the regressor 's input data ; 7 :",0
16797,"Set ?? t , i = ? * i ? ?",0
16798,"t , i as regressor 's target increment ; 8 : end for 9 : end for",0
16799,Landmarks - Attention Network,0
16800,"In this section , we will describe the structure of the proposed regressor .",0
16801,Our goal is to learn a mapping between appearance features and landmarks ' location increment .,0
16802,Previous works usually first obtain robust initialization location by extracting features in the whole image and then refine the location by many refinement networks or stack all landmarks patches to directly extract all landmarks features .,0
16803,They either require a number of model parameters or generate indiscriminative features .,0
16804,Thus we propose a Landmarks - Attention Network ( LAN ) to overcome the above two drawbacks .,0
16805,Our regressor is a single CNN which concurrently pays attention to appearance feature around each facial landmark .,0
16806,"Specifically , for each landmarks patch , we extract features by several convolutional and pooling layers , then concatenate these independent feature vectors and add two fully connected layers to learn a holistic location increment .",0
16807,The structure of each feature extraction sub -network is illustrated in ( b ) and the detailed information of the sub-network is presented in .,0
16808,"Compared to the previous networks , our proposed model has three advantages :",0
16809,"( 1 ) The landmarks feature extracted by independent sub - networks can be more discriminative , as showed in ( 2 ) Concatenating all independent features vectors and adding fully connected layers can obtain a holistic landmarks location increment , especially when some landmarks are occluded or blurred ; ( 3 ) Our network is very light , whose parameters number ( 3.72 M in total ) is far less than other CNN models ( e.g. , AlexNet ( Krizhevsky , Sutskever , and Hinton 2012 ) contains about 60M parameters and VGGNet contains about 138M parameters ) .",0
16810,Training,0
16811,The training process is illustrated in ( a ) and ( b ) .,0
16812,"Since sampling period T can be large enough , online random sampling process can generate nearly unlimited training data T t=1 N i=1 ( P t , i , ?? t , i ) . Then , the above described LAN is trained to learn the descent directions in coarse and fine stages together .",0
16813,This process can be formulated as,0
16814,"where R ? is the target self - iterative regressor ( i.e. , LAN ) , and t indicates the t th sampling period .",0
16815,"Since the training space of SIR includes most possible landmarks distribution from coarse stages to fine stages , the training process will generate a Descent Direction Map ( DDM ) in the sampling space where each sample 's descent direction roughly points toward the ground truth .",0
16816,"As illustrated in , SIR is more robust than CR because the former can cover more training space and is n't affected by the optimization path .",0
16817,"When the previous regressor predicts false descent directions , SIR can still converge to the ground truth while CR is prone to drift away .",0
16818,"( a ) Cascaded Regression ( b ) Self - Iterative Regression : ( a ) Typical cascaded regression process : starting from initial value , parameters are updated and close to the ground truth ( such as init ? C 1 ? C 2 ? gt ) by regressors R k ( k = 1 , 2 , 3 , ... ) .",0
16819,"Once one regressor predicts the false direction , the final result is prone to drift away ; ( b ) SIR Descent Direction Map : the training space of SIR includes distribution from coarse stages to fine stages and all descent directions are pointed to ground truth .",0
16820,Self - Iterative Updating,0
16821,"For the testing process , similar to the cascaded regression methods , starting from initial location parameters ?",0
16822,"0 , we iteratively update the location parameters ?",0
16823,k and extract new patches P k till converges .,0
16824,"The process is presented in Algorithm 2 , and facial landmarks location parameter is updated by , ?",0
16825,"k+1 = ? k + R ? ( P k ) , k = 0 , 1 , . ( 7 )",0
16826,"Algorithm 2 Self - Iterative updating process of SIR Input : Regressor R ? , Initial location ?",0
16827,"0 , Total iteration times K Output : Prediction of facial landmarks ' location ?",0
16828,K 1 : fork = 0 to K ?,0
16829,1 do 2:,0
16830,Extract patches,0
16831,P kin location ?,0
16832,k 3:,0
16833,Experiments,0
16834,"In this section , we perform experiments to demonstrate the effectiveness of the proposed SIR compared to state - of - theart methods .",0
16835,"Specifically , we evaluate the proposed method model by ( 1 ) comparing the performance of SIR vs. stateof - the - art and baseline cascaded regression ; ( 2 ) comparing the number of model parameters and memory storage of pretrain models ; and ( 3 ) studying the effect of the proposed feature extraction network ( LAN ) , the number of iteration times and sampling space parameter .",0
16836,Datasets .,0
16837,"The 300 - W dataset is short for 300 faces in - thewild , which is designed for evaluating the performance of facial landmarks detection .",0
16838,"The training set ( 3 , 148 faces in total ) consists of AFW dataset ( Ramanan 2012 ) , HELEN training set ) and LFPW training set .",0
16839,"Two testing sets are established , i.e. , public testing set ( 689 faces in total ) including HELEN testing set , LFPW testing set ) and IBUG dataset ; and competition testing set ( 600 faces in total ) including 300 indoor and 300 outdoor faces images .",0
16840,Metrics .,0
16841,Normalized Mean Error ( NME ) measures landmarks ' mean location error normalized by inter-pupil ( eyes centers ) distance or interocular ( outer eye corners ) distance .,0
16842,"Cumulative Error Distribution ( CED ) curve is the cumulative distribution function of the normalized error , which can avoid heavily impacted by some big failures .",0
16843,"We also calculated another two evaluation metrics , namely Area - Under - the - Curve ( AUC ? ) and Failure Rate ( FR ? ) .",0
16844,"Similar as MDM ) , we consider mean point - to - point error greater than 0.08 as a failure , i.e. , ? = 0.08 . Implementation Detail .",0
16845,"We perform the experiments based on a machine with Core i7 - 5930 k CPU , 32 GB memory and GTX 1080 GPU with 8G video memory .",1
16846,The detected faces are resized into 256 256 and the location patch size is 57 57 .,1
16847,"For CNN structure , the Rectified Linear Unit ( ReLU ) is adopted as the activation function , and the optimizer is the Adadelta ( Zeiler 2012 ) approach , learning rate is set to 0.1 and weight decay is set to 1 e ?",1
16848,4 . Training the CNN requires around 2 days .,1
16849,Comparison with State - of - the - arts,0
16850,"As shown in , we compare the proposed method with several state - of - the - art facial landmarks detection methods in the public testing set .",0
16851,"Specifically , the common subset consists of LFPW testing set ( 224 faces ) and HELEN testing set ( 330 faces ) and the challenging subset is IBUG dataset ( 135 faces ) .",0
16852,Thus the the full set ( 689 faces ) of the union of the common ( 554 faces ) and challenging subsets ( 135 faces ) .,0
16853,The NME results shows that SIR performs comparatively with RAR ) and outperform other existing methods .,1
16854,"Besides , more visual results are also illustrated in .",0
16855,"In the more challenging IBUG subset , our method achieves robust performance in large pose , expression and illumination environment .",1
16856,"On the other hand , we evaluate SIR in the competition testing set .",0
16857,"As shown in , the SIR method outperform the state - of - the - art methods according to the CED curve .",1
16858,"Moreover , presents the quantitative results for both the 51 - point and 68 - point error metrics ( i.e. , AUC and Failure Rate at a threshold of 0.08 of the normalised error ) , compared to existing methods ) .",0
16859,The promising performances on two metrics indicate the effectiveness of the proposed method .,0
16860,Comparison with Cascaded Regression,0
16861,"As discussed before , previous cascaded regression methods adding more regressors can achieve better performance , but increase the number of model parameters , computing resources and storage space , especially for deep learning based methods .",0
16862,"Different from them , our method obtains state - of - the - art performance by iterative call the same regressor rather than adding anymore regressors .",0
16863,"Our method reduces the model complexity while keeps the performance in two folds : ( 1 ) the proposed network focuses on the landmarks ' local feature , which significantly reduces the dimension of final feature layer ;",0
16864,"( 2 ) only one CNN module is required to iteratively predict the location parameters , while cascaded regression usually requires at least three regressors .",0
16865,"To prove the effectiveness of SIR , we add a baseline CR method which extracts features by the same LAN while adopts cascaded regression framework .",0
16866,Both baseline CR and SIR is updated for 4 times before the stable performance .,0
16867,"As shown in , our method requires parameters and memory far less than other cascaded regression based methods .",0
16868,Discussion and Analysis,0
16869,"In this section , we perform analyses on the effect of several important modules in our method to the final performance .",0
16870,Effect of different feature extraction networks .,0
16871,"In SIR framework , we adopt the Landmarks - Attention Network ( we call it SIR - LAN ) to extract landmarks patches features separately , while some works stack all landmarks patches and then extract the whole features directly ( we call it SIR - Stack ) , as illustrated in .",0
16872,"To demonstrate the effectiveness of our network , we conduct an experiment by SIR framework to compare the above two networks with the same number of CNN layers and model parameters , the structure of SIR - Stack is showed in .",0
16873,"The result illustrated in shows that the proposed network extracting patches features separately performs significantly better than previous methods extracting patches feature together ( e.g. , MDM ) . Effect of iteration times .",0
16874,"From , we can find that the accuracy will be improved by adding iteration times before the stable performance ( i.e. , 4 iterations ) is achieved .",0
16875,"When increasing iteration times , more model memory will be added in baseline CR .",0
16876,Figure 7 : Effect of iteration times .,0
16877,Top : Comparison between SIR and baseline CR inaccuracy .,0
16878,"With the increase of iteration times , both SIR and baseline CR can decrease the detection error and SIR performs better than baseline CR .",0
16879,Bottom : Comparison between SIR and baseline CR in in Model Memory .,0
16880,"Increasing the iteration times will increase its model memory of baseline CR , while SIR does n't because it can iteratively call itself .",0
16881,Effect of Gaussian sampling space parameters .,0
16882,"As one of the most important processes , random sampling space significantly affects the final robustness and accuracy .",0
16883,"As shown in , the NME results are presented by varying the standard deviation ? of Gaussian sampling .",0
16884,Appropriate values lead to promising performance so that we set ? = 0.2 in our method .,0
16885,Conclusion,0
16886,"In this paper , we develop a SIR framework solve the nonlinear least squares problems .",0
16887,"Compared with cascaded regression , it only needs to train a single regressor to learn descent directions in coarse stages to fine stages together , and refines the target parameters iteratively by call the same regressor .",0
16888,"Experimental results in the facial landmarks detection task demonstrate that the proposed self - iterative regressor achieves comparable accuracy to state - of - the - art methods , but significantly reduces the number of parameters and memory storage of the pre-trained models .",0
16889,"In the future , we will extend the proposed method to other applications , such as human pose prediction , structure from motion and 3D face reconstruction .",0
16890,title,0
16891,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,1
16892,abstract,0
16893,"Figure 1 : A few results from our VRN - Guided method , on a full range of pose , including large expressions .",0
16894,Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .,1
16895,"Current systems often assume the availability of multiple facial images ( sometimes from the same subject ) as input , and must address a number of methodological challenges such as establishing dense correspondences across large facial poses , expressions , and non-uniform illumination .",0
16896,In general these methods require complex and inefficient pipelines for model building and fitting .,0
16897,"In this work , we propose to address many of these limitations by training a Convolutional Neural Network ( CNN ) on an appropriate dataset consisting of 2D images and 3D facial models or scans .",0
16898,"Our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3D Morphable Model .",0
16899,We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image .,0
16900,"We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality , especially for the cases of large poses and facial expressions .",0
16901,Code and models will be made available at,0
16902,Introduction,0
16903,3 D face reconstruction is the problem of recovering the 3D facial geometry from 2D images .,0
16904,"Despite many years of research , it is still an open problem in Vision and Graphics research .",0
16905,"Depending on the setting and the assumptions made , there are many variations of it as well as a multitude of approaches to solve it .",0
16906,This work is on 3D face reconstruction using only a single image .,0
16907,"Under this setting , the problem is considered far from being solved .",0
16908,"In this paper , we propose to approach it , for the first time to the best of our knowledge , by directly learning a mapping from pixels to 3D coordinates using a Convolutional Neural Network ( CNN ) .",0
16909,"Besides its simplicity , our approach works with totally unconstrained images downloaded from the web , including facial images of arbitrary poses , facial expressions and occlusions , as shown in .",0
16910,Motivation .,0
16911,"No matter what the underlying assumptions are , what the input ( s ) and output ( s ) to the algorithm are , 3 D face reconstruction requires in general complex pipelines and solving non-convex difficult optimization problems for both model building ( during training ) and model fitting ( during testing ) .",0
16912,"In the following paragraph , we provide examples from 5 predominant approaches :",0
16913,1 .,0
16914,"In the 3D Morphable Model ( 3 DMM ) , the most popular approach for estimating the full 3D facial structure from a single image ( among others ) , training includes an iterative flow procedure for dense image correspondence which is prone to failure .",0
16915,"Additionally , testing requires a careful initialisation for solving a difficult highly non-convex optimization problem , which is slow .",0
16916,"2 . The work of , a popular approach for 2.5 D reconstruction from a single image , formulates and solves a carefully initialised ( for frontal images only ) non-convex optimization problem for recovering the lighting , depth , and albedo in an alternating manner where each of the sub-problems is a difficult optimization problem per se .",0
16917,"3 . In , a quite popular recent approach for creating a neutral subject - specific 2.5 D model from a near frontal image , an iterative procedure is proposed which entails localising facial landmarks , face frontalization , solving a photometric stereo problem , local surface normal estimation , and finally shape integration .",0
16918,"4 . In , a state - of - the - art pipeline for reconstructing a highly detailed 2.5 D facial shape for each video frame , an average shape and an illumination subspace for the specific person is firstly computed ( offline ) , while testing is an iterative process requiring a sophisticated pose estimation algorithm , 3 D flow computation between the model and the video frame , and finally shape refinement by solving a shape - from - shading optimization problem .",0
16919,"5 . More recently , the state - of - the - art method of that produces the average ( neutral ) 3 D face from a collection of personal photos , firstly performs landmark detection , then fits a 3 DMM using a sparse set of points , then solves an optimization problem similar to the one in , then performs surface normal estimation as in and finally performs surface reconstruction by solving another energy minimisation problem .",0
16920,Simplifying the technical challenges involved in the aforementioned works is the main motivation of this paper .,0
16921,Main contributions,0
16922,"We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry , and an appropriate CNN architecture that is trained to regress directly from a 2 D facial image to the corresponding 3D volume .",1
16923,An overview of our method is shown in .,0
16924,"In summary , our contributions are :",0
16925,"Given a dataset consisting of 2D images and 3D face scans , we investigate whether a CNN can learn directly , in an end - to - end fashion , the mapping from image pixels to the full 3 D facial structure geometry ( including the non-visible facial parts ) .",0
16926,"Indeed , we show that the answer to this question is positive .",0
16927,"We demonstrate that our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry bypassing the construction ( during training ) and fitting ( during testing ) of a 3 DMM .",0
16928,We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image .,0
16929,3 DMM fitting is not used .,0
16930,Our method uses only 2D images as input to the proposed CNN architecture .,0
16931,"We show how the related task of 3D facial landmark localisation can be incorporated into the proposed framework and help improve reconstruction quality , especially for the cases of large poses and facial expressions .",0
16932,"We report results fora large number of experiments on both controlled and completely unconstrained images from the web , illustrating that our method outperforms prior work on single image 3 D face reconstruction by a large margin .",0
16933,Closely related work,0
16934,"This section reviews closely related work in 3 D face reconstruction , depth estimation using CNNs and work on 3D representation modelling with CNNs .",0
16935,3 D face reconstruction .,0
16936,"A full literature review of 3D face reconstruction falls beyond the scope of the paper ; we simply note that our method makes minimal assumptions i.e. it requires just a single 2D image to reconstruct the full 3 D facial structure , and works under arbitrary poses and expressions .",0
16937,"Under the single image setting , the most related works to our method are based on 3 DMM fitting and the work of which performs joint face reconstruction and alignment , reconstructing however a neutral frontal face .",0
16938,"The work of describes a multi-feature based approach to 3 DMM fitting using non-linear least - squares optimization ( Levenberg - Marquardt ) , which given appropriate initialisation produces results of good accuracy .",0
16939,"More recent work has proposed to estimate the update for the 3 DMM parameters using CNN regression , as opposed to non-linear optimization .",0
16940,"In , the 3 DMM parameters are estimated in six steps each of which employs a different CNN .",0
16941,"Notably , estimates the 3 DMM parameters on a sparse set of landmarks , i.e. the purpose of is 3 D face alignment rather than face reconstruction .",0
16942,The method of is currently considered the state - of - the - art in 3 DMM fitting .,0
16943,It is based on a single CNN that is iteratively applied to estimate the model parameters using as input the 2D image and a 3D - based representation produced at the previous iteration .,0
16944,"Finally , a state - of - the - art cascaded regression landmark - based 3 DMM fitting method is proposed in .",0
16945,Our method is different from the aforementioned methods in the following ways :,0
16946,Our method is direct .,0
16947,"It does not estimate 3 DMM parameters and , in fact , it completely bypasses the fitting of a 3 DMM .",0
16948,"Instead , our method directly produces a 3D volumetric representation of the facial geometry .",0
16949,"Because of this fundamental difference , our method is also radically different in terms of the CNN architecture used : we used one that is able to make spatial predictions at a voxel level , as opposed to the networks of which holistically predict the 3 DMM parameters .",0
16950,Our method is capable of producing reconstruction results for completely unconstrained facial images from the web covering the full spectrum of facial poses with arbitrary facial expression and occlusions .,0
16951,"When compared to the state - of - the - art CNN method for 3 DMM fitting of , we report large performance improvement .",0
16952,"Compared to works based on shape from shading , our method can not capture such fine details .",0
16953,"However , we believe that this is primarily a problem related to the dataset used rather than of the method .",0
16954,"Given training data like the one produced by , then we believe that our method has the capacity to learn finer facial details , too .",0
16955,CNN - based depth estimation .,0
16956,Our work has been inspired by the work of who showed that a CNN can be directly trained to regress from pixels to depth values using as input a single image .,0
16957,Our work is different from in 3 important respects :,0
16958,"Firstly , we focus on faces ( i.e. deformable objects ) whereas on general scenes containing mainly rigid objects .",0
16959,"Secondly , learn a mapping from 2D images to 2D depth maps , whereas we demonstrate that one can actually learn a mapping from 2 D to the full 3 D facial structure including the non-visible part of the face .",0
16960,"Thirdly , use a multi-scale approach by processing images from low to high resolution .",0
16961,"In contrast , we process faces at fixed scale ( assuming that this is provided by a face detector ) , but we build our CNN based on a state - of - the - art bottom - up top - down module that allows analysing and combining CNN features at different resolutions for eventually making predictions at voxel level .",0
16962,Recent work on 3D .,0
16963,We are aware of only one work which regresses a volume using a CNN .,0
16964,The work of uses an LSTM to regress the 3D structure of multiple ob - ject classes from one or more images .,0
16965,This is different from our work in at least two ways .,0
16966,"Firstly , we treat our reconstruction as a semantic segmentation problem by regressing a volume which is spatially aligned with the image .",0
16967,"Secondly , we work from only one image in one single step , regressing a much larger volume of 192 192 200 as opposed to the 32 32 32 used in .",0
16968,The work of decomposes an input 3 D shape into shape primitives which along with a set of parameters can be used to re-assemble the given shape .,0
16969,"Given the input shape , the goal of is to regress the shape primitive parameters which is achieved via a CNN .",0
16970,The method of extends classical work on heatmap regression by proposing a 4D representation for regressing the location of sparse 3D landmarks for human pose estimation .,0
16971,"Different from , we demonstrate that a 3D volumetric representation is particular effective for learning dense 3 D facial geometry .",0
16972,"In terms of 3 DMM fitting , very recent work includes which uses a CNN similar to the one of for producing coarse facial geometry but additionally includes a second network for refining the facial geometry and a novel rendering layer for connecting the two networks .",0
16973,Another recent work is which uses a very deep CNN for 3 DMM fitting .,0
16974,Method,0
16975,This section describes our framework including the proposed data representation used .,0
16976,Dataset,0
16977,Our aim is to regress the full 3 D facial structure from a 2D image .,0
16978,"To this end , our method requires an appropriate dataset consisting of 2D images and 3D facial scans .",0
16979,"As our target is to apply the method on completely unconstrained images from the web , we chose the dataset of for forming our training and test sets .",0
16980,"The dataset has been produced by fitting a 3 DMM built from the combination of the Basel and FaceWarehouse models to the unconstrained images of the 300 W dataset using the multi-feature fitting approach of , careful initialisation and by constraining the solution using a sparse set of landmarks .",0
16981,"Face profiling is then used to render each image to 10 - 15 different poses resulting in a large scale dataset ( more than 60,000 2D facial images and 3D meshes ) called 300W - LP .",0
16982,"Note that because each mesh is produced by a 3 DMM , the vertices of all produced meshes are in dense correspondence ; however this is not a prerequisite for our method and unregistered raw facial scans could be also used if available ( e.g. the BU - 4 DFE dataset ) .",0
16983,Proposed volumetric representation,0
16984,Our goal is to predict the coordinates of the 3D vertices of each facial scan from the corresponding 2D image via CNN regression .,0
16985,"As a number of works have pointed out ( see for example ) , direct regression of all 3D points concatenated as a vector using the standard L2 loss might cause difficulties in learning because a single correct value for each 3 D vertex must be predicted .",0
16986,"Additionally , such an approach requires interpolating all scans to a vector of a fixed dimension , a pre-processing step not required by our method .",0
16987,Note that similar learning problems are encountered when a CNN is used to regress model parameters like the 3 DMM parameters rather than the actual vertices .,0
16988,"In this case , special care must betaken to weight parameters appropriately using the Mahalanobis distance or in general some normalisation method , see for example .",0
16989,We compare the performance of our method with that of a similar method in Section 4 .,0
16990,"To alleviate the aforementioned learning problem , we propose to reformulate the problem of 3D face reconstruction as one of 2 D to 3D image segmentation : in particular , we convert each 3 D facial scan into a 3 D binary volume V whd by discretizing the 3D space into voxels {w , h , d} , assigning a value of 1 to all points enclosed by the 3D facial scan , and 0 otherwise .",0
16991,"That is to say V whd is the ground truth for voxel {w , h , d} and is equal to 1 , if voxel {w , h , d} belongs to the 3D volumetric representation of the face and 0 otherwise ( i.e. it belongs to the background ) .",0
16992,The conversion is shown in .,0
16993,Notice that the process creates a volume fully aligned with the 2D image .,0
16994,The importance of spatial alignment is analysed in more detail in Section 5 .,0
16995,The error caused by discretization fora randomly picked facial scan as a function of the volume size is shown in .,0
16996,"Given that the error of state - of - the - art methods is of the order of a few mms , we conclude that discretization by 192 192 200 produces negligible error .",0
16997,"Given our volumetric facial representation , the problem of regressing the 3D coordinates of all vertices of a facial scan is reduced to one of 3D binary volume segmentation .",0
16998,"We approach this problem using recent CNN architectures from semantic image segmentation and their extensions , as described in the next subsection .",0
16999,Volumetric Regression Networks,0
17000,"In this section , we describe the proposed volumetric regression network , exploring several architectural variations described in detail in the following subsections :",0
17001,Volumetric Regression Network ( VRN ) .,0
17002,We wish to learn a mapping from the 2D facial image to its corresponding 3D volume f : I ?,0
17003,V .,0
17004,"Given the training set of 2D images and constructed volumes , we learn this mapping using a CNN .",0
17005,"Our CNN architecture for 3D segmentation is based on the "" hourglass network "" of an extension of the fully convolutional network of using skip connections and residual learning .",0
17006,Our volumetric architecture consists of two hourglass modules which are stacked together without intermediate supervision .,0
17007,The input is an RGB image and the output is a volume of 192 192 200 of real values .,0
17008,This architecture is shown in .,0
17009,"As it can be observed , the network has an encoding / decoding structure where a set of convolutional layers are firstly used to compute a feature representation of fixed dimension .",0
17010,"This representation is further processed back to the spatial domain , re-establishing spatial correspondence between the input image and the output volume .",0
17011,Features are hierarchically combined from different resolutions to make per-pixel predictions .,0
17012,"The second hourglass is used to refine this output , and has an identical structure to that of the first one .",0
17013,We train our volumetric regression network using the sigmoid cross entropy loss function :,0
17014,"where V whd is the corresponding sigmoid output at voxel {w , h , d} of the regressed volume .",0
17015,"At test time , and given an input 2 D image , the network regresses a 3D volume from which the outer 3 D facial mesh is recovered .",0
17016,"Rather than making hard ( binary ) predictions at pixel level , we found that the soft sigmoid output is more useful for further processing .",0
17017,Both representations are shown in where clearly the latter results in smoother results .,0
17018,"Finally , from the 3D volume , a mesh can be formed by generating the iso-surface of the volume .",0
17019,"If needed , correspondence between this variable length mesh and a fixed mesh can be found using Iterative Closest Point ( ICP ) .",0
17020,VRN - Multitask .,0
17021,"We also propose a Multitask VRN , shown in , consisting of three hourglass modules .",0
17022,The first hourglass provides features to a fork of two hourglasses .,0
17023,"The first of this fork regresses the 68 i BUG landmarks as 2D Gaussians , each on a separate channel .",0
17024,"The second hourglass of this fork directly regresses the 3D structure of the face as a volume , as in the aforementioned unguided volumetric regression method .",0
17025,The goal of this multitask network is to learn more reliable features which are better suited to the two tasks .,0
17026,VRN - Guided .,0
17027,We argue that reconstruction should benefit from firstly performing a simpler face analysis task ; in particular we propose an architecture for volumetric regression guided by facial landmarks .,0
17028,"To this end , we train a stacked hourglass network which accepts guidance from landmarks during training and inference .",0
17029,"This network has a similar architecture to the unguided volumetric regression method , however the input to this architecture is an RGB image stacked with 68 channels , each containing a Gaussian (? = 1 , approximate diameter of 6 pixels ) centred on each :",0
17030,Some visual results from the AFLW2000 - 3D dataset generated using our VRN - Guided method .,0
17031,of the 68 landmarks .,0
17032,This stacked representation and architecture is demonstrated in .,0
17033,During training we used the ground truth landmarks while during testing we used a stacked hourglass network trained for facial landmark localisation .,0
17034,We call this network VRN - Guided .,0
17035,Training,0
17036,"Each of our architectures was trained end - to - end using RMSProp with an initial learning rate of 10 ? 4 , which was lowered after 40 epochs to 10 ?5 .",1
17037,"During training , random augmentation was applied to each input sample ( face image ) and its corresponding target ( 3D volume ) : we applied in - plane rotation r ? [ ?45 , ... , 45 ] , translation t z , t y ? [ ? 15 , ... , 15 ] and scale s ? [ 0.85 , ... , 1.15 ] jitter .",1
17038,"In 20 % of cases , the input and target were flipped horizontally .",1
17039,"Finally , the input samples were adjusted with some colour scaling on each RGB channel .",1
17040,"In the case of the VRN - Guided , the landmark detection module was trained to regress Gaussians with standard deviation of approximately 3 pixels (? = 1 ) .",1
17041,Results,0
17042,"We performed cross - database experiments only , on 3 different databases , namely AFLW2000 - 3D , BU - 4DFE , and Florence reporting the performance of all the proposed 0.1012 0.1227 0.0975 EOS 0.0971 0.1560 0.1253 networks ( VRN , VRN - Multitask and VRN - Guided ) along with the performance of two state - of - the - art methods , namely 3DDFA and EOS .",0
17043,"Both methods perform 3 DMM fitting ( 3 DDFA uses a CNN ) , a process completely bypassed by VRN .",0
17044,Our results can be found in and Figs. 7 and 8 .,0
17045,Visual results of the proposed VRN - Guided on some very challenging images from AFLW2000 - 3D can be seen in .,0
17046,Examples of failure cases along with a visual comparison between VRN and VRN - Guided can be found in the supplementary material .,0
17047,"From these results , we can conclude the following :",0
17048,Volumetric Regression Networks largely outperform,1
17049,"3DDFA and EOS on all datasets , verifying that directly regressing the 3D facial structure is a much easier problem for CNN learning .",1
17050,"2 . All VRNs perform well across the whole spectrum of facial poses , expressions and occlusions .",1
17051,"Also , there are no significant performance discrepancies across different datasets ( ALFW2000 - 3D seems to be slightly more difficult ) .",0
17052,"3 . The best performing VRN is the one guided by detected landmarks , however at the cost of higher computational complexity :",1
17053,VRN - Guided uses another stacked hourglass network for landmark localization .,0
17054,"4 . VRN - Multitask does not always perform particularly better than the plain VRN ( in fact on BU - 4 DFE it performs worse ) , not justifying the increase of network complexity .",1
17055,It seems that it might be preferable to train a network to focus on the task in hand .,0
17056,Details about our experiments are as follows :,0
17057,Datasets .,0
17058,( a ) AFLW2000 - 3D :,0
17059,"As our target was to test our network on totally unconstrained images , we firstly conducted experiments on the AFLW2000 - 3D dataset which contains 3 D facial meshes for the first 2000 images from AFLW .",0
17060,( b ) BU - 4DFE :,0
17061,We also conducted experiments on rendered images from BU - 4DFE .,0
17062,We rendered each participant for both Happy and Surprised expressions with three different pitch rotations between ? 20 and 20 degrees .,0
17063,"For each pitch , seven roll rotations from ?80 to 80 degrees were also rendered .",0
17064,Large variations in lighting direction and colour were added randomly to make the images more challenging .,0
17065,"( c ) Florence : Finally , we conducted experiments on rendered images from the Florence dataset .",0
17066,Facial images were rendered in a similar fashion to the ones of BU - 4 DFE but for slightly different parameters :,0
17067,"Each face is rendered in 20 difference poses , using a pitch of - 15 , 20 or 25 degrees and each of the five evenly spaced rotations between - 80 and 80 .",0
17068,Error metric .,0
17069,"To measure the accuracy of reconstruction for each face , we used the Normalised Mean Error ( NME ) defined as the average per vertex Euclidean distance between the estimated and ground truth reconstruction normalised by the outer 3D interocular distance :",0
17070,"where N is the number of vertices per facial mesh , dis the 3D interocular distance and x k , y k are vertices of the grouthtruth and predicted meshes .",0
17071,"The error is calculated on the face region only on approximately 19,000 vertices per facial mesh .",0
17072,"Notice that when there is no point correspondence between the ground truth and the estimated mesh , ICP was used but only to establish the correspondence , i.e. the rigid alignment was not used .",0
17073,"If the rigid alignment is used , we found that , for all methods , the error decreases but it turns out that the relative difference in performance remains the same .",0
17074,"For completeness , we included these results in the supplementary material .",0
17075,Comparison with state - of - the - art .,0
17076,We compared against state - of - the - art 3D reconstruction methods for which code is publicly available .,0
17077,"These include the very recent methods of 3DDFA , and EOS [ 8 ] 1 .",0
17078,Importance of spatial alignment,0
17079,The 3D reconstruction method described in regresses a 3D volume of fixed orientation from one or more images using an LSTM .,0
17080,"This is different to our approach of taking a single image and regressing a spatially aligned volume , which we believe is easier to learn .",0
17081,"To explore what the repercussions of ignoring spatial alignment are , we trained a variant of VRN which regresses a frontal version of the face , i.e. a face of fixed orientation as in",0
17082,"Although this network produces a reasonable face , it can only capture diminished expression , and the shape for all faces appears to remain almost identical .",0
17083,This is very noticeable in .,0
17084,"Numeric comparison is shown in ( left ) , as VRN without alignment .",0
17085,We believe that this further confirms that spatial alignment is of paramount importance when performing 3D reconstruction in this way .,0
17086,Ablation studies,0
17087,"In this section , we report the results of experiments aiming to shed further light into the performance of the proposed networks .",0
17088,"For all experiments reported , we used the best performing VRN - Guided .",0
17089,Effect of pose .,1
17090,"To measure the influence of pose on the reconstruction error , we measured the NME for different yaw angles using all of our Florence renderings .",0
17091,"As shown in , the performance of our method decreases as the pose increases .",1
17092,"This is to be expected , due to less of the face being visible which makes evaluation for the invisible part difficult .",0
17093,We believe that our error is still very low considering these poses .,0
17094,Effect of expression .,1
17095,Certain expressions are usually considered harder to accurately reproduce in 3D face reconstruction .,0
17096,"To measure the effect of facial expressions on performance , we rendered frontal images in difference expressions from BU - 4DFE ( since Florence only exhibits a neutral expression ) and measured the performance for each expression .",0
17097,"This kind of extreme acted facial expressions generally do not occur in the training set , yet as shown in , the performance variation across different expressions is quite minor .",1
17098,Effect of Gaussian size for guidance .,1
17099,"We trained a VRN - Guided , however , this time , the facial landmark detector network of the VRN - Guided regresses larger Gaussians ( ? = 2 as opposed to the normal ? = 1 ) .",0
17100,"The performance of the 3D reconstruction dropped by a negligible amount , suggesting that as long as the Gaussians are of a sensible size , guidance will always help .",1
17101,Normalised NME :,0
17102,The effect of facial expression on reconstruction accuracy in terms of NME on the BU - 4DFE dataset .,0
17103,The VRN - Guided network was used .,0
17104,Conclusions,0
17105,We proposed a direct approach to 3D facial reconstruction from a single 2D image using volumetric CNN regression .,0
17106,"To this end , we proposed and exhaustively evaluated three different networks for volumetric regression , reporting results that show that the proposed networks perform well for the whole spectrum of facial pose , and can deal with facial expressions as well as occlusions .",0
17107,We also compared the performance of our networks against that of recent state - of - the - art methods based on 3 DMM fitting reporting large performance improvement on three different datasets .,0
17108,Future work may include improving detail and establishing a fixed correspondence from the isosurface of the mesh .,0
17109,title,0
17110,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,1
17111,abstract,0
17112,Face alignment algorithms locate a set of landmark points in images of faces taken in unrestricted situations .,0
17113,"State - of - the - art approaches typically fail or lose accuracy in the presence of occlusions , strong deformations , large pose variations and ambiguous configurations .",0
17114,"In this paper we present 3DDE , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ensembles of regression trees .",0
17115,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a convolutional neural network .,0
17116,With this initialization we address self - occlusions and large face rotations .,0
17117,"Further , the regressor implicitly imposes a prior face shape on the solution , addressing occlusions and ambiguous face configurations .",0
17118,Its coarse - to - fine structure tackles the combinatorial explosion of parts deformation .,0
17119,"In the experiments performed , 3 DDE improves the state - of - the - art in 300W , COFW , AFLW and WFLW data sets .",0
17120,"Finally , we perform cross - dataset experiments that reveal the existence of a significant data set bias in these benchmarks .",0
17121,Introduction,0
17122,Face alignment algorithms precisely locate a set of points of interest in the images of faces taken in unrestricted conditions .,0
17123,"It has received much attention from the research community since it is a preliminary step for estimating 3 D facial structure and many other face image analysis problems such as verification and recognition , attributes estimation or facial expression recognition , to name a few .",0
17124,"Present approaches typically fail or lose precision in the presence of occlusions , strong deformations produced by facial expressions , large pose variations and ambiguous configurations caused , for example , by strong make - up or the existence of other nearby faces .",0
17125,"Top performers in the most popular benchmarks are based on Convolutional Neural Networks ( CNNs ) and Ensemble of Regression Trees ( ERT ) , see e.g. , Tables 1 , 2 , 3 , 4 and 5 .",0
17126,The large effective receptive field of deep models ) enable them to model context better and produce robust landmark estimations .,0
17127,"However , in these models it is not easy to enforce facial shape consistency , something that limits their accuracy in the presence of occlusions and ambiguous facial configurations .",0
17128,"ERT - based models , on the other hand , are difficult to initialize , but may implicitly impose face shape consistency in their estimations .",0
17129,This increases their performance in occluded and ambiguous situations .,0
17130,"They are also much more efficient than deep models and , as we demonstrate in our experiments , with a good initialization they are also very accurate .",0
17131,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .",1
17132,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .",0
17133,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,1
17134,"With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .",0
17135,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .",1
17136,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .",1
17137,A preliminary version of our work appeared in .,0
17138,Here we refine and extend it in several ways .,0
17139,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,1
17140,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,1
17141,We also extend the evaluation including the newly released WFLW database and a detailed ablation study .,0
17142,"Finally , 3 DDE may also be trained in presence of missing and occluded landmarks in the training set .",0
17143,This has enabled us to perform cross - dataset experiments that reveal the existence of significant data set bias that may limit the generalization capabilities of regressors trained on present data bases .,0
17144,"To the best of our knowledge , this is the first time such a problem has been raised in the field .",0
17145,Related Work,0
17146,Face alignment has been a topic of intense research for more than twenty years .,0
17147,Initial successful results were based on 2D and 3D generative approaches such as the Active Appearance Models ( AAM ) or the 3D Morphable Models ( 3 DMM ) .,0
17148,Recent approaches are based on a cascaded combination of discriminative regressors .,0
17149,"In the earliest case these regressors are Random Ferns , Ensembles of Regression Trees or linear models .",0
17150,"Key ideas in this approach are indexing image description relative to the current shape estimate , and the use of a regressor whose predictions lie on the subspace spanned by the training face shapes , this is the so - called Cascade Shape Regressor ( CSR ) framework .",0
17151,improved the original cascade framework by proposing a realtime ensemble of regression trees .,0
17152,used locally binary features to boost the performance up to 3000 FPS .,0
17153,included occlusion estimation and decreased the influence of occluded landmarks .,0
17154,refine the initial location of face landmarks using a random forest and SIFT features .,0
17155,also use SIFT features and learn the linear regressor dividing the search space into individual regions with similar gradient directions .,0
17156,"Overall , this set of approaches are very sensitive to the starting point of the regression process .",0
17157,For this reason an important part of recent work revolves around how to find good initializations .,0
17158,"However , they are extremely efficient and may take advantage of implicit shape constraints .",0
17159,The recent development of deep learning techniques has also impacted the face alignment field with the widespread use of CNN - based regressors .,0
17160,were pioneers to apply a three - level CNN for locating landmarks .,0
17161,proposed a multi - task solution to deal with face alignment and attributes classification .,0
17162,use global and local face parts regressors for fine - grained facial deformation estimation .,0
17163,transform the landmarks rather than the input image for the refinement cascade .,0
17164,and are the first approaches that fuse the feature extraction and regression steps of CSR into a recurrent neural network trained end - to - end .,0
17165,and use a global similarity transform to normalize landmark locations followed by a VGG - based and a Stacked Hourglass network respectively to regress the final shape .,0
17166,"derive face landmarks from boundary lines , which helps to remove the ambiguities in the landmark definition .",0
17167,"Deep CNN models have large effective receptive fields that let them model context better and convey these approaches with a high degree of robustness to face rotation , scale , deformation and initialization .",0
17168,"However , when used in a cascaded framework they may notably increase the computational requirements .",0
17169,"Moreover , it is not clear how to impose facial shape consistency on the estimated set of landmarks .",0
17170,"Hence , the regressor accuracy maybe harmed in the presence of occlusions or ambiguities .",0
17171,There is also an increasing number of works based on 3D face models .,0
17172,"In the simplest case , they fit a mean model to the estimated image landmarks positions or jointly regress the pose and shape of the face .",0
17173,and fit a 3 DMM in a cascaded way .,0
17174,These approaches provide 3 D pose information that maybe used to estimate landmark self - occlusions or to train simpler regressors specialized in a given head orientation .,0
17175,"However , building and fitting a 3 D face model is a difficult task and the results of the full 3D approaches in current benchmarks are not as good as those described above .",0
17176,Our proposal tries to leverage on the good properties of the three approaches described above .,0
17177,Using a CNN - based initialization we inherit the robustness of deep learning models .,0
17178,Like the simple 3D approaches we fit a rigid 3 D face model to initialize the regressor and estimate the initial face orientation to address selfocclusions and ambiguities .,0
17179,"Finally , we use a cascaded ERT within a coarse - to - fine framework to achieve accuracy and efficiency while avoiding the combinatorial explosion of independent parts deformations .",0
17180,3D deeply - initialized,0
17181,Ensemble,0
17182,In this section we present 3DDE .,0
17183,"It consists of two main steps : CNN - based rigid face pose computation and ERT - based non-rigid face deformation estimation , both shown in .",0
17184,Rigid pose computation,0
17185,ERT - based regressors require a good initialization to converge .,0
17186,We propose the use of face landmarks location probability maps to generate plausible shape initialization candidates .,0
17187,"We define a UNet - like architecture , with a loss function that handles missing landmarks .",0
17188,"We train this CNN to obtain a set of probability maps , P ( I ) , that model the position of each landmark in the input image ( see ) .",0
17189,The maximum of each smoothed probability map determines our initial landmark positions .,0
17190,"Note in that these predictions are sensitive to occlusions , ambiguities and may not be a valid face shape .",0
17191,"Compared to typical CNN - based approaches , e.g. , , our CNN is much simpler , since we only require a rough estimation of landmark locations .",0
17192,"To start the ERT with a plausible face , we compute the initial shape by fitting a rigid 3D head model to the estimated 2D landmarks locations .",0
17193,To this end we use the soft POSIT algorithm proposed by within a robust scheme .,0
17194,"Unlike , here we use a set of the distinct landmarks to establish the correspondences between the CNN predictions and the 3D face model .",0
17195,"This avoids problems related to ambiguous landmarks around the jaw that do not correspond always to the same 3D points and produce wrong initializations , mainly in profile faces .",0
17196,"Moreover , we have also implemented a RANSAC - like procedure , that runs soft POSIT several times with subsets of correspondences , to obtain a robust estimation ( see Algorithm 1 ) .",0
17197,Let X ?,0
17198,"R L3 be the 3D coordinates of the L landmarks on the 3D face model , x ?",0
17199,R L2 their 2 D projections onto the image plane and v ?,0
17200,"{ 0 , 1 } L their visibilities .",0
17201,"We produce sub - sets of correspondences ( x s , X s ) from the distinct landmarks shown in , estimate the 3D face model pose ( R , t ) with softPOSIT and evaluate the goodness of each estimation as the sum of landmarks probabilities ,",0
17202,where x z ( l ) are the 2D coordinates of the l - th landmark and Pl ( I ) is the probability map for landmark l .,0
17203,"Finally , we select the rigid transformation ( R , t ) with highest p ( x z ) .",0
17204,"As a result , we project the 3D model onto the image using the most likely estimated rigid transformation .",0
17205,"This provides the ERT with a rough estimation of the scale , translation and 3D pose of the target face ( see ) , and the visibility estimation of the self - occluded parts of the face .",0
17206,"Let x 0 = g 0 ( P ( I ) , X ) be the initial shape , the output of the initialization function g 0 after processing the input image I. With our initialization we enforce two key requirements for the convergence of the ERT .",0
17207,"First , that x 0 lies on the face with an approximately correct 3 D face pose .",0
17208,"Second , that x 0 is a valid face shape .",0
17209,The latter guarantees that the predictions in the next step of the algorithm will also be valid face shapes .,0
17210,ERT - based non-rigid shape estimation,0
17211,.,0
17212,"Each shape s i has it s own training image , I i , ground truth shape , x g i , ground truth visibility label , v g i , annotated landmark label , w g i ?",0
17213,"{ 0 ,",0
17214,"1 } L , initial shape , x 0 i , and visibilities , v 0 i , for training the ERT regressor .",0
17215,"In our implementa - tion we use shape - indexed features , ?( P( I i ) , x ti , w g i ) , that depend on the current shape x ti of the landmarks in image I i and whether they are annotated or not , w g i .",0
17216,We divide the regression process into a maximum of T stages .,0
17217,"We learn an ensemble of K regression trees for the t- th stage ,",0
17218,) and x j are the coordinates of the landmarks estimated in j-th stage .,0
17219,"To train the ERT we use the N training shapes in S to generate an augmented training set of samples , SA , and a validation set , S V , with cardinality N A = |S A | and NV = |S V | respectively .",0
17220,The total number of samples is NT = N A + NV .,0
17221,"Instead of using a fixed number of stages , like , we stop training when the validation error stops improving .",0
17222,In this way the regressor has a variable number of stages .,0
17223,We compute the initialization for each sample using the 3D projections produced by g 0 ( see generated initializations in ) .,0
17224,We also improve the data augmentation used in .,0
17225,"To this end we add random noise to the yaw , pitch and roll angles , of the rotation matrix R * estimated with g 0 , to generate new training initializations for each sample in SA .",0
17226,"Following et al. and , we attach to each landmark in S the binary labels {v , w} ? { 0 , 1 } that model respectively whether it is visible and annotated .",0
17227,We learn these labels in the ERT together with the landmark location .,0
17228,Each initial shape is progressively refined by estimating a shape and visibility incre -,0
17229,represents the current shape of the i - th sample ( see Algorithm 2 ) .,0
17230,"C v t is trained to minimize only the landmark position errors but on each tree leaf , in addition to the mean shape , we also output the mean of all training shapes visibilities , v g i , that belong to that node .",0
17231,We define,0
17232,"as the set of all current shapes and corresponding visibility vectors for all training and validation data , respectively .",0
17233,Algorithm 2 Training an Ensemble of Regression Trees,0
17234,"Input : S , T // Generate an augmented training set of samples SA , SV = dataAugmentation ( S ) repeat // Extract training ( FA ) and validation ( FV ) fea -",0
17235,"Compared with conventional ERT approaches , our ensemble is simpler .",0
17236,"It will require fewer trees because we only have to estimate the nonrigid face deformation , since the 3D rigid component has already been estimated in the previous step .",0
17237,In the following we describe the details .,0
17238,Initial shapes for regression,0
17239,The selection of the starting point in the ERT is fundamental to reach a good solution .,0
17240,"The simplest choice is the mean of the ground truth training shapes , x 0 = N i=1 x g i / N .",0
17241,"However , , or randomly deform the initial shape .",0
17242,"In our approach we initialize the ERT using the algorithm described in section 3.1 , that provides a robust pose and a valid shape for initialization ( see ) .",0
17243,"Hence , the ERT only needs to estimate the non-rigid deformation component of the face .",0
17244,Feature Extraction,0
17245,ERT efficiency depends on the feature extraction step .,0
17246,"In general , descriptor features such as SIFT used by and improve face alignment results , but have higher computational cost compared to simpler features such as plain pixel value differences .",0
17247,"In our case , a simple feature suffices , since shape landmarks are close to their ground truth location .",0
17248,We use the probability maps P ( I ) to extract features for the cascade .,0
17249,"To this end , we select a landmark land its associated probability map Pl ( I ) .",0
17250,"The feature is computed as the difference between two pixels values in Pl ( I ) from a FREAK descriptor pattern around l , similar to those in .",0
17251,"However , ours are defined on the probability maps , P ( I ) , instead of the image , I .",0
17252,We let the training algorithm select the most informative landmark and pair of pixels in each iteration .,0
17253,Learn a coarse - to - fine regressor,0
17254,"To train the t - th stage regressor , C v t , we fit an ERT .",0
17255,"Thus , the goal is to sequentially learn a series of weak learners to greedily minimize the regression loss function :",0
17256,where is the Hadamard product .,0
17257,There are different ways of minimizing Equation 1 . present a general framework based on Gradient Boosting for learning an ensemble of regression trees .,0
17258,establish an optimization method based on Gaussian Processes also learning an ensemble of regression trees but outperforming previous literature by reducing the overfitting .,0
17259,In our approach we adopt a Gradient Boosting scheme ( see Algorithm 3 ) .,0
17260,A crucial problem when training a global face landmark regressor is the lack of examples showing all possible combinations of face parts deformations .,0
17261,"Hence , these regressors quickly overfit and generalize poorly to combinations of part deformations not present in the training set .",0
17262,To address this problem we introduce the coarse - tofine ERT architecture .,0
17263,The goal is to be able to cope with combinations of face part deformations not seen during training .,0
17264,A single monolithic regressor is notable to estimate these local deformations ( see the difference between monolithic and coarse - to - fine NME curves in ) .,0
17265,Our algorithm is agnostic in the number of parts and stages of the coarse - to - fine estimation .,0
17266,Algorithm 3 details the training of P face parts regressors ( each one with a subset of the landmarks ) to build a coarseto - fine regressor .,0
17267,"Note that A k?1 in this context is the shape and visibility vectors from the last regressor output ( e.g. , the previous part regressor or a previous full stage regressor ) .",0
17268,In our implementation the coarse - to - fine scheme has two stages .,0
17269,"The coarse stage has one part , P = 1 , that involves all landmarks and K 1 trees .",0
17270,"The fine stage hasten parts , P = 10 , left / right eyebrow , left / right eye , nose , top / bottom mouth , left / right ear and chin ( see ) , with K 2 trees .",0
17271,Algorithm 3 Training,0
17272,P parts regressors,0
17273,"Input : SA , FA , At ? 1 , ? , K , P for k=1 to K do for p =1 to P do // Compute residuals : // is the Hadamard product / / ( p ) selects elements of vectors in that part",0
17274,"// Update samples with the regression tree estimation , // ? , shrinkage factor to scale each tree contribution :",0
17275,"The P = 10 face parts of 300W , COFW , AFLW and WFLW data bases in the fine stage of our coarse - to - fine ERT .",0
17276,Fit a regression tree,0
17277,"The training objective for the k - th regression tree is to minimize the sum of squared residuals , taking into account the annotated landmark labels :",0
17278,We learn each regression binary tree by recursively splitting the training set into the left ( l ) and right ( r ) child nodes .,0
17279,The tree node split function is designed to minimize E k from Equation 2 in the selected landmark .,0
17280,"To train a regression tree node we randomly generate a set of candidate split functions , each of them involving four parameters ? = ( ? , p 1 , p 2 , l ) , where p 1 and p 2 are pixels coordinates on a fixed FREAK structure around the l - th landmark coordinates in x k?1 i .",0
17281,The feature value corresponding to ?,0
17282,for the i - th training sample is,0
17283,", the difference of probability values in the maps for the given landmark .",0
17284,"Finally , we compute the split function thresholding the feature value , f i ( ? ) > ? .",0
17285,Given N ?,0
17286,"SA the set of training samples at anode , fitting a tree node for the k - th tree , consists of finding the parameter ?",0
17287,that minimizes,0
17288,"where N ? , l and N ? , r are , respectively , the samples sent to the left and right child nodes due to the decision induced by ?.",0
17289,"The mean residual ? , b fora candidate split function and a subset of training data is given by",0
17290,"Once we know the optimal split each leaf node stores the mean residual , ? , b , as the output of the regression for any example reaching that leaf .",0
17291,We also output the mean visibility of the samples reaching the tree leaf .,0
17292,Experiments,0
17293,"To train and evaluate our proposal , we perform experiments with 300W , COFW , AFLW and WFLW that are considered the most challenging public data sets :",0
17294,300 W .,0
17295,"It provides 68 manually annotated landmarks , .",0
17296,We follow the most established approach and divide the 300 W annotations into 3148 training and 689 testing images ( public competition ) .,0
17297,Evaluation is also performed on the 300W private competition using the previous 3837 images as training and 600 newly updated images as testing set . AFLW .,0
17298,"It provides a collection of 25993 in - the - wild faces , with 21 facial landmarks annotated depending on their visibility , .",0
17299,"We have found several annotations errors and , consequently , removed these faces from our experiments .",0
17300,From the remaining faces we randomly choose 19312 images for training / validation and 4828 instances for testing .,0
17301,WFLW .,0
17302,"It consists of 7500 extremely challenging training and 2500 testing faces divided into six subgroups , pose , expression , illumination , make - up , occlusion and blur , with 98 fully manual annotated landmarks , .",0
17303,Evaluation,0
17304,We use the Normalized Mean Error ( NME ) as a metric to measure the shape estimation error,0
17305,It computes the mean euclidean distance between the ground - truth and estimated landmark positions normalized by d i .,0
17306,"We report our results using different values of d i : the ground truth distance between the eye centers ( pupils ) , the ground truth distance between the outer eye corners ( corners ) and the ground truth bounding box size ( height ) .",0
17307,"In addition , we also compare our results using Cumulative Error Distribution ( CED ) curves .",0
17308,We calculate AU C ?,0
17309,as the area under the CED curve for images with an NME smaller than ? and F R ?,0
17310,as the failure rate representing the percentage of testing faces with NME greater than ?.,0
17311,We use precision / recall percentages to compare occlusion prediction .,0
17312,To train our algorithm we shuffle the training set of each database and split it into 90 % trainset and 10 % validation - set .,0
17313,Implementation,0
17314,All experiments have been carried outwith the settings described in this section .,0
17315,"For each data set , we train from scratch the CNN selecting the model parameters with lowest validation error .",0
17316,We crop faces using the ground truth bounding boxes annotations enlarged by 30 % .,0
17317,"We generate different training samples in each epoch by applying random in plane rotations between 45 , scale changes by 15 % and translations by 5 % of bounding box size , randomly mirroring images horizontally and generating random rectangular occlusions .",0
17318,"We use Adam stochastic optimization with ? 1 = 0.9 , ? 2 = 0.999 and = 1 e ? 8 parameters .",1
17319,We train until convergence with an initial learning rate ? = 0.001 .,1
17320,"When validation error levels out for 10 epochs , we multiply the learning rate by decay = 0.05 .",1
17321,In the CNN the cropped input face is reduced from 160160 to 11 pixels gradually dividing by half their size across B = 8 branches applying astride 2 convolution with kernel size 22 1 .,0
17322,We apply batch normalization after each convolution .,1
17323,All layers contain 68 filters to describe the required landmark features .,1
17324,"We apply a Gaussian filter with ? = 33 to the output probability maps to stabilize the initialization , g 0 .",1
17325,We train the coarse - to - fine ERT with the Gradient Boosting algorithm ) .,1
17326,It requires a maximum of T = 20 stages of K = 50 regression trees per stage .,1
17327,The depth of trees is set to 4 .,1
17328,"The number of tests to choose the best split parameters , ? , is set to 200 .",1
17329,We resize each image to set the face size to 160160 pixels .,1
17330,"For feature extraction , the FREAK pattern diameter is reduced gradually in each stage ( i.e. , in the last stages the pixel pairs for each feature are closer ) .",0
17331,We generate Z = 25 initializations in the robust soft POSIT scheme of g 0 .,1
17332,"We augment the shapes of each face training image to create a set , SA , of at least N A = 60000 samples to train the cascade .",1
17333,To avoid overfitting we use a shrinkage factor ? = 0.1 and subsampling factor ? = 0.5 in the ERT .,1
17334,"Our regressor triggers the coarse - to - fine strategy once the training error is below the validation error , e.g. , t = 5 in .",0
17335,"Training the CNN and the coarse - to - fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080 Ti ( 11 GB ) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20 GHz ( 210 cores / 20 threads , 128 GB of RAM ) with a batch size of 32 images .",1
17336,"At runtime our method process test images on average at a rate of 12.5 FPS , where the CNN takes 75 ms and the ERT 5 ms per face image using C ++ , Tensorflow and OpenCV libraries .",0
17337,Experiments using public code,0
17338,Published results in the literature are sometimes not fully comparable .,0
17339,"In this section we use publicly available code to ensure a fair comparison between 3 DDE and DCFE , LAB , DAN , RCN , c GPRT , RCPR and ERT with the same settings ( including same training , valida - tion and bounding boxes ) , in different benchmarks : 300W public , 300 W private , COFW and WFLW .",0
17340,Note that LAB only provides a trained model for the WFLW data set .,0
17341,"In addition , DAN provides code using 68 landmarks , for this reason we only report results in 300W .",0
17342,In we plot the CED curves for all data bases .,0
17343,In the legend we provide the AU C and F R values for each algorithm .,0
17344,"The selected algorithms are representative of the three main families of solutions : a ) ensembles of regression trees ( c GPRT , RCPR , ERT ) , b) CNN - based approaches ( LAB , DAN , RCN ) and c ) mixed approaches with deep nets and ensembles of regression trees ( 3DDE , DCFE ) .",0
17345,"Overall , 3 DDE is better than any other providing a public implementation in the literature .",1
17346,"We improve over our preliminary algorithm , , because of the better 3D initialization and regularization ( see a complete analysis in section 4.5 ) .",0
17347,"In general we are able to improve by a large margin other ERT methods as RCPR , ERT or c GPRT because of the better initialization and the robust features provided by the CNN .",1
17348,"We also outperform RCN ( without any denoising model ) , a CNN architecture like the one used in 3DDE .",1
17349,"Even DAN and LAB , that implement a cascade of CNN regressors , can not compete with the regularization obtained by using the cascade of ERT in 3 DDE ( see ) .",1
17350,The fact that the largest margin is in COFW reflects the importance of the implicit shape model in our cascade to address occlusions .,0
17351,Experiments using published results,0
17352,In this section we compare 3 DDE with other methods in the literature by using their published results .,0
17353,"Since our method is able to train with unannotated landmarks and visibilities , we are able to train and evaluate all data sets in the literature .",0
17354,First we test our method against the 300W benchmark .,0
17355,Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition ( see ) and in the full subset of the 300W public test set ( see ) .,1
17356,This is due to the excellent accuracy achieved by the coarse - to - fine ERT scheme enforcing valid face shapes and the deep robust features extracted from the CNN .,0
17357,"In the challenging subset of the 300W public competition , SHN gets better results than 3DDE .",1
17358,This is due to 3 DDE failing to estimate good landmark probability maps for images with large scale variations .,0
17359,"Our method exhibits superior capability in handling typical cases in the database , since we achieve the best NME full set results in We may assess the improvement achieved by the 3D initialization and the coarse - to - fine ERT by comparing the results of 3 DDE in the full subset of 300W , 4.39 , with Honari 's RCN using the denoising model , 5.41 .",0
17360,It roughly represents a 19 % improvement in the inter-pupils NME .,0
17361,compares the performance of our model using the COFW data set .,0
17362,This is the standard to evaluate occlusions .,0
17363,"3 DDE obtains the best results , NME 5.11 , establishing anew state - of - the - art .",0
17364,This shows the importance of the face shape model implicit in the cascade of 5.28 - 17.00 - 7.58 - 43.12 10.45 SDM 5.60 - 15.40 - 7.52 - 42.94 10.89 ECSAN 5.42 - 11.80 - 6.67,0
17365,--- ERT,0
17366,---- 6.40 --- LBF 4.95 - 11.98 - 6.32,0
17367,---cGPRT,0
17368,---- 5.71 ---CFSS 4.73 - 9.98 - 5.76 - 49.87 5.08 DDN,0
17369,---- 5.65,0
17370,---TCDCN 4.80 - 8.60 - 5.54,0
17371,--- MDM,0
17372,------52.12 4.21 3DDFA 5.09 - 8.07 - 5.63 ---RCN 4.67 - 8.44 - 5.41 --- DAN 4.42 3.19 7.57 5.24 5.03 3.59 55.33 1.16 TSR 4.36 - 7.56 - 4.99,0
17373,--- RAR 4.12 - 8.35 - 4.94 --- SHN 4.12 - 7.00 4.90 4.68 --- DCFE 3.83 2.76 7.54 5.22 4.55 3.24 60.13 1.59 PCD - CNN ERT to cope with severe occlusions .,0
17374,"In terms of landmark visibility estimation , we have obtained better precision with an overall better recall than the best previous approach , DCFE .",0
17375,"Again , the regularization together with the new initialization contributes to improve DCFE .",0
17376,In we show the results of our evaluation with AFLW .,0
17377,"This is a challenging data set not only because of its size and the large variability of face poses , but also because of the large number of samples with occluded landmarks , that are unannotated .",0
17378,"Although the results in are not strictly comparable , because each paper uses its own train and test subsets , we get an NME of 2.06 with the full 21 landmarks set .",0
17379,"Again , it is anew state - of - the - art , since most competing approaches do not use the two most difficult landmarks , each located in one earlobe ( see 19 landmarks results in ) .",0
17380,We have also evaluated 3 DDE without the two earlobe landmarks .,0
17381,"In this case we get an NME of 2.01 , the best reported result .",0
17382,"Finally , we have also evaluated 3 DDE with the newly released WFLW data set .",0
17383,"In enables us to evaluate different sources of variability ( i.e. , expressions , illumination , make - up , occlusions and blur ) .",0
17384,"In we provide the results of various competing methods , normalized by the eye corners distance .",0
17385,3DDE outperforms its competitors in all the WFLW subsets by a large margin .,0
17386,We hypothesize that the reason for this is that the hybrid approach in 3 DDE can be trained with less samples that some of its most prominent competitors and at the same time provide a very accurate face shape ( see ) .,0
17387,"Moreover , we Method pupils occlusion NM E AU C8 F R8 precision / recall RCPR 8.50 --80/40 TCDCN 8.05 --- RAR 6.03 ---DAC - CSR 6.03 --- Wu et al .",0
17388,5.93 --80/49.11 SHN,0
17389,"5.6 ---PCD-CNN - 4.45 CFSS 3.92 - CCL 2.72 -DAC - CSR 2.27 - Binary-CNN - 2.85 PCD-CNN - 2.40 TSR 2.17 - DCFE 2.12 2.17 3DDE 2.01 2.06 Figure 5 : First row shows LAB results , second row 3 DDE results .",0
17390,We report the corresponding NME normalized by the eye corners distance .,0
17391,Blue and green colours represent ground truth and predictions respectively .,0
17392,"achieve the best AU C in all subsets , which determines that 3 DDE is the best approach under all capture conditions ( easy / frontal and difficult / profile ) including all subsets that contain several types of difficulties .",0
17393,Ablation study,0
17394,"3DDE is based on three key ideas : 3D initialization , a cascaded ERT regressor operating on probabilistic CNN features and a coarse - to - fine scheme .",0
17395,In this section we analyze the contribution of each one to the overall performance of our algorithm .,0
17396,In we show the results obtained by different configurations of our framework when evaluated on WFLW .,0
17397,"We have chosen WFLW in our study because it allows the analysis of results stratified by different types of difficulties ( i.e. , facial expressions , large poses , illumination changes , etc . ) .",0
17398,"In this case , since there are many profile faces , we use the height as normalization for the NME .",0
17399,"So , the numerical values are not di-rectly comparable to those in .",0
17400,"MS stands for "" mean shape initialization "" of the ERT .",0
17401,3 D means to initialize the ERT with the procedure in section 3.1 .,0
17402,SE denotes using plain gray level features for the ERT whereas DE denotes using probability maps produced by the CNN to train the ERT .,0
17403,Finally CF stands for using the coarseto - fine scheme .,0
17404,"When combined with the cascaded ERT , the 3D initialization is key to achieve top overall performance , see CNN + MS + DE vs CNN + 3D + DE in the full subset .",1
17405,"The reason for this is that , in the 3D case , the initialization takes care of the rigid component of face pose so that the ERT cascade only models non-rigid deformations .",0
17406,"Moreover , the projection of the 3D face model is a correct 2 D shape , a requirement for the ERT to converge to a valid face shape .",0
17407,"Of course , the 3D initialization is fundamental to achieve good performance in presence of large face rotations .",1
17408,"So , it provides the largest improvement in the pose subset .",0
17409,The use of CNN probability maps improves the NME in the full data set in about 20 % ( see CNN+ 3D + SE vs CNN + 3D + DE ) .,0
17410,"The large receptive fields of CNNs are specially helpful in challenging situations , specifically those in the pose and occlusion subsets .",1
17411,"The coarse - to - fine strategy in our cascaded ERT provides significative local improvements in difficult cases , with rare facial part combinations ( see ) .",1
17412,"For this reason , the largest gain of CNN+3D+ DE + CF vs CNN+ 3D + DE occurs in the expressions subset .",0
17413,"Although this strategy provides improvements in all the database subsets , the actual NME differences are washed out when averaged over the number of landmarks in the face and the number of images in the subset .",0
17414,"They maybe appreciated by looking into specific data subsets or samples ( see ) , such",0
17415,Method Full,0
17416,Pose Expression Illumination Make - up Occlusion,0
17417,Blur corners corners corners corners corners corners corners NM E AU C 10 F R 10 NM E AU C 10 F R 10 NM E AU C 10 F R 10 NM E AU C 10 F R 10 NM E AU C 10 F R 10 NM E AU C 10 F R 10 NM E AU C 10 F R 10 ESR 10.29 30.02 as the left eyebrow / eye location improvement in ( best viewed after zoom - in ) .,0
17418,"Finally , we analyze the NME distribution produced by the rigid initialization and the final 3 DDE model ( see ) .",0
17419,"Using the model trained for the WFLW experiment , we align the 2500 test samples of WFLW and plot the distribution of NMEs , produced both with the CNN + 3D regressor ( soft POSIT result ) and the full CNN+ 3D + DE + CF regressor ( 3 DDE result ) .",0
17420,The values of percentiles 10 and 90 of the NME distribution are 3.71 and 6.87 for the CNN + 3D regressor and 1.03 and 3.32 for the CNN+3D+ DE + CF one .,0
17421,"So , on average , the full regressor reduces in about 60 % the NME achieved by the rigid initialization .",0
17422,Cross - dataset evaluation,0
17423,In this section we perform cross - dataset experiments to evaluate the quality of present benchmarks and the generalization of the regressors trained on them .,0
17424,"Here we benefit from the fact that 3 DDE maybe trained in a semi-supervised way , i.e. , using data sets with missing or unlabeled landmarks .",0
17425,To this end we select 24 distinct facial landmarks ( see ) .,0
17426,We consider them distinct because they maybe accurately located by a human annotator .,0
17427,We train and evaluate 3 DDE respectively with the training and test sets of each database .,0
17428,"We have also performed one more experiment training 3 DDE with the training sets of all data bases and evaluating it successively with the tests sets of each of them , we denote this experiment with label All .",0
17429,In we show the results of our evaluation .,0
17430,"The smallest database , COFW , has the worst cross - dataset results .",0
17431,"On the other hand , the data set with greatest diversity , WFLW , has the best results .",0
17432,"Moreover , the model All , trained with the training sets of all data bases , is able to improve , in all cross - dataset experiments , the models trained in a single data set .",0
17433,"However , the most prominent outcome of this experiment is that we always achieve the best result when training with the train subset of the same database .",0
17434,"This holds even when compared against the model trained with all data sets , confirming the existence of the so - called "" data set bias "" in current benchmarks .",0
17435,Ina final experiment we use model,0
17436,All to evaluate the NME of each landmark using the test sets of all data sets ( see ) .,0
17437,"The landmarks with highest NME are those related to the ears , the bottom of the mouth and the chin .",0
17438,Conclusions,0
17439,"We have introduced 3DDE , a robust face alignment method that leverages on good properties of CNNs , cascade of ERT and 3D face models .",0
17440,The CNN provides robust landmark estimations We use the height as normalization for the NME .,0
17441,with weak face shape enforcement .,0
17442,"The ERT is able to enforce the face shape and achieve better accuracy in landmark detection , but it only converges with a good initialization .",0
17443,"Finally , 3D models exploit face orientation information to improve self - occlusion estimation .",0
17444,3 DDE is initialized by robustly fitting a 3 D face model to the probability maps produced by the CNN .,0
17445,The 3D model enables 3 DDE to handle self - occlusions and successfully deal with both frontal and profile faces .,0
17446,"Once initialized , the cascade of ERT only models the non-rigid component of face motion .",0
17447,"It provides various benefits , namely , it enforces shape consistency , maybe trained with unlabeled landmarks , estimate landmark visibility and efficiently parallelize the execution of the regression trees within each stage .",0
17448,We have additionally introduced a coarse - to - fine scheme within the cascade of ERT that is able to deal with the combinatorial explosion of local parts deformation .,0
17449,"In this case , the usual monolithic ERT will perform poorly when fitting faces with combinations of facial part deformations not present in the training set .",0
17450,This is a fundamental limitation of implicit shape models addressed by 3DDE .,0
17451,"In the experiments we have shown that 3 DDE improves , as far as we know , the state - of - theart performance in 300W , COFW , AFLW and WFLW data sets .",0
17452,In our ablation analysis we have shown that all the components of the system critically contribute to the final result .,0
17453,The availability of large annotated data sets has encouraged research in this area with important performance improvements in recent years .,0
17454,"However , as shown in , this problem is still far from being completely solved .",0
17455,A critical question here is whether the models trained with present data sets will generalize to the situations present in real - life operation .,0
17456,The cross - dataset experiments performed reveal the existence of a significant data set bias in present benchmarks that limit the generalization of models trained with them .,0
17457,"So , further work in this direction is required to improve the performance of present face alignment algorithms . :",0
17458,"Representative results considered errors using 3DDE in 300W , COFW , AFLW and WFLW testing subsets .",0
17459,"Blue colour represents ground truth , green and red colours point out visible and non-visible shape predictions respectively .",0
17460,title,0
17461,BERT for Coreference Resolution : Baselines and Analysis,1
17462,abstract,0
17463,"We apply BERT to coreference resolution , achieving strong improvements on the OntoNotes ( + 3.9 F1 ) and GAP ( + 11.5 F1 ) benchmarks .",0
17464,"A qualitative analysis of model predictions indicates that , compared to ELMo and BERT - base , BERT - large is particularly better at distinguishing between related but distinct entities ( e.g. , President and CEO ) .",0
17465,"However , there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .",0
17466,Our code and models are publicly available 1 .,0
17467,Introduction,0
17468,"Recent BERT - based models have reported dramatic gains on multiple semantic benchmarks including question - answering , natural language inference , and named entity recognition .",0
17469,"Apart from better bidirectional reasoning , one of BERT 's major improvements over previous methods is passage - level training , 2 which allows it to better model longer sequences .",0
17470,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .",1
17471,We present two ways of extending the c 2f - coref model in .,1
17472,The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,0
17473,The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .,0
17474,BERT - large improves over ELMo - based c 2f - coref 3.9 % on OntoNotes and 11.5 % on GAP ( both absolute ) .,0
17475,1 https://github.com/mandarjoshi90/coref,1
17476,"2 Each BERT training example consists of around 512 word pieces , while ELMo is trained on single sentences .",0
17477,"A qualitative analysis of BERT and ELMobased models suggests that BERT - large ( unlike BERT - base ) is remarkably better at distinguishing between related yet distinct entities or concepts ( e.g. , Repulse Bay and Victoria Harbor ) .",0
17478,"However , both models often struggle to resolve coreferences for cases that require world knowledge ( e.g. , the developing story and the scandal ) .",0
17479,"Likewise , modeling pronouns remains difficult , especially in conversations .",0
17480,We also find that BERT - large benefits from using longer context windows ( 384 word pieces ) while BERT - base performs better with shorter contexts ( 128 word pieces ) .,0
17481,"Yet , both variants perform much worse with longer context windows ( 512 tokens ) in spite of being trained on 512 - size contexts .",0
17482,"Moreover , the overlap variant , which artificially extends the context window beyond 512 tokens provides no improvement .",0
17483,This indicates that using larger context windows for pretraining might not translate into effective long - range features for a downstream task .,0
17484,"Larger models also exacerbate the memory - intensive nature of span representations , which have driven recent improvements in coreference resolution .",0
17485,"Together , these observations suggest that there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .",0
17486,Method,0
17487,"For our experiments , we use the higher - order coreference model in which is the current state of the art for the English OntoNotes dataset .",0
17488,We refer to this as c 2 f - coref in the paper .,0
17489,Overview of c2f- coref,0
17490,"For each mention span x , the model learns a distribution P ( ) over possible antecedent spans Y :",0
17491,"The scoring function s ( x , y ) between spans x and y uses fixed - length span representations , g x and g y to represent its inputs .",0
17492,These consist of a concatenation of three vectors : the two LSTM states of the span endpoints and an attention vector computed over the span tokens .,0
17493,"It computes the score s ( x , y ) by the mention score of x ( i.e. how likely is the span x to be a mention ) , the mention score of y , and the joint compatibility score of x and y ( i.e. assuming they are both mentions , how likely are x and y to refer to the same entity ) .",0
17494,The components are computed as follows :,0
17495,"where FFNN ( ) represents a feedforward neural network and ?( x , y) represents speaker and metadata features .",0
17496,These span representations are later refined using antecedent distribution from a spanranking architecture as an attention mechanism .,0
17497,Applying BERT,0
17498,We replace the entire LSTM - based encoder ( with ELMo and GloVe embeddings as input ) in c2fcoref with the BERT transformer .,0
17499,We treat the first and last word - pieces ( concatenated with the attended version of all word pieces in the span ) as span representations .,0
17500,"Documents are split into segments of max segment len , which we treat as a hyperparameter .",0
17501,We experiment with two variants of splitting :,0
17502,Independent,0
17503,The independent variant uses nonoverlapping segments each of which acts as an independent instance for BERT .,0
17504,The representation for each token is limited to the set of words that lie in its segment .,0
17505,"As BERT is trained on sequences of at most 512 word pieces , this variant has limited encoding capacity especially for tokens that lie at the start or end of their segments .",0
17506,Overlap,0
17507,The overlap variant splits the document into overlapping segments by creating a Tsized segment after every T / 2 tokens .,0
17508,"These segments are then passed onto the BERT encoder independently , and the final token representation is derived by element - wise interpolation of representations from both overlapping segments .",0
17509,Let r 1 ?,0
17510,Rd and r 2 ?,0
17511,Rd be the token representations from the overlapping BERT segments .,0
17512,The final representation r ?,0
17513,Rd is given by :,0
17514,where w ?,0
17515,R 2dd is a trained parameter and [ ; ] represents concatenation .,0
17516,This variant allows the model to artificially increase the context window beyond the max segment len hyperparameter .,0
17517,All layers in both model variants are then finetuned following .,0
17518,Experiments,0
17519,"We evaluate our BERT - based models on two benchmarks : the paragraph - level GAP dataset , and the documentlevel English OntoNotes 5.0 dataset .",0
17520,OntoNotes examples are considerably longer and typically require multiple segments to read the entire document .,0
17521,Implementation and Hyperparameters,0
17522,We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .,1
17523,"We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the BERT parameters and the task parameters respectively .",1
17524,We found that this made a sizable impact of 2 - 3 % over using the same learning rate for all parameters .,0
17525,"We trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for BERT - base and BERT - large respectively .",1
17526,"As span representations are memory intensive , we truncate documents randomly to eleven segments for BERT - base and",0
17527,"Martschat and Strube 2017 ) , which does not use contextualized representations .",0
17528,"In addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .",0
17529,Paragraph Level : GAP,1
17530,GAP ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from Wikipedia snippets .,0
17531,"Examples in the GAP dataset fit within a single BERT segment , thus eliminating the need for cross - segment inference .",0
17532,"Following , we trained our BERT - based c 2f - coref model on OntoNotes .",0
17533,5,0
17534,The predicted clusters were scored against GAP examples according to the official evaluation script .,0
17535,Table 2 shows that BERT improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .,1
17536,These results are inline with large gains reported for a variety of semantic tasks by BERTbased models .,0
17537,Document Level : OntoNotes,1
17538,OntoNotes ( English ) is a document - level dataset from the CoNLL - 2012 shared task on coreference resolution .,0
17539,"It consists of about one million words of newswire , magazine articles , broadcast news , broadcast conversations , web data and conversational speech data , and the New Testament .",0
17540,"The main evaluation is the average F1 of three metrics - MUC , B 3 , and CEAF ?",0
17541,4 on the test set according to the official CoNLL - 2012 evaluation scripts .,0
17542,shows that BERT - base offers an improvement of 0.9 % over the ELMo - based c2 fcoref model .,1
17543,"Given how gains on coreference resolution have been hard to come by as evidenced by the table , this is still a considerable improvement .",0
17544,"However , the magnitude of gains is relatively modest considering BERT 's arguably better architecture and many more trainable parameters .",0
17545,This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks .,0
17546,"BERT - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .",1
17547,We also observe that the overlap variant offers no improvement over independent .,1
17548,"Concurrent with our work , , who use higher - order entity - level representations over "" frozen "" BERT features , also report large gains over c 2 f - coref .",0
17549,"While their feature - based approach is more memory efficient , the fine - tuned model seems to yield better results .",0
17550,"Also concurrent , Span BERT , another self - supervised method , pretrains span representations achieving state of the art results ( Avg. F1 79.6 ) with the independent variant .",0
17551,Analysis,0
17552,"We performed a qualitative comparison of ELMo and BERT models ) on the OntoNotes English development set by manually assigning error categories ( e.g. , pronouns , mention paraphrasing ) to incorrect predicted clusters .",0
17553,"Overall , we found 93 errors for BERT - base and 74 for BERT - large from the same 15 documents .",0
17554,Each incorrect cluster can belong to multiple categories .,0
17555,Strengths,0
17556,"We did not find salient qualitative differences between ELMo and BERT - base models , which is consistent with the quantitative results .",0
17557,"BERT - large improves over BERT - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .",0
17558,"In particular , the BERT - large variant is better at distinguishing related , but distinct , entities .",0
17559,shows several examples where the BERT - base variant merges distinct entities ( like Ocean Theater and Marine Life Center ) into a single cluster .,0
17560,BERT - large seems to be able to avoid such merging on a more regular basis .,0
17561,Weaknesses,0
17562,"An analysis of errors on the OntoNotes English development set suggests that better modeling of document - level context , conversations , and entity paraphrasing might further improve the state of the art .",0
17563,Longer documents in OntoNotes generally contain larger and more spread - out clusters .,0
17564,"We focus on three observations -( a ) shows how models perform distinctly worse on longer documents , ( b ) both models are unable to use larger segments more effectively ) and perform worse when the max segment len of 450 and 512 are used , and , ( c ) using overlapping segments to provide additional context does not improve results .",0
17565,Recent work suggests that BERT 's inability to use longer sequences effectively is likely a by - product pretraining on short sequences for a vast majority of updates .,0
17566,Comparing preferred segment lengths for base and large variants of BERT indicates that larger models might better encode longer contexts .,0
17567,"However , larger models also exacerbate the memoryintensive nature of span representations , 7 which have driven recent improvements in coreference resolution .",0
17568,These observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .,0
17569,"Modeling pronouns especially in the context of conversations , continues to be difficult for all models , perhaps partly because c 2 f - coref does very little to model dialog structure of the document .",0
17570,"Lastly , a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing .",0
17571,"For example , bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities , especially considering that such learning signal is rather sparse in the training set .",0
17572,Related Work,0
17573,Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution .,0
17574,The base coreference model used in this paper from belongs to this family of models .,0
17575,"More recently , advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations .",0
17576,"Of these , BERT notably uses pretraining on passage - level sequences ( in conjunction with a bidirectional masked language modeling objective ) to more effectively model long - range dependencies .",0
17577,SpanBERT focuses on pretraining span representations achieving current state of the art results on OntoNotes with the independent variant .,0
17578,title,0
17579,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,1
17580,abstract,0
17581,A long - standing challenge in coreference resolution has been the incorporation of entity - level information - features defined over clusters of mentions instead of mention pairs .,0
17582,We present a neural network based coreference system that produces high - dimensional vector representations for pairs of coreference clusters .,0
17583,"Using these representations , our system learns when combining clusters is desirable .",0
17584,We train the system with a learning - to - search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high - scoring final coreference partition .,0
17585,The system substantially outperforms the current state - of - the - art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand - engineered features .,0
17586,Introduction,0
17587,"Coreference resolution , the task of identifying which mentions in a text refer to the same realworld entity , is fundamentally a clustering problem .",0
17588,"However , many recent state - of - the - art coreference systems operate solely by linking pairs of mentions together .",0
17589,"An alternative approach is to use agglomerative clustering , treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity .",0
17590,"Such systems can take advantage of entity - level information , i.e. , features between clusters of mentions instead of between just two mentions .",0
17591,"As an example for why this is useful , it is clear that the clusters { Bill Clinton } and { Clinton , she } are not referring to the same entity , but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent .",0
17592,Previous work has incorporated entity - level information through features that capture hard constraints like having gender or number agreement between clusters .,0
17593,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .",1
17594,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .",1
17595,"Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .",1
17596,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",1
17597,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,1
17598,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,0
17599,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,1
17600,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,1
17601,"Our system uses little manual feature engineering , which means it is easily extended to multiple languages .",0
17602,We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset .,0
17603,The cluster - ranking model significantly outperforms a mention - ranking model that does not use entity - level information .,0
17604,We also show that using an easy - first strategy improves the performance of the cluster - ranking model .,0
17605,"Our final system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese , substantially outperforming other state - of - the - art systems .",0
17606,1,0
17607,System Architecture,0
17608,Our cluster - ranking model is a single neural network that learns which coreference cluster merges are desirable .,0
17609,"However , it is helpful to think of the network as being composed of distinct subnetworks .",0
17610,The mention - pair encoder produces distributed representations for pairs of mentions by passing relevant features through a feedforward neural network .,0
17611,"The cluster - pair encoder produces distributed representations for pairs of clusters by applying a pooling operation over the representations of relevant mention pairs , i.e. , pairs where one mention is in each cluster .",0
17612,The clusterranking model then scores pairs of clusters by passing their representations through a single neural network layer .,0
17613,We also train a mention - ranking model that scores pairs of mentions by passing their representations through a single neural network layer .,0
17614,"Its parameters are used to initialize the clusterranking model , and the scores it produces are used to prune which candidate cluster merges the cluster - ranking model considers , allowing the cluster - ranking model to run much faster .",0
17615,The system architecture is summarized in .,0
17616,Mention - Pair Encoder,0
17617,"Given a mention m and candidate antecedent a , the mention - pair encoder produces a distributed representation of the pair rm ( a , m ) ?",0
17618,"Rd with a feedforward neural network , which is shown in .",0
17619,"The candidate antecedent maybe any mention that occurs before min the document or NA , indicating that m has no antecedent .",0
17620,"We also experimented with models based on Long Short - Term Memory recurrent neural networks ( Hochreiter and Schmidhuber , 1997 ) , but found these to perform slightly worse when used in an end - to - end coreference system due to heavy overfitting to the training data .",0
17621,Input Layer .,0
17622,"For each mention , the model extracts various words and groups of words thatare fed into the neural network .",0
17623,Each word is represented by a vector w i ?,0
17624,R dw .,0
17625,Each group of words is represented by the average of the vectors of each word in the group .,0
17626,"For each mention and pair of mentions , a small number of binary features and distance features are also extracted .",0
17627,Distances and mention lengths are binned into one of the buckets and then encoded in a one - hot vector in addition to being included as continuous features .,0
17628,The full set of features is as follows :,0
17629,Embedding Features :,0
17630,"Word embeddings of the headword , dependency parent , first word , last word , two preceding words , and two following words of the mention .",0
17631,"Averaged word embeddings of the five preceding words , five following words , all words in the mention , all words in the mention 's sentence , and all words in the mention 's document .",0
17632,Additional Mention,0
17633,Features :,0
17634,"The type of the mention , the mention 's position ( index of the mention divided by the number of mentions in the document ) , whether the mentions is contained in another mention , and the length of the mention in words .",0
17635,"Document Genre : The genre of the mention 's document ( broadcast news , newswire , web data , etc . ) .",0
17636,Cluster - Ranking Model,0
17637,Mention - Ranking Model,0
17638,"Rather than training a cluster - ranking model from scratch , we first train a mention - ranking model that assigns each mention its highest scoring candidate antecedent .",0
17639,There are two key advantages of doing this .,0
17640,"First , it serves as pretraining for the cluster - ranking model ; in particular the mentionranking model learns effective weights for the mention - pair encoder .",0
17641,"Second , the scores produced by the mention - ranking model are used to provide a measure of which coreference decisions are easy ( allowing for an easy - first clustering strategy ) and which decisions are clearly wrong ( these decisions can be pruned away , significantly reducing the search space of the cluster - ranking model ) .",0
17642,"The mention - ranking model assigns a score s m ( a , m ) to a mention m and candidate an - tecedent a representing their compatibility for coreference .",0
17643,"This is produced by applying a single fully connected layer of size one to the representation rm ( a , m ) produced by the mention - pair encoder :",0
17644,the mention - ranking model links each mention with its highest scoring candidate antecedent .,0
17645,Training Objective .,0
17646,"We train the mentionranking model with the slack - rescaled maxmargin training objective from , which encourages separation between the highest scoring true and false antecedents of the current mention .",0
17647,"Suppose the training set consists of N mentions m 1 , m 2 , ... , m N .",0
17648,"Let A ( m i ) denote the set of candidate antecedents of a mention mi ( i.e. , mentions preceding mi and NA ) , and T ( m i ) denote the set of true antecedents of mi ( i.e. , mentions preceding mi thatare coreferent with it or { NA } if mi has no antecedent ) .",0
17649,Lett i be the highest scoring true antecedent of mention,0
17650,Then the loss is given by,0
17651,"for "" false new , "" "" false anaphoric , "" "" wrong link , "" and correct coreference decisions .",0
17652,The different error penalties allow the system to be tuned for coreference evaluation metrics by biasing it towards making more or fewer coreference links .,0
17653,"to using more precise mention detection , which results in fewer links to NA .",0
17654,Training Details .,0
17655,"We initialized our word embeddings with 50 dimensional ones produced by word2vec on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot ( Al - Rfou et al. , 2013 ) for Chinese .",0
17656,Averaged word embeddings were held fixed during training while the embeddings used for single words were updated .,0
17657,"We set our hidden layer sizes to M 1 = 1000 , M 2 = d = 500 and minimized the training objective using RMS - Prop .",0
17658,"To regularize the network , we applied L2 regularization to the model weights and dropout with a rate of 0.5 on the word embeddings and the output of each hidden layer .",0
17659,Pretraining .,0
17660,"As in , we found that pretraining is crucial for the mentionranking model 's success .",0
17661,"We pretrained the network in two stages , minimizing the following objectives from Clark and Manning :",0
17662,"Where F ( m i ) is the set of false antecedents form i and p ( a , mi ) = sigmoid ( s ( a , mi ) ) .",0
17663,"The top - pairs objective is a middle ground between the all - pairs classification and mention ranking objectives : it only processes high - scoring mentions , but is probabilistic rather than max - margin .",0
17664,We first pretrained the network with all - pairs classification for 150 epochs and then with top - pairs classification for 50 epochs .,0
17665,See Section 6.1 for experiments on the two - stage pretraining .,0
17666,"Although a strong coreference system on its own , the mention - ranking model has the dis advantage of only considering local information between pairs of mentions , so it can not consolidate information at the entity - level .",0
17667,We address this problem by training a cluster - ranking model that scores pairs of clusters instead of pairs of mentions .,0
17668,"Given two clusters of mentions c i and c j , the cluster - ranking model produces a score s c ( c i , c j ) representing their compatibility for coreference .",0
17669,"This is produced by applying a single fully connected layer of size one to the representation r c ( c i , c j ) produced by the cluster - pair encoder :",0
17670,where W c is a 1 2 d weight matrix .,0
17671,"Our cluster - ranking approach also uses a measure of anaphoricity , or how likely it is for a mention m to have an antecedent .",0
17672,This is defined as,0
17673,Building Representations,0
17674,"In this section , we describe the neural networks producing distributed representations of pairs of mentions and pairs of coreference clusters .",0
17675,We assume that a set of mentions has already been extracted from each document using a method such as the one in .,0
17676,Distance Features :,0
17677,"The distance between the mentions in sentences , the distance between the mentions in intervening mentions , and whether the mentions overlap .",0
17678,Speaker Features :,0
17679,Whether the mentions have the same speaker and whether one mention is the other mention 's speaker as determined by string matching rules from .,0
17680,String Matching,0
17681,Features :,0
17682,"Head match , exact string match , and partial string match .",0
17683,"The vectors for all of these features are concatenated to produce an I-dimensional vector h 0 , the input to the neural network .",0
17684,"If a = NA , the features defined over mention pairs are not included .",0
17685,"For this case , we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores .",0
17686,Our set of hand - engineered features is much smaller than the dozens of complex features typically used in coreference systems .,0
17687,"However , we found these features were crucial forgetting good model performance .",0
17688,See Section 6.1 for a feature ablation study .,0
17689,Hidden Layers .,0
17690,The input gets passed through three hidden layers of rectified linear ( ReLU ) units .,0
17691,Each unit in a hidden layer is fully connected to the previous layer :,0
17692,"The output of the last hidden layer is the vector representation for the mention pair : rm ( a , m ) = h 3 ( a , m ) . :",0
17693,Cluster - pair encoder .,0
17694,Cluster - Pair Representation,0
17695,Mention - Pair Representations,0
17696,Cluster - Pair Encoder,0
17697,Given two clusters of mentions c,0
17698,The architecture of the encoder is summarized in .,0
17699,The cluster - pair encoder first combines the information contained in the matrix of,0
17700,.,0
17701,This is done by applying a pooling operation .,0
17702,"In particular it concatenates the results of max - pooling and average - pooling , which we found to be slightly more effective than using either one alone :",0
17703,Cluster - Ranking Policy Network,0
17704,"At test time , the cluster ranker iterates through every mention in the document , merging the current mention 's cluster with a preceding one or performing no action .",0
17705,We view this procedure as a sequential decision process where at each step the algorithm observes the current state x and performs some action u.,0
17706,"Specifically , we define a state x = ( C , m ) to consist of C = {c 1 , c 2 , ...} , the set of existing coreference clusters , and m , the current mention being considered .",0
17707,"At a start state , each cluster in PASS .",0
17708,This leaves the clustering unchanged .,0
17709,"After determining the new clustering C based on the existing clustering C and action u , we consider another mention m to get the next state x = ( C , m ) .",0
17710,"Using the scoring functions s c and s NA , we define a policy network ?",0
17711,that assigns a probability distribution over U ( x ) as follows :,0
17712,"During inference , ?",0
17713,is executed by taking the highest - scoring ( most probable ) action at each step .,0
17714,Easy - First Cluster Ranking,0
17715,The last detail needed is the ordering in which to consider mentions .,0
17716,"Cluster - ranking models in prior work order the mentions according to their positions in the document , processing them leftto - right .",0
17717,"However , we instead sort the mentions in descending order by their highest scoring candidate coreference link according to the mention - ranking model .",0
17718,This causes inference to occur in an easyfirst fashion where hard decisions are delayed until more information is available .,0
17719,Easy - first orderings have been shown to improve the performance of other incremental coreference strategies because they reduce the problem of errors compounding as the algorithm runs .,0
17720,We also find it beneficial to prune the set of candidate antecedents A ( m ) for each mention m .,0
17721,"Rather than using all previously occurring mentions as candidate antecedents , we only include high - scoring ones , which greatly reduces the size of the search space .",0
17722,This allows for much faster learning and inference ; we are able to remove over 95 % of candidate actions with no decrease in the model 's performance .,0
17723,"For both of these two preprocessing steps , we use s ( a , m ) ? s ( NA , m ) as the score of a coreference link between a and m .",0
17724,Deep Learning to Search,0
17725,We face a sequential prediction problem where future observations ( visited states ) depend on previous actions .,0
17726,This is challenging because it violates the common i.i.d. assumption made in machine learning .,0
17727,"Learning - to - search algorithms are effective for this sort of problem , and have been applied successfully to coreference resolution as well as other structured prediction tasks in natural language processing Algorithm 1 Deep Learning to Search for i = 1 to num epochs do Initialize the current training set ? = ? for each example ( x , y) ?",0
17728,D do Run the policy ?,0
17729,"to completion from start state x to obtain a trajectory of states {x 1 , x 2 , ... , x n } for each state x i in the trajectory do for each possible action u ? U ( x i ) do Execute u on xi and then run the reference policy ?",0
17730,"ref until reaching an end state e Assign u a cost by computing the loss on the end state : l ( u ) = L(e , y) end for Add the state x i and associated costs l to ?",0
17731,"end for end for Update ? with gradient descent , minimizing ( x , l ) ?? u?U ( x ) ? ( u|x ) l ( u ) end for",0
17732,"Chang et al. , 2015 a ) .",0
17733,"We train the cluster - ranking model using a learning - to - search algorithm inspired by SEARN , which is described in Algorithm",0
17734,1 . The algorithm takes as input a dataset D of start states x ( in our case documents with each mention in its own singleton coreference cluster ) and structured labels y ( in our case gold coreference clusters ) .,0
17735,It s goal is to train the policy ?,0
17736,"so when it executes from x , reaching a final state e , the resulting loss L (e , y) is small .",0
17737,We use the negative of the B 3 coreference metric for this loss ) .,0
17738,"Although our system evaluation also includes the MUC and CEAF ? 4 ( Luo , 2005 ) metrics , we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF ?",0
17739,4 is slow to compute .,0
17740,"For each example ( x , y ) ?",0
17741,"D , the algorithm obtains a trajectory of states x 1 , x 2 , ... , x n visited by the current policy by running it to completion ( i.e. , repeatedly taking the highest scoring action until reaching an end state ) from the start state x .",0
17742,"This exposes the model to states at train time similar to the ones it will face at test time , allowing it to learn how to cope with mistakes .",0
17743,"Given a state x in a trajectory , the algorithm then assigns a cost l ( u ) to each action u ? U ( x ) by executing the action , "" rolling out "" from the resulting state with a reference policy ?",0
17744,"ref until reaching an end state e , and computing the resulting loss L (e , y ) .",0
17745,"This rolling out procedure allows the model to learn how a local action will affect the final score , which can not be otherwise computed because coreference evaluation metrics do not de -compose over cluster merges .",0
17746,The policy network is then trained to minimize the risk associated with taking each action : u?U ( x ) ?( u |x ) l ( u ) .,0
17747,Reference policies typically refer to the gold labels to find actions thatare likely to be beneficial .,0
17748,Our reference policy ?,0
17749,"ref takes the action that increases the B 3 score the most each step , breaking ties randomly .",0
17750,It is generally recommended to use a stochastic mixture of the reference policy and the current learned policy during rollouts when the reference policy is not optimal .,0
17751,"However , we find only using the reference policy ( which is close to optimal ) to be much more efficient because it does not require neural network computations and is deterministic , which means the costs of actions can be cached .",0
17752,Training details .,0
17753,We update ?,0
17754,using RMSProp and apply dropout with a rate of 0.5 to the input layer .,0
17755,"For most experiments , we initialize the mention - pair encoder component of the clusterranking model with the learned weights from the mention - ranking model , which we find to greatly improve performance ( see Section 6.2 ) .",0
17756,Runtime .,0
17757,The full cluster - ranking system runs end - to - end in slightly under 1 second per document on the English test set when using a GPU ( including scoring all pairs of mentions with the mention - ranking model for search - space pruning ) .,0
17758,This means the bottleneck for the over all system is the syntactic parsing required for mention detection ( about 4 seconds per document on the English test set ) .,0
17759,"The models are evaluated using three of the most popular coreference metrics : MUC , B 3 , and Entity - based CEAF ( CEAF ?",0
17760,"4 ) . We generally report the average F 1 score ( CoNLL F 1 ) of the three , which is common practice in coreference evaluation .",0
17761,"We used the most recent version of the CoNLL scorer ( version 8.01 ) , which implements the original definitions of the metrics .",0
17762,Mention Detection .,0
17763,Our experiments were run using system - produced predicted mentions .,0
17764,"We used the rule - based mention detection algorithm from , which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities and pleonastic it pronouns .",0
17765,Mention - Ranking Model Experiments,0
17766,Feature Ablations .,0
17767,We performed a feature ablation study to determine the importance of the hand - engineered features included in our model .,0
17768,The results are shown in .,0
17769,"We find the small number of non-embedding features substantially improves model performance , especially the distance and string matching features .",0
17770,"This is unsurprising , as the additional features are not easily captured byword embeddings and historically such features have been very important in coreference resolvers .",0
17771,The Importance of Pretraining .,0
17772,We evaluate the benefit of the two - step pretraining for the - 0.32 - 0.25 : CoNLL F 1 scores of the cluster - ranking model on the dev sets with various ablations .,0
17773,"- PRETRAINING : initializing model parameters randomly instead of from the mention - ranking model , - EASY - FIRST : iterating through mentions in order of occurrence instead of according to their highest scoring candidate coreference link , - L2S : training on a fixed trajectory of correct actions instead of using learning to search .",0
17774,mention - ranking model and report results in .,0
17775,"Consistent with , we find pretraining to greatly improve the model 's accuracy .",0
17776,"We note in particular that the model benefits from using both pretraining steps from Section 4 , which more smoothly transitions the model from a mention - pair classification objective that is easy to optimize to a max-margin objective better suited for a ranking task .",0
17777,Cluster - Ranking Model Experiments,0
17778,"We evaluate the importance of three key details of the cluster ranker : initializing it with the mentionranking model 's weights , using an easy - first ordering of mentions , and using learning to search .",0
17779,The results are shown in .,0
17780,Pretrained Weights .,0
17781,We compare initializing the cluster - ranking model randomly with initializing it with the weights learned by the mentionranking model .,0
17782,Using pretrained weights greatly improves performance .,0
17783,We believe the clusterranking model has difficulty learning effective weights from scratch due to noise in the signal coming from cluster - level decisions ( an over all bad cluster merge may still involve a few cor-rect pairwise links ) and the smaller amount of data used to train the cluster - ranking model ( many possible actions are pruned away during preprocessing ) .,0
17784,"We believe the score would be even lower without search - space pruning , which stops the model from considering many bad actions .",0
17785,Easy - First Cluster Ranking .,0
17786,We compare the effectiveness of easy - first cluster - ranking with the commonly used left - to - right approach .,0
17787,Using a left - to - right strategy simply requires changing the preprocessing step ordering the mentions so mentions are sorted by their position in the document instead of their highest scoring coreference link according to the mention - ranking model .,0
17788,We find the easy - first approach slightly outperforms using a left - to - right ordering of mentions .,0
17789,We believe this is because delaying hard decisions until later reduces the problem of early mistakes causing later decisions to be made incorrectly .,0
17790,Learning to Search .,0
17791,"We also compare learning to search with the simpler approach of training the model on a trajectory of gold coreference decisions ( i.e. , training on a fixed cost-sensitive classification dataset ) .",0
17792,Using this approach significantly decreases performance .,0
17793,We attribute this to the model not learning how to deal with mistakes when it only sees correct decisions during training .,0
17794,Capturing Semantic Similarity,0
17795,"Using semantic information to improve coreference accuracy has had mixed in results in previous research , and has been called an "" uphill battle "" in coreference resolution .",0
17796,"However , word embeddings are well known for being effective at capturing semantic relatedness , and we show here that neural network coreference models can take advantage of this .",0
17797,"Perhaps the case where semantic similarity is most important is in linking nominals with no head match ( e.g. , "" the nation "" and "" the country "" ) .",0
17798,We compare the performance of our neural network model with our earlier statistical system at classifying mention pairs of this type as being coreferent or not .,0
17799,The neural network shows substantial improvement ( 18.9 F 1 vs. 10.7 F 1 ) on this task compared to the more modest improvement it gets at classifying any pair of mentions as coreferent ( 68.7 F 1 vs. 66.1 F 1 ) .,0
17800,Some example wins are shown in .,0
17801,These types of coreference links are quite rare in the CoNLL data ( about 1.2 % of the positive coref - Antecedent Anaphor the country 's leftist rebels the guerrillas the company the New York firm the suicide bombing the attack the gun the rifle the U.S. carrier the ship :,0
17802,"Examples of nominal coreferences with no head match that the neural model gets correct , but the system from gets incorrect .",0
17803,"erence links in the test set ) , so the improvement does not significantly contribute to the final system 's score , but it does suggest progress on this difficult type of coreference problem .",0
17804,Final System Performance,0
17805,In we compare the results of our system with state - of - the - art approaches for English and Chinese .,0
17806,Our mention - ranking model surpasses all previous systems .,1
17807,"We attribute its improvement over the neural mention ranker from to our model using a deeper neural network , pretrained word embeddings , and more sophisticated pretraining .",0
17808,"The cluster - ranking model improves results further across both languages and all evaluation metrics , demonstrating the utility of incorporating entity - level information .",1
17809,The improvement is largest in CEAF ?,0
17810,"4 , which is encouraging because CEAF ?",0
17811,"4 is the most recently proposed metric , designed to correct flaws in the other two .",0
17812,We believe entity - level information is particularly useful for preventing bad merges between large clusters ( see for an example ) .,0
17813,"However , it is worth noting that in practice the much more complicated cluster - ranking model brings only fairly modest gains in performance .",0
17814,Related Work,0
17815,"There has been extensive work on machine learning approaches to coreference resolution , with mentionranking models being particularly popular .",0
17816,"We train a neural mention - ranking model inspired by as a starting point , but then use it to pretrain a cluster - ranking model that benefits from entity - level information .",0
17817,Wise - :,0
17818,"Thanks to entity - level information , the cluster - ranking model correctly declines to merge these two large clusters when running on the test set .",0
17819,"However , the mention - ranking model incorrectly links the Russian President and President Clinton 's , which greatly reduces the final precision score .",0
17820,man et al .,0
17821,extend their mention - ranking model by incorporating entity - level information produced by a recurrent neural network running over the candidate antecedent - cluster .,0
17822,"However , this is an augmentation to a mention - ranking model , and not fundamentally a clustering model as our cluster ranker is .",0
17823,Entity - level information has also been incorporated in coreference systems using joint inference and systems that buildup coreference clusters incrementally .,0
17824,"We take the latter approach , and in particular combine the cluster - ranking ( Rahman and and easy - first clustering strategies .",0
17825,These prior systems all express entity - level information in the form of hand - engineered features and constraints instead of entity - level distributed representations thatare learned from data .,0
17826,We train our system using a learning - to - search algorithm similar to SEARN .,0
17827,"Learning - to - search style algorithms have been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test - time by , , and .",0
17828,Other works use structured perceptron models for the same purpose .,0
17829,Conclusion,0
17830,We have presented a coreference system that captures entity - level information with distributed representations of coreference cluster pairs .,0
17831,"These learned , dense , high - dimensional feature vectors provide our cluster - ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones .",0
17832,The model is trained with a learning - to - search algorithm that allows it to learn how local decisions will affect the final coreference score .,0
17833,We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state - of - the - art .,0
17834,title,0
17835,End - to - end Neural Coreference Resolution,1
17836,abstract,0
17837,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,1
17838,The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each .,0
17839,The model computes span embeddings that combine context - dependent boundary representations with a headfinding attention mechanism .,0
17840,It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions .,0
17841,"Experiments demonstrate state - of - the - art performance , with again of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5 - model ensemble , despite the fact that this is the first approach to be successfully trained with no external resources .",0
17842,Introduction,0
17843,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,1
17844,"All recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .",0
17845,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .",1
17846,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,1
17847,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",1
17848,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .",1
17849,"The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .",1
17850,"In our analyses , we show empirically that these learned attention weights correlate strongly with traditional headedness definitions .",0
17851,"Scoring all span pairs in our end - to - end model is impractical , since the complexity would be quartic in the document length .",0
17852,"Therefore we factor the model over unary mention scores and pairwise antecedent scores , both of which are simple functions of the learned span embedding .",0
17853,"The unary mention scores are used to prune the space of spans and antecedents , to aggressively reduce the number of pairwise computations .",0
17854,Our final approach outperforms existing models by 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5 - model ensemble .,0
17855,"It is not only accurate , but also relatively interpretable .",0
17856,"The model factors , for example , directly indicate whether an absent coreference link is due to low mention scores ( for either span ) or a low score from the mention ranking component .",0
17857,The head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .,0
17858,"We leverage this over all interpretability to do detailed quantitative and qualitative analyses , providing insights into the strengths and weak - nesses of the approach .",0
17859,Related Work,0
17860,Machine learning methods have along history in coreference resolution ( see for a detailed survey ) .,0
17861,"However , the learning problem is challenging and , until very recently , handengineered systems built on top of automatically produced parse trees outperformed all learning approaches .",0
17862,"showed that highly lexical learning approaches reverse this trend , and more recent neural models have achieved significant performance gains .",0
17863,"However , all of these models use parsers for head features and include highly engineered mention proposal algorithms .",0
17864,1 Such pipelined systems suffer from two major drawbacks : ( 1 ) parsing mistakes can introduce cascading errors and ( 2 ) many of the handengineered rules do not generalize to new languages .,0
17865,A non-pipelined system that jointly models mention detection and coreference resolution was first proposed by .,0
17866,They introduce a search - based system that predicts the coreference structure in a left - to - right transition system that can incorporate global features .,0
17867,"In contrast , our approach performs well while making much stronger independence assumptions , enabling straightforward inference .",0
17868,"More generally , a wide variety of approaches for learning coreference models have been proposed .",0
17869,"They can typically be categorized as ( 1 ) mention - pair classifiers , ( 2 ) entity - level models , ( 3 ) latent - tree models , or ( 4 ) mention - ranking models .",0
17870,"Our span - ranking approach is most similar to mention ranking , but we reason over a larger space by jointly detecting mentions and predicting coreference .",0
17871,Task,0
17872,We formulate the task of end - to - end coreference resolution as a set of decisions for every possible span in the document .,0
17873,The input is a document D containing T words along with metadata such as speaker and genre information .,0
17874,Let N = T ( T + 1 ) 2 be the number of possible text spans in D .,0
17875,"Denote the start and end indices of a span i in D respectively by and END ( i ) , for 1 ?",0
17876,i ?,0
17877,N .,0
17878,We assume an ordering of the spans based on START ( i ) ; spans with the same start index are ordered by END ( i ) .,0
17879,The task is to assign to each span i an antecedent y i .,0
17880,"The set of possible assignments for each y i is Y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?",0
17881,and all preceding spans .,0
17882,"True antecedents of span i , i.e. span j such that 1 ? j ? i ?",0
17883,"1 , represent coreference links between i and j .",0
17884,The dummy antecedent ?,0
17885,represents two possible scenarios : ( 1 ) the span is not an entity mention or ( 2 ) the span is an entity mention but it is not coreferent with any previous span .,0
17886,"These decisions implicitly define a final clustering , which can be recovered by grouping all spans thatare connected by a set of antecedent predictions .",0
17887,Model,0
17888,"We aim to learn a conditional probability distribution P ( y 1 , . . . , y N | D ) whose most likely configuration produces the correct clustering .",0
17889,We use a product of multinomials for each span :,0
17890,"where s ( i , j ) is a pairwise score for a coreference link between span i and span j in document D .",0
17891,We omit the document D from the notation when the context is unambiguous .,0
17892,"There are three factors for this pairwise coreference score : ( 1 ) whether span i is a mention , ( 2 ) whether span j is a mention , and ( 3 ) whether j is an antecedent of i:",0
17893,"Here s m ( i ) is a unary score for span i being a mention , and s a ( i , j ) is pairwise score for span j being an antecedent of span i .",0
17894,By fixing the score of the dummy antecedent ?,0
17895,"to 0 , the model predicts the best scoring antecedent if any non-dummy scores are positive , and it abstains if they are all negative .",0
17896,A challenging aspect of this model is that it s size is O ( T 4 ) in the document length .,0
17897,"As we will see in Section 5 , the above factoring enables aggressive pruning of spans thatare unlikely to belong to a coreference cluster according the mention score s m ( i ) .",0
17898,Scoring Architecture,0
17899,We propose an end - toend neural architecture that computes the above scores given the document and its metadata .,0
17900,"At the core of the model are vector representations g i for each possible span i , which we describe in detail in the following section .",0
17901,"Given these span representations , the scoring functions above are computed via standard feed - forward neural networks :",0
17902,"where denotes the dot product , denotes element - wise multiplication , and FFNN denotes a feed - forward neural network that computes a nonlinear mapping from input to output vectors .",0
17903,"The antecedent scoring function s a ( i , j ) includes explicit element - wise similarity of each span g i g j and a feature vector ?( i , j ) encoding speaker and genre information from the metadata and the distance between the two spans .",0
17904,Span Representations,0
17905,Two types of information are crucial to accurately predicting coreference links : the context surrounding the mention span and the internal structure within the span .,0
17906,We use a bidirectional LSTM to encode the lexical information of both the inside and outside of each span .,0
17907,We also include an attention mechanism over words in each span to model head words .,0
17908,"We assume vector representations of each word {x 1 , . . . , x T } , which are composed of fixed pretrained word embeddings and 1 - dimensional convolution neural networks ( CNN ) over characters ( see Section 7.1 for details )",0
17909,"To compute vector representations of each span , we first use bidirectional LSTMs to encode every word in its context :",0
17910,"where ? ? {? 1 , 1 } indicates the directionality of each LSTM , and x * t is the concatenated output of the bidirectional LSTM .",0
17911,"We use independent LSTMs for every sentence , since cross - sentence context was not helpful in our experiments .",0
17912,Syntactic heads are typically included as features in previous systems .,0
17913,"Instead of relying on syntactic parses , our model learns a taskspecific notion of headedness using an attention mechanism over words in each span :",0
17914,wherex i is a weighted sum of word vectors in span i .,0
17915,"The weights a i ,t are automatically learned and correlate strongly with traditional definitions of head words as we will see in Section 9.2 .",0
17916,The above span information is concatenated to produce the final representation g i of span i:,0
17917,"This generalizes the recurrent span representations recently proposed for questionanswering , which only include the boundary representations x * START ( i ) and x * END ( i ) .",0
17918,We introduce the soft headword vector xi and a feature vector ?( i ) encoding the size of span i.,0
17919,Inference,0
17920,The size of the full model described above is O ( T 4 ) in the document length T .,0
17921,"To maintain computation efficiency , we prune the candidate spans greedily during both training and evaluation .",0
17922,We only consider spans with up to L words and compute their unary mention scores s m ( i ) ( as defined in Section 4 ) .,0
17923,"To further reduce the number of spans to consider , we only keep up to ?",0
17924,T spans with the highest mention scores and consider only up to K antecedents for each .,0
17925,We also enforce non-crossing bracketing structures with a simple suppression scheme .,0
17926,"We accept spans in decreasing order of the mention scores , unless , when considering span i , there exists a previously accepted span j such that",0
17927,"Despite these aggressive pruning strategies , we maintain a high recall of gold mentions in our experiments ( over 92 % when ? = 0.4 ) .",0
17928,"For the remaining mentions , the joint distribution of antecedents for each document is computed in a forward pass over a single computation graph .",0
17929,The final prediction is the clustering produced by the most likely configuration .,0
17930,Learning,0
17931,"In the training data , only clustering information is observed .",0
17932,"Since the antecedents are latent , we optimize the marginal log -likelihood of all correct antecedents implied by the gold clustering :",0
17933,where GOLD ( i ) is the set of spans in the gold cluster containing span i .,0
17934,"If span i does not belong to a gold cluster or all gold antecedents have been pruned , GOLD ( i ) = {?}.",0
17935,"By optimizing this objective , the model naturally learns to prune spans accurately .",0
17936,"While the initial pruning is completely random , only gold mentions receive positive updates .",0
17937,"The model can quickly leverage this learning signal for appropriate credit assignment to the different factors , such as the mention scores s m used for pruning .",0
17938,Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over all model with respect to mention detection .,0
17939,It also prevents the span pruning from introducing noise .,0
17940,"For example , consider the case where span i has a single gold antecedent that was pruned , so GOLD ( i ) = {?}.",0
17941,"The learning objective will only correctly push the scores of non-gold antecedents lower , and it can not incorrectly push the score of the dummy antecedent higher .",0
17942,"This learning objective can be considered a span - level , cost - insensitive analog of the learning objective proposed by .",0
17943,"We experimented with these cost-sensitive alternatives ,",0
17944,"including margin - based variants , but a simple maximum - likelihood objective proved to be most effective .",0
17945,Experiments,0
17946,We use the English coreference resolution data from the CoNLL - 2012 shared task in our experiments .,0
17947,"This dataset contains 2802 training documents , 343 development documents , and 348 test documents .",0
17948,The training documents contain on average 454 words and a maximum of 4009 words .,0
17949,Hyperparameters,1
17950,Word representations,0
17951,"The word embeddings area fixed concatenation of 300 - dimensional GloVe embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .",1
17952,Outof - vocabulary words are represented by a vector of zeros .,1
17953,"In the character CNN , characters are represented as learned 8 - dimensional embeddings .",1
17954,"The convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .",1
17955,Hidden dimensions,0
17956,The hidden states in the LSTMs have 200 dimensions .,1
17957,Each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .,1
17958,Feature encoding,0
17959,We encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .,0
17960,"Following , the distance features are binned into the following buckets .",0
17961,"All features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .",0
17962,Pruning,0
17963,"We prune the spans such that the maximum span width L = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents K = 250 .",0
17964,"During training , documents are randomly truncated to up to 50 sentences .",0
17965,Learning,0
17966,We use ADAM for learning with a minibatch size of 1 .,1
17967,The LSTM weights are initialized with random orthonormal matrices as described in .,1
17968,We apply 0.5 dropout to the word embeddings and character CNN outputs .,1
17969,We apply 0.2 dropout to all hidden layers and feature embeddings .,1
17970,Dropout masks are shared across timesteps to preserve long - distance information as described in .,1
17971,The learning rate is decayed by 0.1 % every 100 steps .,1
17972,"The model is trained for up to 150 epochs , with early stopping based on the development set .",1
17973,All code is implemented in Tensor - Flow and is publicly available .,1
17974,3,0
17975,Ensembling,0
17976,We also report ensemble experiments using five models trained with different random initializations .,0
17977,Ensembling is performed for both the span pruning and antecedent decisions .,0
17978,"At test time , we first average the mention scores s m ( i ) over each model before pruning the spans .",0
17979,"Given the same pruned spans , each model then computes the antecedent scores s a ( i , j ) separately , and they are averaged to produce the final scores .",0
17980,Results,1
17981,"We report the precision , recall , and F1 for the standard MUC , B 3 , and CEAF ?",0
17982,4 metrics using the official CoNLL - 2012 evaluation scripts .,0
17983,The main evaluation is the average F1 of the three metrics .,0
17984,Coreference Results,1
17985,Table 1 compares our model to several previous systems that have driven substantial improvements over the past several years on the OntoNotes benchmark .,0
17986,We outperform previous systems in all metrics .,1
17987,"In particular , our single model improves the state - of - the - art average F1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .",1
17988,"The most significant gains come from improvements in recall , which is likely due to our end - toend setup .",1
17989,"During training , pipelined systems typically discard any mentions that the mention detector misses , which for consists of more than 9 % of the labeled mentions in the training data .",0
17990,"In contrast , we only discard mentions that exceed our maximum mention width of 10 , which accounts for less than 2 % of the training mentions .",0
17991,The contribution of joint mention scoring is further discussed in Section 8.3,0
17992,Ablations,0
17993,"To show the importance of each component in our proposed model , we ablate various parts of the architecture and report the average F1 on the development set of the data ( see ) .",0
17994,Features,0
17995,"The distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .",1
17996,They contribute 3.8 F1 to the final result .,1
17997,Word representations,0
17998,"Since our word embeddings are fixed , having access to a variety of word embeddings allows for a more expressive model without overfitting .",0
17999,We hypothesis that the different learning objectives of the GloVe and Turian embeddings provide orthogonal information ( the former is word - order insensitive while the latter is word - order sensitive ) .,0
18000,Both embeddings contribute to some improvement in development F1 .,0
18001,The character CNN provides morphological information and away to backoff for out - ofvocabulary words .,0
18002,"Since coreference decisions often involve rare named entities , we see a contribution of 0.9 F1 from character - level modeling .",1
18003,Metadata Speaker and genre indicators many not be available in downstream applications .,0
18004,"We show that performance degrades by 1.4 F1 without them , but is still on par with previous state - of - theart systems that assume access to this metadata .",0
18005,Head - finding attention,0
18006,Ablations also show a 1.3 F1 degradation in performance without the attention mechanism for finding task - specific heads .,1
18007,"As we will see in Section 9.4 , the attention mechanism should not be viewed as simply an approximation of syntactic heads .",0
18008,"In many cases , it is beneficial to pay attention to multiple words thatare useful specifically for coreference but are not traditionally considered to be syntactic heads .",0
18009,Avg. F1 ?,0
18010,Our model ( joint mention scoring ) 67.7 w/ rule - based mentions 66.7 - 1.0 w/ oracle mentions 85.2 + 17.5,0
18011,Comparing Span Pruning Strategies,0
18012,"To tease apart the contributions of improved mention scoring and improved coreference decisions , we compare the results of our model with alternate span pruning strategies .",0
18013,"In these experiments , we use the alternate spans for both training and evaluation .",0
18014,"As shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( Raghunathan et al. , 2010 ) degrades performance by 1 F1 .",1
18015,"We also provide oracle experiment results , where we keep exactly the mentions thatare present in gold coreference clusters .",0
18016,"With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .",1
18017,Analysis,0
18018,"To highlight the strengths and weaknesses of our model , we provide both quantitative and qualitative analyses .",0
18019,"In the following discussion , we use predictions from the single model rather than the ensembled model .",0
18020,Mention Recall,0
18021,"The training data only provides a weak signal for spans that correspond to entity mentions , since singleton clusters are not explicitly labeled .",0
18022,"As a byproduct of optimizing marginal likelihood , our model automatically learns a useful ranking of spans via the unary mention scores from Section 4 .",0
18023,"The top spans , according to the mention scores , cover a large portion of the mentions in gold clusters , as shown in .",0
18024,"Given a similar number of spans kept , our recall is comparable to the rulebased mention detector ( Raghunathan et al. , 2010 ) that produces 0.26 spans per word with a recall of 89.2 % .",0
18025,"As we increase the number of spans per word (? in Section 5 ) , we observe higher recall but with diminishing returns .",0
18026,"In our experiments , keeping 0.4 spans per word results in 92.7 % recall in the development data .",0
18027,Mention Precision,0
18028,"While the training data does not offer a direct measure of mention precision , we can use the gold syntactic structures provided in the data as a proxy .",0
18029,Spans with high mention scores should correspond to syntactic constituents .,0
18030,"In , we show the precision of topscoring spans when keeping 0.4 spans per word .",0
18031,"For spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .",0
18032,"Longer spans , which are all relatively rare , prove more difficult for the model , and precision drops to 46 % for spans with 10 words .",0
18033,Head Agreement,0
18034,We also investigate how well the learned head preferences correlate with syntactic heads .,0
18035,"For each of the top - scoring spans in the development data that correspond to gold constituents , we compute the word with the highest attention weight .",0
18036,We plot in the proportion of these words that match syntactic heads .,0
18037,"Agreement ranges between 68 - 93 % , which is surprisingly 1 ( A fire in a Bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .",0
18038,Most of the deceased were killed in the crush as workers tried to flee ( the blaze ) in the four - story building .,0
18039,A fire in ( a Bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .,0
18040,Most of the deceased were killed in the crush as workers tried to flee the blaze in ( the four - story building ) .,0
18041,2,0
18042,We are looking for ( a region of central Italy bordering the Adriatic Sea ) .,0
18043,"( The area ) is mostly mountainous and includes Mt. Corno , the highest peak of the Apennines .",0
18044,"( It ) also includes a lot of sheep , good clean - living , healthy sheep , and an Italian entrepreneur has an idea about how to make a little money of them .",0
18045,3,0
18046,( The flight attendants ) have until 6:00 today to ratify labor concessions .,0
18047,( The pilots ' ) union and ground crew did so yesterday .,0
18048,"Also such location devices , ( some ships ) have smoke floats ( they ) can toss out so the man overboard will be able to use smoke signals as away of trying to , let the rescuer locate ( them ) .",0
18049,"high , since no explicit supervision of syntactic heads is provided .",0
18050,The model simply learns from the clustering data that these head words are useful for making coreference decisions .,0
18051,Qualitative Analysis,0
18052,Our qualitative analysis in highlights the strengths and weaknesses of our model .,0
18053,Each row is a visualization of a single coreference cluster predicted by the model .,0
18054,"Bolded spans in parentheses belong to the predicted cluster , and the redness of a word indicates its weight from the headfinding attention mechanism ( a i ,t in Section 4 ) .",0
18055,Strengths,0
18056,The effectiveness of the attention mechanism for making coreference decisions can be seen in Example 1 .,0
18057,"The model pays attention to fire in the span A fire in a Bangladeshi garment factory , allowing it to successfully predict the coreference link with the blaze .",0
18058,"For a subspan of that mention , a Bangladeshi garment factory , the model pays most attention instead to factory , allowing it successfully predict the coreference link with the four - story building .",0
18059,The task - specific nature of the attention mechanism is also illustrated in Example 4 .,0
18060,"The model generally pays attention to coordinators more than the content of the coordination , since coordinators , such as and , provide strong cues for plurality .",0
18061,"The model is capable of detecting relatively long and complex noun phrases , such as a region of central Italy bordering the Adriatic Sea in Example 2 .",0
18062,"It also appropriately pays atten - tion to region , showing that the attention mechanism provides more than content - word classification .",0
18063,The context encoding provided by the bidirectional LSTMs is critical to making informative headword decisions .,0
18064,Weaknesses,0
18065,"A benefit of using neural models for coreference resolution is their ability to use word embeddings to capture similarity between words , a property that many traditional featurebased models lack .",0
18066,"While this can dramatically increase recall , as demonstrated in Example 1 , it is also prone to predicting false positive links when the model conflates paraphrasing with relatedness or similarity .",0
18067,"In Example 3 , the model mistakenly predicts a link between",0
18068,The flight attendants and The pilots '.,0
18069,"The predicted head words attendants and pilots likely have nearby word embeddings , which is a signal used - and often overused - by the model .",0
18070,"The same type of error is made in Example 4 , where the model predicts a coreference link between Prince Charles and his new wife Camilla and Charles and Diana , two noncoreferent mentions thatare similar in many ways .",0
18071,"These mistakes suggest substantial room for improvement with word or span representations that can cleanly distinguish between equivalence , entailment , and alternation .",0
18072,"Unsurprisingly , our model does little in the uphill battle of making coreference decisions requiring world knowledge .",0
18073,"In Example 5 , the model incorrectly decides that them ( in the context of let the rescuer locate them ) is coreferent with some ships , likely due to plurality cues .",0
18074,"However , an ideal model that uses common - sense reasoning would instead correctly infer that a rescuer is more likely to look for the man overboard rather than the ship from which he fell .",0
18075,This type of reasoning would require either ( 1 ) models that integrate external sources of knowledge with more complex inference or ( 2 ) a vastly larger corpus of training data to overcome the sparsity of these patterns .,0
18076,Conclusion,0
18077,We presented a state - of - the - art coreference resolution model that is trained end - to - end for the first time .,0
18078,Our final model ensemble improves performance on the OntoNotes benchmark by over 3 F1 without external preprocessing tools used by previous systems .,0
18079,We showed that our model implicitly learns to generate useful mention candidates from the space of all possible spans .,0
18080,"A novel head - finding attention mechanism also learns a taskspecific preference for head words , which we empirically showed correlate strongly with traditional head - word definitions .",0
18081,"While our model substantially pushes the stateof - the - art performance , the improvements are potentially complementary to a large body of work on various strategies to improve coreference resolution , including entity - level inference and incorporating world knowledge , which are important avenues for future work .",0
18082,title,0
18083,Coreference Resolution with Entity Equalization,1
18084,abstract,0
18085,"A key challenge in coreference resolution is to capture properties of entity clusters , and use those in the resolution process .",0
18086,"Here we provide a simple and effective approach for achieving this , via an "" Entity Equalization "" mechanism .",0
18087,The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster .,0
18088,"We show how this can be done in a fully differentiable end - to - end manner , thus enabling high - order inferences in the resolution process .",0
18089,"Our approach , which also employs BERT embeddings , results in new stateof - the - art results on the CoNLL - 2012 coreference resolution task , improving average F1 by 3.6 % .",0
18090,1,0
18091,Introduction,0
18092,Coreference resolution is the task of grouping mentions into entities .,0
18093,A key challenge in this task is that information about an entity is spread across multiple mentions .,0
18094,"Thus , deciding whether to assign a given mention to a candidate entity could require entity - level information that needs to be aggregated from all mentions .",0
18095,Most coreference resolution systems rely on pairwise scoring of entity mentions .,0
18096,As such they are prone to missing global entity information .,0
18097,"The problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .",0
18098,"Specifically , tackled this problem by iteratively averaging the antecedents of each mention to create mention representations thatare more "" global "" ( i.e. , reflect information about the entity to which the mention refers ) .",0
18099,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .",1
18100,"Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .",1
18101,"It is not immediately obvious how to perform this equalization , which relies on the entity - to- mention mapping , but we provide a natural smoothed representation of this mapping , and demonstrate how to use it for equalization .",0
18102,"Now that each mention contains information about all its corresponding entities , we can use a standard pairwise scoring model , and this model will be able to use global entity - level information .",0
18103,"Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .",1
18104,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .",1
18105,It is challenging to apply BERT to the coreference resolution setting because BERT is limited to a fixed sequence length which is shorter than most coreference resolution documents .,0
18106,We show that this can be done by using BERT in a fully convolutional manner .,1
18107,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .",0
18108,"In summary , our contributions are : a.",0
18109,A simple and intuitive approach for entity - level representation via the notion of Entity - Equalization .,0
18110,b.,0
18111,The first use of BERT embeddings in coreferenceresolution .,0
18112,"c. New state - of - the - art performance on the CoNLL - 2012 coreference resolution task , improving over previous F1 performance by 3.6 % .",0
18113,Background,0
18114,"Following , we cast the coreference resolution task as finding a set of antecedent assignments y i for each span i in the document .",0
18115,"The set of possible values for each y i is Y ( i ) = { , 1 , . . . , i ? 1 } , a dummy antecedent and all preceding spans .",0
18116,"Non-dummy antecedents represent coreference links between i and y i , whereas indicates that the span is either not a mention , or is a first mention in a newly formed cluster .",0
18117,"Whenever a new cluster is formed it receives a new index , and every mention with y i = receives the index of its antecedents .",0
18118,Thus the process results in clusters of coreferent entities .,0
18119,Baseline Model,0
18120,We briefly describe the baseline model ) which we will later augment with Entity - Equalization and BERT features .,0
18121,"Let s ( i , j ) denote a pairwise score between two spans i and j.",0
18122,"Next , for each span i define the distribution P ( y i ) over antecedents :",0
18123,The score is a function of the span representations defined as follows .,0
18124,For each span i let g i ?,0
18125,Rd denote its corresponding representation vector ( see for more details about the model architecture ) .,0
18126,"computes the antecedent score s ( i , j ) = f s ( g i , g j ) as a pairwise function of the span representations , i.e. not directly incorporating any information about the entities to which they might belong .",0
18127,"improved upon this model by "" refining "" the span representations as follows .",0
18128,The expected antecedent representation a i of each span i is computed by using the current antecedent distribution P ( y i ) as an attention mechanism :,0
18129,The current span representation g i is then updated via interpolation with its expected antecedent representation a i :,0
18130,"where f i = ff ( g i , a i ) is a learned gate vector .",0
18131,"Thus , the refined representation g i is an elementwise weighted average of the current span representation and its direct antecedents .",0
18132,Using this representation the refined antecedent distribution can be calculated as follows :,0
18133,Entity Equalization,0
18134,"The idea behind the refinement procedure in was to create features thatare closer to cluster - level representations and hence more "" global "" .",0
18135,This was partially achieved by considering not only the current span but also its antecedents .,0
18136,We would like take this idea one step further and create refined span representations that contain information about the entire cluster to which it belongs .,0
18137,One way to achieve this is by simply representing each mention via the sum of the mentions currently in its coreference cluster .,0
18138,"Formally , let C ( i ) be a coreference cluster ( as defined by the antecedent distribution P ( y i ) ) such that i ?",0
18139,"C ( i ) , and replace Equation 1 with :",0
18140,"As a result each span will now contain information about all of its current coreference cluster , effectively equalizing the representations of different spans belonging to the same cluster .",0
18141,"However , note that it is not clear how to train such a procedure end - to - end because the clustering process is not differentiable .",0
18142,"To overcome this problem , we use a differentiable relaxation of the clustering process and use the resulting soft clustering matrix to create a fully differentiable cluster representation .",0
18143,We call this refinement procedure Entity Equalization and provide a detailed description in the next section .,0
18144,"To illustrate the difference between Entity Equalization and antecedent averaging , consider the following example : "" [ John ] went to the park and [ he ] got tired .",0
18145,"[ John ] decided to go back home . """,0
18146,Now assume that the model outputs the following antecedent distribution P ( y i ) :,0
18147,"John 1 he John 2 John 1 1 0 0 he 1 0 0 John 2 1 0 0 there is only one coreference cluster induced by this antecedent matrix , C = { John 1 , he , John 2 }. A cluster representation for John 2 would be the sum of the representations of all three mentions .",0
18148,"However , using antecedent averaging , the representation of John 2 will be a weighted average of the representations of John 2 and John 1 , because only John 1 is an antecedent of John 2 .",0
18149,Implementing Equalization,0
18150,"In order to achieve differentiable cluster representations , we need a differentiable soft - clustering process .",0
18151,"introduced such a relaxation given an antecedent distribution , based on the following observation : in a document containing m mentions there are m potential entities E 1 , ... , E m where E i has mention i as the first mention .",0
18152,Let Q (i ?,0
18153,"E j ) be the probability that mention i corresponds to entity E j ( that is , to the entity that has j as its first mention ) .",0
18154,showed that this probability can be computed recursively based on the antecedent distribution P ( y i ) as follows :,0
18155,Note that this is a fully differentiable procedure that calculates the clustering distribution for each entity i .,0
18156,The distribution Q ( i ?,0
18157,"E j ) above leads to a simple differentiable implementation of the equalization operation in ( 3 ) , as described next .",0
18158,"In order to use entity representations for equalizing mention representations , we need a representation for each entity E i at each time step t , so we wo n't represent a mention using mentions not yet encountered .",0
18159,We denote it by :,0
18160,"Finally , an entity representation for each mention i is calculated using the entity distribution of mention i and the global entity representations :",0
18161,It can be seen that the above a i will indeed lead to ( 3 ) when the distributions P ( y ) are deterministic .,0
18162,Using BERT,0
18163,Embeddings,0
18164,Our coreference model relies on input representations for each input token .,0
18165,used the ELMo context - dependent embeddings for this purpose .,0
18166,"Here we propose to use the more recent BERT embeddings instead , which have achieved state of the art performance on many natural language processing tasks .",0
18167,BERT is a bidirectional contextual language model based on the Transformer architecture .,0
18168,Using BERT for coreference resolution is not trivial : BERT can only run on sequences of fixed length which is determined in the pretraining process .,0
18169,"In the pre-trained model published by , this limitation is 512 tokens , which is shorter than many of the documents in the CoNLL - 2012 coreference resolution task .",0
18170,"Even without considering the pre-training limitation , because the attention mechanism grows as the square of the sequence length , and because of the large number of parameters of the BERT model , running it on very large sequences is not feasible on most machines due to memory constraints .",0
18171,"In order to obtain BERT embeddings for sequences of unlimited length , we propose to use BERT in a convolutional mode as follows .",0
18172,Let D be a fixed window length .,0
18173,We obtain a representation for token i by applying BERT to the sequence of tokens from D to the left of i to D to the right of i .,0
18174,"We then take the four last layers of the BERT model for token i and apply a learnable weighted averaging to those , similar to the process used in ELMo .",0
18175,"The output of the network is taken as the representation of token i , and replaced the ELMo representation in the model of Section 3.1 .",0
18176,"We use D = 64 , since using the maximum size of D = 256 is computationally intensive , and good results are already obtained with 64 .",0
18177,2,0
18178,Related Work,0
18179,Several works have addressed the issue of entitylevel representation .,0
18180,In an RNN is used to model each entity .,0
18181,"While this allows complex entity representations , the assignment of a mention to an RNN is a hard decision , and as such can not be optimized in an end - to - end manner .",0
18182,use whole - entity representations as obtained from agglomerative clustering .,0
18183,"But again the clustering operation in non-differentiable , requiring the use of imitation learning .",0
18184,"In , entity refinement is more restricted , as it is only obtained from the attention vector at each step .",0
18185,"Thus , we believe that our model is the first to use entity - level representations that correspond directly to the inferred clusters , and are end - to - end differentiable .",0
18186,Mention - entity mappings have been used in the context of optimizing coreference performance measures .,0
18187,Here we show that these mappings can also be used for the resolution model itself .,0
18188,"We note that we did not try to optimize for coreference measures as in , and this is likely to further improve results .",0
18189,Experiments,0
18190,Data for all our experiments is taken from the English portion of the CoNLL - 2012 coreference resolution tasks .,0
18191,"Our experimental setup is very similar to , and our code is built on theirs .",0
18192,We did not change the optimizer or any of the training hyperparameters .,0
18193,The following changes were made to the model :,0
18194,We used BERT word embeddings instead of ELMo as input to the LSTM ( see Section 4 ) .,0
18195,We replaced the span representation refinement mechanism with our Entity Equalization approach ( see Section 3 ) .,0
18196,Results,1
18197,"Following Pradhan et al. , we report precision , recall and F1 of the MUC , B 3 and CEAF ? 4 metrics , and average the F 1 score of all three metrics to get the main evaluation metric used in the CoNLL - 2012 coreference resolution task .",0
18198,We calculated the metrics using the official evaluation scripts of CoNLL - 2012 .,0
18199,Results on the test set are shown in .,0
18200,"Our baseline is the span - ranking model from with ELMo input features and second - order span representations , which achieves 73.0 % Avg.",1
18201,F1 . Replacing the ELMo features with BERT features achieves 76. 25 % average F1 .,1
18202,"Removing the second - order span - representations while using BERT features achieves 76.37 % F1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .",1
18203,"Replacing secondorder span representations with Entity Equalization achieves 76. 64 % average F1 , while also consistently achieving the highest F 1 score on all three evaluation metrics .",1
18204,"Our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average F1 .",1
18205,Conclusion,0
18206,In this work we presented a new state - of - the - art coreference resolution system .,0
18207,Key to our approach is the idea that each mention should contain information about all its coreferring mentions .,0
18208,Here we implemented this idea by summing all mention representations within a cluster .,0
18209,In the future we plan to further enrich these representations by considering information from across the document .,0
18210,"Furthermore , we can consider more structured representations of entities that reflect entity attributes and inter-entity relations .",0
18211,title,0
18212,Deep Reinforcement Learning for Mention - Ranking Coreference Models,0
18213,abstract,0
18214,Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning .,1
18215,In this paper we instead apply reinforcement learning to directly optimize a neural mention - ranking model for coreference evaluation metrics .,0
18216,We experiment with two approaches : the REINFORCE policy gradient algorithm and a rewardrescaled max - margin objective .,0
18217,"We find the latter to be more effective , resulting in significant improvements over the current state - of the - art on the English and Chinese portions of the CoNLL 2012 Shared Task .",0
18218,1 Code and trained models are available at https,0
18219,Introduction,0
18220,"Coreference resolution systems typically operate by making sequences of local decisions ( e.g. , adding a coreference link between two mentions ) .",0
18221,"However , most measures of coreference resolution performance do not decompose over local decisions , which means the utility of a particular decision is not known until all other decisions have been made .",0
18222,"Due to this difficulty , coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision .",0
18223,These losses contain hyperparameters thatare carefully selected to ensure the model performs well according to coreference evaluation metrics .,0
18224,"This complicates training , especially across different languages and datasets where systems may work best with different settings of the hyperparameters .",0
18225,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .",1
18226,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",1
18227,We also test the REINFORCE policy gradient algorithm .,1
18228,Our model is a neural mention - ranking model .,1
18229,Mention - ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters .,0
18230,Hence they operate in a simple setting where coreference decisions are made independently .,0
18231,"Although they are less expressive than entity - centric approaches to coreference ( e.g. , Haghighi and Klein , 2010 ) , mention - ranking models are fast , scalable , and simple to train , causing them to be the dominant approach to coreference in recent years .",0
18232,Having independent actions is particularly useful when applying reinforcement learning because it means a particular action 's effect on the final reward can be computed efficiently .,0
18233,We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task .,0
18234,The REINFORCE algorithm is competitive with a heuristic loss function while the reward - rescaled objective significantly outperforms both 1 .,0
18235,We attribute this to reward rescaling being well suited for a ranking task due to its max - margin loss as well as benefiting from directly optimizing for coreference metrics .,0
18236,"Error analysis shows that using the reward - rescaling loss results in a similar number of mistakes as the heuristic loss , but the mistakes tend to be less severe .",0
18237,Neural Mention - Ranking Model,0
18238,"We use the neural mention - ranking model described in , which we briefly go over in this section .",0
18239,"Given a mention m and candidate antecedent c , the mention - ranking model produces a score for the pair s ( c , m ) indicating their compatibility for coreference with a feedforward neural network .",0
18240,"The candidate antecedent maybe any mention that occurs before min the document or NA , indicating that m has no antecedent .",0
18241,Input Layer .,0
18242,"For each mention , the model extracts various words ( e.g. , the mention 's head word ) and groups of words ( e.g. , all words in the mention 's sentence ) thatare fed into the neural network .",0
18243,Each word is represented by a vector w i ?,0
18244,R dw .,0
18245,Each group of words is represented by the average of the vectors of each word in the group .,0
18246,"In addition to the embeddings , a small number of additional features are used , including distance , string matching , and speaker identification features .",0
18247,See for the full set of features and an ablation study .,0
18248,"These features are concatenated to produce an Idimensional vector h 0 , the input to the neural network .",0
18249,"If c = NA , features defined over pairs of mentions are not included .",0
18250,"For this case , we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores .",0
18251,Hidden Layers .,0
18252,The input gets passed through three hidden layers of rectified linear ( ReLU ) units .,0
18253,Each unit in a hidden layer is fully connected to the previous layer :,0
18254,Scoring Layer .,0
18255,The final layer is a fully connected layer of size 1 :,0
18256,where W 4 is a 1 M 3 weight matrix .,0
18257,"At test time , the mention - ranking model links each mention with its highest scoring candidate antecedent .",0
18258,Learning Algorithms,0
18259,Mention - ranking models are typically trained with heuristic loss functions thatare tuned via hyperparameters .,0
18260,"These hyperparameters are usually given as costs for different error types , which are used to bias the coreference system towards making more or fewer coreference links .",0
18261,In this section we first describe a heuristic loss function incorporating this idea from .,0
18262,We then propose new training procedures based on reinforcement learning that instead directly optimize for coreference evaluation metrics .,0
18263,Heuristic Max - Margin Objective,0
18264,"The heuristic loss from Wiseman et al. is governed by the following error types , which were first proposed by .",0
18265,"Suppose the training set consists of N mentions m 1 , m 2 , ... , m N . Let C ( m i ) denote the set of candidate antecedents of a mention mi ( i.e. , mentions preceding mi and NA ) and T ( m i ) denote the set of true antecedents of mi ( i.e. , mentions preceding mi thatare coreferent with it or { NA } if mi has no antecedent ) .",0
18266,Then we define the following costs for linking mi to a candidate antecedent c ?,0
18267,C ( m i ) :,0
18268,"for "" false new , "" "" false anaphor , "" "" wrong link "" , and correct coreference decisions .",0
18269,The heuristic loss is a slack - rescaled max - margin objective parameterized by these error costs .,0
18270,Lett i be the highest scoring true antecedent of mi :,0
18271,Then the heuristic loss is given as,0
18272,Finding Effective Error Penalties .,0
18273,We fix ?,0
18274,WL = 1.0 and search for ?,0
18275,FA and ?,0
18276,"FN out of { 0.1 , 0.2 , ... , 1.5 } with a variant of grid search .",0
18277,Each new trial uses the unexplored set of hyperparame - ters that has the closest Manhattan distance to the best setting found so far on the dev set .,0
18278,The search is halted when all immediate neighbors ( within 0.1 distance ) of the best setting have been explored .,0
18279,"We found (? FN , ? FA , ? WL ) = ( 0.8 , 0.4 , 1.0 ) to be best for English and (? FN , ? FA , ? WL ) = ( 0.8 , 0.5 , 1.0 ) to be best for Chinese on the CoNLL 2012 data .",0
18280,Reinforcement Learning,0
18281,"Finding the best hyperparameter settings for the heuristic loss requires training many variants of the model , and at best results in an objective that is correlated with coreference evaluation metrics .",0
18282,"To address this , we pose mention ranking in the reinforcement learning framework and propose methods that directly optimize the model for coreference metrics .",0
18283,"We can view the mention - ranking model as an agent taking a series of actions a 1:T = a 1 , a 2 , ... , a T , where T is the number of mentions in the current document .",0
18284,Each action a i links the ith mention in the document mi to a candidate antecedent .,0
18285,"Formally , we denote the set of actions available for the ith mention as A i = { ( c , mi ) : c ?",0
18286,"C ( m i ) } , where an action ( c , m ) add s a coreference link between mentions c and m .",0
18287,"The mentionranking model assigns each action the score s ( c , m ) and takes the highest - scoring action at each step .",0
18288,"Once the agent has executed a sequence of actions , it observes a reward R ( a 1:T ) , which can be any function .",0
18289,"We use the B 3 coreference metric for this reward ( Bagga and Baldwin , 1998 ) .",0
18290,Although our system evaluation also includes the MUC and CEAF ?,0
18291,"4 ) metrics , we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF ?",0
18292,4 is slow to compute .,0
18293,Reward Rescaling .,0
18294,"Crucially , the actions taken by a mention - ranking model are independent .",0
18295,This means it is possible to change any action a i to a different one a i ?,0
18296,"A i and see what reward the model would have gotten by taking that action instead : R ( a 1 , ... , a i?1 , a i , a i + 1 , ... , a T ) .",0
18297,We use this idea to improve the slack - rescaling parameter ? in the maxmargin loss L ( ? ) .,0
18298,"Instead of setting its value based on the error type , we compute exactly how much each action hurts the final reward :",0
18299,where a 1:T is the highest scoring sequence of actions according to the model 's current parameters .,0
18300,Otherwise the model is trained in the same way as with the heuristic loss .,0
18301,The REINFORCE Algorithm .,0
18302,We also explore using the REINFORCE policy gradient algorithm .,0
18303,We can define a probability distribution over actions using the mention - ranking model 's scoring function as follows :,0
18304,It does this through gradient ascent .,0
18305,"Computing the full gradient is prohibitive because of the expectation over all possible action sequences , which is exponential in the length of the sequence .",0
18306,"Instead , it gets an unbiased estimate of the gradient by sampling a sequence of actions a 1:T according top ?",0
18307,and computing the gradient only over the sample .,0
18308,"We take advantage of the independence of actions by using the following gradient estimate , which has lower variance than the standard REINFORCE gradient estimate :",0
18309,"where bi is a baseline used to reduce the variance , which we set to E a i ?",0
18310,A,0
18311,"i ?p ? R ( a 1 , ... , a i , ... , a T ) .",0
18312,Experiments and Results,0
18313,"We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data and evaluate with the MUC , B 3 , and CEAF ?",0
18314,4 metrics .,0
18315,Our experiments were run using predicted mentions from Stanford 's rule - based coreference system .,0
18316,"We follow the training methodology from Clark and Manning , dropout with a rate of 0.5 , and pretraining with the all pairs classification and top pairs classification tasks .",0
18317,"However , we improve on the previous system by using using better mention detection , more effective hyperparameters , and more epochs of training .",0
18318,Results,1
18319,"We compare the heuristic loss , REINFORCE , and reward rescaling approaches on both datasets .",0
18320,"We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .",1
18321,We attribute the modest improvement of REIN - FORCE to it being poorly suited for a ranking task .,0
18322,"During training it optimizes the model 's performance in expectation , but at test - time it takes the most probable sequence of actions .",0
18323,"This mismatch occurs even at the level of an individual decision : the model only links the current mention to a single antecedent , but is trained to assign high probability to all correct antecedents .",0
18324,"We believe the benefit of REINFORCE being guided by coreference evaluation metrics is offset by this dis advantage , which does not occur in the max-margin approaches .",0
18325,"The reward - rescaled max - margin loss combines the best of both worlds , resulting in superior performance .",1
18326,The Benefits of Reinforcement Learning,0
18327,In this section we examine the reward - based cost function ?,0
18328,rand perform error analysis to determine how reward rescaling improves the mention - ranking model 's accuracy .,0
18329,Comparison with Heuristic Costs .,0
18330,We compare the reward - based cost function ?,0
18331,r with the error types used in the heuristic loss .,0
18332,"For English , the average value of ?",0
18333,r is 0.79 for FN errors and 0.38 for FA errors when the costs are scaled so the average value of a WL error is 1.0 .,0
18334,"These are very close to the hyperparameter values (? FN , ? FA , ? WL ) = ( 0.8 , 0.4 , 1.0 ) found by grid search .",0
18335,"However , there is a high variance in costs for each error type , suggesting that using a fixed penalty for each type as in the heuristic loss is insufficient ( see ) .",0
18336,Avoiding Costly Mistakes .,0
18337,"Embedding the costs of actions into the loss function causes the rewardrescaling model to prioritize getting the more important coreference decisions ( i.e. , the ones with the biggest impact on the final score ) correct .",0
18338,"As a result , it makes fewer costly mistakes at test time .",0
18339,Costly mistakes often involve large clusters of mentions : incorrectly combining two coreference clusters of size ten is much worse than incorrectly combining two clusters of size one .,0
18340,"However , the cost of an action also depends on other factors such as the number of errors already present in the clusters and the utilities of the other available actions .",0
18341,shows the breakdown of errors made by the heuristic and reward - rescaling models on the test set .,0
18342,"The reward - rescaling model makes slightly more errors , meaning its improvement in performance must come from its errors being less severe .",0
18343,Example Improvements .,0
18344,shows two classes of mentions where the reward - rescaling loss particularly improves over the heuristic loss .,0
18345,"Proper nouns have a higher average cost for "" false new "" errors ( 0.90 ) than other mentions types ( 0.77 ) .",0
18346,"This is perhaps because proper nouns are important for connecting clusters of mentions far apart in a document , so incorrectly linking a proper noun to NA could result in a large decrease in recall .",0
18347,"Because it more heavily weights these high - cost errors during training , the reward - rescaling model makes fewer "" false new "" errors for proper nouns than the heuristic loss .",0
18348,"Although there is an increase in other kinds of errors as a result , most of these are low - cost "" false anaphoric "" errors .",0
18349,"The pronouns in the "" telephone conversation "" genre often group into extremely large coreference clusters , which means a "" wrong link "" error can have a large negative effect on the score .",0
18350,This is reflected in its high average cost of 1.21 .,0
18351,"After prioritizing these examples during training , the reward - rescaling model creates significantly fewer wrong links than the heuristic loss , which is trained using a fixed cost of 1.0 for all wrong links .",0
18352,Related Work,0
18353,Mention - ranking models have been widely used for coreference resolution .,0
18354,"These models are typically trained with heuristic loss functions that assign costs to different error types , as in the heuristic loss we describe in Section 3.1 .",0
18355,To the best of our knowledge reinforcement learning has not been applied to coreference resolution before .,0
18356,"However , imitation learning algorithms such as SEARN ) have been used to train coreference resolvers .",0
18357,"These algorithms also directly optimize for coreference evaluation metrics , but they require an expert policy to learn from instead of relying on rewards alone .",0
18358,Conclusion,0
18359,"We propose using reinforcement learning to directly optimize mention - ranking models for coreference evaluation metrics , obviating the need for hyperparameters that must be carefully selected for each particular language , dataset , and evaluation metric .",0
18360,"Our reward - rescaling approach also increases the model 's accuracy , resulting in significant gains over the current state - of - the - art .",0
18361,title,0
18362,Learning Global Features for Coreference Resolution,1
18363,abstract,0
18364,There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters .,1
18365,"Yet , state - of - the - art performance can be achieved with systems treating each mention prediction independently , which we attribute to the inherent difficulty of crafting informative clusterlevel features .",0
18366,"We instead propose to use recurrent neural networks ( RNNs ) to learn latent , global representations of entity clusters directly from their mentions .",0
18367,"We show that such representations are especially useful for the prediction of pronominal mentions , and can be incorporated into an end - to - end coreference system that outperforms the state of the art without requiring any additional search .",0
18368,: um and [ I ] 1 think that is what 's - Go ahead [ Linda ] 2 .,0
18369,LW : Welland uh thanks goes to [ you ] 1 and to [ the media ] 3 to help [ us ] 4 ... So [ our ] 4 hat is off to all of [ you ] 5 as well .,0
18370,Introduction,0
18371,"While structured , non-local coreference models would seem to hold promise for avoiding many common coreference errors ( as discussed further in Section 3 ) , the results of employing such models in practice are decidedly mixed , and state - of - the - art results can be obtained using a completely local , mention - ranking system .",0
18372,"In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .",1
18373,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",1
18374,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .",1
18375,We incorporate these representations into a mention - ranking style coreference system .,1
18376,"The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .",1
18377,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .",1
18378,"As such , unlike several recent approaches , which may require complicated inference during training , we are able to train our model in much the same way as a vanilla mentionranking model .",0
18379,Experiments compare the use of learned global features to several strong baseline systems for coreference resolution .,0
18380,"We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions , which remain a persistent source of errors for modern coreference systems .",0
18381,"Our final system improves over 0.8 points in CoNLL score over the current state of the art , and the improvement is statistically significant on all three CoNLL metrics .",0
18382,Background and Notation,0
18383,Coreference resolution is fundamentally a clustering task .,0
18384,"Given a sequence ( x n ) N n=1 of ( intra-document ) mentions - that is , syntactic units that can refer or be referred to - coreference resolution involves partitioning ( x n ) into a sequence of clusters ( X ( m ) )",0
18385,M m= 1 such that all the mentions in any particular cluster X ( m ) refer to the same underlying entity .,0
18386,"Since the mentions within a particular cluster maybe ordered linearly by their appearance in the document , 1 we will use the notation X ( m ) j to refer to the j'th mention in the m'th cluster .",0
18387,"A valid clustering places each mention in exactly one cluster , and so we may represent a clustering with a vector z ?",0
18388,"{ 1 , . . . , M } N , where z n = miff x n is a member of X .",0
18389,Coreference systems attempt to find the best clustering z * ?,0
18390,"Z under some scoring function , with Z the set of valid clusterings .",0
18391,"One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention x n ; because x n may not be anaphoric ( and therefore have no antecedents ) , a "" dummy "" antecedent may also be predicted .",0
18392,"The aforementioned strategy is adopted by "" mention - ranking "" systems , which , formally , predict an antecedent ? ?",0
18393,"Y ( x n ) for each mention x n , where Y ( x n ) = { 1 , . . . , n ? 1 , }.",0
18394,"Through transitivity , these decisions induce a clustering over the document .",0
18395,"Mention - ranking systems make their antecedent predictions with a local scoring function f ( x n , y) defined for any mention x n and any antecedent y ?",0
18396,Y ( x n ) .,0
18397,"While such a scoring function clearly ignores much structural information , the mentionranking approach has been attractive for at least two reasons .",0
18398,"First , inference is relatively simple and efficient , requiring only a left - to - right pass through a document 's mentions during which a mention 's antecedents ( as well as ) are scored and the highest scoring antecedent is predicted .",0
18399,"Second , from a linguistic modeling perspective , mention - ranking models learn a scoring function that requires a mention x n to be compatible with only one of its coreferent antecedents .",0
18400,"This contrasts with mention - pair models ( e.g. , ) , which score all pairs of mentions in a cluster , as well as with certain cluster - based models ( see discussion in ) .",0
18401,"Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions , which we might like to model as linking to a single nominal or proper antecedent , for example , but not necessarily to all other coreferent mentions .",0
18402,"Accordingly , in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking , while allowing the model to utilize global , structural information relating to z in making its predictions .",0
18403,"We therefore investigate objective functions of the form arg max y 1 , ... ,y N N n= 1 f ( x n , y n ) + g ( x n , y n , z 1:n?1 ) , where g is a global function that , in making predictions for x n , may examine ( features of ) the clustering z 1:n?",0
18404,1 induced by the antecedent predictions made through y n?1 .,0
18405,The Role of Global Features,0
18406,Here we motivate the use of global features for coreference resolution by focusing on the issues that may arise when resolving pronominal mentions in a purely local way .,0
18407,See and for more general motivation for using global models .,0
18408,Pronoun Problems,0
18409,Recent empirical work has shown that the resolution of pronominal mentions accounts for a substantial percentage of the total errors made by modern mention - ranking systems .,0
18410,"show that on the CoNLL 2012 English development set , almost 59 % of mention - ranking precision errors and almost 24 % of recall errors involve pronominal mentions .",0
18411,"Martschat and found a similar pattern in their comparison of mention - ranking , mention - pair , and latent - tree models .",0
18412,"To see why pronouns can be so problematic , consider the following passage from the "" Broadcast Conversation "" portion of the CoNLL development set ( bc / msnbc/0000/018 ) ; below , we enclose mentions in brackets and give the same subscript to coclustered mentions .",0
18413,( This example is also shown in . ),0
18414,"This example is typical of Broadcast Conversation , and it is difficult because local systems learn to myopically link pronouns such as 5 to other instances of the same pronoun thatare close by , such as [ you ] 1 .",0
18415,"While this is often a reasonable strategy , in this case predicting [ you ] 1 to bean antecedent of [ you ] 5 would result in the prediction of an incoherent cluster , since [ you ] 1 is coreferent with the singular [ I ] 1 , and [ you ] 5 , as part of the phrase "" all of you , "" is evidently plural .",0
18416,"Thus , while there is enough information in the text to correctly predict [ you ] 5 , doing so crucially depends on having access to the history of predictions made so far , and it is precisely this access to history that local models lack .",0
18417,"More empirically , there are non-local statistical regularities involving pronouns we might hope models could exploit .",0
18418,"For instance , in the CoNLL training data over 70 % of pleonastic "" it "" instances and over 74 % of pleonastic "" you "" instances follow ( respectively ) previous pleonastic "" it "" and "" you "" instances .",0
18419,"Similarly , over 78 % of referential "" I "" instances and over 68 % of referential "" he "" instances corefer with previous "" I "" and "" he "" instances , respectively .",0
18420,"Accordingly , we might expect non-local models with access to global features to perform significantly better .",0
18421,"However , models incorporating nonlocal features have a rather mixed track record .",0
18422,"For instance , found that cluster - level features improved their results , whereas Martschat and found that they did not .",0
18423,found that incorporating cluster - level features beyond those involving the precomputed mention - pair and mention - ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance .,0
18424,"Furthermore , among recent , state - of - theart systems , mention - ranking systems ( which are completely local ) perform at least as well as their more structured counterparts .",0
18425,Issues with Global Features,0
18426,"We believe a major reason for the relative ineffectiveness of global features in coreference problems is that , as noted by , cluster - level features can be hard to define .",0
18427,"Specif-ically , it is difficult to define discrete , fixed - length features on clusters , which can be of variable size ( or shape ) .",0
18428,"As a result , global coreference features tend to be either too coarse or too sparse .",0
18429,"Thus , early attempts at defining cluster - level features simply applied the coarse quantifier predicates all , none , most to the mention - level features defined on the mentions ( or pairs of mentions ) in a cluster .",0
18430,"For example , a cluster would have the feature ' most - female = true ' if more than half the mentions ( or pairs of mentions ) in the cluster have a ' female = true ' feature .",0
18431,"On the other extreme , define certain cluster - level features by concatenating the mention - level features of a cluster 's constituent mentions in order of the mentions ' appearance in the document .",0
18432,"For example , if a cluster consists , in order , of the mentions ( the president , he , he ) , they would define a cluster - level "" type "" feature ' C - P - P=true ' , which indicates that the cluster is composed , in order , of a common noun , a pronoun , and a pronoun .",0
18433,"While very expressive , these concatenated features are often quite sparse , since clusters encountered during training can be of any size .",0
18434,Learning Global Features,0
18435,"To circumvent the aforementioned issues with defining global features , we propose to learn cluster - level feature representations implicitly , by identifying the state of a ( partial ) cluster with the hidden state of an RNN that has consumed the sequence of mentions composing the ( partial ) cluster .",0
18436,"Before providing technical details , we provide some preliminary evidence that such learned representations capture important contextual information by displaying in the learned final states of all clusters in the CoNLL development set , projected using T - SNE ( van der Maaten and Hinton , 2012 ) .",0
18437,Each point in the visualization represents the learned features for an entity cluster and the head words of mentions are shown for representative points .,0
18438,"Note that the model learns to roughly separate clusters by simple distinctions such as predominant type and number ( it , they , etc ) , but also captures more subtle relationships such as grouping geographic terms and long strings of pronouns .",0
18439,Recurrent Neural Networks,0
18440,A recurrent neural network is a parameterized nonlinear function RNN that recursively maps an input sequence of vectors to a sequence of hidden states .,0
18441,"Let ( m j ) J j=1 be a sequence of J input vectors m j ? RD , and let h 0 = 0 . Applying an RNN to any such sequence yields",0
18442,where ?,0
18443,"is the set of parameters for the model , which are shared overtime .",0
18444,"There are several varieties of RNN , but by far the most commonly used in natural - language processing is the Long Short - Term Memory network ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) , particularly for language modeling ( e.g. , ) and machine translation ( e.g. , Sutskever et al. ) , and we use LSTMs in all experiments .",0
18445,RNNs for Cluster Features,0
18446,Our main contribution will be to utilize RNNs to produce feature representations of entity clusters which will provide the basis of the global term g.,0
18447,Recall that we view a cluster X ( m ) as a sequence of mentions ( X ( m ) j ) J j=1 ( ordered in linear document or - der ) .,0
18448,We therefore propose to embed the state ( s ) of X ( m ) by running an RNN over the cluster in order .,0
18449,In order to run an RNN over the mentions we need an embedding function h c to map a mention to a real vector .,0
18450,"First , following define ? a ( x n ) : X ?",0
18451,"{ 0 , 1 } F as a standard set of local indicator features on a mention , such as its headword , its gender , and soon .",0
18452,( We elaborate on features below . ),0
18453,We then use a non-linear feature embedding h c to map a mention x n to a vector - space representation .,0
18454,"In particular , we define",0
18455,where W c and b care parameters of the embedding .,0
18456,"We will refer to the j'th hidden state of the RNN corresponding to X ( m ) ash ( m ) j , and we obtain it according to the following formula",0
18457,"again assuming that h ( m ) 0 = 0 . Thus , we will effectively run an RNN over each ( sequence of mentions corresponding to a ) cluster X ( m ) in the document , and thereby generate a hidden state h ( m ) j corresponding to each step of each cluster in the document .",0
18458,"Concretely , this can be implemented by maintaining M RNNs - one for each cluster - that all share the parameters ?.",0
18459,The process is illustrated in the top portion of .,0
18460,Coreference with Global Features,0
18461,We now describe how the RNN defined above is used within an end - to - end coreference system .,0
18462,Full Model and Training,0
18463,Recall that our inference objective is to maximize the score of both a local mention ranking term as well as a global term based on the current clusters :,0
18464,"f ( x n , y n ) + g ( x n , y n , z 1:n?1 )",0
18465,"We begin by defining the local model f ( x n , y) with the two layer neural network of , which has a specialization for the nonanaphoric case , as follows :",0
18466,"DA : um and [ I ] , h",0
18467,( 1 ) 2,0
18468,"[ Linda ] , h",0
18469,( 2 ) 1,0
18470,"[ you ] , h . There are currently four entity clusters in scope X ( 1 ) , X ( 2 ) , X ( 3 ) , X ( 4 ) based on unseen previous decisions ( y ) .",0
18471,"Each cluster has a corresponding RNN state , two of which ( h ( 1 ) and h ( 4 ) ) have processed multiple mentions ( with X ( 1 ) notably including a singular mention ) .",0
18472,"At the bottom , we show the complete mention - ranking process .",0
18473,"Each previous mention is considered as an antecedent , and the global term considers the antecedent clusters ' current hidden state .",0
18474,Selecting is treated with a special case NA ( x n ) .,0
18475,"Above , u and v are the parameters of the model , and ha and hp are learned feature embeddings of the local mention context and the pairwise affinity between a mention and an antecedent , respectively .",0
18476,"These feature embeddings are defined similarly to h c , as",0
18477,where ?,0
18478,a ( mentioned above ) and ?,0
18479,"pare "" raw "" ( that is , unconjoined ) features on the context of x n and on the pairwise affinity between mentions x n and antecedent y , respectively .",0
18480,Note that ha and h c use the same raw features ; only their weights differ .,0
18481,We now specify our global scoring function g based on the history of previous decisions .,0
18482,"Define h ( m ) <n as the hidden state of cluster m before a decision is made for x n - that is , h",0
18483,< n is the state of cluster m's RNN after it has consumed all mentions in the cluster preceding x n .,0
18484,We define gas,0
18485,where NA gives a score for assigning based on a non-linear function of all of the current hidden states :,0
18486,See for a diagram .,0
18487,"The intuition behind the first casein g is that in considering whether y is a good antecedent for x n , we add a term to the score that examines how well x n matches with the mentions already in X ( zy ) ; this matching score is expressed via a dot-product .",0
18488,2,0
18489,"In the second case , when predicting that x n is non-anaphoric , we add the NA term to the score , which examines the ( sum of ) the current states h",0
18490,<n of all clusters .,0
18491,"This information is useful both because it allows the non-anaphoric score to incorporate information about potential antecedents , and because the occurrence of certain singleton - clusters often predicts the occurrence of future singleton - clusters , as noted in Section 3 .",0
18492,The whole system is trained end - to - end on coreference using backpropagation .,0
18493,"For a given training document , let z ( o ) be the oracle mapping from mention to cluster , which induces an oracle clustering .",0
18494,"While at training time we do have oracle clusters , we do not have oracle antecedents ( y ) N n= 1 , so following past work we treat the oracle antecedent as latent .",0
18495,"We train with the following slack - rescaled , margin objective :",0
18496,where the latent antecedent y n is defined as,0
18497,"if x n is anaphoric , and is otherwise .",0
18498,"The term ?( x n ,? ) gives different weight to different error types .",0
18499,"We use a ? with 3 different weights (? 1 , ? 2 , ? 3 ) for "" false link "" ( FL ) , "" false new "" ( FN ) , and "" wrong link "" ( WL ) mistakes , which correspond to predicting an antecedent when non-anaphoric , when anaphoric , and the wrong antecedent , respectively .",0
18500,Note that in training we use the oracle clusters z ( o ) .,0
18501,"Since these are known a priori , we can precompute all the hidden states h ( m ) j in a document , which makes training quite simple and efficient .",0
18502,"This approach contrasts in particular with the work of Bjrkelund and Kuhn ( 2014 ) - who also incorporate global information in mention - ranking - in that they train against latent trees , which are not annotated and must be searched for during training .",0
18503,"On the other hand , training on oracle clusters leads to a mismatch between training and test , which can hurt performance .",0
18504,Search,0
18505,"When moving from a strictly local objective to one with global features , the test - time search problem becomes intractable .",0
18506,"The local objective requires O ( n 2 ) time , whereas the full clustering problem is NP - Hard .",0
18507,"Past work with global features has used integer linear programming solvers for exact search , or beam search with ( delayed ) early update training for an approximate solution .",0
18508,"In contrast , we simply use greedy search at test time , which also requires O ( n 2 ) time .",0
18509,"The full algorithm Algorithm 1 Greedy search with global RNNs 1 : procedure GREEDYCLUSTER ( x1 , . . . , xN ) 2 :",0
18510,"Initialize clusters X ( 1 ) . . . as empty lists , hidden states h ( 0 ) , . . . as 0 vectors in RD , z as map from mention to cluster , and cluster counter M ?",0
18511,0 3 :,0
18512,for n = 2 . . .,0
18513,"N do 4 : y * ? arg max f ( xn , y ) + g ( xn , y , z 1:n? 1 )",0
18514,5 :,0
18515,m ? zy * 6:,0
18516,if y * = then 7:,0
18517,append xn to X ( m ) 10 :,0
18518,zn ? m 11:,0
18519,is shown in Algorithm,0
18520,"1 . The greedy search algorithm is identical to a simple mention - ranking system , with the exception of line 11 , which updates the current RNN representation based on the previous decision that was made , and line 4 , which then uses this cluster representation as part of scoring .",0
18521,Experiments,0
18522,Methods,0
18523,We run experiments on the CoNLL 2012 English shared task .,0
18524,"The task uses the OntoNotes corpus , consisting of 3,493 documents in various domains and formats .",0
18525,We use the experimental split provided in the shared task .,0
18526,"For all experiments , we use the Berkeley Coreference System for mention extraction and to compute features ?",0
18527,a and ? p .,0
18528,Features,0
18529,"We use the raw BASIC + feature sets described by , with the following modifications :",0
18530,We remove all features from ?,0
18531,"p that concatenate a feature of the antecedent with a feature of the current mention , such as bi-head features .",0
18532,"We add true - cased head features , a current speaker indicator feature , and a 2 - character they underperformed .",0
18533,"We also experimented with training approaches and model variants that expose the model to its own predictions , but found that these yielded a negligible performance improvement. , , , and .",0
18534,F 1 gains are significant ( p < 0.05 under the bootstrap resample test ) compared with for all metrics .,0
18535,"genre ( out of {bc , bn , mz , nw , pt , tc , wb} ) indicator to ?",0
18536,p and ? a .,0
18537,"We add features indicating if a mention has a substring overlap with the current speaker (? p and ? a ) , and if an antecedent has a substring overlap with a speaker distinct from the current mention 's speaker (? p ) .",0
18538,"We add a single centered , rescaled document position feature to each mention when learning h c .",0
18539,We calculate a mention x n 's rescaled document position as 2 n ? N ?1 N ?1 .,0
18540,"These modifications result in there being approximately 14 K distinct features in ? a and approximately 28 K distinct features in ? p , which is far fewer features than has been typical in past work .",0
18541,"For training , we use document - size minibatches , which allows for efficient pre-computation of RNN states , and we minimize the loss described in Section 5 with AdaGrad ( after clipping LSTM gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .",1
18542,"We find that the initial learning rate chosen for AdaGrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .",1
18543,"In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .",1
18544,"We use a single - layer LSTM ( without "" peep - hole "" connections ) , as implemented in the element - rnn library .",1
18545,"For regularization , we apply Dropout with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot -product scores .",1
18546,"Following we use the costweights ? = 0.5 , 1.2 , 1 in defining ? , and we use their pre-training scheme as well .",0
18547,"For final results , we train on both training and development portions of the CoNLL data .",0
18548,Scoring uses the official CoNLL 2012 script .,0
18549,Code for our system is available at https : //github.com/swiseman/nn_coref .,1
18550,"The system makes use of a GPU for training , and trains in about two hours .",1
18551,Results,1
18552,"In we present our main results on the CoNLL English test set , and compare with other recent stateof - the - art systems .",0
18553,"We see a statistically significant improvement of over 0.8 Co NLL points over the previous state of the art , and the highest F 1 scores to date on all three CoNLL metrics .",1
18554,We now consider in more detail the impact of global features and RNNs on performance .,0
18555,"For these experiments , we report MUC , B 3 , and CEAF e F 1 scores in as well as errors broken down by mention type and by whether the mention is anaphoric or not in .",0
18556,are defined in Section 5.1 .,0
18557,"We typically think of FL and WL as representing precision errors , and FN as representing recall errors .",0
18558,Our experiments consider several different settings .,0
18559,"First , we consider an oracle setting ( "" RNN , OH "" in tables ) , in which the model receives z ( o ) 1:n?1 , the oracle partial clustering of all mentions preceding x n in the document , and is therefore not forced to rely on its own past predictions when predicting x n .",0
18560,This provides us with an upper bound on the performance achievable with our model .,0
18561,"Next , we consider the performance of the model under a greedy inference strategy ( RNN , GH ) , as in Algorithm",0
18562,"1 . Finally , for baselines we consider the mention - ranking system ( MR ) of using our updated feature - set , as well as a non-local baseline with oracle history ( Avg , OH ) , which averages the representations h c ( x j ) for all x j ? X ( m ) , rather than feed them through an RNN ; errors are still backpropagated through the h c representations during learning .",0
18563,"In we see that the RNN improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .",1
18564,"While WL errors also decrease for both these mention - categories under the RNN model , FN errors increase .",0
18565,"Importantly , the RNN performance is significantly better than that of the Avg baseline , which barely improves over mention - ranking , even with oracle history .",1
18566,This suggests that modeling the sequence of mentions in a cluster is advantageous .,0
18567,"We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .",0
18568,Qualitative Analysis,0
18569,"In this section we consider in detail the impact of the g term in the RNN scoring function on the two error categories that improve most under the RNN model ( as shown in ) , namely , pronominal WL errors and pronominal FL errors .",0
18570,We consider an example from the CoNLL development set in each category on which the baseline MR model makes an error but the greedy RNN model does not .,0
18571,"The example in involves the resolution of the ambiguous pronoun "" his , "" which is bracketed and in bold in the figure .",0
18572,"Whereas the baseline MR model incorrectly predicts "" his "" to corefer with the closest gender - consistent antecedent "" Justin "" thus making a WL error - the greedy RNN model : Magnitudes of gradients of NA score applied to bold "" It 's "" with respect to final mention in three preceding clusters .",0
18573,See text for full description .,0
18574,"correctly predicts "" his "" to corefer with "" Mr. Kaye "" in the previous sentence .",0
18575,"( Note that "" the official "" also refers to Mr. Kaye ) .",0
18576,"To get a sense of the greedy RNN model 's decision - making on this example , we color the mentions the greedy RNN model has predicted to corefer with "" Mr. Kaye "" in green , and the mentions it has predicted to corefer with "" Justin "" in blue .",0
18577,"( Note that the model incorrectly predicts the initial "" I "" mentions to corefer with "" Justin . "" )",0
18578,"Letting X ( 1 ) refer to the blue cluster , X ( 2 ) refer to the green cluster , and x n refer to the ambiguous mention "" his , "" we further shade each mention x j in X ( 1 ) so that it s intensity corresponds to h c ( x n ) Th ( 1 ) <k , where k = j + 1 ; mentions in X ( 2 ) are shaded analogously .",0
18579,"Thus , the shading shows how highly g scores the compatibility between "" his "" and a cluster X ( i ) as each of X ( i ) 's mentions is added .",0
18580,"We see that when the initial "" Justin "" mentions are added to X ( 1 ) the g-score is relatively high .",0
18581,"However , after "" The company "" is correctly predicted to corefer with "" Justin , "" the score of X ( 1 ) drops , since companies are generally not coreferent with pronouns like "" his. "" shows an example ( consisting of a telephone conversation between "" A "" and "" B "" ) in which the bracketed pronoun "" It 's "" is being used pleonastically .",0
18582,"Whereas the baseline MR model predicts "" It 's "" to corefer with a previous "" it "" - thus making a FL error - the greedy RNN model does not .",0
18583,In the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention .,0
18584,"This visualization resembles the "" saliency "" technique of , and it attempts to gives a sense of the contribution of a ( preceding ) cluster in the calculation of the NA score .",0
18585,"We see that the potential antecedent "" S - Bahn "" has a large gradient , but also that the initial , obviously pleonastic use of "" it 's "" has a large gradient , which may suggest that earlier , easier predictions of pleonasm can inform subsequent predictions .",0
18586,Related Work,0
18587,"In addition to the related work noted throughout , we add supplementary references here .",0
18588,"Unstructured approaches to coreference typically divide into mention - pair models , which classify ( nearly ) every pair of mentions in a document as coreferent or not , and mention - ranking models , which select a single antecedent for each anaphoric mention ( Denis and Baldridge , 2008 ; .",0
18589,"Structured approaches typically divide between those that induce a clustering of mentions , and , more recently , those that learn a latent tree of mentions .",0
18590,There have also been structured approaches that merge the mention - ranking and mention - pair ideas in someway .,0
18591,"For instance , rank clusters rather than mentions ; use the output of both mention - ranking and mention pair systems to learn a clustering .",0
18592,"The application of RNNs to modeling ( the trajectory of ) the state of a cluster is apparently novel , though it bears some similarity to the recent work of , who use LSTMs to embed the state of a transition based parser 's stack .",0
18593,Conclusion,0
18594,"We have presented a simple , state of the art approach to incorporating global information in an end - to - end coreference system , which obviates the need to define global features , and moreover allows for simple ( greedy ) inference .",0
18595,"Future work will examine improving recall , and more sophisticated approaches to global training .",0
18596,title,0
18597,Higher - order Coreference Resolution with Coarse - to - fine Inference,1
18598,abstract,0
18599,We introduce a fully differentiable approximation to higher - order inference for coreference resolution .,0
18600,Our approach uses the antecedent distribution from a span - ranking architecture as an attention mechanism to iteratively refine span representations .,0
18601,This enables the model to softly consider multiple hops in the predicted clusters .,0
18602,"To alleviate the computational cost of this iterative process , we introduce a coarse - to - fine approach that incorporates a less accurate but more efficient bilinear factor , enabling more aggressive pruning without hurting accuracy .",0
18603,"Compared to the existing state - of - the - art span - ranking approach , our model significantly improves accuracy on the English OntoNotes benchmark , while being far more computationally efficient .",0
18604,Introduction,0
18605,"Recent coreference resolution systems have heavily relied on first order models , where only pairs of entity mentions are scored by the model .",0
18606,These models are computationally efficient and scalable to long documents .,0
18607,"However , because they make independent decisions about coreference links , they are susceptible to predicting clusters thatare locally consistent but globally inconsistent .",0
18608,shows an example from that illustrates this failure case .,0
18609,"The plurality of [ you ] is underspecified , making it locally compatible with both [ I ] and [ all of you ] , while the full cluster would have mixed plurality , resulting in global inconsistency .",0
18610,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,1
18611,"At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .",1
18612,Speaker 2 : Welland uh thanks goes to and to the media to help us ...,0
18613,So our hat is off to [ all of you ] as well .,0
18614,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",1
18615,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,1
18616,This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,1
18617,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .",1
18618,Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark .,0
18619,"We observe a significant increase in average F1 with a second - order model , but returns quickly diminish with a third - order model .",0
18620,"Additionally , our analysis shows that the coarse - to - fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning , compared to the distance - based heuristic pruning from previous work .",0
18621,Background,0
18622,Task definition,0
18623,"We formulate the coreference resolution task as a set of antecedent assignments y i for each of span i in the given document , following .",0
18624,"The set of possible assignments for each y i is Y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?",0
18625,and all preceding spans .,0
18626,Non-dummy antecedents represent coreference links between i and y i .,0
18627,The dummy antecedent ?,0
18628,represents two possible scenarios : ( 1 ) the span is not an entity mention or ( 2 ) the span is an entity mention but it is not coreferent with any previous span .,0
18629,"These decisions implicitly define a final clustering , which can be recovered by grouping together all spans thatare connected by the set of antecedent predictions .",0
18630,Baseline,0
18631,"We describe the baseline model , which we will improve to address the modeling and computational limitations discussed previously .",0
18632,The goal is to learn a distribution P ( y i ) over antecedents for each span i :,0
18633,"where s ( i , j ) is a pairwise score for a coreference link between span i and span j.",0
18634,"The baseline model includes three factors for this pairwise coreference score : ( 1 ) s m ( i ) , whether span i is a mention , ( 2 ) s m ( j ) , whether span j is a mention , and ( 3 ) s a ( i , j ) whether j is an antecedent of i:",0
18635,"In the special case of the dummy antecedent , the score s ( i , ? ) is instead fixed to 0 .",0
18636,A common component used throughout the model is the vector representations g i for each possible span i .,0
18637,These are computed via bidirectional LSTMs ) that learn context - dependent boundary and head representations .,0
18638,The scoring functions s m and s a take these span representations as input :,0
18639,"where denotes element - wise multiplication , FFNN denotes a feed - forward neural network , and the antecedent scoring function s a ( i , j ) includes explicit element - wise similarity of each span g i g j and a feature vector ?( i , j ) encoding speaker and genre information from the metadata and the distance between the two spans .",0
18640,The model above is factored to enable a twostage beam search .,0
18641,A beam of up to M potential mentions is computed ( where M is proportional to the document length ) based on the spans with the highest mention scores s m ( i ) .,0
18642,Pairwise coreference scores are only computed between surviving mentions during both training and inference .,0
18643,"Given supervision of gold coreference clusters , the model is learned by optimizing the marginal log - likelihood of the possibly correct antecedents .",0
18644,This marginalization is required since the best antecedent for each span is a latent variable .,0
18645,Higher - order Coreference Resolution,0
18646,"The baseline above is a first - order model , since it only considers pairs of spans .",0
18647,First - order models are susceptible to consistency errors as demonstrated in .,0
18648,"Unlike in sentence - level semantics , where higher - order decisions can be implicitly modeled by the LSTMs , modeling these decisions at the document - level requires explicit inference due to the potentially very large surface distance between mentions .",0
18649,"We propose an inference procedure that allows the model to condition on higher - order structures , while being fully differentiable .",0
18650,"This inference involves N iterations of refining span representations , denoted as g n i for the representation of span i at iteration n .",0
18651,"At iteration n , g n i is computed with an attention mechanism that averages over previous representations g n?1 j weighted according to how likely each mention j is to bean antecedent for i , as defined below .",0
18652,The baseline model is used to initialize the span representation at g 1 i .,0
18653,The refined span representations allow the model to also iteratively refine the antecedent distributions P n ( y i ) :,0
18654,where sis the coreference scoring function of the baseline architecture .,0
18655,"The scoring function uses the same parameters at every iteration , but it is given different span representations .",0
18656,"At each iteration , we first compute the expected antecedent representation an i of each span i by using the current antecedent distribution P n ( y i ) as an attention mechanism : an i = y i ? Y ( i ) P n ( y i ) g n y i",0
18657,The current span representation g n i is then updated via interpolation with its expected antecedent representation an i :,0
18658,The learned gate vector f n i determines for each dimension whether to keep the current span information or to integrate new information from its expected antecedent .,0
18659,"At iteration n , g n i is an element - wise weighted average of approximately n span representations ( assuming P n ( y i ) is peaked ) , allowing P n ( y i ) to softly condition on up ton other spans in the predicted cluster .",0
18660,"Span - ranking can be viewed as predicting latent antecedent trees , where the predicted antecedent is the parent of a span and each tree is a predicted cluster .",0
18661,"By iteratively refining the span representations and antecedent distributions , another way to interpret this model is that the joint distribution i P N ( y i ) implicitly models every directed path of up to length N + 1 in the latent antecedent tree .",0
18662,4 Coarse - to - fine Inference,0
18663,The model described above scales poorly to long documents .,0
18664,"Despite heavy pruning of potential mentions , the space of possible antecedents for every surviving span is still too large to fully consider .",0
18665,"The bottleneck is in the antecedent score s a ( i , j ) , which requires computing a tensor of size M M ( 3 |g | + |?|) .",0
18666,"This computational challenge is even more problematic with the iterative inference from Section 3 , which requires recomputing this tensor at every iteration .",0
18667,Heuristic antecedent pruning,0
18668,"To reduce computation , heuristically consider only the nearest K antecedents of each span , resulting in a smaller input of size M K ( 3 |g | + |?| ) .",0
18669,The main drawback to this solution is that it imposes an a priori limit on the maximum distance of a coreference link .,0
18670,"The previous work only considers up to K = 250 nearest mentions , whereas Coarse - to - fine ( various K ) Coarse - to - fine ( chosen K ) Heuristic ( various K ) Heuristic ( chosen K ) : Comparison of accuracy on the development set for the two antecedent pruning strategies with various beams sizes K .",0
18671,"The distance - based heuristic pruning performance drops by almost 5 F1 when reducing K from 250 to 50 , while the coarse - to - fine pruning results in an insignificant drop of less than 0.2 F1 . coreference links can reach much further in natural language discourse .",0
18672,Coarse - to - fine antecedent pruning,0
18673,We instead propose a coarse - to - fine approach that can be learned end - to - end and does not establish an a priori maximum coreference distance .,0
18674,The key component of this coarse - to - fine approach is an alternate bilinear scoring function :,0
18675,where W c is a learned weight matrix .,0
18676,"In contrast to the concatenation - based s a ( i , j ) , the bilinear s c ( i , j ) is far less accurate .",0
18677,"A direct replacement of s a ( i , j ) with s c ( i , j ) results in a performance loss of over 3 F1 in our experiments .",0
18678,"However , s c ( i , j ) is much more efficient to compute .",0
18679,"Computing s c ( i , j ) only requires manipulating matrices of size M |g | and M M .",0
18680,"Therefore , we instead propose to use s c ( i , j ) to compute a rough sketch of likely antecedents .",0
18681,This is accomplished by including it as an additional factor in the model :,0
18682,"Similar to the baseline model , we leverage this additional factor to perform an additional beam pruning step .",0
18683,The final inference procedure involves a three - stage beam search :,0
18684,Second stage,0
18685,"Keep the top K antecedents of each remaining span i based on the first three factors , s m ( i ) + s m ( j ) + s c ( i , j ) .",0
18686,Third stage,0
18687,"The over all coreference s ( i , j ) is computed based on the remaining span pairs .",0
18688,The soft higher - order inference from Section 3 is computed in this final stage .,0
18689,"While the maximum - likelihood objective is computed over only the span pairs from this final stage , this coarse - to - fine approach expands the set of coreference links that the model is capable of learning .",0
18690,It achieves better performance while using a much smaller K ( see ) .,0
18691,Experimental Setup,0
18692,We use the English coreference resolution data from the CoNLL - 2012 shared task in our experiments .,0
18693,The code for replicating these results is publicly available .,0
18694,"Our models reuse the hyperparameters from , with a few exceptions mentioned below .",0
18695,"In our results , we report two improvements thatare orthogonal to our contributions .",0
18696,We used embedding representations from a language model at the input to the LSTMs ( ELMo in the results ) .,0
18697,We changed several hyperparameters :,0
18698,1 . increasing the maximum span width from 10 to 30 words .,0
18699,1 https://github.com/kentonl/e2e-coref,0
18700,2 . using 3 highway LSTMs instead of 1 .,0
18701,3 . using Glo Ve word embeddings with a window size of 2 for the headword embeddings and a window size of 10 for the LSTM inputs .,0
18702,The baseline model considers up to 250 antecedents per span .,0
18703,"As shown in , the coarse - to - fine model is quite insensitive to more aggressive pruning .",0
18704,"Therefore , our final model considers only 50 antecedents per span .",0
18705,"On the development set , the second - order model ( N = 2 ) outperforms the first - order model by 0.8 F1 , but the third order model only provides an additional 0.1 F1 improvement .",0
18706,"Therefore , we only compute test results for the secondorder model .",0
18707,Results,1
18708,"We report the precision , recall , and F1 of the the MUC , B 3 , and CEAF ?",0
18709,4 metrics using the official CoNLL - 2012 evaluation scripts .,0
18710,The main evaluation is the average F1 of the three metrics .,0
18711,Results on the test set are shown in .,0
18712,We include performance of systems proposed in the past 3 years for reference .,0
18713,"The baseline relative to our contributions is the span - ranking model from augmented with both ELMo and hyperparameter tuning , which achieves 72.3 F1 .",1
18714,"Our full approach achieves 73.0 F1 , setting a new state of the art for coreference resolution .",1
18715,"Compared to the heuristic pruning with up to 250 antecedents , our coarse - to - fine model only computes the expensive scores s a ( i , j ) for 50 antecedents .",0
18716,"Despite using far less computation , it outperforms the baseline because the coarse scores s c ( i , j ) can be computed for all antecedents , enabling the model to potentially predict a coreference link between any two spans in the document .",1
18717,"As a result , we observe a much higher recall when adopting the coarse - to - fine approach .",1
18718,We also observe further improvement by including the second - order inference ( Section 3 ) .,1
18719,"The improvement is largely driven by the over all increase in precision , which is expected since the higher - order inference mainly serves to rule out inconsistent clusters .",0
18720,It is also consistent with findings from who report mainly improvements in precision when modeling latent trees to achieve a similar goal .,0
18721,Related Work,0
18722,"In addition to the end - to - end span - ranking model ) that our proposed model builds upon , there is a large body of literature on coreference resolvers that fundamentally rely on scoring span pairs .",0
18723,"Motivated by structural consistency issues discussed above , significant effort has also been devoted towards cluster - level modeling .",0
18724,"Since global features are notoriously difficult to define , they often depend heavily on existing pairwise features or architectures .",0
18725,We similarly use an existing pairwise span - ranking architecture as a building block for modeling more complex structures .,0
18726,"In contrast to who use highly expressive recurrent neural networks to model clusters , we show that the addition of a relatively lightweight gating mechanism is sufficient to effectively model higher - order structures .",0
18727,Conclusion,0
18728,We presented a state - of - the - art coreference resolution system that models higher order interactions between spans in predicted clusters .,0
18729,"Additionally , our proposed coarse - to - fine approach alleviates the additional computational cost of higher - order inference , while maintaining the end - to - end learnability of the entire model .",0
18730,title,0
18731,A Mention - Ranking Model for Abstract Anaphora Resolution,1
18732,abstract,0
18733,"Resolving abstract anaphora is an important , but difficult task for text understanding .",0
18734,"Yet , with recent advances in representation learning this task becomes a more tangible aim .",0
18735,A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its ( typically non-nominal ) antecedent .,0
18736,We propose a mention - ranking model that learns how abstract anaphors relate to their antecedents with an LSTM - Siamese Net .,0
18737,We overcome the lack of training data by generating artificial anaphoric sentenceantecedent pairs .,0
18738,Our model outperforms state - of - the - art results on shell noun resolution .,0
18739,We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus .,0
18740,This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders .,0
18741,"We found model variants that outperform the baselines for nominal anaphors , without training on individual anaphor data , but still lag behind for pronominal anaphors .",0
18742,Our model selects syntactically plausible candidates and - if disregarding syntax - discriminates candidates using deeper features .,0
18743,Introduction,0
18744,"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work .",1
18745,"world , which is arguably the most frequently occurring type .",0
18746,"Distinct from these are diverse types of abstract anaphora ( AA ) where reference is made to propositions , facts , events or properties .",0
18747,An example is given in ( 1 ) below .,0
18748,"While recent approaches address the resolution of selected abstract shell nouns ( Kolhatkar and Hirst , 2014 ) , we aim to resolve a wide range of abstract anaphors , such as the NP this trend in ( 1 ) , as well as pronominal anaphors ( this , that , or it ) .",0
18749,"Henceforth , we refer to a sentence that contains an abstract anaphor as the anaphoric sentence ( AnaphS ) , and to a constituent that the anaphor refers to as the antecedent ( Antec ) ( cf. ( 1 ) ) .",0
18750,"( 1 ) Ever-more powerful desktop computers , designed with one or more microprocessors as their "" brains "" , are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes .",0
18751,""" [ Antec The guys that make traditional hardware are really being obsoleted by microprocessor - based machines ] "" , said Mr. Benton .",0
18752,[ AnaphS,0
18753,"As a result of this trend AA , longtime powerhouses HP , IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor - based systems of their own . ]",0
18754,A major obstacle for solving this task is the lack of sufficient amounts of annotated training data .,0
18755,We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types .,0
18756,"This enables us to use neural methods which have shown great success in related tasks : coreference resolution ( Clark and Manning , 2016 a ) , textual entailment , learning textual similarity , and discourse relation sense classification .",0
18757,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .",1
18758,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",1
18759,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,1
18760,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,1
18761,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,1
18762,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,1
18763,displays our architecture .,0
18764,"In contrast to other work , our method for generating training data is not confined to specific types of anaphora such as shell nouns or anaphoric connectives .",0
18765,It produces large amounts of instances and is easily adaptable to other languages .,1
18766,"This enables us to build a robust , knowledge - lean model for abstract anaphora resolution that easily extends to multiple languages .",0
18767,We evaluate our model on the shell noun resolution dataset of and show that it outperforms their state - of - the - art results .,0
18768,"Moreover , we report results of the model ( trained on our newly constructed dataset ) on unrestricted abstract anaphora instances from the ARRAU corpus .",0
18769,To our knowledge this provides the first state - of - the - art benchmark on this data subset .,0
18770,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,1
18771,Related and prior work,0
18772,"Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types , their degrees of abstractness , and general dis - 2 course properties .",0
18773,"In contrast to nominal anaphora , abstract anaphora is difficult to resolve , given that agreement and lexical match features are not applicable .",0
18774,"Annotation of abstract anaphora is also difficult for humans , and thus , only few smaller - scale corpora have been constructed .",0
18775,We evaluate our models on a subset of the AR - RAU corpus ) that contains abstract anaphors and the shell noun corpus used in .,0
18776,We are not aware of other freely available abstract anaphora datasets .,0
18777,Little work exists for the automatic resolution of abstract anaphora .,0
18778,"Early work ( Eckert and has focused on spoken language , which exhibits specific properties .",0
18779,"Recently , event coreference has been addressed using feature - based classifiers .",0
18780,"Event coreference is restricted to a subclass of events , and usually focuses on coreference between verb ( phrase ) and noun ( phrase ) mentions of similar abstractness levels ( e.g. purchase - acquire ) with no special focus on ( pro )nominal anaphora .",0
18781,"Abstract anaphora typically involves a full - fledged clausal antecedent that is referred to by a highly abstract ( pro ) nominal anaphor , as in ( 1 ) .",0
18782,proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses .,0
18783,"However , instead of selecting the correct antecedent clause ( s ) ( our task ) for a given event , their model is restricted to classifying the event into six abstract categories : this these changes , responses , analysis , context , finding , observation , based on its surrounding context .",0
18784,"While related , their task is not comparable to the full - fledged abstract anaphora resolution task , since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types .",0
18785,More related to our work is who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset .,0
18786,"They employ features modeling distance , containment , discourse structure , and - less effectively - content and lexical correlates .",0
18787,"Closest to our work is ( KZH13 ) and Kolhatkar and Hirst ( 2014 ) ( KH14 ) on shell noun resolution , using classical machine learning techniques .",0
18788,"Shell nouns are abstract nouns , such as fact , possibility , or issue , which can only be interpreted jointly with their shell content ( their embedded clause as in ( 2 ) or antecedent as in ) .",0
18789,"KZH13 refer to shell nouns whose antecedent occurs in the prior discourse as anaphoric shell nouns ( ASNs ) ( cf. ( 3 ) ) , and cataphoric shell nouns ( CSNs ) otherwise ( cf .",0
18790,( 2 ) ) .,0
18791,5,0
18792,( 2 ) Congress has focused almost solely on the fact that [ special education is expensive - and that it takes away money from regular education . ],0
18793,"( 3 KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone , and the assumption that ASNs share linguistic properties with their embedded ( CSN ) counterparts .",0
18794,"They manually developed rules to identify the embedded clause ( i.e. cataphoric antecedent ) of CSNs and trained SVM rank ( Joachims , 2002 ) on such instances .",0
18795,The trained SVM rank model is then used to resolve ASNs .,0
18796,"KH14 generalized their method to be able to create training data for any given shell noun , however , their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora .",0
18797,Stede and Grishina ( 2016 ) study a related phenomenon for German .,0
18798,They examine inherently anaphoric connectives ( such as demzufolge - according to which ) that could be used to access their abstract antecedent in the immediate context .,0
18799,"Yet , such connectives are restricted in type , and the study shows that such connectives are often ambiguous with nominal anaphors and require sense dis ambiguation .",0
18800,We conclude that they can not be easily used to acquire antecedents automatically .,0
18801,"In our work , we explore a different direction : we construct artificial training data using a general pattern that identifies embedded sentence constituents , which allows us to extract relatively secure training data for abstract anaphora that captures a wide range of anaphora - antecedent rela-tions , and apply this data to train a model for the resolution of unconstrained abstract anaphora .",0
18802,Recent work in entity coreference resolution has proposed powerful neural network - based models that we will adapt to the task of abstract anaphora resolution .,0
18803,"Most relevant for our task is the mention - ranking neural coreference model proposed in , and their improved model in Clark and Manning ( 2016 a ) , which integrates a loss function which learns distinct feature representations for anaphoricity detection and antecedent ranking .",0
18804,Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations .,0
18805,"It is widely used in vision , and in NLP for semantic similarity , entailment , query normalization and QA .",0
18806,Mention - Ranking Model,0
18807,"Given an anaphoric sentence s with a marked anaphor ( mention ) and a candidate antecedent c , the mention - ranking ( MR ) model assigns the pair ( c , s ) a score , using representations produced by an LSTM - Siamese Net .",0
18808,The highest - scoring candidate is assigned to the marked anaphor in the anaphoric sentence .,0
18809,displays the model .,0
18810,We learn representations of an anaphoric sentence sand a candidate antecedent c using a bidirectional Long Short - Term Memory .,0
18811,"One bi - LSTM is applied to the anaphoric sentence sand a candidate antecedent c , hence the term siamese .",0
18812,"Each word is represented with a vector w i constructed by concatenating embeddings of the word , of the context of the anaphor ( average of embeddings of the anaphoric phrase , the previous and the next word ) , of the head of the anaphoric phrase 6 , and , finally , an embedding of the constituent tag of the candidate , or the S constituent tag if the word is in the anaphoric sentence .",0
18813,"For each sequence s orc , the word vectors w i are sequentially fed into the bi - LSTM , which produces outputs from the forward pass , ? ?",0
18814,"hi , and outputs ? ?",0
18815,hi from the backward pass .,0
18816,The final output of the i - th word is defined as,0
18817,"To get a representation of the full sequence , h s or h c , all outputs are averaged , except for those that correspond to padding tokens .",0
18818,"To prevent forgetting the constituent tag of the sequence , we concatenate the corresponding tag embedding with h s or h c ( we call this a shortcut for the tag information ) .",0
18819,The resulting vector is fed into a feed - forward layer of exponential linear units ( ELUs ) to produce the final representationh s orh c of the sequence .,0
18820,"Fromh c andh s we compute a vector h c , s = [ |h c ?h s | ;h ch s ] , where |-| denotes the absolute values of the element - wise subtraction , and the element - wise multiplication .",0
18821,"Then h c , s is fed into a feed - forward layer of ELUs to obtain the final joint representation , h c , s , of the pair ( c , s ) .",0
18822,"Finally , we compute the score for the pair ( c , s ) that represents relatedness between them , by applying a single fully connected linear layer to the joint representation :",0
18823,"where Wis a 1 d weight matrix , and d the dimension of the vectorh c , s .",0
18824,"We train the described mention - ranking model with the max-margin training objective from , used for the antecedent ranking subtask .",0
18825,Suppose that the training set,0
18826,", where a i is the i - th abstract anaphor , s i the corresponding anaphoric sentence , T ( a i ) the set of antecedents of a i and N ( a i ) the set of candidates thatare not antecedents ( negative candidates ) .",0
18827,"Lett i = arg max t?T ( a i ) score ( t i , s i ) be the highest scor - ing antecedent of a i .",0
18828,Then the loss is given by,0
18829,Training data construction,0
18830,"We create large - scale training data for abstract anaphora resolution by exploiting a common construction , consisting of a verb with an embedded sentence ( complement or adverbial ) ( cf. ) .",0
18831,"We detect this pattern in a parsed corpus , ' cut off ' the S constituent and replace it with a suitable anaphor to create the anaphoric sentence ( AnaphS ) , while S yields the antecedent ( Antec ) .",0
18832,"This method covers a wide range of anaphoraantecedent constellations , due to diverse semantic or discourse relations that hold between the clause hosting the verb and the embedded sentence .",0
18833,"First , the pattern applies to verbs that embed sentential arguments .",0
18834,"In ( 4 ) , the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement .",0
18835,"( 4 ) He doubts [ S [ S a Bismarckian superstate will emerge that would dominate Europe ] , but warns of "" a risk of profound change in the [.. ]",0
18836,"European Community from a Germany that is too strong , even if democratic "" ] .",0
18837,"From this we extract the artificial antecedent A Bismarckian superstate will emerge that would dominate Europe , and its corresponding anaphoric sentence He doubts this , but warns of "" a risk of profound change ... even if democratic "" , which we construct by randomly choosing one of a predefined set of appropriate anaphors ( here : this , that , it ) , cf . .",0
18838,"The second row in is used when the head of S is filled by an overt complementizer ( doubts that ) , as opposed to ( 4 ) .",0
18839,The remaining rows in apply to adverbial clauses of different types .,0
18840,"Adverbial clauses encode specific discourse relations with their embedding sentences , often indicated by their conjunctions .",0
18841,"In ( 5 ) , for example , the causal conjunction as relates a cause ( embedded sentence ) and its effect ( embedding sentence ) : We randomly replace causal conjunctions because , as with appropriately adjusted anaphors , e.g. because of that , due to this or therefore that make the causal relation explicit in the anaphor .",0
18842,7,0
18843,"Compared to the shell noun corpus of KZH13 , who made use of a carefully constructed set of extraction patterns , a downside of our method is that our artificially created antecedents are uniformly of type S. However , the majority of abstract anaphora antecedents found in the existing datasets are of type S. Also , our models are intended to induce semantic representations , and so we expect syntactic form to be less critical , compared to a feature - based model .",0
18844,"8 Finally , the general extraction pattern in , covers a much wider range of anaphoric types .",0
18845,"Using this method we generated a dataset of artificial anaphoric sentence - antecedent pairs from the WSJ part of the PTB Corpus , automatically parsed using the Stanford Parser .",0
18846,7,0
18847,"In case of ambiguous conjunctions ( e.g. as interpreted as causal or temporal ) , we generally choose the most frequent interpretation .",0
18848,"This also alleviates problems with languages like German , where ( non - ) embedded sentences differ in surface position of the finite verb .",0
18849,"We can either adapt the order or ignore it , when producing anaphoric sentence - antecedent pairs .",0
18850,We thank the authors for providing the available data .,0
18851,We follow the data preparation and evaluation protocol of Kolhatkar et al. ( 2013 b ) ( KZH13 ) .,0
18852,The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns ( CSNs ) .,0
18853,"In KZH13 , all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model .",0
18854,Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents 10 .,0
18855,To all other candidates we refer to as negative candidates .,0
18856,"For every shell noun , KZH13 used the corresponding part of the CSN data to train SVM rank .",0
18857,The ASN corpus serves as the test corpus .,0
18858,"It was also constructed from the NYT corpus , by selecting anaphoric instances with the pattern "" this shell noun "" for all covered shell nouns .",0
18859,"For validation , crowdsourced annotations for the sentence which contains the antecedent , which KZH13 refer to as a broad region .",0
18860,Candidates for the antecedent were obtained by using all syntactic constituents of the broad region as candidates and ranking them using the SVM rank model trained on the CSN corpus .,0
18861,The top 10 ranked candidates were presented to the crowd workers and they chose the best answer that represents the ASN antecedent .,0
18862,The workers were encouraged to select None when they did not agree with any of the displayed answers and could provide information about how satisfied they were with the displayed candidates .,0
18863,"We consider this dataset as gold , as do KZH13 , although it maybe biased towards the offered candidates .",0
18864,11 b. Abstract anaphora resolution data set .,0
18865,We use the automatically constructed data from the WSJ corpus ( Section 4 ) for training .,0
18866,12 Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus .,0
18867,"We extracted all abstract anaphoric instances from the WSJ part of ARRAU thatare marked with the category abstract or plan , 13 and call the subcorpus ARRAU - AA .",0
18868,We obtained this information from the authors directly .,0
18869,"The authors provided us with the workers ' annotations of the broad region , antecedents chosen by the workers and links to the NYT corpus .",0
18870,The extraction of the anaphoric sentence and the candidates had to be redone .,0
18871,We excluded any documents thatare part of ARRAU .,0
18872,"13 ARRAU distinguishes abstract anaphors and ( mostly ) pronominal anaphors referring to an action or plan , as plan .",0
18873,Candidates extraction .,0
18874,"Following KZH13 , for every anaphor we create a list of candidates by extracting all syntactic constituents from sentences which contain antecedents .",0
18875,"Candidates that differ from antecedents in only one word , or one word and punctuation , were as well considered as antecedents .",0
18876,Constituents thatare not antecedents are considered as negative candidates .,0
18877,Data statistics .,0
18878,"gives statistics of the datasets : the number of anaphors ( row 1 ) , the median length ( in tokens ) of antecedents ( row 2 ) , the median length ( in tokens ) for all anaphoric sentences ( row 3 ) , the median of the number of antecedents and candidates thatare not antecedents ( negatives ) ( rows 4 - 5 ) , the number of pronominal and nominal anaphors ( rows 6 - 7 ) .",0
18879,"Both training sets , artificial and CSN , have only one possible antecedent for which we accept two minimal variants differing in only one word or one word and punctuation .",0
18880,"On the contrary , both test sets by design allow annotation of more than one antecedent that differ in more than one word .",0
18881,"Every anaphor in the artificial training dataset is pronominal , whereas anaphors in CSN and ASN are nominal only .",0
18882,ARRAU - AA has a mixture of nominal and pronominal anaphors .,0
18883,Data pre-processing .,0
18884,Other details can be found in Supplementary Materials .,0
18885,Baselines and evaluation metrics,1
18886,"Following KZH13 , we report success@n ( s@n ) , which measures whether the antecedent , or a candidate that differs in one word 14 , is in the first n ranked candidates , for n ? { 1 , 2 , 3 , 4 }.",0
18887,"Additionally , we report the preceding sentence baseline ( PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline ( TAG BL ) that randomly chooses a candidate with the constituent tag label in {S , VP , ROOT , SBAR } .",1
18888,For TAG BL we report the average of 10 runs with 10 fixed seeds .,0
18889,"PS BL always performs worse than the KZH13 model on the ASN , so we report it only for ARRAU - AA .",0
18890,Training details for our models,0
18891,Hyperparameters tuning .,0
18892,We recorded performance with manually chosen HPs and then tuned HPs with Tree - structured Parzen Estimators ( TPE ) .,0
18893,TPE chooses HPs for the next ( out of 10 ) trails on the basis of the s@1 score on the devset .,0
18894,As devsets we employ the ARRAU - AA corpus for shell noun resolution and the ASN corpus for unrestricted abstract anaphora resolution .,0
18895,For each trial we record performance on the test set .,0
18896,We report the best test s@1 score in 10 trials if it is better than the scores from default HPs .,0
18897,The default HPs and prior distributions for HPs used by TPE are given below .,0
18898,The ( exact ) HPs we used can be found in Supplementary Materials .,0
18899,Input representation .,0
18900,"To construct word vectors w i as defined in Section 3 , we used 100 - dim .",0
18901,"Glo Ve word embeddings pre-trained on the Gigaword and Wikipedia , and did not fine - tune them .",1
18902,"Vocabulary was built from the words in the training data with frequency in { 3 , U ( 1 , 10 ) } , and OOV words were replaced with an UNK token .",1
18903,"Embeddings for tags are initialized with values drawn from the uniform distribution U ? 1 ? d+t , 1 ? d+t , where t is the number of tags 16 and d ? { 50 , qlog - U ( 30 , 100 ) } the size of the tag embeddings .",0
18904,"We experimented with removing embeddings for tag , anaphor and context .",0
18905,Weights initialization .,0
18906,"The size of the LSTMs hidden states was set to { 100 , qlog - U ( 30 , 150 ) } .",1
18907,"We initialized the weight matrices of the LSTMs with random orthogonal matrices , all other weight matrices with the initialization proposed in .",1
18908,The first feed - forward layer size is set to a value in Optimization .,1
18909,"We trained our model in minibatches using Adam ( Kingma and Ba , 2015 ) with the learning rate of 10 ? 4 and maximal batch size 64 .",1
18910,"We clip gradients by global norm , with a clipping value in { 1.0 , U ( 1 , 100 ) } .",1
18911,We train for 10 epochs and choose the model that performs best on the devset .,1
18912,Regularization .,0
18913,"We used the l 2 - regularization with ? ? { 10 ?5 , log - U (10 ?7 , 10 ?2 ) }.",1
18914,"Dropout with a keep probability k p ? { 0.8 , U( 0.5 , 1.0 ) } was applied to the outputs of the LSTMs , both feed - forward layers and optionally to the input with k p ? U (0.8 , 1.0 ) .",1
18915,6 Results and analysis 6.1 Results on shell noun resolution dataset provides the results of the mentionranking model ( MR - LSTM ) on the ASN corpus using default HPs .,0
18916,Column 2 states which model produced the results : KZH13 refers to the best reported results in and TAG BL is the baseline described in Section 5.2 .,0
18917,"In terms of s@1 score , MR - LSTM outperforms both KZH13 's results and TAG BL without even necessitating HP tuning .",1
18918,"For the outlier reason we tuned HPs ( on ARRAU - AA ) for different variants of the architecture : the full architecture , without embedding of the context of the anaphor ( ctx ) , of the anaphor ( aa ) , of both constituent tag em - bedding and shortcut ( tag , cut ) , dropping only the shortcut ( cut ) , using only word embeddings as input ( ctx , aa , tag , cut ) , without the first ( ffl1 ) and second ( ffl2 ) layer .",0
18919,"From we observe : ( 1 ) with HPs tuned on ARRAU - AA , we obtain results well beyond KZH13 , ( 2 ) all ablated model variants perform worse than the full model , ( 3 ) a large performance drop when omitting syntactic information ( tag , cut ) suggests that the model makes good use of it .",1
18920,"However , this could also be due to a bias in the tag distribution , given that all candidates stem from the single sentence that contains antecedents .",0
18921,"The median occurrence of the S tag among both antecedents and negative candidates is 1 , thus the model could achieve 50.00 s@1 by picking S- type constituents , just as TAG BL achieves 42.02 for reason and 48.66 for possibility .",0
18922,Tuning of HPs gives us insight into how different model variants cope with the task .,0
18923,"For example , without tuning the model with and without syntactic information achieves 71.27 and 19.68 ( not shown in table ) s@1 score , respectively , and with tuning : 87.78 and 68.10 .",0
18924,"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance , contrary to the 19.68 s@1 score before tuning .",1
18925,"shows the performance of different variants of the MR - LSTM with HPs tuned on the ASN corpus ( always better than the default HPs ) , when evaluated on 3 different subparts of the ARRAU - AA : all 600 abstract anaphors , 397 nominal and 203 pronominal ones .",0
18926,"HPs were tuned on the ASN corpus for every variant separately , without shuffling of the training data .",0
18927,"For the best performing variant , without syntactic information ( tag , cut ) , we report the results with HPs that yielded the best s@1 test score for all anaphors ( row 4 ) , when training with those HPs on shuffled training data ( row 5 ) , and with HPs that yielded the best s@1 all nominal pronominal ( 203 ) ctx aa tag cut ffl1 ffl 2 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4 24.17 43.67 54.50 63.00 score for pronominal anaphors ( row 6 ) .",0
18928,Results on the ARRAU corpus,1
18929,"The MR - LSTM is more successful in resolving nominal than pronominal anaphors , although the training data provides only pronominal ones .",1
18930,"This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora , such as shell nouns .",0
18931,"Moreover , for shell noun resolution in KZH13 's dataset , the MR - LSTM achieved s@1 scores in the range 76.09-93.14 , while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU - AA .",1
18932,"Although lower performance is expected , since we do not have specific training data for individual nominals in ARRAU - AA , we suspect that the reason for better performance for shell noun resolution in KZH13 is due to a larger number of positive candidates in ASN ( cf. , rows : antecedents / negatives ) .",0
18933,We also note that HPs that yield good performance for resolving nominal anaphors are not necessarily good for pronominal ones ( cf. rows 4 - 6 in ) .,0
18934,"Since the TPE tuner was tuned on the nominal - only ASN data , this suggest that it would be better to tune HPs for pronominal anaphors on a different dataset or stripping the nouns in ASN .",0
18935,"Contrary to shell noun resolution , omitting syntactic information boosts performance in ARRAU - AA .",0
18936,"We conclude that when the model is provided with syntactic information , it learns to pick S- type candidates , but does not continue to learn deeper features to further distinguish them or needs more data to do so .",0
18937,"Thus , the model is notable to point to exactly one antecedent , resulting in a lower s@1 score , but does well in picking a few good candidates , which yields good s@2 - 4 scores .",0
18938,"This is what we can observe from row 2 vs. row 6 in Table 5 : the MR - LSTM without context embedding ( ctx ) achieves a comparable s@ 2 score with the variant that omits syntactic information , but better s@3 - 4 scores .",1
18939,"Further , median occurrence of tags not in {S , VP , ROOT , SBAR } among top - 4 ranked candidates is 0 for the full architecture , and 1 when syntactic information is omitted .",0
18940,"The need for discriminating capacity of the model is more emphasized in ARRAU - AA , given that the median occurrence of S- type candidates among negatives is 2 for nominal and even 3 for pronominal anaphors , whereas it is 1 for ASN .",0
18941,This is inline with the lower TAG BL in ARRAU - AA .,0
18942,"Finally , not all parts of the architecture contribute to system performance , contrary to what is observed for reason .",0
18943,"For nominal anaphors , the anaphor ( aa ) and feed - forward layers ( ffl1 , ffl2 ) are beneficial , for pronominals only the second ffl .",0
18944,Exploring the model,0
18945,We finally analyze deeper aspects of the model :,0
18946,( 1 ) whether a learned representation between the anaphoric sentence and an antecedent establishes a relation between a specific anaphor we want to resolve and the antecedent and ( 2 ) whether the maxmargin objective enforces a separation of the joint representations in the shared space .,0
18947,( 1 ) We claim that by providing embeddings of both the anaphor and the sentence containing the anaphor we ensure that the learned relation between antecedent and anaphoric sentence is dependent on the anaphor under consideration .,0
18948,illustrates the heatmap for an anaphoric sentence with two anaphors .,0
18949,The i - th column of the heatmap corresponds to absolute differences between the output of the bi -LSTM for the i - th word in the anaphoric sentence when the first vs. second anaphor is resolved .,0
18950,"Stronger color indi - cates larger difference , the blue rectangle represents the column for the head of the first anaphor , the dashed blue rectangle the column for the head of the second anaphor .",0
18951,"Clearly , the representations differ when the first vs. second anaphor is being resolved and consequently , joint representations with an antecedent will differ too .",0
18952,( 2 ) It is known that the max-margin objective separates the best - scoring positive candidate from the best - scoring negative candidate .,0
18953,"To investigate what the objective accomplishes in the MR - LSTM model , we analyze the joint representations of candidates and the anaphoric sentence ( i.e. , outputs of ffl2 ) after training .",0
18954,"For a randomly chosen instance from ARRAU - AA , we plotted outputs of ffl2 with the tSNE algorithm .",0
18955,illustrates that the joint representation of the first ranked candidate and the anaphoric sentence is clearly separated from other joint representations .,0
18956,This shows that the maxmargin objective separates the best scoring positive candidate from the best scoring negative candidate by separating their respective joint representations with the anaphoric sentence .,0
18957,Conclusions,0
18958,"We presented a neural mention - ranking model for the resolution of unconstrained abstract anaphora , and applied it to two datasets with different types of abstract anaphora : the shell noun dataset and a subpart of ARRAU with ( pro ) nominal abstract anaphora of any type .",0
18959,To our knowledge this work is the first to address the unrestricted abstract anaphora resolution task with a neural network .,0
18960,Our model also outperforms state - of - the - art results on the shell noun dataset .,0
18961,In this work we explored the use of purely artificially created training data and how far it can bring us .,0
18962,"In future work , we plan to investigate mixtures of ( more ) artificial and natural data from different sources ( e.g. ASN , CSN ) .",0
18963,"On the more challenging ARRAU - AA , we found model variants that surpass the baselines for the entire and the nominal part of ARRAU - AA , although we do not train models on individual ( nominal ) anaphor training data like the related work for shell noun resolution .",0
18964,"However , our model still lags behind for pronominal anaphors .",0
18965,"Our results suggest that models for nominal and pronominal anaphors should be learned independently , starting with tuning of HPs on a more suitable devset for pronominal anaphors .",0
18966,"We show that the model can exploit syntactic information to select plausible candidates , but that when it does so , it does not learn how to distinguish candidates of equal syntactic type .",0
18967,"By contrast , if the model is not provided with syntactic information , it learns deeper features that enable it to pick the correct antecedent without narrowing down the choice of candidates .",0
18968,"Thus , in order to improve performance , the model should be enforced to first select reasonable candidates and then continue to learn features to distinguish them , using a larger training set that is easy to provide .",0
18969,"In future work we will design such a model , and offer it candidates chosen not only from sentences containing the antecedent , but the larger context .",0
18970,title,0
18971,End - to - end Deep Reinforcement Learning Based Coreference Resolution,1
18972,abstract,0
18973,Recent neural network models have significantly advanced the task of coreference resolution .,0
18974,"However , current neural coreference models are typically trained with heuristic loss functions thatare computed over a sequence of local decisions .",0
18975,"In this paper , we introduce an end - to - end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics .",0
18976,"Specifically , we modify the state - of - the - art higherorder mention ranking approach in Lee et al .",0
18977,( 2018 ) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions .,0
18978,"Furthermore , we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum .",0
18979,Our proposed model achieves new state - of - the - art performance on the English OntoNotes v5.0 benchmark .,0
18980,Introduction,0
18981,"Coreference resolution is one of the most fundamental tasks in natural language processing ( NLP ) , which has a significant impact on many downstream applications including information extraction ) , question answering , and entity linking .",0
18982,"Given an input text , coreference resolution aims to identify and group all the mentions that refer to the same entity .",0
18983,"In recent years , deep neural network models for coreference resolution have been prevalent .",0
18984,"These models , however , either assumed mentions were given and only developed a coreference linking model or built a pipeline system to detect mention first then resolved coreferences .",0
18985,"In either case , they depend on hand - crafted fea -tures and syntactic parsers that may not generalize well or may even propagate errors .",0
18986,"To avoid the cascading errors of pipeline systems , recent NLP researchers have developed endto - end approaches , which directly consider all text spans , jointly identify entity mentions and cluster them .",0
18987,The core of those end - to - end models are vector embeddings to represent text spans in the document and scoring functions to compute the mention scores for text spans and antecedent scores for pairs of spans .,0
18988,"Depending on how the span embeddings are computed , the end - to - end coreference models could be further divided into first order methods or higher order methods .",0
18989,"Although recent end - to - end neural coreference models have advanced the state - of - the - art performance for coreference resolution , they are still trained with heuristic loss functions and make a sequence of local decisions for each pair of mentions .",0
18990,"However as studied in ; , most coreference resolution evaluation measures are not accessible over local decisions , but can only be known until all other decisions have been made .",0
18991,"Therefore , the next key research question is how to integrate and directly optimize coreference evaluation metrics in an end - to - end manner .",0
18992,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .",1
18993,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .",1
18994,"Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .",1
18995,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",1
18996,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .",1
18997,We evaluate our end - to - end reinforced coreference resolution model on the English OntoNotes v5.0 benchmark .,0
18998,"Our model achieves the new state - of - the - art F1 - score of 73.8 % , which outperforms previous best - published result ( 73.0 % ) of with statistical significance .",0
18999,Related Work,0
19000,Closely related to our work are the end - to - end coreference models developed by and .,0
19001,"Different from previous pipeline approaches , neural networks to learn mention representations and calculate mention and antecedent scores without using syntactic parsers .",0
19002,"However , their models optimize a heuristic loss based on local decisions rather than the actual coreference evaluation metrics , while our reinforcement model directly optimizes the evaluation metrics based on the rewards calculated from sequences of actions .",0
19003,"Our work is also inspired by and , which resolve coreferences with reinforcement learning techniques .",0
19004,"They view the mention - ranking model as an agent taking a series of actions , where each action links each mention to a candidate antecedent .",0
19005,They also use pretraining for initialization .,0
19006,"Nevertheless , their models assume mentions are given while our work is end - to - end .",0
19007,"Furthermore , we add entropy regularization to encourage more exploration ) and prevent our model from prematurely converging to a sub-optimal ( or bad ) local optimum .",0
19008,Methodology,0
19009,Task definition,0
19010,"Given a document , the task of end - to - end coreference resolution aims to identify a set of mention clusters , each of which refers to the same entity .",0
19011,"Following , we formulate the task as a sequence of linking decisions for each span i to the set of its possible antecedents , denoted as Y ( i ) = { , 1 , , i ? 1 } , a dummy antecedent and all preceding spans .",0
19012,"In particular , the use of dummy antecedent for a span is to handle two possible scenarios : ( i ) the span is not an entity mention or ( ii ) the span is an entity mention but it is not coreferent with any previous spans .",0
19013,The final coreference clusters can be recovered with a backtracking step on the antecedent predictions .,0
19014,Our Model,0
19015,Figure 2 illustrates a demonstration of our iterative coreference resolution model on a document .,0
19016,"Given a document , our model first identifies top scored mentions , and then conducts a sequence of actions a 1:T = {a 1 , a 2 , , a T } over them , where T is the number of mentions and each action at assigns mention t to a candidate antecedent",0
19017,"Once our model has finished all the actions , it observes a reward R ( a 1:T ) .",0
19018,The calculated gradients are then propagated to update model parameters .,0
19019,"We use the average of the three metrics : MUC , B 3 ( Recasens and Hovy , 2011 ) and CEAF ?",0
19020,4 ( Cai and ( 3 ) ( 4 ) ( 5 ) Observe Sample Act ( 3 ) ( 4 ),0
19021,Env update ( 3 ) ( 4 ) ( 1 ) ( 3 ) ( 4 ),0
19022,( 1 ) ( 3 ) ( 4 ),0
19023,( 1 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) :,0
19024,A demonstration of our reinforced coreference resolution method on a document with 6 mentions .,0
19025,"The upper and lower rows correspond to step 5 and 6 respectively , in which the policy network selects mention as the antecedent of mention ( 5 ) and leaves mention as a singleton mention .",0
19026,The red ( gray ) nodes represent processed ( current ) mentions and edges between them indicate current predicted coreferential relations .,0
19027,The gray rectangles around circles are span embeddings and the reward is calculated at the trajectory end . :,0
19028,Architecture of the policy network .,0
19029,The components in dashed square iteratively refine span representations .,0
19030,The last layer is a masked softmax layer that computes probability distribution only over the candidate antecedents for each mention .,0
19031,We omit the span generation and pruning component for simplicity .,0
19032,"Strube , 2010 ) as the reward .",0
19033,"Following Clark and Manning ( 2016 a ) , we assume actions are independent and the next state S t+ 1 is generated based on the natural order of the starting position and then the end position of mentions regardless of action at .",0
19034,Policy Network :,0
19035,"We adopt the state - of - the - art end - to - end neural coreferene scoring architecture from and add a masked softmax layer to compute the probability distribution over actions , as illustrated in .",0
19036,"The success of their approach lies in two aspects : ( i ) a coarse - tofine pruning to reduce the search space , and ( ii ) an iterative procedure to refine the span representation with an self - attention mechanism that av - erages over the previous round 's representations weighted by the normalized coreference scores .",0
19037,"Given the state St and current network parameters ? , the probability of action at choosing y t is :",0
19038,"where s ( i , j ) is the pairwise coreference score between span i and span j defined as following :",0
19039,"For the dummy antecedent , the score s ( i , ) is fixed to 0 .",0
19040,"Here s m ( . ) is the mention score function , s c ( . , .) is a bilinear score function used to prune antecedents , and s a ( . , .) is the antecedent score function .",0
19041,"Let g i denote the refined representation for span i after gating , the three functions are s m ( i ) = ?",0
19042,"Tm FFNN m ( g i ) , s c ( i , j ) = g Ti ? cg j , and s a ( i , j ) is :",0
19043,"where FFNN denotes a feed - forward neural network and denotes the element - wise product . ? m , ?",0
19044,c and ?,0
19045,"a are network parameters . ? ( i , j ) is the feature vector encoding speaker and genre information from metadata .",0
19046,The Reinforced Algorithm :,0
19047,We explore using the policy gradient algorithm to maximize the expected reward :,0
19048,Computing the exact gradient of J ( ? ) is infeasible due to the expectation over all possible action sequences .,0
19049,"Instead , we use Monte - Carlo methods .",0
19050,"The F1 improvement is statistically significant under t- test with p < 0.05 , compared with .",0
19051,to approximate the actual gradient by randomly sampling N s trajectories according top ?,0
19052,and compute the gradient only over the sampled trajectories .,0
19053,"Meanwhile , following , we subtract a baseline value from the reward to reduce the variance of gradient estimation .",0
19054,The gradient estimate is as follows :,0
19055,"where N sis the number of sampled trajectories , ? i = {a i 1 , a i T } is the ith sampled trajectory and b = Ns i =1 R ( ? i ) / N sis the baseline reward .",0
19056,The Entropy Regularization :,0
19057,"To prevent our model from being stuck in highly - peaked polices towards a few actions , an entropy regularization term is added to encourage exploration .",0
19058,The final regularized policy gradient estimate is as follows :,0
19059,where ? expr ?,0
19060,0 is the regularization parameter that controls how diverse our model can explore .,0
19061,The larger the ?,0
19062,"expr is , the more diverse our model can explore .",0
19063,"If ? expr ? ? , all actions will be sampled uniformly regardless of current policies .",0
19064,"To the contrary , if ? expr = 0 , all actions will be sampled based on current polices .",0
19065,Pretraining :,0
19066,We pretrain the policy network parameterized by ?,0
19067,using the loss function below :,0
19068,"where N is the number of mentions , I ( i , j ) = 1 if mention i and j are coreferred , and 0 otherwise .",0
19069,Y i is the set of candidate antecedents of mention i .,0
19070,Experiments,1
19071,"We evaluate our model on the English OntoNotes v5.0 , which contains 2,802 training documents , 343 development documents , and 348 test documents .",0
19072,We reuse the hyperparameters and evaluation metrics from with a few exceptions .,0
19073,"First , we pretrain our model using Eq. ( 4 ) for around 200 K steps and use the learned parameters for initialization .",1
19074,"Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .",1
19075,"We use three standard metrics : MUC ( Grishman and Sundheim , 1995 ) , B 3 and CEAF ?",0
19076,"4 . For each metric , we report the precision , recall and F1 score .",0
19077,The final evaluation is the average F1 of the above three metrics .,0
19078,Results,1
19079,"In , we compare our model with the coreference systems that have produced significant improvement over the last 3 years on the OntoNotes benchmark .",0
19080,The reported results are either adopted from their papers or reproduced from their code .,0
19081,"The first section of the table lists the pipeline models , while the second section lists the end - to - end approaches .",0
19082,The third section lists the results of our model with different variants .,0
19083,"Note that 's method contains 3 tasks : named entity recognition , relation inference and coreference resolution and we dis able the relation inference task and train the other two tasks .",0
19084,"Built on top of the model in but excluding ELMo , our base reinforced model improves the average F 1 score around 2 points ( statistical significant t- test with p < 0.05 ) compared with .",1
19085,"Besides , it is even comparable with the end - to - end multi-task coreference model that has ELMo support , which demonstrates the power of reinforcement learning combined with the state - of - the - art end - to - end model in .",0
19086,"Regarding our model , using entropy regularization to encourage exploration can improve the result by 1 point .",1
19087,"Moreover , introducing the context - dependent ELMo embedding to our base model can further boosts the performance , which is consistent with the results in .",1
19088,"We also notice that our full model 's improvement is mainly from higher precision scores and reasonably good recall scores , which indicates that our reinforced model combined with more active exploration produces better coreference scores to reduce false positive coreference links .",0
19089,"Overall , our full model achieves the state - of the - art performance of 73.8 % F1 - score when using ELMo and entropy regularization ( compared to models marked with * in , and our approach simultaneously obtains the best F1 -score of 70.5 % when using fixed word embedding only .",1
19090,Model,0
19091,Avg. F1 Full Model 74.1 w/o entropy regularization 73.1 w/o coarse - to - fine pruning 73.8 w/o second - order inference 73.6 w/ o ELMo embedding,0
19092,71.0 : Ablation study on the development set .,0
19093,""" Coarse - to - fine pruning "" and "" second - order inference "" are adopted from Impact of the parameter ?",0
19094,expr :,0
19095,Since the parameter ?,0
19096,"expr directly controls how diverse the model is explored during training , it is necessary to study it s effect on the model performance .",0
19097,shows the avg. F1 score on the development set for our full model and .,0
19098,We observe that ?,0
19099,expr does have a strong effect on the performance and the best value is around 10 ? 4 .,0
19100,"Besides , our full model consistently outperforms over a wide range of ?",0
19101,expr .: Avg. F1 score on the development set with different regularization parameter ?,0
19102,expr .,0
19103,"The result of is also plotted for comparison , which is a flat line since it does not depend on ?",0
19104,expr .,0
19105,Analysis and Discussion,0
19106,Ablation Study :,0
19107,"To understand the effect of different components , we conduct an ablation study on the development set as illustrated in .",0
19108,"Clearly , removing entropy regularization deteriorates the average F 1 score by 1 % .",0
19109,"Also , dis abling coarse - to - fine pruning or second - order inference decreases 0.3/0.5 F1 score .",0
19110,"Among all the components , ELMo embedding makes the most contribution and improves the result by 3.1 % .",0
19111,Conclusion,0
19112,We present the first end - to - end reinforcement learning based coreference resolution model .,0
19113,Our model transforms the supervised higher order coreference model to a policy gradient model that can directly optimizes coreference evaluation metrics .,0
19114,Experiments on the English OntoNotes benchmark demonstrate that our full model integrated with entropy regularization significantly outperforms previous coreference systems .,0
19115,"There are several potential improvements to our model as future work , such as incorporating mention detection result as apart of the reward .",0
19116,Another interesting direction would be introducing intermediate step rewards for each action to better guide the behaviour of the RL agent .,0
19117,title,0
19118,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,1
19119,abstract,0
19120,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",1
19121,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .",1
19122,Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word .,0
19123,"With this approach , the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document .",0
19124,"Experiments show that learning cross - sentence dependency enriches information contained by the word representations , and improves the performance of the co-reference resolution model compared with our baseline .",0
19125,Introduction,0
19126,Co-reference resolution requires models to cluster mentions that refer to the same physical entities .,1
19127,The models based on neural networks typically require different levels of semantic representations of input sentences .,0
19128,"The models usually need to calculate the representations of word spans , or mentions , given pre-trained character and wordlevel embeddings before predicting antecedents .",0
19129,"The mention - level embeddings are used to make coreference decisions , typically by scoring mention pairs and making links .",0
19130,Long short - term memories ( LSTMs ) are often used to encode the syntactic and semantic information of input sentences .,0
19131,Articles and conversations include more than one sentences .,0
19132,"Considering the accuracy and efficiency of co-reference resolution models , the encoder LSTM usually processes input sentences separately as a batch .",0
19133,"The dis advantage of this method is that the models do not consider the dependency among words from different sentences , which plays a significant role in word representation learning and co-reference predicting .",0
19134,"For example , pronouns are often linked to entities mentioned in other sentences , while their initial word vectors lack dependency information .",0
19135,"As a result , a word representation model can not learn an informative embedding of a pronoun without considering cross - sentence dependency in this case .",0
19136,It is also problematic if we encode the input document considering cross - sentence dependency and treat the entire document as one sentence .,0
19137,An input article or conversation can be too long for a single LSTM cell to memorize .,0
19138,"If the LSTM updates itself for too many steps , gradients will vanish or explode , and the coreference resolution model will be very difficult to optimize .",0
19139,Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .,0
19140,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .",1
19141,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",1
19142,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .",1
19143,Experiments showed that this approach improved the performance of co-reference resolution models .,0
19144,2 Related Work,0
19145,Co- reference Resolution,0
19146,A popular method of co-reference resolution is mention ranking .,0
19147,"Reading each mention , the model calculates coreference scores for all antecedent mentions , and picks the mention with the highest positive score to be its co-reference .",0
19148,Many recent works are based on this approach .,0
19149,designed a set of feature templates to improve the mention - ranking model .,0
19150,proposed a mention - ranking model by jointly learning mention heads and co-references .,0
19151,proposed a reinforcement learning framework for the mention ranking approach .,0
19152,"Based on similar ideas but without using parsing features , the authors of proposed the current state - of - the - art model which uses neural networks to embed mentions and calculate mention and antecedent scores .",0
19153,applied ELMo embeddings to improve within - sentence dependency modeling and word representation learning .,0
19154,and proposed models using global entity - level features .,0
19155,Language Representation Learning,0
19156,Distributed word embeddings has been used as the basic unit of language representation for over a decade .,0
19157,"Pre-trained word embeddings , for example GloVe and Skip - Gram are widely used as the input of natural language processing models .",0
19158,Long short - term memory ( LSTM ) networks are widely used for sentence modeling .,0
19159,A single - layer LSTM network was applied in the previous state - of - theart co-reference model to generate word and mention representations .,0
19160,"To capture dependency of longer distances , proposed a recurrent model that outputs hidden states by skipping input tokens .",0
19161,"Recently , memory networks have been applied in language modeling .",0
19162,"Applying an attention mechanism on memory cells , memory networks allow the model to focus on significant words or segments for classification and generation tasks .",0
19163,Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction .,0
19164,Learning Cross - Sentence dependency,0
19165,"To improve the word representation learning model for better co-reference resolution performance , we propose two word representation models that learn cross - sentence dependency .",0
19166,Linear Sentence Linking,0
19167,"Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM , the most direct way to consider cross - sentence dependency is to initialize LSTM states with the encodings of adjacent sentences .",0
19168,We name this method linear sentence linking ( LSL ) .,0
19169,"In LSL , we encode input sentences with a 2 layer bidirectional LSTM .",0
19170,"Give input sentences [ s 1 , s 2 . . . s n ] , the outputs of the first layer are",0
19171,"In the second LSTM layer , the initial state of the forward LSTM of s i is initialized as",0
19172,while the backward state is initialized as,0
19173,"where c i 0 stands for the initial cell of the ith layer , and x stands for the final output of the LSTMs in first layer .",0
19174,We then concatenate the outputs of the forward and backward LSTMs in the second layer as the word representations for coreference prediction .,0
19175,Attentional Sentence Linking,0
19176,It is difficult for LSTMs to embed enough information about along sentence into a lowdimensional distributed vector .,0
19177,"To collect richer knowledge from neighbor sentences , we propose along short - term recurrent memory module and an attention mechanism to improve sentence linking .",0
19178,"To describe the architecture of the proposed model , we focus on adjacent input sentences s i ?1 and s i .",0
19179,"We present the input embeddings of the j - th word in the i - th sentence with x i , j .",0
19180,Long Short - Term Memory RNNs,0
19181,"To solve the traditional recurrent neural networks , proposed the LSTM architecture .",0
19182,"The detail of recurrent state updating in LSTMs ht = f lstm ( x t , h t?1 , c t?1 ) is shown in following equations .",0
19183,where x t is the input embedding and ht is the output representation of the t- th word .,0
19184,LSTMs with Cross - Sentence Attention,0
19185,We design an LSTM module with cross - sentence attention for capturing cross - sentence dependency .,0
19186,We name this method attentional sentence linking ( ASL ) .,0
19187,"Considering input word x i ,t in the ith sentence and all words from the previous sentence X i?1 = [ x i ?1 , 1 , x i?1 , 2 , . . . , x i?1 , m ] , we regard the matrix X i?1 as an external memory module and calculate an attention on its cells , where each cell contains a word embedding .",0
19188,"With the attention distribution ? , we can get a vector summarizing related information from",0
19189,The model decides if it needs to pay more attention on the current input or cross - sentence information with a context gate .,0
19190,? ( ) stands for the Sigmoid function .,0
19191,The word representation of the target word is calculated as,0
19192,where f lstm stands for standard LSTM update described in section 3.2.1 .,0
19193,Co- reference Prediction,0
19194,"In this work , we apply the mention - ranking endto - end co-reference resolution ( E2E - CR ) model proposed by for co-reference prediction .",0
19195,The word representations applied in E2E - CR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs .,0
19196,"In our work , we represent words by concatenating pre-trained word embeddings and the outputs of LSL - and ASL - LSTMs .",0
19197,Experiments,0
19198,We train and evaluate our model on the English corpus of the CoNLL - 2012 shared task .,0
19199,We implement our model based on the published implementation of the baseline E2E - CR model 1 .,0
19200,Our implementation is also available online for reproducing the results reported in this paper 2 .,0
19201,"In this section , we first describe our hyperparameter setup , and then show the experimental results of previous work and our proposed models .",0
19202,Model and Hyperparameter Setup,0
19203,"In practice , the LSTM modules applied in our model have 200 output units .",1
19204,"In ASL , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .",1
19205,The initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .,1
19206,"The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .",1
19207,We randomly select up to 40 continuous sentences for training if the input is too long .,1
19208,"In co-reference prediction , we select 250 candidate antecedents as our baseline model .",0
19209,Experiment Results and Discussion,1
19210,We evaluate our model on the test set of the CoNLL - 2012 shared task .,0
19211,The performance of previous work and our model are shown in .,0
19212,"We mainly focus on the average F 1 score of MUC , B 3 , and CEAF metrics .",0
19213,"Comparing with the baseline model that achieved 67.2 % F1 score , the ASL model improved the performance by 0.6 % and achieved 67.8 % average F1 .",1
19214,Experiments : Experimental results of previous models and cross - sentence dependency learning models on the CoNLL - 2012 shared task .,0
19215,- I remember receiving an SMS like this one last year before it snowed since snowfall would affect road conditions in Beijing to a large extent .,0
19216,- Uh- huh .,0
19217,"However , it did not give people such a special feeling as it did this time .",0
19218,- Reporters are tired of the usual stand ups .,0
19219,- They want to be riding on a train or walking in the rain or something to get attention .,0
19220,- Planned terrorist bombing that ripped a 20 x 40 - foot hole in the Navy destroyer USS Cole in the Yemeni port of Aden .,0
19221,- The ship was therefor refueling .,0
19222,- Yemeni authorities claimed they have detained over 70 people for questioning .,0
19223,- These include some Afghan - Arab volunteers .,0
19224,"show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .",1
19225,"Experiments also indicated that the ASL model has better performance than the LSL model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .",1
19226,This gives the model a better ability to model cross - sentence dependency .,0
19227,Examples for comparing the performance of the ASL model and the baseline are shown in .,0
19228,Each example contains two continuous sentences with co-references distritubed in different sentences .,0
19229,Underlined spans in bold are target mentions and annotated co-references .,0
19230,"Spans in green are ASL predictions , and spans in red are baseline predictions .",0
19231,"A prediction on "" - "" means that no mention is predicted as a co-reference .",0
19232,"shows that the baseline model , which does not consider cross - sentence dependency , has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence .",0
19233,The pretrained embeddings of pronouns are not informative enough .,0
19234,"In the first example , "" it "" is not semantically similar with "" SMS "" in Glo Ve without any context , and in this case , "" it "" and "" SMS "" are in different sentences .",0
19235,"As a result , if reading this two sentences separately , it is hard for the encoder to represent "" it "" with the semantics of "" SMS "" .",0
19236,"This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention , or can not find any co-reference .",0
19237,"However , with ASL , the model learns the semantics of pronouns with an attention to words in other sentences .",0
19238,"With the proposed context gate , ASL takes knowledge from context sentences if local inputs are not informative enough .",0
19239,"Based on word represents enhanced with cross - sentence dependency , the co-reference scoring model can make better predictions .",0
19240,Conclusion and Future Work,0
19241,We proposed linear and attentional sentence linking models for learning word representations that captures cross - sentence dependency .,0
19242,"Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state - of - the - art co-reference resolution model , indicating that cross - sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences .",0
19243,"It worth exploring if our model can improve the performance of other natural language processing applications whose inputs contain multiple sentences , for example , reading comprehension , dialog generation , and sentiment analysis .",0
19244,title,0
19245,Graph Convolutional Networks for Text Classification,1
19246,abstract,0
19247,Text classification is an important and classical problem in natural language processing .,0
19248,"There have been a number of studies that applied convolutional neural networks ( convolution on regular grid , e.g. , sequence ) to classification .",0
19249,"However , only a limited number of studies have explored the more flexible graph convolutional neural networks ( convolution on non-grid , e.g. , arbitrary graph ) for the task .",0
19250,"In this work , we propose to use graph convolutional networks for text classification .",0
19251,"We build a single text graph for a corpus based on word co-occurrence and document word relations , then learn a Text Graph Convolutional Network ( Text GCN ) for the corpus .",0
19252,"Our Text GCN is initialized with one - hot representation for word and document , it then jointly learns the embeddings for both words and documents , as supervised by the known class labels for documents .",0
19253,Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state - of - the - art methods for text classification .,0
19254,"On the other hand , Text GCN also learns predictive word and document embeddings .",0
19255,"In addition , experimental results show that the improvement of Text GCN over state - of - the - art comparison methods become more prominent as we lower the percentage of training data , suggesting the robustness of Text GCN to less training data in text classification .",0
19256,Introduction,0
19257,Text classification is a fundamental problem in natural language processing ( NLP ) .,0
19258,"There are numerous applications of text classification such as document organization , news filtering , spam detection , opinion mining , and computational phenotyping ) .",0
19259,An essential intermediate step for text classification is text representation .,0
19260,"Traditional methods represent text with hand - crafted features , such as sparse lexical features ( e.g. , bag - of - words and n-grams ) .",0
19261,"Recently , deep learning models have been widely used to learn text representations , including convolutional neural networks ( CNN ) ( Kim 2014 ) and recurrent neural networks ( RNN ) such as long short - term memory ( LSTM ) .",0
19262,"As CNN and RNN prioritize locality and sequentiality , these deep learning models can capture semantic and syntactic information in local consecutive word sequences well , but may ignore global word cooccurrence in a corpus which carries non-consecutive and long - distance semantics .",0
19263,"Recently , a new research direction called graph neural networks or graph embeddings has attracted wide attention .",0
19264,Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph embeddings .,0
19265,"In this work , we propose a new graph neural networkbased method for text classification .",1
19266,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .",1
19267,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",1
19268,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,1
19269,We then turn text classification problem into anode classification problem .,1
19270,The method can achieve strong classification performances with a small proportion of labeled documents and learn interpretable word and document node embeddings .,0
19271,Our source code is available at https://github. com/yao8839836/text_gcn .,1
19272,"To summarize , our contributions are as follows :",0
19273,We propose a novel graph neural network method for text classification .,0
19274,"To the best of our knowledge , this is the first study to model a whole corpus as a heterogeneous graph and learn word and document embeddings with graph neural networks jointly .",0
19275,"Results on several benchmark datasets demonstrate that our method outperforms state - of - the - art text classification methods , without using pre-trained word embeddings or external knowledge .",0
19276,Our method also learn predictive word and document embeddings automatically .,0
19277,Related Work Traditional Text Classification,0
19278,Traditional text classification studies mainly focus on feature engineering and classification algorithms .,0
19279,"For feature engineering , the most commonly used feature is the bagof - words feature .",0
19280,"In addition , some more complex features have been designed , such as n-grams and entities in ontologies ) .",0
19281,There are also existing studies on converting texts to graphs and perform feature engineering on graphs and subgraphs .,0
19282,"Unlike these methods , our method can learn text representations as node embeddings automatically .",0
19283,Deep Learning for Text Classification,0
19284,Deep learning text classification studies can be categorized into two groups .,0
19285,One group of studies focused on models based on word embeddings .,0
19286,Several recent studies showed that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings .,0
19287,Some authors aggregated unsupervised word embeddings as document embeddings then fed these document embeddings into a classifier .,0
19288,Others jointly learned word / document and document label embeddings ) .,0
19289,"Our work is connected to these methods , the major difference is that these methods build text representations after learning word embeddings while we learn word and document embeddings simultaneously for text classification .",0
19290,Another group of studies employed deep neural networks .,0
19291,Two representative deep networks are CNN and RNN .,0
19292,( Kim 2014 ) used CNN for sentence classification .,0
19293,The architecture is a direct application of CNNs as used in computer vision but with one dimensional convolutions .,0
19294,"and ) designed character level CNNs and achieved promising results. , and used LSTM , a specific type of RNN , to learn text representation .",0
19295,"To further increase the representation flexibility of such models , attention mechanisms have been introduced as an integral part of models employed for text classification .",0
19296,"Although these methods are effective and widely used , they mainly focus on local consecutive word sequences , but do not explicitly use global word co-occurrence information in a corpus .",0
19297,Graph Neural Networks,0
19298,The topic of Graph Neural Networks has received growing attentions recently .,0
19299,A number of authors generalized well - established neural network models like CNN that apply to regular grid structure ( 2 -d mesh or 1 -d sequence ) to work on arbitrarily structured graphs .,0
19300,"In their pioneering work , Kipf and Welling presented a simplified graph neural network model , called graph convolutional networks ( GCN ) , which achieved state - of - the - art classification results on a number of benchmark graph datasets .",0
19301,"GCN was also explored in several NLP tasks such as semantic role labeling , relation classification and machine translation , where GCN is used to encode syntactic structure of sentences .",0
19302,Some recent studies explored graph neural networks for text classification .,0
19303,"However , they either viewed a document or a sentence as a graph of word nodes or relied on the not -routinely - available document citation relation to construct the graph .",0
19304,"In contrast , when constructing the corpus graph , we regard the documents and words as nodes ( hence heterogeneous graph ) and do not require inter-document relations .",0
19305,Method Graph Convolutional Networks ( GCN ),0
19306,A GCN ( Kipf and Welling 2017 ) is a multilayer neural network that operates directly on a graph and induces embedding vectors of nodes based on properties of their neighborhoods .,0
19307,"Formally , consider a graph G = ( V , E ) , where V ( | V | = n ) and E are sets of nodes and edges , respectively .",0
19308,"Every node is assumed to be connected to itself , i.e. , ( v , v ) ?",0
19309,E for any v. Let X ?,0
19310,"R nm be a matrix containing all n nodes with their features , where m is the dimension of the feature vectors , each row xv ?",0
19311,R m is the feature vector for v.,0
19312,"We introduce an adjacency matrix A of G and its degree matrix D , where D ii = j A ij .",0
19313,The diagonal elements of A are set to 1 because of self - loops .,0
19314,GCN can capture information only about immediate neighbors with one layer of convolution .,0
19315,"When multiple GCN layers are stacked , information about larger neighborhoods are integrated .",0
19316,"For a one - layer GCN , the new k-dimensional node feature matrix L ( 1 ) ?",0
19317,R nk is computed as,0
19318,is the normalized symmetric adjacency matrix and W 0 ?,0
19319,R mk is a weight matrix . ?,0
19320,"is an activation function , e.g. a ReLU ?( x ) = max ( 0 , x ) .",0
19321,"As mentioned before , one can incorporate higher order neighborhoods information by stacking multiple GCN layers :",0
19322,where j denotes the layer number and L ( 0 ) = X.,0
19323,Text Graph Convolutional Networks ( Text GCN ),0
19324,"We build a large and heterogeneous text graph which contains word nodes and document nodes so that global word co-occurrence can be explicitly modeled and graph convolution can be easily adapted , as shown in .",0
19325,The number of nodes in the text graph | V | is the number of documents ( corpus size ) plus the number of unique words ( vocabulary size ) in a corpus .,0
19326,We simply set feature matrix X = I as an identity matrix which means every word or document is represented as a one - hot vector as the input to Text GCN .,0
19327,We build edges among nodes based on word occurrence in documents ( document - word edges ) and word co-occurrence in the whole corpus ( word - word edges ) .,0
19328,"The weight of the edge between a document node and a word node is the term frequency - inverse document frequency ( TF - IDF ) of the word in the document , where term frequency is the number of times the word appears in the document , inverse document frequency is the logarithmically scaled inverse fraction of the number of documents that contain the word .",0
19329,We found using TF - IDF weight is better than using term frequency only .,0
19330,"To utilize global word co-occurrence information , we use a fixed size sliding window on all documents in the corpus to gather co-occurrence statistics .",0
19331,"We employ point - wise mutual information ( PMI ) , a popular measure for word associations , to calculate weights between two word nodes .",0
19332,We also found using PMI achieves better results than using word co-occurrence count in our preliminary experiments .,0
19333,"Formally , the weight of edge between node i and node j is defined as",0
19334,"The PMI value of a word pair i , j is computed as",0
19335,"where # W ( i ) is the number of sliding windows in a corpus that contain word i , # W ( i , j ) is the number of sliding windows that contain both word i and j , and # W is the total number of sliding windows in the corpus .",0
19336,"A positive PMI value implies a high semantic correlation of words in a corpus , while a negative PMI value indicates little or no semantic correlation in the corpus .",0
19337,"Therefore , we only add edges between word pairs with positive PMI values .",0
19338,"After building the text graph , we feed the graph into a simple two layer GCN as in , the second layer node ( word / document ) embeddings have the same size as the labels set and are fed into a softmax classifier :",0
19339,where = D ? 1 2 AD ?,0
19340,"1 2 is the same as in equation 1 , and softmax (",0
19341,The loss function is defined as the cross - entropy error over all labeled documents :,0
19342,"where Y Dis the set of document indices that have labels and F is the dimension of the output features , which is equal to the number of classes .",0
19343,Y is the label indicator matrix .,0
19344,The weight parameters W 0 and W 1 can be trained via gradient descent .,0
19345,"In equation 7 , E 1 = XW 0 contains the first layer document and word embeddings and E 2 = ReLU ( XW 0 ) W 1 contains the second layer document and word embeddings .",0
19346,The over all Text GCN model is schematically illustrated in .,0
19347,A two - layer GCN can allow message passing among nodes thatare at maximum two steps away .,0
19348,"Thus although there is no direct document - document edges in the graph , the two - layer GCN allows the information exchange between pairs of documents .",0
19349,In our preliminary experiment .,0
19350,"We found that a two - layer GCN performs better than a onelayer GCN , while more layers did not improve the performances .",0
19351,This is similar to results in and .,0
19352,Experiment,0
19353,In this section we evaluate our Text Graph Convolutional Networks ( Text GCN ) on two experimental tasks .,0
19354,Specifically we want to determine :,0
19355,"Can our model achieve satisfactory results in text classification , even with limited labeled data ?",0
19356,Can our model learn predictive word and document embeddings ?,0
19357,Baselines .,1
19358,We compare our Text GCN with multiple stateof - the - art text classification and embedding methods as follows :,1
19359,TF - IDF + LR : bag - of - words model with term frequencyinverse document frequency weighting .,1
19360,Logistic Regression is used as the classifier .,1
19361,CNN : Convolutional Neural Network ( Kim 2014 ) .,1
19362,We explored CNN -rand which uses randomly initialized word embeddings and CNN - non- static which uses pre-trained word embeddings .,1
19363,LSTM : The LSTM model defined in which uses the last hidden state as the representation of the whole text .,1
19364,We also experimented with the model with / without pre-trained word embeddings .,0
19365,"Bi- LSTM : a bi-directional LSTM , commonly used in text classification .",1
19366,We input pre-trained word embeddings to Bi - LSTM .,0
19367,"PV - DBOW : a paragraph vector model proposed by , the orders of words in text are ignored .",1
19368,We used Logistic Regression as the classifier .,1
19369,"PV - DM : a paragraph vector model proposed by , which considers the word order .",1
19370,We used Logistic Regression as the classifier .,1
19371,"PTE : predictive text embedding , which firstly learns word embedding based on heterogeneous text network containing words , documents and labels as nodes , then averages word embeddings as document embeddings for text classification .",1
19372,"fast Text : a simple and efficient text classification method , which treats the average of word / n- grams embeddings as document embeddings , then feeds document embeddings into a linear classifier .",1
19373,We evaluated it with and without bigrams .,0
19374,"SWEM : simple word embedding models , which employs simple pooling strategies operated over word embeddings .",1
19375,"LEAM : label - embedding attentive models , which embeds the words and labels in the same joint space for text classification .",1
19376,It utilizes label descriptions .,1
19377,"Graph - CNN - C : a graph CNN model that operates convolutions over word embedding similarity graphs ( Defferrard , Bresson , and Vandergheynst 2016 ) , in which Chebyshev filter is used .",1
19378,Graph - CNN - S : the same as Graph - CNN - C but using Spline filter ) .,1
19379,Graph - CNN - F : the same as Graph - CNN - C but using Fourier filter .,1
19380,Datasets .,0
19381,"We ran our experiments on five widely used benchmark corpora including 20 - Newsgroups ( 20 NG ) , Ohsumed , R52 and R8 of Reuters 21578 and Movie Review ( MR ) .",0
19382,"The 20 NG dataset 1 ( bydate version ) contains 18,846 documents evenly categorized into 20 different categories .",0
19383,"In total , 11,314 documents are in the training set and 7,532 documents are in the test set . The Ohsumed corpus 2 is from the MEDLINE data base , which is a bibliographic data base of important medical literature maintained by the National Library of Medicine .",0
19384,"In this work , we used the 13,929 unique cardiovascular diseases abstracts in the first 20,000 abstracts of the year 1991 .",0
19385,Each document in the set has one or more associated categories from the 23 disease categories .,0
19386,"As we focus on single - label text classification , the documents belonging to multiple categories are excluded so that 7,400 documents belonging to only one category remain .",0
19387,"3,357 documents are in the training set and 4,043 documents are in the test set . R52 and R8 3 ( all - terms version ) are two subsets of the Reuters 21578 dataset .",0
19388,"R8 has 8 categories , and was split to 5,485 training and 2,189 test documents .",0
19389,"R52 has 52 categories , and was split to 6,532 training and 2,568 test documents .",0
19390,"MR is a movie review dataset for binary sentiment classification , in which each review only contains one sentence .",0
19391,"The corpus has 5,331 positive and 5,331 negative reviews .",0
19392,We used the training / test split in 5 .,0
19393,We first preprocessed all the datasets by cleaning and tokenizing text as ( Kim 2014 ) .,0
19394,"We then removed stop words defined in NLTK 6 and low frequency words appearing less than 5 times for 20 NG , R8 , R52 and Ohsumed .",0
19395,"The only exception was MR , we did not remove words after cleaning and tokenizing raw text , as the documents are very short .",0
19396,The statistics of the preprocessed datasets are summarized in .,0
19397,Settings .,0
19398,"For Text GCN , we set the embedding size of the first convolution layer as 200 and set the window size as 20 .",1
19399,We also experimented with other settings and found that small changes did not change the results much .,0
19400,"We tuned other parameters and set the learning rate as 0.02 , dropout For baseline models , we used default parameter settings as in their original papers or implementations .",1
19401,"For baseline models using pre-trained word embeddings , we used 300 dimensional Glo Ve word embeddings ( Pennington , Socher , and Manning 2014 )",1
19402,7 .,0
19403,Test Performance . presents test accuracy of each model .,0
19404,"Text GCN performs the best and significantly outperforms all baseline models ( p < 0.05 based on student t- test ) on four datasets , which showcases the effectiveness of the proposed method on long text datasets .",1
19405,"For more in - depth performance analysis , we note that TF - IDF + LR performs well on long text datasets like 20 NG and can outperform CNN with randomly initialized word embeddings .",0
19406,"When pre-trained Glo Ve word embeddings are provided , CNN performs much better , especially on Ohsumed and 20 NG .",1
19407,"CNN also achieves the best results on short text dataset MR with pre-trained word embeddings , which shows it can 7 http://nlp.stanford.edu/data/glove.6B.zip model consecutive and short - distance semantics well .",0
19408,"Similarly , LSTM - based models also rely on pre-trained word embeddings and tend to perform better when documents are shorter .",1
19409,"PV - DBOW achieves comparable results to strong baselines on 20 NG and Ohsumed , but the results on shorter text are clearly inferior to others .",1
19410,This is likely due to the fact that word orders are important in short text or sentiment classification .,0
19411,"PV - DM performs worse than PV - DBOW , the only comparable results are on MR , where word orders are more essential .",1
19412,The results of PV - DBOW and PV - DM indicate that unsupervised document embeddings are not very discriminative in text classification .,0
19413,PTE and fast Text clearly outperform PV - DBOW and PV - DM because they learn document embeddings in a supervised manner so that label information can be utilized to learn more discriminative embeddings .,0
19414,"The two recent methods SWEM and LEAM perform quite well , which demonstrates the effectiveness of simple pooling methods and label descriptions / embeddings .",0
19415,Graph - CNN models also show competitive performances .,1
19416,"This suggests that building word similarity graph using pretrained word embeddings can preserve syntactic and semantic relations among words , which can provide additional information in large external text data .",0
19417,The main reasons why Text GCN works well are two fold :,0
19418,"1 ) the text graph can capture both document - word relations and global word - word relations ; 2 ) the GCN model , as a special form of Laplacian smoothing , computes the new features of anode as the weighted average of itself and its second order neighbors .",0
19419,"The label information of document nodes can be passed to their neighboring word nodes ( words within the documents ) , then relayed to other word nodes and document nodes thatare neighbor to the first step neighboring word nodes .",0
19420,"Word nodes can gather comprehensive document label information and act as bridges or key paths in the graph , so that label information can be propagated to the entire graph .",0
19421,"However , we also observed that Text GCN did not outperform CNN and LSTM - based models on MR .",0
19422,"This is because GCN ignores word orders thatare very useful in sentiment classification , while CNN and LSTM model consecutive word sequences explicitly .",0
19423,"Another reason is that the edges in MR text graph are fewer than other text graphs , which limits the message passing among the nodes .",0
19424,There are only few document - word edges because the documents are very short .,0
19425,The number of word - word edges is also limited due to the small number of sliding windows .,0
19426,"Nevertheless , CNN and LSTM rely on pre-trained word embeddings from external corpora while Text GCN only uses information in the target input corpus .",0
19427,Parameter Sensitivity .,0
19428,shows test accuracies with different sliding window sizes on R8 and MR .,0
19429,"We can see that test accuracy first increases as window size becomes larger , but the average accuracy stops increasing when window size is larger than 15 .",0
19430,"This suggests that too small window sizes could not generate sufficient global word cooccurrence information , while too large window sizes may add edges between nodes thatare not very closely related .",0
19431,depicts the classification performance on R8 and MR with different dimensions of the - first layer embeddings .,0
19432,We observed similar trends as in .,0
19433,"Too low dimensional embeddings may not propagate label information to the whole graph well , while high dimensional embeddings do not improve classification performances and may cost more training time .",0
19434,Effects of the Size of Labeled Data .,0
19435,"In order to evaluate the effect of the size of the labeled data , we tested several best performing models with different proportions of the training data .",0
19436,"reports test accuracies with 1 % , 5 % , 10 % and 20 % of original 20 NG and R8 training set .",0
19437,We note that Text GCN can achieve higher test accuracy with limited labeled documents .,0
19438,"For instance , Text GCN achieves a test accuracy of 0.8063 0.0025 on 20 NG with only 20 % training documents and a test accuracy of 0.8830 0.0027 on R8 with only 1 % training documents which are higher than some baseline models with even the full training documents .",0
19439,"These encouraging results are similar to results in where GCN can perform quite well with low label rate , which again suggests that GCN can propagate document label information to the entire graph well and our word document graph preserves global word co-occurrence information .",0
19440,Document Visualization .,0
19441,We give an illustrative visualization of the document embeddings leaned by Text GCN .,0
19442,We use t- SNE tool to visualize the learned document embeddings .,0
19443,"shows the visualization of 200 dimensional 20 NG test document embeddings learned by GCN ( first layer ) , PV - DBOW and PTE .",0
19444,We also show 20 dimensional second layer test document embeddings of Text GCN .,0
19445,"We observe that Text GCN can learn more discriminative document embeddings , and the second layer embeddings are more distinguishable than the first layer .",0
19446,Word Visualization .,0
19447,We also qualitatively visualize word embeddings learned by Text GCN.,0
19448,shows the t - SNE visualization of the second layer word embeddings learned from 20 NG .,0
19449,We set the dimension with the highest value as a word 's label .,0
19450,"We can see that words with the same label are close to each other , which means most words are closely related to some certain document classes .",0
19451,We also show top 10 words with highest values under each class in .,0
19452,We note that the top 10 words are interpretable .,0
19453,"For example , "" jpeg "" , "" graphics "" and "" image "" in column 1 can represent the meaning of their label "" comp.graphics "" well .",0
19454,Words in other columns can also indicate their label 's meaning .,0
19455,Discussion .,0
19456,"From experimental results , we can see the proposed Text GCN can achieve strong text classification results and learn predictive document and word embeddings .",0
19457,"However , a major limitation of this study is that the GCN model is inherently transductive , in which test document nodes ( without labels ) are included in GCN training .",0
19458,Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents .,0
19459,Possible solutions to the problem are introducing inductive or fast GCN model .,0
19460,Conclusion and Future Work,0
19461,"In this study , we propose a novel text classification method termed Text Graph Convolutional Networks ( Text GCN ) .",0
19462,We build a heterogeneous word document graph for a whole corpus and turn document classification into anode classification problem .,0
19463,Text GCN can capture global word cooccurrence information and utilize limited labeled documents well .,0
19464,A simple two - layer Text GCN demonstrates promising results by outperforming numerous state - of - theart methods on multiple benchmark datasets .,0
19465,"In addition to generalizing Text GCN model to inductive settings , some interesting future directions include improving the classification performance using attention mechanisms ) and developing unsupervised text GCN framework for representation learning on largescale unlabeled text data .",0
19466,title,0
19467,A C - LSTM Neural Network for Text Classification,1
19468,abstract,0
19469,Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling .,0
19470,"Convolutional neural network ( CNN ) and recurrent neural network ( RNN ) are two mainstream architectures for such modeling tasks , which adopt totally different ways of understanding natural languages .",0
19471,"In this work , we combine the strengths of both architectures and propose a novel and unified model called C - LSTM for sentence representation and text classification .",0
19472,"C - LSTM utilizes CNN to extract a sequence of higher - level phrase representations , and are fed into along short - term memory recurrent neural network ( LSTM ) to obtain the sentence representation .",0
19473,C - LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics .,0
19474,We evaluate the proposed architecture on sentiment classification and question classification tasks .,0
19475,The experimental results show that the C - LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks .,0
19476,Introduction,0
19477,"As one of the core steps in NLP , sentence modeling aims at representing sentences as meaningful features for tasks such as sentiment classification .",0
19478,"Traditional sentence modeling uses the bag - ofwords model which often suffers from the curse of dimensionality ; others use composition based methods instead , e.g. , an algebraic operation over semantic word vectors to produce the semantic sentence vector .",0
19479,"However , such methods may not perform well due to the loss of word order information .",0
19480,More recent models for distributed sentence representation fall into two categories according to the form of input sentence : sequence - based models and tree - structured models .,0
19481,Sequence - based models construct sentence representations from word sequences by taking in account the relationship between successive words .,0
19482,Tree- structured models treat each word token as anode in a syntactic parse tree and learn sentence representations from leaves to the root in a recursive manner .,0
19483,Convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) have emerged as two widely used architectures and are often combined with sequence - based or tree - structured models .,0
19484,"Owing to the capability of capturing local correlations of spatial or temporal structures , CNNs have achieved top performance in computer vision , speech recognition and NLP .",0
19485,"For sentence modeling , CNNs perform excellently in extracting n-gram features at different positions of a sentence through convolutional filters , and can learn short and long - range relations through pooling operations .",0
19486,CNNs have been successfully combined with both sequence - based model and tree - structured model in sentence modeling .,0
19487,The other popular neural network architecture - RNN - is able to handle sequences of any length and capture long - term dependencies .,0
19488,"To avoid the problem of gradient exploding or vanishing in the standard RNN , Long Short - term Memory RNN ( LSTM ) and other variants were designed for better remembering and memory accesses .",0
19489,"Along with the sequence - based or the tree - structured models , RNNs have achieved remarkable results in sentence or document modeling .",0
19490,"To conclude , CNN is able to learn local response from temporal or spatial data but lacks the ability of learning sequential correlations ; on the other hand , RNN is specilized for sequential modelling but unable to extract features in a parallel way .",0
19491,"It has been shown that higher - level modeling of x t can help to disentangle underlying factors of variation within the input , which should then make it easier to learn temporal structure between successive time steps .",0
19492,"For example , Sainath et al. have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs .",0
19493,We explore training the LSTM model directly from sequences of higherlevel representaions while preserving the sequence order of these representaions .,0
19494,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",1
19495,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .",1
19496,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,1
19497,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",1
19498,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .",0
19499,"We choose sequence - based input other than relying on the syntactic parse trees before feeding in the neural network , thus our model does n't rely on any external language knowledge and complicated pre-processing .",1
19500,"In our experiments , we evaluate the semantic sentence representations learned from C - LSTM with two tasks : sentiment classification and 6 - way question classification .",0
19501,Our evaluations show that the C - LSTM model can achieve excellent results with several benchmarks as compared with a wide range of baseline models .,0
19502,"We also show that the combination of CNN and LSTM outperforms individual multi - layer CNN models and RNN models , which indicates that LSTM can learn longterm dependencies from sequences of higher - level representations better than the other models .",0
19503,Related Work,0
19504,"Deep learning based neural network models have achieved great success in many NLP tasks , including learning distributed word , sentence and document representation , statistical machine translation , sentiment classification , etc .",0
19505,"Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , text categorization .",0
19506,"In many recent sentence representation learning works , neural network models are constructed upon either the input word sequences or the transformed syntactic parse tree .",0
19507,"Among them , convolutional neural network ( CNN ) and recurrent neural network ( RNN ) are two popular ones .",0
19508,The capability of capturing local correlations along with extracting higher - level correlations through pooling empowers CNN to model sentences naturally from consecutive context windows .,0
19509,"In , Collobert et al. applied convolutional filters to successive windows for a given sequence to extract global features by max - pooling .",0
19510,"As a slight variant , proposed a CNN architecture with multiple filters ( with a varying window size ) and two ' channels ' of word vectors .",0
19511,"To capture word relations of varying sizes , proposed a dynamic k-max pooling mechanism .",0
19512,"In a more recent work , Tao et al .",0
19513,apply tensor - based operations between words to replace linear operations on concatenated word vectors in the standard convolutional layer and explore the non-linear interactions between nonconsective n-grams .,0
19514,also explores convolutional models on tree - structured sentences .,0
19515,"As a sequence model , RNN is able to deal with variable - length input sequences and discover long - term dependencies .",0
19516,Various variants of RNN have been proposed to better store and access memories .,0
19517,"With the ability of explicitly modeling time - series data , RNNs are being increasingly applied to sentence modeling .",0
19518,"For example , adjusted the standard LSTM to tree - structured topologies and obtained superior results over a sequential LSTM on related tasks .",0
19519,"In this paper , we stack CNN and LSTM in a unified architecture for semantic sentence modeling .",0
19520,The combination of CNN and LSTM can be seen in some computer vision tasks like image caption and speech recognition .,0
19521,Most of these models use multi - layer CNNs and train CNNs and RNNs separately or throw the output of a fully connected layer of CNN into RNN as inputs .,0
19522,"Our approach is different : we apply CNN to text data and feed consecutive window features directly to LSTM , and so our architecture enables LSTM to learn long - range dependencies from higher - order sequential features .",0
19523,"In , the authors suggest that sequence - based models are sufficient to capture the compositional semantics for many NLP tasks , thus in this work the CNN is directly built upon word sequences other than the syntactic parse tree .",0
19524,Our experiments on sentiment classification and 6 - way question classification tasks clearly demonstrate the superiority of our model over single CNN or LSTM model and other related sequence - based models .,0
19525,C- LSTM,0
19526,Model,0
19527,"The architecture of the C - LSTM model is shown in , which consists of two main components : convolutional neural network ( CNN ) and long shortterm memory network ( LSTM ) .",0
19528,The following two subsections describe how we apply CNN to extract higher - level sequences of word features and LSTM to capture long - term dependencies over window feature sequences respectively .,0
19529,N- gram Feature Extraction through Convolution,0
19530,The one - dimensional convolution involves a filter vector sliding over a sequence and detecting features at different positions .,0
19531,Let xi ?,0
19532,Rd be the d-dimensional word vectors for the i - th word in a sentence .,0
19533,Let x ?,0
19534,R,0
19535,Ld denote the input sentence where L is the length of the sentence .,0
19536,"Let k be the length of the filter , and the vector m ?",0
19537,R kd is a filter for the convolution operation .,0
19538,"For each position j in the sentence , we have a window vector w j with k consecutive word vectors , denoted as :",0
19539,"Here , the commas represent row vector concatenation .",0
19540,A filter m convolves with the window vectors ( k - grams ) at each position in a valid way to generate a feature map c ?,0
19541,R L?k+1 ; each element c j of the feature map for window vector w j is produced as follows :,0
19542,"where is element - wise multiplication , b ?",0
19543,"R is a bias term and f is a nonlinear transformation function that can be sigmoid , hyperbolic tangent , etc .",0
19544,"In our case , we choose ReLU ( Nair and Hinton , 2010 ) as the nonlinear function .",0
19545,The C - LSTM model uses multiple filters to generate multiple feature maps .,0
19546,"For n filters with the same length , the generated n feature maps can be rearranged as feature representations for each window w j ,",0
19547,"Here , semicolons represent column vector concatenation and c i is the feature map generated with the i - th filter .",0
19548,Each row W j of W ? R ( L?k+ 1 ) n is the new feature representation generated from n filters for the window vector at position j.,0
19549,The new successive higher - order window representations then are fed into LSTM which is described below .,0
19550,A max - over - pooling or dynamic k-max pooling is often applied to feature maps after the convolution to select the most or the k-most important features .,0
19551,"However , LSTM is specified for sequence input , and pooling will break such sequence organization due to the discontinuous selected features .",0
19552,"Since we stack an LSTM neural neural network on top of the CNN , we will not apply pooling after the convolution operation .",0
19553,Long Short - Term Memory Networks,0
19554,Recurrent neural networks ( RNNs ) are able to propagate historical information via a chain - like neural network architecture .,0
19555,"While processing sequential data , it looks at the current input x t as well as the previous output of hidden state h t?1 at each time step .",0
19556,"However , standard RNNs becomes unable to learn long - term dependencies as the gap between two time steps becomes large .",0
19557,"To address this issue , LSTM was first introduced in ) and reemerged as a successful architecture since obtained remarkable performance in statistical machine translation .",0
19558,"Although many variants of LSTM were proposed , we adopt the standard architecture in this work .",0
19559,The LSTM architecture has a range of repeated modules for each time step as in a standard RNN .,0
19560,"At each time step , the output of the module is controlled by a set of gates in Rd as a function of the old hidden state h t?1 and the input at the current time step x t : the forget gate ft , the input gate it , and the output gate o t .",0
19561,These gates collectively decide how to update the current memory cell ct and the current hidden state ht .,0
19562,We used to denote the memory dimension in the LSTM and all vectors in this architecture share the same dimension .,0
19563,The LSTM transition functions are defined as follows :,0
19564,"Here , ?",0
19565,"is the logistic sigmoid function that has an output in [ 0 , 1 ] , tanh denotes the hyperbolic tangent function that has an output in [ ? 1 , 1 ] , and ?",0
19566,denotes the elementwise multiplication .,0
19567,"To understand the mechanism behind the architecture , we can view ft as the function to control to what extent the information from the old memory cell is going to be thrown away , it to control how much new information is going to be stored in the current memory cell , and o t to control what to output based on the memory cell ct .",0
19568,"LSTM is explicitly designed for time - series data for learning long - term dependencies , and therefore we choose LSTM upon the convolution layer to learn such dependencies in the sequence of higher - level features .",0
19569,Learning C- LSTM for Text Classification,0
19570,"For text classification , we regard the output of the hidden state at the last time step of LSTM as the document representation and we add a softmax layer on top .",0
19571,We train the entire model by minimizing the cross - entropy error .,0
19572,"Given a training sample x ( i ) and it s true label y ( i ) ? { 1 , 2 , , k} where k is the number of possible labels and the estimated probabilities y ( i ) j ?",0
19573,"[ 0 , 1 ] for each label j ? { 1 , 2 , , k} , the error is defined as :",0
19574,"where 1 { condition } is an indicator such that 1 { condition is true } = 1 otherwise 1 { condition is false } = 0 . We employ stochastic gradient descent ( SGD ) to learn the model parameters and adopt the optimizer RM - Sprop ( Tieleman and Hinton , 2012 ) .",0
19575,Padding and Word Vector Initialization,0
19576,"First , we use maxlen to denote the maximum length of the sentence in the training set .",0
19577,"As the convolution layer in our model requires fixed - length input , we pad each sentence that has a length less than maxlen with special symbols at the end that indicate the unknown words .",0
19578,"For a sentence in the test dataset , we pad sentences thatare shorter than maxlen in the same way , but for sentences that have a length longer than maxlen , we simply cut extra words at the end of these sentences to reach maxlen .",0
19579,We initialize word vectors with the publicly available word2vec vectors 1 thatare pre-trained using about 100B words from the Google News Dataset .,0
19580,The dimensionality of the word vectors is 300 .,0
19581,"We also initialize the word vector for the unknown words from the uniform distribution [ - 0.25 , 0.25 ] .",0
19582,We then fine - tune the word vectors along with other model parameters during training .,0
19583,Regularization,0
19584,"For regularization , we employ two commonly used techniques : dropout ( Hinton et al. , 2012 ) and L2 weight regularization .",0
19585,We apply dropout to prevent co-adaptation .,0
19586,"In our model , we either apply dropout to word vectors before feeding the sequence of words into the convolutional layer or to the output of LSTM before the softmax layer .",0
19587,The L2 regularization is applied to the weight of the softmax layer .,0
19588,Experiments,0
19589,"We evaluate the C - LSTM model on two tasks : ( 1 ) sentiment classification , and ( 2 ) question type classification .",0
19590,"In this section , we introduce the datasets and the experimental settings .",0
19591,Datasets,0
19592,Sentiment Classification :,0
19593,Our task in this regard is to predict the sentiment polarity of movie reviews .,0
19594,We use the Stanford Sentiment Treebank ( SST ) benchmark .,0
19595,"This dataset consists of 11855 movie reviews and are split into train ( 8544 ) , dev ( 1101 ) , and test ( 2210 ) .",0
19596,"Sentences in this corpus are parsed and all phrases along with the sentences are fully annotated with 5 labels : very positive , positive , neural , negative , very negative .",0
19597,We consider two classification tasks on this dataset : fine - grained classification with 5 labels and binary classification by removing neural labels .,0
19598,"For the binary classification , the dataset has a split of train ( 6920 ) / dev ( 872 ) / test ( 1821 ) .",0
19599,"Since the data is provided in the format of sub-sentences , we train the model on both phrases and sentences but only test on the sentences as in several previous works .",0
19600,Question type classification :,0
19601,"Question classification is an important step in a question answering system that classifies a question into a specific type , e.g.",0
19602,""" what is the highest waterfall in the United States ? "" is a question that belongs to "" location "" .",0
19603,"For this task , we use the benchmark TREC .",0
19604,"TREC divides all questions into 6 categories , including location , human , entity , abbreviation , description and numeric .",0
19605,The training dataset contains 5452 labelled questions while the testing dataset contains 500 questions .,0
19606,Experimental Settings,0
19607,"We implement our model based on Theano ) - a python library , which supports efficient symbolic differentiation and transparent use of a GPU .",1
19608,"To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .",1
19609,"For text preprocessing , we only convert all characters in the dataset to lowercase .",0
19610,"For SST , we conduct hyperparameter ( number of filters , filter length in CNN ; memory dimension in LSTM ; dropout rate and which layer to apply , etc. ) tuning on the validation data in the standard split .",0
19611,"For TREC , we holdout 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data .",0
19612,"In our final settings , we only use one convolutional layer and one LSTM layer for both tasks .",1
19613,"For the filter size , we investigated filter lengths of 2 , 3 and 4 in two cases : a ) single convolutional layer with the same filter length , and b ) multiple convolutional layers with different lengths of filters in parallel .",0
19614,Here we denote the number of filters of length i by n i for ease of clarification .,0
19615,"For the first case , each n-gram window is transformed into n i convoluted :",0
19616,Comparisons with baseline models on Stanford Sentiment Treebank .,0
19617,Fine - grained is a 5 - class classification task .,0
19618,Binary is a 2 - classification task .,0
19619,The second block contains the recursive models .,0
19620,The third block are methods related to convolutional neural networks .,0
19621,The fourth block contains methods using LSTM ( the first two methods in this block also use syntactic parsing trees ) .,0
19622,The first block contains other baseline methods .,0
19623,The last block is our model .,0
19624,features after convolution and the sequence of window representations is fed into LSTM .,0
19625,"For the latter case , since the number of windows generated from each convolution layer varies when the filter length varies ( see L ? k + 1 below equation ) , we cut the window sequence at the end based on the maximum filter length that gives the shortest number of windows .",0
19626,Each window is represented as the concatenation of outputs from different convolutional layers .,0
19627,We also exploit different combinations of different filter lengths .,0
19628,We further present experimental analysis of the exploration on filter size later .,0
19629,"According to the experiments , we choose a single convolutional layer with filter length",0
19630,"3 . For SST , the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150 , too .",0
19631,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,0
19632,"For TREC , the number of filters is set to be 300 and the memory dimension is set to be 300 .",1
19633,The word vector layer and the LSTM layer are dropped outwith a probability of 0.5 .,1
19634,We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks .,1
19635,Results and Model Analysis,1
19636,"In this section , we show our evaluation results on sentiment classification and question type classification tasks .",0
19637,"Moreover , we give some model analysis on the filter size configuration .",0
19638,Sentiment Classification,1
19639,The results are shown in .,0
19640,We compare our model with a large set of well - performed models on the Stanford Sentiment Treebank .,0
19641,"Generally , the baseline models consist of recursive models , convolutional neural network models , LSTM related models and others .",0
19642,The recursive models employ a syntactic parse tree as the sentence structure and the sentence representation is computed recursively in a bottom - up manner along the parse tree .,0
19643,"Under this category , we choose recursive autoencoder ( RAE ) , matrix - vector ( MV - RNN ) , tensor based composition ( RNTN ) and multi -layer stacked ( DRNN ) recursive neural network as baselines .",0
19644,"Among CNNs , we compare with ing , Tao 's CNN ( Molding - CNN ) with low - rank tensor based non-linear and non-consecutive convolutions .",0
19645,"Among LSTM related models , we first compare with two tree - structured LSTM models ( Dependence Tree - LSTM and Constituency Tree - LSTM ) that adjust LSTM to tree - structured network topologies .",0
19646,Then we implement one - layer LSTM and Bi - LSTM by ourselves .,0
19647,"Since we could not tune the result of Bi - LSTM to be as good as what has been reported in even if following their untied weight configuration , we report our own results .",0
19648,"For other baseline methods , we compare against SVM with unigram and bigram features , NBoW with average word vector features and paragraph vector that infers the new paragraph vector for unseen documents .",0
19649,"To the best of our knowledge , we achieve the fourth best published result for the 5 - class classification task on this dataset .",1
19650,"For the binary classification task , we achieve comparable results with respect to the state - of - the - art ones .",1
19651,"From , we have the following observations :",0
19652,"( 1 ) Although we did not beat the state - of - the - art ones , as an endto - end model , the result is still promising and comparable with thoes models that heavily rely on linguistic annotations and knowledge , especially syntactic parse trees .",0
19653,This indicates C - LSTM will be more feasible for various scenarios .,0
19654,( 2 ) Comparing our results against single CNN and LSTM models shows that LSTM does learn long - term dependencies across sequences of higher - level representations better .,0
19655,We could explore in the future how to learn more compact higher - level representations by replacing standard convolution with other non-linear feature mapping functions or appealing to tree - structured topologies before the convolutional layer .,0
19656,Question Type Classification,1
19657,The prediction accuracy on TREC question classification is reported in .,0
19658,We compare our model with a variety of models .,0
19659,"The SVM classifier uses unigrams , bigrams , wh-word , headword , POS tags , parser , hypernyms , Word Net synsets as engineered features and 60 hand - coded rules .",0
19660,Ada - CNN is a self - adaptiive hierarchical sentence model with gating networks .,0
19661,Other baseline models have been introduced in the last task .,0
19662,"From , we have the following observations :",0
19663,"( 1 ) Our result consistently outperforms all published neural baseline models , which means that C - LSTM captures intentions of TREC questions well .",1
19664,( 2 ) Our result is close to that of the state - of - the - art SVM that depends on highly engineered features .,1
19665,"Such engineered features not only demands human laboring but also leads to the error propagation in the existing NLP tools , thus could n't generalize well in other datasets and tasks .",0
19666,"With the ability of automatically learning semantic sentence representations , C - LSTM does n't require any human - designed features and has a better scalibility .",0
19667,Model Analysis,0
19668,Here we investigate the impact of different filter configurations in the convolutional layer on the model performance .,0
19669,"In the convolutional layer of our model , filters are used to capture local n-gram features .",0
19670,"Intuitively , multiple convolutional layers in parallel with differ - ent filter sizes should perform better than single convolutional layers with the same length filters in that different filter sizes could exploit features of different n-grams .",0
19671,"However , we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases .",1
19672,We show in the prediction accuracies on the 6 - way question classification task using different filter configurations .,0
19673,Note that we also observe the similar phenomenon in the sentiment classification task .,0
19674,"For each filter configuration , we report in the best result under extensive grid - search on hyperparameters .",0
19675,It it shown that single convolutional layer with filter length 3 performs best among all filter configurations .,1
19676,"For the case of multiple convolutional layers in parallel , it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters , which further confirms that tri-gram features do play a significant role in capturing local features in our tasks .",1
19677,We conjecture that LSTM could learn better semantic sentence representations from sequences of tri-gram features .,0
19678,Conclusion and Future Work,0
19679,"We have described a novel , unified model called C - LSTM that combines convolutional neural network with long short - term memory network ( LSTM ) .",0
19680,C - LSTM is able to learn phrase - level features through a convolutional layer ; sequences of such higherlevel representations are then fed into the LSTM to learn long - term dependencies .,0
19681,We evaluated the learned semantic sentence representations on sentiment classification and question type classification tasks with very satisfactory results .,0
19682,We could explore in the future ways to replace the standard convolution with tensor - based operations or tree - structured convolutions .,0
19683,We believe LSTM will benefit from more structured higher - level representations .,0
19684,title,0
19685,Task - oriented Word Embedding for Text Classification,1
19686,abstract,0
19687,Distributed word representation plays a pivotal role in various natural language processing tasks .,0
19688,"In spite of its success , most existing methods only consider contextual information , which is suboptimal when used in various tasks due to alack of task - specific features .",0
19689,The rational word embeddings should have the ability to capture both the semantic features and task - specific features of words .,0
19690,"In this paper , we propose a task - oriented word embedding method and apply it to the text classification task .",0
19691,"With the function - aware component , our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary .",0
19692,We evaluate our method using five text classification datasets .,0
19693,The experiment results show that our method significantly outperforms the state - of - the - art methods .,0
19694,* Corresponding author,0
19695,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,0
19696,Licence details : http://creativecommons.org/licenses/by/4.0/,0
19697,Text Classification Expected Distribution Word Embeddings Actual Distribution,0
19698,AI : a combination of active learning and self learning for named entity recognition on twitter using conditional random fields learning :,0
19699,Law : we assess the relationship between legal origin and a range of correlated indicators of social responsibility,0
19700,Introduction,0
19701,Learning word representation is a fundamental step in various natural language processing tasks .,0
19702,"Tremendous advances have been made by distributed representations ( also known as word embeddings ) which learn a transformation of each word from raw text data to a dense , lower - dimensional vector space .",0
19703,"Most existing methods leverage contextual information from the corpus and other complementary information , such as subword information , implicitly syntactic dependencies , and semantic relations .",0
19704,"In traditional evaluations such as word similarity and word analogy , the aforementioned context - aware word embeddings work well since semantic information plays a vital role in these tasks , and this information is naturally addressed byword contexts .",0
19705,"However , in real - world applications , such as text classification and information retrieval , word contexts alone are insufficient to achieve success in the absence of task - specific features .",0
19706,illustrates this problem with the classification task as an example .,0
19707,Several sentences from different categories are given at the far left of the figure where the words in bold are salient words for the category distinction .,0
19708,We also illustrated expected word distribution of these salient words in the embedding space .,0
19709,"To obtain a good classification performance , the expected word distribution should have a clear classification boundary : words within the same category are close to each other and faraway from words in other categories as illustrated in .",0
19710,"However , the actual distribution obtained from Word2 Vec at the far right of is normally not satisfactory because Word2 Vec only focuses on context similarity .",0
19711,"For example , although learning and educational are with similar context as recognized by Word2 Vec , they are salient words to distinguish categories of AI and Sociology , so they should be faraway from each other .",0
19712,"Apparently , using word embedding directly from Word2 Vec would not obtain good performance on the text classification task due to the fact that words ' functional features in the real tasks are ignored in the training process .",0
19713,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .",1
19714,It learns the distributed representation of words according to the given specific :,0
19715,The example sentences from the text classification dataset .,0
19716,Words in bold are salient words to distinguish the sentence category .,0
19717,Their most similar words in the Word2 Vec space are shown in the right - hand column .,0
19718,"The word color indicates the category , and the words in black are general words for the task .",0
19719,NLP task .,0
19720,"Specifically , we focus on text classification .",0
19721,"In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .",1
19722,"In the joint learning framework , the contextual information is captured following the context prediction task introduced by .",0
19723,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .",1
19724,"To give an intuitive understanding on how our method works from the classification perspective , we design a 5 Abstracts",0
19725,Group dataset ( detailed in Section 4.1 ) and conduct a qualitative analysis .,0
19726,Experiments show qualitative improvements of our method over context - based Skip - gram method on word neighbors for classification .,0
19727,"We also perform empirical comparisons on five text classification datasets , which demonstrate the effectiveness of our method over the other state - of the - art methods .",0
19728,The contributions of this paper can be summarized as following :,0
19729,We propose a task - oriented word embedding method that is specially designed for text classification .,0
19730,It introduces the function - aware component and highlights word 's functional attributes in the embedding space by regularizing the distribution of words to have a clear classification boundary .,0
19731,We design a 5 Abstracts,0
19732,"Group dataset and present a qualitative analysis , giving an intuitive understanding on how our method works from the classification perspective .",0
19733,Experimental results on five text classification datasets also show that the proposed method is more optimal for classification on account of revealing functional attributes of words .,0
19734,Related Work,0
19735,Word embeddings that provide continuous low - dimensional vector representations of words have been widely studied by NLP communities .,0
19736,The last few years have seen the development of word embedding methods purely based on the co-occurrence information in a corpus .,0
19737,Some studies also pay attention to the semantic knowledge stored in the knowledge bases .,0
19738,"For example , refine word representations using relational information from semantic lexicons , represent semantic knowledge as a number of ordinal similarity inequalities of related word pairs to learn semantic word embeddings .",0
19739,Recent works have thrown light on the problems associated with directly applying word embeddings into real - world applications .,0
19740,demonstrated that the globally trained word embedding underperform corpus and query - specific embeddings for retrieval tasks .,0
19741,They proposed locally training word embeddings in a query - specific manner for the query expansion task .,0
19742,"indicated that the underlying assumption in typical word embedding methods is not equal to the need of IR tasks , and they proposed relevance - based models to learn word representations based on querydocument relevance information , which is the primary objective of most IR task .",0
19743,"For the sentiment analysis task , refined word embedding to avoid generating similar vector representations for sentimentally opposite words .",0
19744,"For the contradiction detection task , developed contradiction - specific word embedding to recognize contradiction relations between a pair of sentences .",0
19745,"These studies show that general trained word embeddings can not be optimized for a specific task , thus , they are likely to be suboptimal .",0
19746,"To meet the needs of real - world applications , rational word embeddings should have the ability to capture both the semantics of words and the task - specific features of words .",0
19747,"In this work , we focus on task - oriented word embedding for the text classification task .",0
19748,Several attempts have shown that revised word embeddings can boost the performance of classification .,0
19749,"For example , topical information is shown to be effective in generating high quality word embeddings , which can enhance the performance of text classification .",0
19750,"On the other hand , to enhance the performance of sentiment classification , Chih et al. proposed a word embedding refinement model to refine existing semantically oriented word vectors using sentiment lexicons in order to distinguish words with similar vector representations but opposite sentiment polarities .",0
19751,Our work departs from previous work in that it directly models task - specific features to construct the embedding space with a clear boundary for classification .,0
19752,Method,0
19753,"Given the unlabeled corpus C and the labeled training set D = { D 1 , D 2 , , D g } with g text categories , our method aims to train the task - oriented d-dimensional word embedding w i ?",0
19754,Rd for the i - th word w i in vocabulary V .,0
19755,"Formally , the document collection belonging to the k -th category is denoted as D k .",0
19756,"Our proposed joint learning framework contains two components , i.e. , the context - aware component and function - aware component .",0
19757,The context - aware part models the co-occurrence in corpus C and captures the word semantic features .,0
19758,The function - aware part reveals the word 's functional attributes following the task - specific features observed in D .,0
19759,We next describe these two parts respectively .,0
19760,Context - aware Component,0
19761,Our method uses the Word2 Vec method to model the context information and uses log - linear models to produce word embeddings .,0
19762,It applies a sliding window moving on the corpus .,0
19763,The word in the center of the window is the target word and the others are context words .,0
19764,"Word2 Vec has two versions , i.e. , CBOW and Skip - gram .",0
19765,"The CBOW model uses the average / sum of context words as input to predict the target , and the Skip - gram model uses the target word as input to predict each context word .",0
19766,"To simplify , we represent the objective of each prediction as",0
19767,.,0
19768,( 1 ),0
19769,"In CBOW , w is the target word , and c is the vector of the context words , and in Skip - gram , w is each word in the context , and c is the vector of the target word .",0
19770,Function - aware Component,0
19771,"In the function - aware component , we define salient words as those words with the ability to distinguish the document category .",0
19772,These salient words are first extracted from the labeled training set D in an offline process .,0
19773,Then the correlations among these words are used to model the functional features in the embedding space .,0
19774,Each salient word w of the k -th category is offline extracted according to the following the two principles :,0
19775,"( 1 ) The term frequency of the word win this category ( i.e. , D k ) is much higher than that in other categories ; ( 2 ) w is common in other categories , expressed as a small variance of term frequencies in other categories .",0
19776,"Formally , we design the following formula to measure the importance of word w to the k -th category as a salient word :",0
19777,"where ti is the term frequency in the i - th category , T ?k ( w ) is the collection of term frequencies except the k - th category ( i.e. , T ?k ( w ) = {t j | 1 ? j ? g , j = k} ) , and var ( ) is the variance .",0
19778,"According to this importance score , we generate a salience words set by selecting the top N words for each category , denoted as S k = {w j | 1 ? j ? N } , k ?",0
19779,"[ 1 , g] .",0
19780,"Then , for the task , the words in the salient words set S = { S 1 , S 2 , , S g } have the ability to distinguish different categories .",0
19781,The salient words are next utilized to capture the functional relations between words in the embedding space .,0
19782,"In the learning framework , if the predicted word w is in S , the function - aware component will be activated .",0
19783,"As to modeling the correlations of function - salient words , we expect to constrain w to be close to the words in the same category and faraway from the words in different categories .",0
19784,"According to this idea , we construct a set P ( w ) with n word - pairs for each salient word w .",0
19785,Each word - pair contains a positive word u and a negative word v.,0
19786,"The positive words are randomly selected from S which belong to the same category with w , and the negative words are randomly sampled from other categories .",0
19787,We maximize a margin - based ranking criterion over the training set S:,0
19788,where ?,0
19789,"is a margin hyper parameter , n is the size of sample set P ( w ) , and s ( , ) is similarity measure .",0
19790,"Following the recommendations in prior work on word similarity measurement , we apply the cosine similarity of a pair of words by computing s ( a , b ) = ab | a | | b | .",0
19791,"The objective function favors higher values of the similarity for positive word - pairs than for negative word - pairs , and is thus a natural implementation of the intended criterion .",0
19792,Joint Learning,0
19793,"The context - aware component and the function - aware component are jointly optimized , so we then obtain the following object function :",0
19794,where ?,0
19795,"is a set of all parameters in L context and L function , and ?",0
19796,is the combination parameter which balances the contribution of each component in the training process .,0
19797,The goal of the training objective is to maximize L with respect to the model parameters .,0
19798,The optimization process is conducted via Stochastic Gradient Descent ( SGD ) .,0
19799,The optimization of L context follows the negative sampling introduced in .,0
19800,"If the predicted word w is in the salient words set S , the corresponding optimization process for L function will be activated , and the parameters are updated as w ? w + ? ? L ?w , u ? u + ? ? L ?u , and v ? v + ? ?L ?v , where ?",0
19801,"is the learning rate , and the gradients are calculated as follows :",0
19802,"where w is the predicted word , u is its positive word and v is its negative word .",0
19803,"Since we apply cosine distance to compute the similarity between two words , the optimization can be derived as follows :",0
19804,Algorithm 1 Task - oriented Word Embedding Method .,0
19805,Input : Corpus,0
19806,"C , the labeled training set D with g categories , dimensionality d , sampling times n , and word vocabulary V Output : Embeddings w ?",0
19807,Rd of all words in the vocabulary V .,0
19808,Initialization : randomly set w ?,0
19809,"Rd for all words in V ; generate the salient words set S ; constructing T prediction tasks using a sliding window . fort = 1 , 2 , . . . , T do optimize L context using negative sample method introduced in if win S then for n do sampling the positive word u and the negative word v. optimize L function using Eq. ( 5 ) to update w , u , v. end for end if end for return w for all words in V .",0
19810,"where S a , b = ab | a| | b | .",0
19811,"The pseudo code for our word embedding method is shown in Algorithm 1 , and the source code is available on the Github 1 .",0
19812,Experiments,0
19813,Datasets,0
19814,"To undertake an extensive evaluation , we investigate the empirical performances of our proposed method on five text classification datasets .",0
19815,"The detailed statistics of all the datasets are listed in ( 1 ) The 20 NewsGroup 2 is a popular text classification dataset which contains 18,846 documents from 20 different newsgroups .",0
19816,Each document contains several sentences .,0
19817,"The dataset is separated into a training set of 11,314 documents and a test set of 7,532 documents .",0
19818,"( 2 ) The 5 Abstracts Group dataset is academic papers from five different domains collected from the Web of Science namely , business , artificial intelligence , sociology , transport and law .",0
19819,We extracted the abstract and title fields of each paper as a document .,0
19820,"The dataset contains 6,256 documents , and we randomly selected 500 papers in each category as the training set , and the others as the test set .",0
19821,The dataset is published on the Github 3 .,0
19822,"( 3 ) The IMDB 4 contains movie reviews with binary classes ( i.e. , positive and negative ) .",0
19823,"It consists of 50,000 movie reviews , and each movie review has several sentences .",0
19824,( 4 ) The MR 5 dataset consists of movie reviews from Rotten Tomato website with two classes labeled by .,0
19825,Each review contains only one sentence .,0
19826,( 5 ) The SST 6 dataset contains the movie reviews in the Stanford Sentiment Treebank labeled by comprising one sentence for each review .,0
19827,50 % of the MR and SST datasets are partitioned randomly into the training set and 50 % into the test set .,0
19828,Baseline Methods,0
19829,"To evaluate our method , we consider the following baselines : ( 1 ) the BOW method is employed as a basic baseline .",1
19830,It represents each document as a bag of words and the weighting scheme is TFIDF .,1
19831,"We select the top 2,000 words according to the TFIDF scores as features ;",0
19832,( 2 ) the Word2 Vec method is a neural network language method which learns word embeddings by maximizing the conditional probability leveraging contextual information .,1
19833,"It comprises two models , i.e. , CBOW which predicts the target word using context information , and the Skip - gram ( denoted as SG ) which predicts each context word using the target word ; ( 3 ) the Glo Ve method is a state - of - the - art matrix factorization method .",0
19834,"It leverages global count information aggregated from the entire corpus as wordword occurrence matrix to learn word embeddings ; ( 4 ) the Topical Word Embedding method ( denoted as TWE ) ) learns a topic model from the training set , then generates word embeddings by jointly considering words and topics in a neural network ; ( 5 ) the Retrofit method is a popular method that refines pre-trained word embeddings using relational information from the knowledge base ( e.g. , WordNet used in our experiments ) .",0
19835,Experimental Settings,0
19836,"In this paper , we use the text classification task to evaluate the performance of word embeddings .",0
19837,"Word embeddings are used to construct the document embeddings d by simply averaging all word embeddings in the given document , i.e. , d = 1 |d | w?d w , where w is a word in document d .",0
19838,"We regard document embedding as a document feature and trained a linear classifier using Liblinear 7 , since the feature size is large , and Liblinear can quickly train a linear classifier with high dimension features .",0
19839,The classifier is then used to predict the class labels of documents in the test set .,0
19840,"The multi-group classification performance was evaluated in terms of four measures : accuracy ( Acc. ) , precision ( Prec. ) , recall ( Rec. ) and F - measure ( F1 ) , and the binary classification performance was evaluated by accuracy ( Acc. ) .",0
19841,All the measures are computed by averaging the metrics of each class and are weighted by the number of true instances for each class .,0
19842,"For each dataset , all documents are joined together as a corpus for embedding training .",0
19843,"We tokenized the corpus with the Stanford Tokenizer 8 and converted it to lowercase , then removed the stop words .",0
19844,"For a fair comparison , all word embeddings adhere to the following settings : the dimensionality of vectors is 300 , the size of the context window is 5 , the number of negative samples is 25 .",0
19845,"In our method , an offline process is used to extract salient word set S from labeled training set D.",0
19846,"To obtain an intuitive understanding of these salient words , we list the top ten words for each category in the 5 Abstracts",0
19847,Group dataset .,0
19848,The result is displayed in .,0
19849,"We vary parameter N ( detailed in section 3.2 ) in the range between 30 and 200 , and show the performance in .",0
19850,Our method achieves the best performance when N is set to 150 for the 5 Abstracts,0
19851,Group dataset .,0
19852,"If the value of N is too large , this may hinder the performance because too much noise will be involved .",0
19853,The recommended N is 150 with the constraint that the total size of S is under 1200 based on practical experience .,0
19854,"There are two hyper - parameters in our method , i.e. , the combination parameter ? in Eq. ( 4 ) and the size n of sample set P ( w ) in Eq. ( 3 ) .",0
19855,We carefully tune these parameters by fixing one and varying the other .,0
19856,The parameters corresponding to the best accuracy in 20 New s Group are used to report the final settings .,0
19857,"As shown in , the optimal values for ?",0
19858,"were tuned from 0 to 1 , with a step size of 0.1 .",0
19859,"The proposed method based on Skip - gram and CBOW reaches optimal performance when ? = 0.4 and ? = 0.3 , respectively .",0
19860,"We tuned the value for n from 50 to 300 , and the methods achieve the best performance when n = 150 .",0
19861,"We follow the optimal settings in this work , with recommended settings of ? ? ( 0.3 , 0.4 ) and n ? ( 100 , 150 ) .",0
19862,Overall Performance,0
19863,We compared our proposed method with the baseline methods .,0
19864,shows the evaluation results .,0
19865,"Based on the experiment results , we make several observations :",0
19866,"( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .",1
19867,"In particular , the ToWE - SG method significantly outperforms the other baselines on the 20 New s Group , 5 Abstract s Group , and MR .",1
19868,"This is mainly attributed to the task - specific modeling mechanism , which enables our models to capture functional features among words , therefore , it can more accurately distinguish classes .",0
19869,"( 2 ) The word embedding methods outperform the basic bag - of - words methods in most cases , indicating the superiority of distributed word representation over the one - hot representation .",1
19870,"Moreover , the manager ( Business ) layer ( AI ) congress ( Law ) poverty ( Sociology ) accident methods which integrate the abundant information discovered from the datasets ( i.e. , TWE and ToWE ) achieve better performance compared to those that only consider contextual information , such as GloVe , CBOW , and SG .",0
19871,This demonstrates the effectiveness of refining context - aware word embeddings with task information .,0
19872,( 3 ) The Retrofit method is the knowledge - base enhanced word embedding method .,0
19873,"Our method achieves better performance over Retrofit method , indicating that the task - specific features could be more effective compared with general semantic relations constructed by humans in the knowledge bases .",1
19874,"( 4 ) In sentence classification , such as the MR and SST datasets , it is obvious that TWE achieves a relatively lower performance .",0
19875,This observation shows that topical information enhanced word embedding does not accurately represent a short text .,0
19876,"Our method outperforms the TWE method on both the document - level and sentence - level tasks , which shows the stability and reliability of modeling taskspecific features in real - world applications .",1
19877,Case Study,0
19878,A case study was conducted to qualitatively analyze in - depth why task - oriented word embedding methods surpass typical context - aware word embedding methods .,0
19879,We selected several salient words from different categories in the 5 Abstracts,0
19880,"Group dataset , and then compared the top ten similar words obtained by ToWE - SG and SG , respectively .",0
19881,The results are displayed in .,0
19882,"We observe that the similar words selected by the ToWE - SG method belong to the same category , while the SG method may select words from different categories .",0
19883,"Taking the word manager as an example , the most similar words selected by ToWE - SG all belong to the Business category , whereas the SG method selects helps , create which can hardly be regarded as being in the Business category .",0
19884,This demonstrates that our method is capable of capturing a clear boundary in the embedding space .,0
19885,"For further investigation , we compared the classification performance of these two word embeddings in each category .",0
19886,"As shown in , ToWE - SG outperforms SG in all these categories .",0
19887,"This indicates that by forcing words in the same category to have similar representations , the classifier achieves better performance .",0
19888,Conclusion,0
19889,"In this paper , we proposed a novel approach for learning task - oriented word embedding , especially for the text classification task .",0
19890,"Instead of learning embedding vectors merely based on context information , we incorporate task - specific features into the training process in order to reveal the words functional attributes in the embedding space .",0
19891,The results of the experiments with different datasets show that the proposed method outperforms the existing state - of - the - art word embedding learning methods on text classification tasks .,0
19892,"In the future , we will study how to effectively construct the task - oriented word embeddings with the help of transferable task - features across domains .",0
19893,6 Acknowledgment,0
19894,title,0
19895,Practical Text Classification With Large Pre-Trained Language Models,1
19896,abstract,0
19897,Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .,0
19898,"We demonstrate that large - scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets , including those with label class imbalance and domain - specific context .",0
19899,"By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .",0
19900,"These results are competitive with state of the art models , including strong F 1 scores on difficult ( emotion ) categories such as Fear ( 0.73 ) , Disgust ( 0.77 ) and Anger ( 0.78 ) , as well as competitive results on rare categories such as Anticipation ( 0.42 ) and Surprise ( 0.37 ) .",0
19901,"Furthermore , we demonstrate our application on a real world text classification task .",0
19902,"We create a narrowly collected text dataset of real tweets on several topics , and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin .",0
19903,"We also perform a variety of additional studies , investigating properties of deep learning architectures , datasets and algorithms for achieving practical multidimensional sentiment classification .",0
19904,"Overall , we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on realworld sentiment classification .",0
19905,Introduction,0
19906,"Recent work has shown that language models - both RNN variants like the multiplicative LSTM ( m LSTM ) , as well as the attention - based Transformer network ) - can be trained efficiently over very large datasets , and that the resulting models can be transferred to downstream language understanding problems , often matching or exceeding the previous state of the art approaches on academic datasets .",0
19907,"However , how well do these models perform on practical text classification problems , with real world data ?",0
19908,"Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",0
19909,All rights reserved .,0
19910,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .",1
19911,"We examine our performance on these tasks , both against large academic datasets , and on an original text dataset that we compiled from social media messages about several specific topics , such as video games .",0
19912,We demonstrate that our approach matches the state of the art on the academic datasets without domain - specific training and without excessive hyper - parameter tuning .,0
19913,"Meanwhile on the social media dataset , our approach outperforms commercially available APIs by significant margins , even when those models are re-calibrated to the test set .",0
19914,"Furthermore , we notice that 1 ) the Transformer model generally out - performs the m LSTM model , especially when fine - tuning on multidimensional emotion classification , and 2 ) fine - tuning the model significantly improves performance on the emotion tasks , both for the m LSTM and the Transformer model .",0
19915,"We suggest that our approach creates models with good generalization to increasingly difficult text classification problems , and we offer ablation studies to demonstrate that effect .",0
19916,"It is difficult to fit a single model for text classification across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .",0
19917,"For example , words such as war and sick are not necessarily negative in the context of video games , which are significantly represented in our dataset .",0
19918,"By training a language model across a large text dataset , we expose our model to many contexts .",0
19919,Perhaps a small amount of downstream transfer is enough to choose the right context features for emotion classification in the appropriate setting .,0
19920,"Our work shows that unsupervised language modeling combined with finetuning offers a practical solution to specialized text classification problems , including those with large category class imbalance , and significant human label dis agreement .",0
19921,Background,0
19922,Supervised learning is difficult to apply to NLP problems because labels are expensive .,0
19923,"Following , and , we train unsupervised text models on large amounts of unlabelled text data , and transfer the model features to small supervised text problems .",0
19924,The supervised text classification problem used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .,0
19925,Some of these binary text examples are subtle .,0
19926,"Prior works show that unsupervised language models can learn nuanced features of text , such as word ordering and double negation , just from the underlying task of next - word prediction .",0
19927,"However , while this includes difficult examples , it does not necessarily represent sentiment on practical text problems .",0
19928,The source material ( professionally written movie reviews ) does not include colloquial language .,0
19929,The dataset excludes Neutral sentiment texts and those with weak directional sentiment .,0
19930,The dataset does not include dimensions of sentiment apart from Positive and Negative .,0
19931,Plutchik 's Wheel of Emotions,0
19932,We focus our multidimension emotion classification on Plutchik 's wheel of emotions .,0
19933,"This taxonomy , in use since 1979 , aims to classify human emotions as a combination of four dualities :",0
19934,"Joy - Sadness , Anger - Fear , Trust - Disgust , and Surprise - Anticipation .",0
19935,"According to the basic emotion model , while humans experience hundreds of emotions , some emotions are more fundamental than others .",0
19936,"The commercial general purpose emotion classification API that we compare against , IBM 's Watson 1 , offers classification scores for the Joy , Sadness , Fear , Disgust and Anger emotions - all present in Plutchik 's wheel (.",0
19937,Sem Eval Multidimension Emotion Dataset,0
19938,"The Se-m Eval Task 1:E - c problem ) offers a training set of 6,857 tweets , with binary labels for the eight Plutchik categories , plus Optimism , Pessimism , and Love .",0
19939,This dataset was created through a process of text selection and human labeling .,0
19940,We show our results on this dataset and compare it to the current state of the art performance .,0
19941,"While it is not possible to report rater agreement on these categories for the compilation of the dataset , the authors note that 2 out of 7 raters had to agree for a positive label to be applied , as requiring larger agreement caused a scarcity of .",0
19942,labels for some categories .,0
19943,This indicates that some of the categories had significant rater dis agreement between the human raters .,0
19944,"The dataset also included a substantial degree of label class imbalance , with some categories like Anger ( 37 % ) , Disgust ( 38 % ) , Joy ( 36 % ) and Sadness ( 29 % ) represented often in the dataset , while others like Trust ( 5 % ) and Surprise ( 5 % ) present much less frequently ) .",0
19945,This class imbalance and human rater dis agreement is not uncommon for real world text classification problems 2 .,0
19946,Company Tweet Dataset,0
19947,"In addition to the SemEval tweet dataset , we wanted to see how our model would perform on a similar but domain - specific task : Plutchik emotion classification on tweets relevant to a particular company .",0
19948,"We collected tweets on a variety of topics , including :",0
19949,Video game tweets Tweets about the company stock,0
19950,"We submitted the first batch of 4,000 tweets to human",0
19951,2 We submitted the SemEval training set for re-labeling using our rater instructions .,0
19952,"See for an estimate of rater dis agreement over the SemEval training set . 13,326 11.7 12.9 6.8 2.9 20.6 4.2 5.0 7.6 8.9/47.0 raters on the FigureEight 3 platform , with rules similar to those used by SemEval , which also used the FigureEight platform for human labeling .",0
19953,"Specifically , we verified that raters passed our golden set ( answering 70 % of test questions correctly ) .",0
19954,We applied positive labels for each category where 2 out of 5 raters agreed .,0
19955,"This is slightly less permissive than the 2 out of 7 raters used by SemEval , because we did not have a budget for 7 raters per tweet .",0
19956,"After the first pass , we noticed that random sampling led to some categories being severely under - sampled , below 5 % of tweets .",0
19957,"Thus we employed a bootstrapping technique to pre-classify tweets by category using our current model , and choose tweets with more likely emotion tweets for classification .",0
19958,See Active Learning section for details .,0
19959,"We also sampled 5,000 tweets balanced by source category , since video game tweets have much more emotion , thus dominated the bootstrapped selections .",0
19960,"Henceforth , we refer to the combined company tweets dataset consisting of : 4,021 random tweets 5,024 tweets selected for higher emotion content 4,281 tweets selected for source category balance Finetuning Recent work has shown promising results using unsupervised language modeling , followed by transfer learning to natural language tasks ( Radford , Jzefowicz , and Sutskever 2017 ) , ) .",0
19961,"Furthermore , these models benefit when the entire model is fine - tuned on the transfer task , as demonstrated in .",0
19962,"Specifically , these methods have beaten the state of the art on binary sentiment classification .",0
19963,"These models have also attained the best over all score on the GLUE Benchmark 4 ) , comprised of a variety of text understanding tasks , including entailment and question answering .",0
19964,Methodology,0
19965,"We use a larger batch size with shorter sequence length , specifically a global batch of 512 and sequence length 64 tokens ( tokenized with a 32,000 BPE vocabulary , as detailed in Characters and Subword Units .",0
19966,"The shorter sequence length works well because the transfer target are tweets , which are short pieces of text .",0
19967,"We trained our language model on the Amazon Reviews dataset rather than other large datasets like BooksCorpus , because reviews are rich in emotional context .",0
19968,"We also train an m LSTM network on the same dataset , based on the model from .",0
19969,We chose to compare these particular models because they work in fundamentally different ways and because they collectively hold state of the art results on many significant academic NLP benchmarks .,0
19970,We wanted to test these models on difficult classification problems with real - world data .,0
19971,Unsupervised Pretraining .,0
19972,The language modeling objective can be summarized as a maximum likelihood estimation problem for a sequence of tokens .,0
19973,We treat our model as a function with two parts : an encoder f e and decoder f d .,0
19974,"The encoder forms the bulk of the model , including the token embedding dictionary as the first module .",0
19975,The decoder is simply a softmax linear layer that projects the encoder output into the dimension equal to the vocabulary size .,0
19976,The objective to maximize is as follows .,0
19977,"where h l t is a hidden layer activation in the final layer off e , indexed 1 . . . l for timestep t.",0
19978,The model is tasked with predicting the next token given all of the ones prior by outputting a probability distribution over the vocabulary of tokens .,0
19979,"Doing this for each timestep t produces each term in the sum of the log - likelihood formulation , and so maximizing the correct probabilities is away to understand the joint probability distribution of sequences in this corpus of text .",0
19980,Characters and Subword Units .,0
19981,"While , and ) have shown state of the art results for language modeling and task transfer with character - level m LSTM models , we found that our ) uses a bytepair encoding vocabulary with 40,000 word pieces for their state of the art results on language transfer tasks with a Transformer model .",0
19982,Our work closely follows their model .,0
19983,Supervised Finetuning .,0
19984,"After the pretraining , we initialize a new decoder f d to be exclusively trained on the supervised problem .",0
19985,"Depending on the task , this decoder maybe a single linear layer with activation or an MLP .",0
19986,We also retain the original decoder f d and continue to train it by using language modeling as an auxiliary loss when finetuning on the new corpus .,0
19987,Error signals from both decoders are backpropagated into the language model .,0
19988,The differences between the hyperparameters for finetuning and language modeling are described in .,0
19989,ELMo Baseline,0
19990,"We also compare our language models to ELMo ) , a contextualized word representation based on a deep bidirectional language model , trained on large text corpus .",0
19991,We use a publicly available pretrained Multihead vs .,0
19992,Single Head Finetuning Decoders,0
19993,The tweet datasets are an example of a multilabel classification problem .,0
19994,"We can formulate the problem for the finetuning decoder , f d as either a collection of single binary problems or multiple problems put together .",0
19995,The single binary problem formulation allows for a focus on one class and end - to - end optimization will only have one error signal .,0
19996,"However , because the label classes are imbalanced in all categories , this may lead to a sparse gradient signal for the positive label , which may impact recall and precision .",0
19997,Increasing the size off d to more than one linear layer leads to rapid overfitting and lower validation performance .,0
19998,The combined binary problems formulation ( henceforth described as multihead ) allows for a richer error signal that propagates more information through the encoder f e and sentiment representation inf d .,0
19999,"In this setup , constructing a Multilayer network is far more useful , and can bethought of as specifically creating sentiment features to be used at the final layer to predict the presence of the individual emotions .",0
20000,"We find that the inclusion of easier , more balanced label categories improves performance on harder ones in .",0
20001,"However , the easier categories have slightly lower performance because the network is not being optimized for only those categories .",0
20002,Thresholding Supervised Results,0
20003,"For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .",0
20004,This makes sense since the label classes for most categories are very imbalanced .,0
20005,"For thresholding , we take a dataset of tweets and split it into training ( 70 % ) , thresholding ( 10 % ) and validation ( 20 % ) sets .",0
20006,"At each epoch of finetuning on the training set , we calculate validation accuracy and save predictions on the threshold set on the epoch for which this is maximized .",0
20007,"To threshold , we search the discretized version of [ 0 , 1 ] : the linear space T = { i 200 : 1 ? i ? 200 } for the positive label threshold for each category .",0
20008,We denoted the threshold which gave the best score on the threshold set as t * .,0
20009,"IBM Watson and Google NLP 6 both offer commercial APIs for binary sentiment analysis , producing scalar values that correspond to a continuous [ - 1 , + 1 ] sentiment score .",0
20010,We applied our thresholding procedure to these scores .,0
20011,In the case of classification with neutrals we create two thresholds 0 < t * 1 < t * 2 < 1 which we individually optimized jointly over T as well .,0
20012,"With the finetuning procedure , we found success with a decoder f d = MLP ( 64 , 2 ) , whose two output units ? p ,?",0
20013,"n are probability estimates of the positive and negative labels y p , y n .",0
20014,"These units both have sigmoid activations , since we denote a neutral as y p = y n =",0
20015,"0 . To threshold these predictions , we searched the cartesian product T T to determine 0 < t * p , t * n < 1 .",0
20016,Active Learning,0
20017,We hypothesized that we could achieve greater precision and recall on our datasets if our class label were more equally balanced .,0
20018,"To this end , we employed an active learning procedure to select unlabeled tweets to be labeled .",0
20019,"The algorithm consisted of first finetuning a language model f = ( f e , f d , f d ) on labeled tweets for 5 epochs .",0
20020,"At peak validation accuracy , we obtain predictions P ?",0
20021,"R 8 nu , for Plutchik sentiment on the unlabeled tweets .",0
20022,"From the labeled dataset , we calculate the negative class percentage for each category v ? R 8 .",0
20023,"Then we obtain category a weighting parameter w = 10 ( v ? 0.5 ) so that w i ? [ ? 5 , 5 ] for i ?",0
20024,1 . . .,0
20025,"8 . Then , we get scores for each unlabeled point as weighted features : s = e w P ?",0
20026,R nu .,0
20027,"This way , positive predictions for sentiment categories are weighted by how much they would contribute towards balancing all of the class distributions .",0
20028,"The scores s are used as weights in a weighted uniform random sampler , and from this , we sampled 5,000 tweets to be labeled .",0
20029,"We found that over all , the method produced tweets with more emotion .",0
20030,"Not only was the positive class balance averaged across label categories higher ( 11.2 % compared to 8.2 % for random sampling ) , but the percentage of tweets which had no emotion was dramatically lower : 35.6 % compared to 52.1 % for random sampling .",0
20031,We hence achieved better class balance than the dataset prior to the augmentation .,0
20032,Results,1
20033,Binary Sentiment Tweets,1
20034,"For binary sentiment , we compare our model on two tasks : the academic SST dataset , which consists of a balanced set of Positive and Negative labels , and the company tweets dataset , which consists of a balance between Positive , Neutral and Negative labels .",0
20035,See.,0
20036,"While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .",1
20037,This is despite optimally calibrating the API results on the test set .,0
20038,Multi - Label Emotion Tweets,1
20039,"The IBM Watson API offers multi-label emotion predictions for five categories : Anger , Disgust , Fear , Joy and Sadness .",0
20040,We compare our models to Watson on these categories for both the SemEval dataset and the company tweets in .,0
20041,We find that our models outperform Watson on every emotion category .,1
20042,Sem Eval Tweets,1
20043,"We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .",0
20044,"These results were computed by the organizers on a golden test set , for which we do not have access to the truth labels .",0
20045,"Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .",1
20046,"This suggests that our model out - performs the other top submission on rare and difficult categories , since macroaverage weighs performance on all classes equally , and the most common categories of Joy , Anger , Disgust and Optimism get relatively higher F 1 scores across all models .",0
20047,We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .,1
20048,"The winner of the Task 1:E - c challenge ) trained a bidirectional LSTM with an 800,000 word embedding vocabulary derived from training word vectors ) on a dataset of 550 million tweets .",0
20049,"Similarly , the second place winner of the SemEval leaderboard trained a word - level bidirectional LSTM with attention , as well as including non-deep learning features into their ensemble .",0
20050,"Both submissions used training data across SemEval tasks , as well as additional training data outside of the training set .",0
20051,"In comparison , we demonstrate that finetuning can be as effective on this task , despite training only on 7,000 tweets .",0
20052,"Furthermore , out language modeling took place on the Amazon Reviews dataset , which does not contain emoji , hashtags or usernames .",0
20053,"We would expect to see improvements if our unsupervised dataset contained emoji , for example .",0
20054,Plutchik on Company Tweets,0
20055,Our models gets lower F 1 scores on the company tweets dataset than on equivalent Se -m Eval categories .,1
20056,"As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .",0
20057,These results are shown in .,0
20058,Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions .,0
20059,We could not conclusively determine whether the singlehead or the multihead Transformer will perform better on a given task .,0
20060,Thus we recommend trying both methods on a new dataset .,0
20061,Analysis,0
20062,Classification Performance by Dataset Size,0
20063,"We would have liked to label more data for the company tweets dataset , and thus looked into how much extra labeling contributes to finetuned model performance accuracy .",0
20064,"First , let us explain the difference between micro and macro averaging of the F 1 scores .",0
20065,We can summarize the F 1 scores of categories c ?,0
20066,C ( or any other metric M ) through macro and micro averaging to obtain M .,0
20067,The macro method weights each class equally by averaging the metric calculated on each individual class .,0
20068,"The micro method accounts for the class imbalances in each category by aggregating all of the true / false positives / negatives first , and then calculating an over all metric .",0
20069,"In one experiment , we decreased the size of the training dataset and observed the resulting macro and micro averaged F 1 scores across all categories on company tweets .",0
20070,The results are shown in .,0
20071,We observe that the macro average is more sensitive to dataset size and falls more quickly than the micro average .,0
20072,The interpretation of this is that categories with worse class imbalance ( which consequently influence macro more than micro average ) benefit more from having a larger training dataset size .,0
20073,This suggests that we may obtain substantially better results with more data in the harder categories .,0
20074,We conducted a related experiment that focused on the difference in category performance when using a single head versus a multihead decoder f d .,0
20075,"We apply the two architectures at different training dataset sizes for three different label categories : Anger , Anticipation and Trust , which we categorize as low , medium and high difficulty , respectively .",0
20076,"As seen in it appears that the difference between the single and multihead becomes more pronounced for more difficult categories , as well as for smaller dataset sizes .",0
20077,"We do not have enough data to make a firm conclusion , but this study suggests that we could get more out of the labeled data that we have , by studying which categories benefit from single head and multihead decoders .",0
20078,"All categories benefit from more training data , but some categories benefit from from marginal labeled data than others .",0
20079,"This suggests further and more rigorous study of the boostrapping methods we used to select tweets for our human labeling budget , as described in the Active Learning section .",0
20080,"Following a similar process , we required 2 out of 5 raters for a positive label , and in the case of binary sentiment labels ( Positive , Neutral , Negative ) , we rounded toward polarized sentiment and away from Neutral labels in the case of a 2 / 3 split .",0
20081,"Applying the SemEval - trained Transformer directly to our company tweets dataset gets reasonably good results ( 0.338 macro average ) , also validating that our labeling technique is similar to that of SemEval .",0
20082,"Looking at rater agreement by dataset , we see that Plutchik category labels contain large rater dis agreement , even among vetted raters who passed the golden set test .",0
20083,"Furthermore , datasets with more emotions ( the SemEval dataset and our active learning sampled company tweets ) contain higher Plutchik dis agreement than random company tweets .",0
20084,"This is likely because raters tend to apply the "" No Emotion "" label when they are not sure about a category .",0
20085,"As shows , the SemEval and active company tweets datasets contain fewer no-emotion tweets than other datsets .",0
20086,"It would be interesting to analyze rater dis agreement by category , how much this effects classifier convergence , whether getting 7 + ratings per tweet helps classifier convergence , and also whether this work could benefit from estimating rater quality via agreement with the crowd , as proposed in .",0
20087,"However this analysis is not straightforward , as the truth data is itself collected through human labeling .",0
20088,"Alongside classifier convergence by dataset size , we think that this could bean interesting area a future research .",0
20089,Difficult tweets and challenging contexts .,0
20090,"There is not sufficient space for a thorough analysis , but we wanted to suggest why general purpose APIs may notwork well on our company tweets dataset .",0
20091,samples the largest binary sentiment dis agreements between human raters and the Wat-son API .,0
20092,"For simplicity , we restrict examples to video game tweets , which comprise 19.1 % of our test set .",0
20093,"As we can see , all of these examples appear to ascribe negative emotion to generally negative terms which , in a video game context , do not indicate negative sentiment .",0
20094,Our purpose is not to castigate the Watson or the GCL APIs .,0
20095,"Rather , we propose that it may not be possible to provide context - independent emotion classification scores that work well across text contexts .",0
20096,"It may work better in practice , on some tasks , to train a large unsupervised model and to use a small amount of labeled data to finetune on the context present in the specific dataset .",0
20097,We would like to quantify this further in future work .,0
20098,"Recent work shows that training an RNN with multiple softmax outputs leads to a much improved BPC on language modeling , especially for diverse datasets and models with large vocabularies .",0
20099,This is because the multiple softmaxes are able to capture a larger number of distinct contexts in the text than a single output .,0
20100,"Perhaps our Transformer also captures the features relevant to a large number of distinct contexts , and the finetuning is able to select the most significant of these features , while ignoring those features that - while adding value in general - are not appropriate in a video game setting .",0
20101,Conclusion,0
20102,In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult text classification tasks .,0
20103,"We noticed that the finetuning was especially effective with the Transformer network , when transferring to downstream tasks with noisy labels and specialized context .",0
20104,We think that this framework makes it easy to customize a text classification model on niche tasks .,0
20105,"Unsupervised language modeling can be done on general text datasets , and requires no labels .",0
20106,"Meanwhile downstream task transfer works well enough , even on small amounts of domain - specific labelled data , to be accessible to most academics and small organization .",0
20107,"It would be great to see this approach applied to a variety of practical text classification problems , much as",0
20108,title,0
20109,Investigating Capsule Networks with Dynamic Routing for Text Classification,1
20110,abstract,0
20111,"In this study , we explore capsule networks with dynamic routing for text classification .",0
20112,"We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information or have not been successfully trained .",0
20113,A series of experiments are conducted with capsule networks on six text classification benchmarks .,0
20114,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",0
20115,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,0
20116,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",0
20117,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,1
20118,Introduction,0
20119,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,0
20120,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",0
20121,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",0
20122,"But it could be very difficult for a computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",0
20123,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",0
20124,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",0
20125,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",0
20126,"A common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",0
20127,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",0
20128,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",0
20129,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .",0
20130,"Here , they form a recursive process to articulate what to be modeled .",0
20131,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",0
20132,It then hierarchically builds such pattern extraction pipelines at multiple levels .,0
20133,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",0
20134,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",0
20135,"On the other hand , methods thatare spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",0
20136,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",0
20137,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,0
20138,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,1
20139,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,1
20140,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,0
20141,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",0
20142,"In our work , we follow a similar spirit to use this technique in modeling texts .",0
20143,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .",1
20144,We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,0
20145,"More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .",0
20146,Our Model,0
20147,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",0
20148,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",0
20149,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",0
20150,"In the rest of this section , we elaborate the key components in detail .",0
20151,N - gram Convolutional Layer,0
20152,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,0
20153,Suppose x ?,0
20154,R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,0
20155,Let xi ?,0
20156,RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,0
20157,Let W a ? R K 1,0
20158,"V be the filter for the convolution operation , where K 1 is the N - gram size while sliding over a sentence for the purpose of detecting features at different positions .",0
20159,A filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ?,0
20160,"R L?K 1 + 1 , each element ma i ?",0
20161,R of the feature map is produced by,0
20162,"where is element - wise multiplication , b 0 is a bias term , and f is a nonlinear activate function ( i.e. , ReLU ) .",0
20163,We have described the process by which one feature is extracted from one filter .,0
20164,"Hence , for a = 1 , . . . , B , totally B filters with the same N - gram size , one can generate B feature maps which can be rearranged as",0
20165,Primary Capsule Layer,0
20166,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,0
20167,Suppose pi ?,0
20168,"Rd denotes the instantiated parameters of a capsule , where d is the dimension of the capsule .",0
20169,Let W b ? R,0
20170,Bd be the filter shared in different sliding windows .,0
20171,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ?",0
20172,"R B , then the corresponding N - gram phrases in the form of capsule are produced with",0
20173,ConvCaps Capsule,0
20174,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ?",0
20175,Rd in the column - list is computed as,0
20176,"where g is nonlinear squash function through the entire vector , b 1 is the capsule bias term .",0
20177,"For all C filters , the generated capsule feature maps can be rearranged as",0
20178,where totally ( L ? K 1 + 1 ) C d-dimensional vectors are collected as capsules in P .,0
20179,Child - Parent Relationships,0
20180,"As argued in , capsule network tries to address the representational limitation and exponential inefficiencies of convolutions with transformation matrices .",0
20181,It allows the networks to automatically learn child - parent ( or partwhole ) relationships .,0
20182,"In text classification tasks , different sentences with the same category are supposed to have the similar topic but with different viewpoints .",0
20183,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ?",0
20184,Rd from it s child capsule i to the parent capsule j.,0
20185,The first one shares weights W t 1 ?,0
20186,"RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",0
20187,"Formally , each corresponding vote can be computed by :",0
20188,where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,0
20189,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ?",0
20190,R HN dd and H is the number of child capsules in the layer below .,0
20191,Dynamic Routing,0
20192,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,0
20193,"For each potential parent , the capsule network can increase or decrease the connection strength by dynamic routing , which is more effective than the primitive routing strategies such as max - pooling in CNN that essentially detects whether a feature is present in any position of the text , but loses spatial information about the feature .",0
20194,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules :,0
20195,Orphan Category,0
20196,"Inspired by , an additional "" orphan "" category is added to the network , which can capture the "" background "" information of the text such as stop words and the words thatare unrelated to specific categories , helping the capsule network model the child - parent relationship more efficiently .",0
20197,"Adding "" orphan "" category in the text is more effective than in image since there is no single consistent "" background "" object in images , while the stop words are consistent in texts such as predicate "" s "" , "" am "" and pronouns "" his "" , "" she "" .",0
20198,Leaky - Softmax,0
20199,We explore Leaky - Softmax in the place of standard softmax while updating connection strength between the children capsules and their parents .,0
20200,"Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .",0
20201,Coefficients Amendment,0
20202,We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the connection strength as Eq.6 .,0
20203,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :",0
20204,for all capsule i in layer land capsule j in,0
20205,"Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",0
20206,where b j|i is the logits of coupling coefficients .,0
20207,Each parent capsule v j in the layer above is a weighted sum over all prediction vectors j|i :,0
20208,"where a j is the probabilities of parent capsules , g is nonlinear squash function through the entire vector .",0
20209,"Once all of the parent capsules are produced , each coupling coefficient b j|i is updated by :",0
20210,"For simplicity of notation , the parent capsules and their probabilities in the layer above are denoted as v , a = Routing ( )",0
20211,"where denotes all of the child capsules in the layer below , v denotes all of the parent - capsules and their probabilities a.",0
20212,Our dynamic routing algorithm is summarized in Algorithm,0
20213,1 .,0
20214,Convolutional Capsule Layer,0
20215,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",0
20216,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,0
20217,Suppose W c 1 ? R Ddd and W c 2 ?,0
20218,R K,0
20219,"2 CDdd denote shared and non-shared weights , respectively , where K 2 C is the number of child capsules in a local region in the layer below , Dis the number of parent capsules which the child capsules are sent to .",0
20220,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b?",0
20221,"where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",0
20222,"Then , we use routingby - agreement to produce parent capsules feature maps totally ( L?K 1 ? K 2 + 2 ) D d-dimensional capsules in this layer .",0
20223,"When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",0
20224,Fully Connected Capsule Layer,0
20225,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,0
20226,R Edd or W d 2 ?,0
20227,R HEdd followed by routing - by - agreement to produce final capsule v j ?,0
20228,Rd and its probability a j ?,0
20229,R for each category .,0
20230,"Here , H is the number of child capsules in the layer below , E is the number of categories plus an extra orphan category .",0
20231,The Architectures of Capsule Network,0
20232,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,0
20233,"Capsule - B Capsule - A starts with an embedding layer which transforms each word in the corpus to a 300 - dimensional ( V = 300 ) word vector , followed by a 3 - gram ( K 1 = 3 ) convolutional layer with 32 filters ( B = 32 ) and astride of 1 with ReLU non-linearity .",0
20234,"All the other layers are capsule layers starting with a B d primary capsule layer with 32 filters ( C = 32 ) , followed by a 3 C d d ( K 2 = 3 ) convolutional capsule layer with 16 filters ( D = 16 ) and a fully connected capsule layer in sequence .",0
20235,Each capsule has 16 - dimensional ( d = 16 ) instantiated parameters and their length ( norm ) can describe the probability of the existence of capsules .,0
20236,"The capsule layers are connected by the transformation matrices , and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism .",0
20237,"The basic structure of Capsule - B is similar to Capsule - A except that we adopt three parallel networks with filter windows ( N ) of 3 , 4 , 5 in the N - gram convolutional layer ( see ) .",0
20238,The final output of the fully connected capsule layer is fed into the average pooling to produce the final results .,0
20239,"In this way , Capsule - B can learn more meaningful and comprehensive text representation .",0
20240,3 Experimental Setup,0
20241,Experimental Datasets,0
20242,"In order to evaluate the effectiveness of our model , we conduct a series of experiments on six bench - marks including : movie reviews ( MR ) , Stanford Sentiment Treebankan extension of MR ( SST - 2 ) , Subjectivity dataset ( Subj ) , TREC question dataset ( TREC ) , customer review ( CR ) , and AG 's news corpus .",0
20243,"These benchmarks cover several text classification tasks such as sentiment classification , question categorization , news categorization .",0
20244,The detailed statistics are presented in,0
20245,Implementation Details,0
20246,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",1
20247,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,1
20248,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,1
20249,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,0
20250,Baseline methods,0
20251,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",1
20252,Experimental Results,1
20253,Quantitative Evaluation,0
20254,"In our experiments , the evaluation metric is classification accuracy .",0
20255,We summarize the experimental results in .,0
20256,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",1
20257,"In particular , our model substantially and consistently outperforms",0
20258,Ablation Study,0
20259,"To analyze the effect of varying different components of our capsule architecture for text classification , we also report the ablation test of the capsule - B model in terms of using different setups of the capsule network .",0
20260,The experimental results are summarized in .,0
20261,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .",0
20262,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,0
20263,Single - Label to Multi - Label Text Classification,1
20264,Capsule network demonstrates promising performance in single - label text classification which as - signs a label from a predefined set to a text ( see ) .,0
20265,"Multi-label text classification is , however , a more challenging practical problem .",0
20266,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",0
20267,"For single - label texts , it is practically easy to collect and annotate the samples .",0
20268,"However , the burden of collection and annotation for a large scale multi-label text dataset is generally extremely high .",0
20269,"How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .",0
20270,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",1
20271,"With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",0
20272,The evaluation is carried on the Reuters - 21578 dataset .,0
20273,"This dataset consists of 10,788 documents from the Reuters financial newswire service , where each document contains either multiple labels or a single label .",0
20274,We reprocess the corpus to evaluate the capability of capsule networks of transferring from single - label to multi-label text classification .,0
20275,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",0
20276,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",0
20277,The characteristics of these two datasets are described in .,0
20278,"Following ( Sorower , 2010 ) , we adopt Micro Averaged Precision ( Precision ) , Micro Averaged Recall ( Recall ) and Micro Averaged F1 scores ( F1 ) as the evaluation metrics for multi-label text classification .",0
20279,"Any of these scores are firstly computed on individual class labels and then averaged over all classes , called label - based measures .",0
20280,"In addition , we also measure the Exact Match Ratio ( ER ) which considers partially correct prediction as incorrect and only counts fully correct samples .",0
20281,The experimental results are summarized in .,0
20282,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",1
20283,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",1
20284,This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,0
20285,The capsule network has much stronger transferring capability than the conventional deep neural networks .,1
20286,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",1
20287,Connection Strength Visualization,1
20288,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",0
20289,"The connection strength shows the importance of each primary capsule for text categories , acting like a parallel attention mechanism .",0
20290,This should allow the capsule networks to recognize multiple categories in the text even though the model is trained on singlelabel documents .,0
20291,"Due to space reasons , we choose a multilabel document from Reuters - Multi - label test set whose category labels ( i.e. , Interest Rates and Money / Foreign Exchange ) are correctly predicted ( fully correct ) by our model with high confidence ( p > 0.8 ) to report in .",0
20292,"The categoryspecific phrases such as "" interest rates "" and "" foreign exchange "" are highlighted with red color .",0
20293,We use the tag cloud to visualize the 3 - gram phrases for Interest Rates and Money / Foreign Exchange categories .,0
20294,"The stronger the connection strength , the bigger the font size .",0
20295,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",1
20296,"The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",0
20297,"From , we observe that the Capsule - B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 iteration .",0
20298,U.K .,0
20299,MONEY RATES FIRM ON LAWSON STERLING TARGETS,0
20300,Interest Rates,0
20301,Money / Foreign Exchange Interest rates on the London money market were slightly firmer on news U.K .,0
20302,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",0
20303,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",0
20304,Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,0
20305,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates .,0
20306,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,0
20307,"Base lending rates even less likely in the near term , dealers said .",0
20308,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",0
20309,Interest rates is not very likely .,0
20310,"The market is expected to continue at around these levels , reflecting the current 10 pct base rate level , for sometime .",0
20311,The key three months interbank rate was 1 / 16 point firmer at 10 9 - 7 /8 pct .,0
20312,Orphan,0
20313,Mergers / Acquisitions Money / Foreign Exchange Trade Interest Rates,0
20314,Related Work,0
20315,"Early methods for text classification adopted the typical features such as bag - of - words , n-grams , and their TF - IDF features as input of machine learning algorithms such as support vector machine ( SVM ) , naive Bayes ( NB ) for classification .",0
20316,"However , these models usually heavily relied on laborious feature engineering or massive extra linguistic resources .",0
20317,Recent advances in deep neural networks and representation learning have substantially improved the performance of text classification tasks .,0
20318,"The dominant approaches are recurrent neural net -works , in particular LSTMs and CNNs. reported on a series of experiments with CNNs trained on top of pre-trained word vectors for sentence - level classification tasks .",0
20319,The CNN models improved upon the state of the art on 4 out of 7 tasks .,0
20320,offered an empirical exploration on the use of character - level convolutional networks ( Convnets ) for text classification and the experiments showed that Convnets outperformed the traditional models .,0
20321,"proposed a simple and efficient text classification method fastText , which could be trained on a billion words within ten minutes .",0
20322,proposed a very deep convolutional networks ( with 29 convolutional layers ) for text classification .,0
20323,generalized the LSTM to the tree - structured network topologies ( Tree - LSTM ) that achieved best results on two text classification tasks .,0
20324,"Recently , a novel type of neural network is proposed using the concept of capsules to improve the representational limitations of firstly introduced the concept of "" capsules "" to address the representational limitations of CNNs and RNNs .",0
20325,Capsules with transformation matrices allowed networks to automatically learn part - whole relationships .,0
20326,"Consequently , proposed capsule networks that replaced the scalar - output feature detectors of CNNs with vector - output capsules and max - pooling with routing - by - agreement .",0
20327,The capsule network has shown its potential by achieving a state - of - the - art result on MNIST data .,0
20328,"Unlike max - pooling in CNN , however , Capsule network do not throwaway information about the precise position of the entity within the region .",0
20329,"For lowlevel capsules , location information is placecoded by which capsule is active .",0
20330,further tested out the application of capsule networks on CIFAR data with higher dimensionality .,0
20331,"proposed a new iterative routing procedure between capsule layers based on the EM algorithm , which achieves significantly better accuracy on the small NORB data set .",0
20332,generalized existing routing methods within the framework of weighted kernel density estimation .,0
20333,"To date , no work investigates the performance of capsule networks in NLP tasks .",0
20334,This study herein takes the lead in this topic .,0
20335,Conclusion,0
20336,"In this paper , we investigated capsule networks with dynamic routing for text classification .",0
20337,Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules .,0
20338,Extensive experiments on six text classification benchmarks show the effectiveness of capsule networks in text classification .,0
20339,"More importantly , capsule networks also show significant improvement when transferring single - label to multi-label text classifications over strong baseline methods .",0
20340,Supplementary Material,0
20341,"To better demonstrate the orphan and other categories with top unigrams , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , similar to the settings in section 5.1 .",0
20342,"Here , the primary capsules denote uni-grams in the form of capsules .",0
20343,"We picked top - 20 uni-gram ( words ) from four categories ( i.e. , Orphan category , Trade category , Money Exchange category and Interest Rates category ) sorted by their connection strengths .",0
20344,"Money / Foreign Exchange Following is the text of a statement by the Group of Seven - the U.S. , Japan , West Germany , France , Britain , Italy and Canada - issued after a Washington meeting yesterday .",0
20345,1 . The finance ministers and central bank governors of seven major industrial countries met today .,0
20346,They continued the process of multilateral surveillance of their economies pursuant to the arrangements for strengthened economic policy coordination agreed at the 1986 Tokyo summit of their heads of state or government .,0
20347,"2 . The ministers and governors reaffirmed the commitment to the cooperative approach agreed at the recent Paris meeting , and noted the progress achieved in implementing the undertakings embodied in the Louvre Agreement .",0
20348,"In this connection they welcomed the proposals just announced by the governing Liberal Democratic Party in Japan for extraordinary and urgent measures to stimulate Japan 's economy through early implementation of a large supplementary budget exceeding those of previous years , as well as unprecedented front - end loading of public works expenditures .",0
20349,They concluded that present and prospective progress in implementing the policy undertakings at the Louvre and in this statement provided a basis for continuing close cooperation to foster the stability of exchange rates .,0
20350,Index,0
20351,title,0
20352,Bag of Tricks for Efficient Text Classification,1
20353,abstract,0
20354,This paper explores a simple and efficient baseline for text classification .,0
20355,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",0
20356,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",0
20357,Introduction,0
20358,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",0
20359,"Recently , models based on neural networks have become increasingly popular .",0
20360,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",0
20361,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",0
20362,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",0
20363,They also have the potential to scale to very large corpus .,0
20364,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",1
20365,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",1
20366,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",0
20367,Model architecture,0
20368,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",0
20369,"However , linear classifiers do not share parameters among features and classes .",0
20370,This possibly limits their generalization in the context of large output space where some classes have very few examples .,0
20371,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,0
20372,shows a simple linear model with rank constraint .,0
20373,The first weight matrix A is a look - up table over the words .,0
20374,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",0
20375,The text representa - tion is an hidden variable which can be potentially be reused .,0
20376,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",0
20377,We use the softmax function f to compute the probability distribution over the predefined classes .,0
20378,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",0
20379,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",0
20380,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,0
20381,Hierarchical softmax,0
20382,"When the number of classes is large , computing the linear classifier is computationally expensive .",0
20383,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",0
20384,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",0
20385,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",0
20386,The hierarchical softmax is also advantageous at test time when searching for the most likely class .,0
20387,Each node is associated with a probability that is the probability of the path from the root to that node .,0
20388,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",0
20389,This means that the probability of anode is always lower than the one of its parent .,0
20390,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,0
20391,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) at test time .",0
20392,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",0
20393,N - gram features,0
20394,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,0
20395,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",0
20396,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,0
20397,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",0
20398,Experiments,0
20399,We evaluate fastText on two different tasks .,0
20400,"First , we compare it to existing text classifers on the problem of sentiment analysis .",0
20401,"Then , we evaluate its capacity to scale to large output space on a tag prediction dataset .",0
20402,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",0
20403,Sentiment analysis,1
20404,Datasets and baselines .,0
20405,We employ the same 8 datasets and evaluation protocol of .,0
20406,We report the n-grams and TFIDF baselines from We also compare to following their evaluation protocol .,0
20407,We report their main baselines as well as their two approaches based on recurrent networks ( Conv - GRNN and LSTM - GRNN ) .,0
20408,Results .,0
20409,We present the results in .,0
20410,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",1
20411,"On this task , adding bigram information improves the performance by 1 - 4 % .",1
20412,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",1
20413,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",1
20414,"Finally , shows that our method is competitive with the methods presented in .",0
20415,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,1
20416,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference in accuracy .",0
20417,We show a few correct and incorrect tag predictions .,0
20418,"up compared to neural network based methods increases with the size of the dataset , going up to at least a 15,000 speed - up .",0
20419,Tag prediction,1
20420,Dataset and baselines .,0
20421,"To test scalability of our approach , further evaluation is carried on the YFCC100M dataset which consists of almost 100M images with captions , titles and tags .",0
20422,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,0
20423,"We remove the words and tags occurring less than 100 times and split the data into a train , validation and test set .",0
20424,"The train set contains 91,188,648 examples ( 1.5B tokens ) .",0
20425,"The validation has 930,497 examples and the test set 543,424 .",0
20426,"The vocabulary size is 297,141 and there are 312,116 unique tags .",0
20427,We will release a script that recreates this dataset so that our numbers could be reproduced .,0
20428,We report precision at 1 .,0
20429,We consider a frequency - based baseline which predicts the most frequent tag .,1
20430,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",1
20431,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",0
20432,Results and training time . and 200 .,1
20433,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",1
20434,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",0
20435,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",0
20436,The speedup of the test phase is even more significant ( a 600 speedup ) .,0
20437,shows some qualitative examples .,0
20438,Discussion and conclusion,0
20439,"In this work , we propose a simple baseline method for text classification .",0
20440,"Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .",0
20441,"In several tasks , fastText obtains performance on par with recently proposed methods inspired by deep learning , while being much faster .",0
20442,"Although deep neural networks have in theory much higher representational power than shallow models , it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them .",0
20443,We will publish our code so that the research community can easily build on top of our work .,0
20444,title,0
20445,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,1
20446,abstract,0
20447,Recurrent Neural Network ( RNN ) is one of the most popular architectures used in Natural Language Processsing ( NLP ) tasks because its recurrent structure is very suitable to process variablelength text .,0
20448,"RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors , which form a matrix .",0
20449,And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .,0
20450,Then most existing models usually utilize one - dimensional ( 1D ) max pooling operation or attention - based operation only on the time - step dimension to obtain a fixed - length vector .,0
20451,"However , the features on the feature vector dimension are not mutually independent , and simply applying 1 D pooling operation over the time - step dimension independently may destroy the structure of the feature representation .",0
20452,"On the other hand , applying two - dimensional ( 2D ) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks .",0
20453,"To integrate the features on both dimensions of the matrix , this paper explores applying 2 D max pooling operation to obtain a fixed - length representation of the text .",0
20454,This paper also utilizes 2D convolution to sample more meaningful information of the matrix .,0
20455,"Experiments are conducted on six text classification tasks , including sentiment analysis , question classification , subjectivity classification and newsgroup classification .",0
20456,"Compared with the state - of - the - art models , the proposed models achieve excellent performance on 4 out of 6 tasks .",0
20457,"Specifically , one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine - grained classification tasks .",0
20458,Introduction,0
20459,"Text classification is an essential component in many NLP applications , such as sentiment analysis , relation extraction and spam detection .",0
20460,"Therefore , it has attracted considerable attention from many researchers , and various types of models have been proposed .",0
20461,"As a traditional method , the bag - of - words ( BoW ) model treats texts as unordered sets of words .",0
20462,"In this way , however , it fails to encode word order and syntactic feature .",0
20463,"Recently , order- sensitive models based on neural networks have achieved tremendous success for text classification , and shown more significant progress compared with BoW models .",0
20464,"The challenge for textual modeling is how to capture features for different text units , such as phrases , sentences and documents .",0
20465,"Benefiting from its recurrent structure , RNN , as an alternative type of neural networks , is very suitable to process the variable - length text .",0
20466,"RNN can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors , which form a matrix .",0
20467,"This matrix includes two dimensions : the time - step dimension and the feature vector dimension , and it will be updated in the process of learning feature representation .",0
20468,"Then RNN utilizes 1 D max pooling operation or attention - based operation , which extracts maximum values or generates a weighted representation over the time - step dimension of the matrix , to obtain a fixed - length vector .",0
20469,"Both of the two operators ignore features on the feature vector dimension , which maybe important for sentence representation , therefore the use of 1 D max pooling and attention - based operators may pose a serious limitation .",0
20470,"Convolutional Neural Networks ( CNN ) utilizes 1 D convolution to perform the feature mapping , and then applies 1 D max pooling operation over the time - step dimension to obtain a fixed - length output .",0
20471,"However the elements in the matrix learned by RNN are not independent , as RNN reads a sentence word byword , one can effectively treat the matrix as an ' image ' .",0
20472,"Unlike in NLP , CNN in image processing tasks applies 2D convolution and 2D pooling operation to get a representation of the input .",0
20473,It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on both the time - step dimension and the feature vector dimension for text classification .,0
20474,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",1
20475,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,1
20476,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,1
20477,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,1
20478,The contributions of this paper can be summarized as follows :,0
20479,"This paper proposes a combined framework , which utilizes BLSTM to capture long - term sentence dependencies , and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks .",0
20480,"To the best of our knowledge , this work is the first example of using 2D convolution and 2D max pooling operation in NLP tasks .",0
20481,"This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .",0
20482,"Compared with the state - of - the - art models , BLSTM - 2DCNN achieves excellent performance on 4 out of 6 tasks .",0
20483,"Specifically , it achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine - grained classification tasks .",0
20484,"To better understand the effect of 2D convolution and 2D max pooling operation , this paper conducts experiments on Stanford Sentiment Treebank fine - grained task .",0
20485,"It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of 2D filter and max pooling size .",0
20486,The remainder of the paper is organized as follows .,0
20487,"In Section 2 , the related work about text classification is reviewed .",0
20488,Section 3 presents the BLSTM - 2DCNN architectures for text classification in detail .,0
20489,Section 4 describes details about the setup of the experiments .,0
20490,Section 5 presents the experimental results .,0
20491,The conclusion is drawn in the section 6 .,0
20492,Related Work,0
20493,Deep learning based neural network models have achieved great improvement on text classification tasks .,0
20494,These models generally consist of a projection layer that maps words of text to vectors .,0
20495,And then combine the vectors with different neural networks to make a fixed - length representation .,0
20496,"According to the structure , they may divide into four categories :",0
20497,"Recursive Neural Networks ( RecNN 1 ) , RNN , CNN and other neural networks .",0
20498,Recursive Neural,0
20499,Networks : RecNN is defined over recursive tree structures .,0
20500,"In the type of recursive models , information from the leaf nodes of a tree and its internal nodes are combined in a bottom - up manner .",0
20501,introduced recursive neural tensor network to build representations of phrases and sentences by combining neighbour constituents based on the parsing tree .,0
20502,"proposed deep recursive neural network , which is constructed by stacking multiple recursive layers on top of each other , to modeling sentence .",0
20503,Recurrent Neural Networks : RNN has obtained much attention because of their superior ability to preserve sequence information overtime .,0
20504,"developed target dependent Long Short - Term Memory Networks ( LSTM ( Hochreiter and Schmidhuber , 1997 ) ) , where target information is automatically taken into account .",0
20505,generalized LSTM to Tree - LSTM where each LSTM unit gains information from its children units .,0
20506,introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification .,0
20507,"introduced a hierarchical network with two levels of attention mechanisms , which are word attention and sentence attention , for document classification .",0
20508,This paper also implements an attention - based model BLSTM - Att like the model in .,0
20509,"Convolution Neural Networks : ) is a feedforward neural network with 2D convolution layers and 2D pooling layers , originally developed for image processing .",0
20510,"Then CNN is applied to NLP tasks , such as sentence classification , and relation classification .",0
20511,The difference is that the common CNN in NLP tasks is made up of 1D convolution layers and 1D pooling layers .,0
20512,defined a CNN architecture with two channels .,0
20513,proposed a dynamic k-max pooling mechanism for sentence modeling .,0
20514,conducted a sensitivity analysis of one - layer CNN to explore the effect of architecture components on model performance .,0
20515,Yin and Schtze ( 2016 ) introduced multichannel embeddings and unsupervised pretraining to improve classification accuracy .,0
20516,conducted a sensitivity analysis of one - layer CNN to explore the effect of architecture components on model performance .,0
20517,Usually there is a misunderstanding that 1D convolutional filter in NLP tasks has one dimension .,0
20518,"Actually it has two dimensions ( k , d ) , where k , d ?",0
20519,R .,0
20520,"As dis equal to the word embeddings size d w , the window slides only on the time - step dimension , so the convolution is usually called 1D convolution .",0
20521,"While din this paper varies from 2 to d w , to avoid confusion with common CNN , the convolution in this work is named as 2D convolution .",0
20522,The details will be described in Section 3.2 .,0
20523,Other Neural Networks :,0
20524,"In addition to the models described above , lots of other neural networks have been proposed for text classification .",0
20525,"introduced a deep averaging network , which fed an unweighted average of word embeddings through multiple hidden layers before classification .",0
20526,"used CNN to extract a sequence of higher - level phrase representations , then the representations were fed into a LSTM to obtain the sentence representation .",0
20527,The proposed model BLSTM - 2DCNN is most relevant to DSCNN and RCNN .,0
20528,"The difference is that the former two utilize LSTM , bidirectional RNN respectively , while this work applies BLSTM , to capture long - term sentence dependencies .",0
20529,"After that the former two both apply 1 D convolution and 1 D max pooling operation , while this paper uses 2D convolution and 2D max pooling operation , to obtain the whole sentence representation .",0
20530,Model,0
20531,"As shown in , the over all model consists of four parts :",0
20532,"BLSTM Layer , Two - dimensional Convolution Layer , Two dimensional max pooling Layer , and Output Layer .",0
20533,The details of different components are described in the following sections .,0
20534,BLSTM Layer,0
20535,LSTM was firstly proposed by to overcome the gradient vanishing problem of RNN .,0
20536,"The main idea is to introduce an adaptive gating mechanism , which decides the degree to keep the previous state and memorize the extracted features of the current data input .",0
20537,"Given a sequence S = {x 1 , x 2 , . . . , x l } , where l is the length of input text , LSTM processes it word byword .",0
20538,"At time - step t , the memory ct and the hidden state ht are updated with the following equations :",0
20539,"where x t is the input at the current time - step , i , f and o is the input gate activation , forget gate activation and output gate activation respectively , ?",0
20540,"is the current cell state , ?",0
20541,denotes the logistic sigmoid function and ?,0
20542,denotes element - wise multiplication .,0
20543,"For the sequence modeling tasks , it is beneficial to have access to the past context as well as the future context .",0
20544,"proposed BLSTM to extend the unidirectional LSTM by introducing a second hidden layer , where the hidden to hidden connections flow in opposite temporal order .",0
20545,"Therefore , the model is able to exploit information from both the past and the future .",0
20546,"In this paper , BLSTM is utilized to capture the past and the future information .",0
20547,"As shown in , the network contains two sub-networks for the forward and backward sequence context respectively .",0
20548,The output of the i th word is shown in the following equation :,0
20549,"Here , element - wise sum is used to combine the forward and backward pass outputs .",0
20550,Convolutional Neural Networks,0
20551,"Since BLSTM has access to the future context as well as the past context , hi is related to all the other words in the text .",0
20552,"One can effectively treat the matrix , which consists of feature vectors , as an ' image ' , so 2D convolution and 2D max pooling operation can be utilized to capture more meaningful information .",0
20553,Two-dimensional,0
20554,Convolution Layer,0
20555,"A matrix H = {h 1 , h 2 , . . . , h l } , H ?",0
20556,"R ld w , is obtained from BLSTM Layer , where d w is the size of word embeddings .",0
20557,Then narrow convolution is utilized to extract local features over H. A convolution operation involves a 2 D filter m ?,0
20558,"R kd , which is applied to a window of k words and d feature vectors .",0
20559,"For example , a feature o i , j is generated from a window of vectors H i:i+k?1 , j:j+d?1 by",0
20560,"where i ranges from 1 to ( l ? k + 1 ) , j ranges from 1 to ( d w ? d + 1 ) , represents dot product , b ?",0
20561,R is a bias and an f is a non-linear function such as the hyperbolic tangent .,0
20562,This filter is applied to each possible window of the matrix H to produce a feature map O :,0
20563,with O ?,0
20564,R ( l?k+ 1 ) ( d w ? d+1 ) .,0
20565,It has described the process of one convolution filter .,0
20566,"The convolution layer may have multiple filters for the same size filter to learn complementary features , or multiple kinds of filter with different size .",0
20567,Two-dimensional,0
20568,Max Pooling Layer,0
20569,Then 2D max pooling operation is utilized to obtain a fixed length vector .,0
20570,For a 2D max pooling p ?,0
20571,"R p 1 p 2 , it is applied to each possible window of matrix",0
20572,O to extract the maximum value :,0
20573,"where down ( ) represents the 2D max pooling function , i = ( 1 , 1 + p 1 , , 1 + ( l ? k + 1 / p 1 ? 1 ) p 1 ) , and j = ( 1 , 1 + p 2 , , 1 + ( d w ? d + 1 / p 2 ? 1 ) p 2 ) .",0
20574,Then the pooling results are combined as follows :,0
20575,where h * ?,0
20576,"R , and the length of h * is ?l ? k + 1 / p 1 ? ?d w ? d + 1 / p 2 ?.",0
20577,Output Layer,0
20578,"For text classification , the output h * of 2D Max Pooling Layer is the whole representation of the input text S. And then it is passed to a softmax classifier layer to predict the semantic relation label ?",0
20579,from a discrete set of classes Y .,0
20580,The classifier takes the hidden state h * as input :,0
20581,A reasonable training objective to be minimized is the categorical cross - entropy loss .,0
20582,The loss is calculated as a regularized sum :,0
20583,where t ?,0
20584,"R m is the one - hot represented ground truth , y ?",0
20585,"R m is the estimated probability for each class by softmax , m is the number of target classes , and ?",0
20586,is an L2 regularization hyper - parameter .,0
20587,Experimental Setup,0
20588,Datasets,0
20589,The proposed models are tested on six datasets .,0
20590,Summary statistics of the datasets are in .,0
20591,MR 2 : Sentence polarity dataset from .,0
20592,The task is to detect positive / negative reviews .,0
20593,SST - 1 3 : Stanford Sentiment Treebank is an extension of MR from .,0
20594,"The aim is to classify a review as fine - grained labels ( very negative , negative , neutral , positive , very positive ) .",0
20595,"SST - 2 : Same as SST - 1 but with neutral reviews removed and binary labels ( negative , positive ) .",0
20596,"For both experiments , phrases and sentences are used to train the model , but only sentences are scored at test time .",0
20597,Thus the training set is an order of magnitude larger than listed in table 1 .,0
20598,Subj 4 : Subjectivity dataset .,0
20599,The task is to classify a sentence as being subjective or objective .,0
20600,TREC 5 : Question classification dataset .,0
20601,"The task involves classifying a question into 6 question types ( abbreviation , description , entity , human , location , numeric value ) .",0
20602,20 Newsgroups,0
20603,6 : The 20 Ng dataset contains messages from twenty newsgroups .,0
20604,We use the bydate version preprocessed by .,0
20605,"We select four major categories ( comp , politics , rec and religion ) followed by .",0
20606,Word Embeddings,0
20607,The word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data .,0
20608,"In particular , our experiments utilize the",0
20609,Hyper-parameter Settings,0
20610,For datasets without a standard development set we randomly select 10 % of the training data as the development set .,0
20611,The evaluation metric of the 20Ng is the Macro - F1 measure followed by the state - of the - art work and the other five datasets use accuracy as the metric .,0
20612,The final hyper -parameters are as follows .,0
20613,"The dimension of word embeddings is 300 , the hidden units of LSTM is 300 .",1
20614,"We use 100 convolutional filters each for window sizes of ( 3 , 3 ) , 2D pooling size of ( 2 , 2 ) .",1
20615,We set the mini-batch size as 10 and the learning rate of AdaDelta as the default value 1.0 .,1
20616,"For regularization , we employ Dropout operation with dropout rate of 0.5 for the word embeddings , 0.2 for the BLSTM layer and 0.4 for the penultimate layer , we also use l 2 penalty with coefficient 10 ? 5 over the parameters .",1
20617,These values are chosen via a grid search on the SST - 1 development set .,0
20618,"We only tune these hyperparameters , and more finer tuning , such as using different numbers of hidden units of LSTM layer , or using wide convolution , may further improve the performance .",0
20619,Results,1
20620,Overall Performance,0
20621,"This work implements four models , BLSTM , BLSTM - Att , BLSTM - 2DPooling , and BLSTM - 2DCNN . presents the performance of the four models and other state - of - the - art models on six classification tasks .",0
20622,The BLSTM - 2DCNN model achieves excellent performance on 4 out of 6 tasks .,1
20623,"Especially , it achieves 52.4 % and 89.5 % test accuracies on SST - 1 and SST - 2 respectively .",1
20624,BLSTM - 2DPooling performs worse than the state - of - the - art models .,1
20625,"While we expect performance gains through the use of 2D convolution , we are surprised at the magnitude of the gains .",0
20626,"BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .",1
20627,"As for Subj and MR datasets , BLSTM - 2DCNN gets a second higher accuracies .",1
20628,"Some of the previous techniques only work on sentences , but not paragraphs / documents with several sentences .",0
20629,"Our question becomes whether it is possible to use our models for datasets that have a substantial number of words , such as 20Ng and where the content consists of many different topics .",0
20630,"For that purpose , this paper tests the four models on document - level dataset 20 Ng , by treating the document as along sentence .",0
20631,"Compared with RCNN , BLSTM - 2DCNN achieves a comparable result .",1
20632,"Besides , this paper also compares with ReNN , RNN , CNN and other neural networks :",0
20633,"Compared with ReNN , the proposed two models do not depend on external language - specific features such as dependency parse trees .",1
20634,"CNN extracts features from word embeddings of the input text , while BLSTM - 2DPooling and BLSTM - 2DCNN captures features from the output of BLSTM layer , which has already extracted features from the original input text .",0
20635,"BLSTM-2DCNN is an extension of BLSTM - 2DPooling , and the results show that BLSTM - 2DCNN can capture more dependencies in text .",0
20636,"Ada Sent utilizes a more complicated model to form a hierarchy of representations , and it outperforms BLSTM - 2DCNN on Subj and MR datasets .",0
20637,"Compared with DSCNN , BLSTM - 2DCNN outperforms it on five datasets .",1
20638,"Compared with these results , 2D convolution and 2D max pooling operation are more effective for modeling sentence , even document .",0
20639,"To better understand the effect of 2D operations , this work conducts a sensitivity analysis on SST - 1 dataset .",0
20640,Re NN RNTN 45.7 85.4 ----DRNN 49.8 86.6 ---- CNN DCNN 48.5 86.8 - 93.0 -- CNN - non-static 48.0 87.2 93.4 93.6 --CNN - MC 47.4 88.1 93.2 92 --TBCNN 51.4 87.9 - 96.0 -- Molding- CNN 51.2 88.6 ----CNN - Ana 49.6 89.4 93.9 --- RNN RCNN 47.21 ----96.49 S-LSTM - 81.9 ---- LSTM 46.4 84.9 ---- BLSTM 49.1 87.5 ---- Tree-LSTM 51.0 88.0 ---- LSTMN 49.3 87.3 ---- Multi- Task 49.6 87.9 94.1 --- Other PV 48.7 87.8 ----DAN 48.2 86.8 ---combine-skip --93.6 92.2 76.5 - Ada Sent -- 95.5 92.4 83.1 - LSTM- RNN 49.9 88.0 ----C-LSTM 49.2 87.8 - 94.6 --DSCNN 49 ..,0
20641,DRNN : Deep recursive neural networks for compositionality in language .,0
20642,DCNN : A convolutional neural network for modeling sentences .,0
20643,CNN -nonstatic / MC : Convolutional neural networks for sentence classification .,0
20644,TBCNN : Discriminative neural sentence modeling by tree - based convolution .,0
20645,"Molding - CNN : Molding CNNs for text : non-linear , non-consecutive convolutions .",0
20646,CNN - Ana : A Sensitivity Analysis of ( and Practitioners ' Guide to ) Convolutional Neural Networks for Sentence Classification .,0
20647,"MVCNN : Multichannel variable - size convolution for sentence classification ( Yin and Schtze , 2016 ) .",0
20648,RCNN : Recurrent Convolutional Neural Networks for Text Classification .,0
20649,S- LSTM : Long short - term memory over recursive structures .,0
20650,LSTM / BLSTM / Tree-LSTM :,0
20651,Improved semantic representations from tree - structured long shortterm memory networks .,0
20652,LSTMN : Long short - term memory - networks for machine reading .,0
20653,Multi - Task : Recurrent Neural Network for Text Classification with Multi - Task Learning .,0
20654,PV : Distributed representations of sentences and documents .,0
20655,DAN : Deep unordered composition rivals syntactic methods for text classification .,0
20656,combine - skip : skip - thought vectors .,0
20657,Ada,0
20658,Sent : Selfadaptive hierarchical sentence model .,0
20659,LSTM - RNN : Compositional distributional semantics with long short term memory .,0
20660,C - LSTM : A C - LSTM Neural Network for Text Classification .,0
20661,DSCNN : Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents . longer than 45 words .,0
20662,"The accuracy here is the average value of the sentences with length in the window [ l ? 2 , l + 2 ] .",0
20663,"Each data point is a mean score over 5 runs , and error bars have been omitted for clarity .",0
20664,Effect of Sentence Length,0
20665,It is found that both BLSTM - 2DPooling and BLSTM - 2DCNN outperform the other two models .,0
20666,This suggests that both 2D convolution and 2D max pooling operation are able to encode semantically - useful structural information .,0
20667,"At the same time , it shows that the accuracies decline with the length of sentences increasing .",0
20668,"In future work , we would like to investigate neural mechanisms to preserve long - term dependencies of text .",0
20669,Effect of 2D Convolutional Filter and 2D Max Pooling Size,0
20670,We are interested in what is the best 2 D filter and max pooling size to get better performance .,0
20671,We conduct experiments on SST - 1 dataset with BLSTM - 2DCNN and set the number of feature maps to 100 .,0
20672,"To make it simple , we set these two dimensions to the same values , thus both the filter and the pooling are square matrices .",0
20673,"For the horizontal axis , c means 2D convolutional filter size , and the five different color bar charts on each c represent different 2 D max pooling size from 2 to 6 .",0
20674,shows that different size of filter and pooling can get different accuracies .,0
20675,"The best accuracy is 52.6 with 2D filter size ( 5 , 5 ) and 2D max pooling size ( 5 , 5 ) , this shows that finer tuning can further improve the performance reported here .",0
20676,"And if a larger filter is used , the convolution can detector more features , and the performance maybe improved , too .",0
20677,"However , the networks will take up more storage space , and consume more time .",0
20678,Conclusion,0
20679,"This paper introduces two combination models , one is BLSTM - 2DPooling , the other is BLSTM - 2DCNN , which can be seen as an extension of BLSTM - 2DPooling .",0
20680,Both models can hold not only the time - step dimension but also the feature vector dimension information .,0
20681,The experiments are conducted on six text classificaion tasks .,0
20682,"The experiments results demonstrate that BLSTM - 2DCNN not only outperforms RecNN , RNN and CNN models , but also works better than the BLSTM - 2DPooling and DSCNN .",0
20683,"Especially , BLSTM - 2DCNN achieves highest accuracy on SST - 1 and SST - 2 datasets .",0
20684,"To better understand the effective of the proposed two models , this work also conducts a sensitivity analysis on SST - 1 dataset .",0
20685,"It is found that large filter can detector more features , and this may lead to performance improvement .",0
20686,title,0
20687,Disconnected Recurrent Neural Networks for Text Categorization,1
20688,abstract,0
20689,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,0
20690,"RNN can model the entire sequence and capture long - term dependencies , but it does not do well in extracting key patterns .",0
20691,"In contrast , convolutional neural network ( CNN ) is good at extracting local and position - invariant features .",0
20692,"In this paper , we present a novel model named disconnected recurrent neural network ( DRNN ) , which incorporates position - invariance into RNN .",0
20693,"By limiting the distance of information flow in RNN , the hidden state at each time step is restricted to represent words near the current position .",0
20694,The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization .,0
20695,Introduction,0
20696,"Text categorization is a fundamental and traditional task in natural language processing ( NLP ) , which can be applied in various applications such as sentiment analysis , question classification and topic classification .",0
20697,"Nowadays , one of the most commonly used methods to handle the task is to represent a text with a low dimensional vector , then feed the vector into a softmax function to calculate the probability of each category .",0
20698,Recurrent neural network ( RNN ) and convolutional neural network ( CNN ) are two kinds of neural networks usually used to represent the text .,0
20699,RNN can model the whole sequence and capture long - term dependencies .,0
20700,"However , modeling the entire sequence sometimes case1 :",0
20701,One of the seven great unsolved mysteries of mathematics may have been cracked by a reclusive Russian .,0
20702,case2 : A reclusive Russian may have cracked one of the seven great unsolved mysteries of mathematics .,0
20703,"can be a burden , and it may neglect key parts for text categorization .",0
20704,"In contrast , CNN is able to extract local and position - invariant features well .",0
20705,"is an example of topic classification , where both sentences should be classified as Science and Technology .",0
20706,"The key phrase that determines the category is unsolved mysteries of mathematics , which can be well extracted by CNN due to position - invariance .",0
20707,"RNN , however , does n't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves .",0
20708,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .",1
20709,"Concretely , we disconnect the information transmission of RNN and limit the maximal transmission step length as a fixed value k , so that the representation at each step only depends on the previous k ?",0
20710,1 words and the current word .,0
20711,"In this way , DRNN can also alleviate the burden of modeling the entire document .",0
20712,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .",1
20713,Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,1
20714,"Therefore , the maximal transmission step length can also be consid-ered as the window size in CNN .",0
20715,Another difference to CNN is that DRNN can increase the window size k arbitrarily without increasing the number of parameters .,0
20716,We also find that there is a trade - off between position - invariance and long - term dependencies in the DRNN .,0
20717,"When the window size is too large , the position - invariance will dis appear like RNN .",0
20718,"By contrast , when the window size is too small , we will lose the ability to model long - term dependencies just like CNN .",0
20719,"We find that the optimal window size is related to the type of task , but affected little by training dataset sizes .",0
20720,"Thus , we can search the optimal window size by training on a small dataset .",0
20721,We conduct experiments on seven large - scale text classification datasets introduced by .,0
20722,The experimental results show that our proposed model outperforms the other models on all of these datasets .,0
20723,Our contributions can be concluded as follows :,0
20724,1 . We propose a novel model to incorporate position - variance into RNN .,0
20725,Our proposed model can both capture long - term dependencies and local information well .,0
20726,2 .,0
20727,"We study the effect of different recurrent units , pooling operations and window sizes on model performance .",0
20728,"Based on this , we propose an empirical method to find the optimal window size .",0
20729,3 .,0
20730,Our proposed model outperforms the other models and achieves the best performance on seven text classification datasets .,0
20731,Related Work,0
20732,"Deep neural networks have shown great success in many NLP tasks such as machine translation , sentiment classification , etc .",0
20733,"Nowadays , nearly most of deep neural networks models are based on CNN or RNN .",0
20734,"Below , we will introduce some important works about text classification based on them .",0
20735,Convolutional Neural Networks,0
20736,CNN has been used in natural language processing because of the local correlation and position - invariance .,0
20737,"first utilize 1D CNN in part of speech ( POS ) , named entity recognition ( NER ) and semantic role labeling ( SRL ) .",0
20738,proposes to classify sentence by encoding a sentence with multiple kinds of convolutional filters .,0
20739,"To capture the relation between words , propose a novel CNN model with a dynamic k-max pooling .",0
20740,introduce an empirical exploration on the use of character - level CNN for text classification .,0
20741,Shallow CNN can not encode long - term information well .,0
20742,"Therefore , propose to use very deep CNN in text classification and achieve good performance .",0
20743,"Similarly , propose a deep pyramid CNN which both achieves good performance and reduces training time .",0
20744,Recurrent Neural Networks RNN is suitable for handling sequence input like natural language .,0
20745,"Thus , many RNN variants are used in text classification .",0
20746,utilize LSTM to model the relation of sentences .,0
20747,"Similarly , propose hierarchical attention model which incorporates attention mechanism into hierarchical GRU model so that the model can better capture the important information of a document .",0
20748,"incorporate the residual networks into RNN , which makes the model handle longer sequence .",0
20749,propose a novel LSTM with a cache mechanism to capture long - range sentiment information .,0
20750,Hybrid model,0
20751,Some researchers attempt to combine the advantages of CNN and RNN. ) extract local and global features by CNN and RNN separately .,0
20752,"firstly model sentences by RNN , and then use CNN to get the final representation .",0
20753,"replace convolution filters with deep LSTM , which is similar to what is proposed in this paper .",0
20754,The main differences are as follows .,0
20755,"Firstly , they regard their models as CNN and set a small window size of 3 , while we propose to use a large window size .",0
20756,We argue that small window size makes the model lose the ability to capture long - term dependencies .,0
20757,"Secondly , we utilize max pooling but not mean pooling , because max pooling can maintain position - invariance better .",0
20758,"Finally , our DRNN model is more general and can make use of different kinds of recurrent units .",0
20759,We find that using GRU as recurrent units outperforms LSTM which is utilized by . :,0
20760,Three model architectures .,0
20761,"In order to ensure the consistency of the hidden output , we pad k ?",0
20762,1 zero vectors on the left of the input sequence for DRNN and CNN .,0
20763,Here window size k is 3 .,0
20764,3 Method,0
20765,Recurrent Neural Network ( RNN ),0
20766,RNN is a class of neural network which models a sequence by incorporating the notion of time step .,0
20767,shows the structure of RNN .,0
20768,"Hidden states at each step depend on all the previous inputs , which sometimes can be a burden and neglect the key information .",0
20769,A variant of RNN has been introduced by with the name of gated recurrent unit ( GRU ) .,0
20770,"GRU is a special type of RNN , capable of learning potential long - term dependencies by using gates .",0
20771,The gating units can control the flow of information and mitigate the vanishing gradients problem .,0
20772,GRU has two types of gates : reset gate rt and update gate z t .,0
20773,The hidden state ht of GRU is computed as,0
20774,"where h t?1 is the previous state , h t is the candidate state computed with new input information and is the element - wise multiplication .",0
20775,The update gate z t decides how much new information is updated .,0
20776,z t is computed as follows :,0
20777,here x t is the input vector at step t.,0
20778,The candidate stateh t is computed b ?,0
20779,where rt is the reset gate which controls the flow of previous information .,0
20780,"Similarly to the update gate , the reset gate rt is computed as :",0
20781,We can see that the representation of step t depends upon all the previous input vectors .,0
20782,"Thus , we can also express the tth step state shown in Equation .",0
20783,Disconntected Recurrent Neural Networks ( DRNN ),0
20784,"To reduce the burden of modeling the entire sentence , we limit the distance of information flow in RNN .",0
20785,"Like other RNN variants , we feed the input sequence into an RNN model and generate an output vector at each step .",0
20786,One important difference from RNN is that the state of our model at each step is only related to the previous k ?,0
20787,1 words but not all the previous words .,0
20788,Here k is a hyperparameter called window size that we need to set .,0
20789,Our proposed model DRNN is illustrated in .,0
20790,Since the output at each step only depends on the previous k ?,0
20791,"1 words and current word , the output can also be regarded as a representation of a phrase with k words .",0
20792,Phrases with the same k words will always have the same representation no matter where they are .,0
20793,"That is , we incorporate the position - invariance into RNN by disconnecting the information flow of RNN .",0
20794,"Similarly , we can get the state ht as follows :",0
20795,"Here k is the window size , and RN N can be naive RNN , LSTM , GRU or any other kinds of recurrent units .",0
20796,Comparison with Convolutional Neural,0
20797,Network,0
20798,"here , we concatenate k word vectors and generate vector ct .",0
20799,Then we can get the output of convolution as follows :,0
20800,where,0
20801,Wis a set of convolution filters and b is a bias vector .,0
20802,Then a pooling operation can be applied after the convolutional layer and generate a fixed size vector .,0
20803,"Similarly to RNN and DRNN , we can also represent the context vector of CNN as followings :",0
20804,"Obviously , the parameters of convolution filters W increase as the window size k increases .",0
20805,"By contrast , for DRNN the parameters do not increase with the increase of window size .",0
20806,"Hence , DRNN can mitigate overfitting problem caused by the increase of parameters .",0
20807,DRNN for Text Classification,0
20808,"DRNN is a general model framework , which can be used for a variety of tasks .",0
20809,"In this paper , we only discuss how to apply DRNN in text categorization .",0
20810,We utilize GRU as recurrent units of DRNN and get the context representation of each step .,0
20811,Every w t - 3 w t - 2 w t - 1 wt ht ht w t - 3 w t - 2 w t - 1 w t : Dropout in DRNN .,0
20812,The dashed arrows indicate connections where dropout is applied .,0
20813,"The left model only applies dropout in input and output layers , but the right model applies dropout in hidden states .",0
20814,context vector can be considered as a representation of a text fragment .,0
20815,Then we feed the context vectors into a multi - layer perceptron ( MLP ) to extract high - level features as illustrated in .,0
20816,"Before feeding the vectors into MLP , we utilize Batch Normalization after DRNN , so that the model can alleviate the internal covariate shift problem .",0
20817,"To get the text representation vector , we apply max pooing after MLP layer to extract the most important information and position - invariant features .",0
20818,"Finally ,",0
20819,We feed the text representation vector into an MLP with rectified linear unit ( ReLU ) activation and send the output of MLP to a softmax function to predict the probability of each category .,0
20820,We use cross entropy loss function as follows :,0
20821,where ?,0
20822,i is the predicted probability and y i is the true probability of class i.,0
20823,"To alleviate the overfitting problem , we apply dropout regularization in DRNN model .",0
20824,"Dropout is usually applied in the input and output layers but not the hidden states of RNN , because the number of previous states is variable .",0
20825,"In contrast , our DRNN model has a fixed window size for output at each step , so we also apply dropout in the hidden states .",0
20826,"In this paper , we apply dropout in the input layer , output layer , and hidden states .",0
20827,The shows the difference to apply dropout between,0
20828,Datasets Introduction,0
20829,We use 7 large - scale text classification datasets which are proposed by .,0
20830,We summarize the datasets in .,0
20831,AG corpus is news and DBPedia is an ontology which comes from the Wikipedia .,0
20832,Yelp and Amazon corpus are reviews for which we should predict the sentiment .,0
20833,"Here P. means that we only need to predict the polarities of the dataset , while F. indicates that we need predict the star number of the review .",0
20834,Yahoo! Answers ( Yah. A. ) is a question answering dataset .,0
20835,"We can see that these datasets contain various domains and sizes , which would be credible to validate our models .",0
20836,Implementation Details,0
20837,We tokenize all the corpus with NLTK 's tokenizer .,0
20838,We limit the vocabulary size of each dataset as shown in .,0
20839,The words not in vocabulary are replaced with a special token UNK .,0
20840,also shows the window sizes that we set for these datasets .,0
20841,"We utilize the 300D Glo Ve 840B vectors ( Pennington et al. , 2014 ) as our pre-trained word embeddings .",1
20842,"For words that do not appear in Glo Ve , we average the vector representations of 8 words around the word in training dataset as its word vector , which has been applied by .",0
20843,"When training our model , word embeddings are updated along with other parameters .",0
20844,"We use Adadelta ( Zeiler , 2012 ) to optimize all the trainable parameters .",1
20845,The hyperparameter of Adadelta is set as Zeiler ( 2012 ) suggest that is 1 e ? 6 and ? is 0.95 .,1
20846,"To avoid the gradient explosion problem , we apply gradient norm clipping .",1
20847,The batch size is set to 128 and all the dimensions of input vectors and hidden shows that our proposed model significantly outperforms all the other models in 7 datasets .,1
20848,DRNN does not have too many hyperparameters .,0
20849,The main hyperparameter is the window size which can be determined by an empirical method .,0
20850,The top block shows the traditional methods and some other neural networks which are not based on RNN or CNN .,0
20851,"The linear model achieves a strong baseline in small datasets , but performs not well in large data .",0
20852,Fast - Text and region embedding methods achieve comparable performance with other CNN and RNN based models .,0
20853,Experimental Results,0
20854,The RNN based models are listed in the second block and CNN based models are in the third block .,0
20855,The D - LSTM is a discriminative LSTM model .,0
20856,Hierarchical attention network ( HAN ) is a hierarchical GRU model with attentive pooling .,0
20857,We can see that very deep CNN ( VDCNN ) performs well in large datasets .,1
20858,"However , VDCNN is a CNN model with 29 convolutional layers , which needs to be tuned more carefully .",0
20859,"By contrast , our proposed model can achieve : DGRU compared with CNN better performance in these datasets by simply setting a large window size .",0
20860,Char-CRNN in the fourth block is a model which combines positioninvariance of CNN and long - term dependencies of RNN .,0
20861,"Nevertheless , they do not achieve great improvements over other models .",0
20862,"They first utilize convolution operation to extract position - invariant features , and then use RNN to capture long - term dependencies .",0
20863,"Here , modeling the whole sequence with RNN leads to a loss of position - invariance .",0
20864,"Compared with their model , our model can better maintain the position - invariance by max pooling .",0
20865,shows that our model achieves 10 - 50 % relative error reduction compared with char - CRNN in these datasets .,1
20866,Comparison with RNN and CNN,0
20867,"In this section , we compare DRNN with CNN , GRU and LSTM .",0
20868,"To make these models comparable , we im - plement these models with the same architecture shown in .",0
20869,We just replace the DRNN with CNN or RNN .,0
20870,we firstly compare DRNN with CNN on AG dataset .,0
20871,shows that DRNN performs far better than CNN .,1
20872,"In addition , the optimal window size of CNN is 3 , while for DRNN the optimal window size is 15 .",0
20873,It indicates that DRNN can model longer sequence as window size increases .,0
20874,"By contrast , simply increasing the window size of CNN only results in overfitting .",0
20875,That is also why design complex CNN models to learn long - term dependencies other than simply increase the window size of convolution filters .,0
20876,"In addition , we also compare our model with GRU and LSTM .",0
20877,The experimental results are shown in .,0
20878,Our model DRNN achieves much better performance than GRU and LSTM .,1
20879,Qualitative Analysis,0
20880,"To investigate why DGRU performs better than CNN and GRU , we do some error analysis on Yelp P. dataset .",0
20881,shows two examples which have been both case1 :,0
20882,I love Hampton Inn but this location is in serious need of remodeling and some deep cleaning .,0
20883,Musty smell everywhere .,0
20884,"case2 : Pretty good service , but really busy and noisy !!",0
20885,It gets a little overwhelming because the salespeople are very knowledgeable and bombard you with useless techy information to I guess impress you ??,0
20886,Anyways I bought the Ipad 3 and it is freaking awesome and makes up for the store .,0
20887,I would give the Ipad 3 a gazillion stars if I could .,0
20888,I left it at home today and got really sad when I was driving away .,0
20889,Boo Hoo !! :,0
20890,Examples of error analysis .,0
20891,The case 1 is a negative review and case 2 is a positive review .,0
20892,The first example is misclassified by CNN and classified correctly by GRU .,0
20893,The second one is just the contrary .,0
20894,DGRU classify both examples correctly .,0
20895,classified correctly by DRNN .,0
20896,The first example is misclassified by CNN and classified correctly by GRU .,0
20897,It is just contrary to the second example .,0
20898,"Considering the first example , CNN may extract some key phrases such as I love and misclassifies the example as P ositive , while GRU can model long sequence and capture the information after but .",0
20899,"For the second example , however , GRU still captures the information after but and neglects the key phrases such as pretty good service and freaking awesome , which leads to the wrong classification .",0
20900,"DGRU can both extract the local key features such as pretty good service and capture long - term information such as the sentence after but , which makes it perform better than GRU and CNN .",0
20901,Component Analysis,0
20902,Recurrent Unit,0
20903,"In this part , we study the impact of different recurrent units on the effectiveness of DRNN .",0
20904,"We choose three types of recurrent units : naive RNN , LSTM and GRU which have been compared by .",0
20905,We carry out the experiments with different window sizes to eliminate the impact of window sizes .,0
20906,All the experiments in this part are conducted on the AG dataset .,0
20907,We find that the disconnected naive RNN performs just a little worse than disconnected LSTM ( DLSTM ) and disconnected GRU ( DGRU ) when the window size is lower than 5 .,0
20908,"However , when the window size is more than 10 , its performance decreases rapidly and the error rate becomes even more than 20 % .",0
20909,We believe that it is due to vanishing gradient problem of naive RNN .,0
20910,"From ( a ) , we can see that window sizes affect the performance of DGRU and DLSTM .",0
20911,"DGRU achieves the best performance when the window size is 15 , while the best window size for DLSTM is 5 .",0
20912,The performance of DGRU is always better than DLSTM no matter what the window size is .,0
20913,We also find that the DGRU model converges faster than DLSTM in the process of training .,0
20914,"Therefore , we apply GRU as recurrent units of DRNN in this paper for all the other experiments .",0
20915,Pooling Method Pooling is a kind of method to subsample the values to capture more important information .,0
20916,"In NLP , pooling can also convert a variable - length tensor or vector into a fixed - length one , so that it can be dealt with more easily .",0
20917,"There 're several kinds of pooling methods such as max pooling , mean pooling and attentive pooling .",0
20918,We still conduct the experiments on AG dataset .,0
20919,shows the experimental results of three pooling methods along with different window sizes .,0
20920,"From ( b ) , we can see that the DRNN model with max pooling performs better than the others .",0
20921,This maybe because that max pooling can capture position - invariant features better .,0
20922,We find attentive pooling is not significantly affected by window sizes .,0
20923,"However , the performance of mean pooling becomes worse as the window becomes larger .",0
20924,Window size analysis,0
20925,"In this section , we mainly study what factors affect the optimal window size .",0
20926,"In addition to the recurrent units and pooling methods discussed above , we believe the optimal window size maybe also related to the amount of training data and the type of task .",0
20927,"In order to study the factors that affect the optimal window size , we conduct experiments on three datasets : AG , DBP and Yelp Polarity .",0
20928,"To eliminate the influence of differrnt training data sizes , we conduct experiments with the same training data size .",0
20929,From ( a ) we can see that the type of task has a great impact on the optimal window size .,0
20930,"For AG and DBPedia , the optimal window size is 15 .",0
20931,"However , for Yelp P. the optimal window size is 40 or even larger .",0
20932,"The result is intuitive , because sentiment analysis such as Yelp often involves long - term dependencies , while topic classification such as AG and DBPedia relys more on the key phrases .",0
20933,From ( b ) and ( c ) we can see the effect of different training data sizes on the optimal window size .,0
20934,"Surprisingly , the effect of different training data sizes on the optimal window size seems little .",0
20935,"We can see that for both DBPedia and Yelp corpus , the trend of error rate with the window size is similar .",0
20936,This shows that the number of training data has little effect on the choice of the optimal window size .,0
20937,It also provides a good empirical way for us to choose the optimal window size .,0
20938,"That is , conducting experiments on a small dataset first to select the optimal window size .",0
20939,Conclusion,0
20940,"In this paper , we incorporate position - invariance into RNN , so that our proposed model DRNN can both capture key phrases and long - term dependencies .",0
20941,We conduct experiments to compare the effects of different recurrent units and pooling operations .,0
20942,"In addition , We also analyze what factors affect the optimal window size of DRNN and present an empirical method to search it .",0
20943,"The experimental results show that our proposed model outperforms CNN and RNN models , and achieve the best performance in seven large - scale text classification datasets .",0
20944,title,0
20945,Neural Attentive Bag - of - Entities Model for Text Classification,1
20946,abstract,0
20947,"This study proposes a Neural Attentive Bagof - Entities model , which is a neural network model that performs text classification using entities in a knowledge base .",0
20948,Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts .,0
20949,"We combine simple high - recall entity detection based on a dictionary , to detect entities in a document , with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities .",0
20950,"We tested the effectiveness of our model using two standard text classification datasets ( i.e. , the 20 Newsgroups and R8 datasets ) and a popular factoid question answering dataset based on a trivia quiz game .",0
20951,"As a result , our model achieved state - of - the - art results on all datasets .",0
20952,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,1
20953,Introduction,0
20954,"Text classification is an important task , and its applications span a wide range of activities such as topic classification , spam detection , and sentiment classification .",0
20955,"Recent studies showed that models based on neural networks can outperform conventional models ( e.g. , nave Bayes ) on text classification tasks .",0
20956,Typical neural network - based text classification models are based on words .,0
20957,"They typically use words in the target documents as inputs , map words into continuous vectors ( embeddings ) , and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings , convolutional neural networks ( CNN ) , and recurrent neural networks ( RNN ) .",0
20958,"Apart from the aforementioned approaches , past studies attempted to use entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) to capture the semantics in documents .",0
20959,These models typically represent a document by using a set of entities ( or bag of entities ) relevant to the document .,0
20960,"The main benefit of using entities instead of words is that unlike words , entities provide unambiguous semantic signals because they are uniquely identified in a KB .",0
20961,One key issue here is to determine the way in which to associate a document with its relevant entities .,0
20962,An existing straightforward approach involves creating a set of relevant entities using an entity linking system to detect and dis ambiguate the names of entities in a document .,0
20963,"However , this approach is problematic because ( 1 ) entity linking systems produce dis ambiguation errors , and ( 2 ) entities appearing in a document are not necessarily relevant to the given document .",0
20964,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .",1
20965,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .",1
20966,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,1
20967,"In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .",1
20968,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .",0
20969,"We validate the effectiveness of our proposed model by addressing two important natural language tasks : a text classification task using two standard datasets ( i.e. , the 20 Newsgroups and R8 datasets ) , and a factoid question answering task based on a popular dataset derived from the quiz bowl trivia quiz game .",0
20970,"As a result , our model achieved state - of - the - art results on both tasks .",0
20971,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,0
20972,Our Approach,0
20973,"Given a document , our model addresses the text classification task by using the following two steps : it first detects entities from the document , and then classifies the document using the proposed model with the detected entities as inputs .",0
20974,Entity Detection,0
20975,"As the target KB , we used the September 2018 version of Wikipedia , which contains a total of 7,333,679 entities .",0
20976,2,0
20977,"Regarding the entity dictionary described in Section 2.1 , we excluded an entity name if its link probability was lower than 1 % and a referent entity if its commonness given the entity name was lower than 3 % for computational efficiency .",0
20978,Entity names were treated as case-insensitive .,0
20979,"As a result , the dictionary contained 18,785,550 entity names , and each name had 1.14 referent entities on average .",0
20980,"Furthermore , to detect entities from a document , we also tested two publicly available entity linking systems , Wikifier and TAGME , instead of using dictionarybased entity detection .",0
20981,"We selected these systems because they are capable of detecting non-named entities ( e.g. , technical terms ) that are useful for addressing the text classification task .",0
20982,4,0
20983,"Here , we used the entities detected and dis ambiguated by these systems as inputs to our neural network model .",0
20984,Model,0
20985,where v w ?,0
20986,Rd is the embedding of word w .,0
20987,We then derive the entity - based representation of D as a weighted average of the embeddings of the entities :,0
20988,where v e ?,0
20989,Rd is the embedding of entity e and a e the normalized attention weight corresponding toe computed using the following softmax - based attention function :,0
20990,where w a ?,0
20991,"R l is a weight vector , b a ?",0
20992,"R is the bias , and ?( e , D ) is a function that generates an l-dimensional vector consisting of the features of the attention function .",0
20993,We use the following two features in the attention function :,0
20994,Cosine : the cosine similarity between the embedding of the entity v e and the wordbased representation of the document z word .,0
20995,Commonness : the probability that the entity name refers to the entity in KB .,0
20996,"Here , our aim is to capture the relevance and the unambiguity of entity e in document D using the attention function .",0
20997,"Thus , the problem is related to the tasks of entity salience detection , which aims to detect entities relevant ( or salient ) to the document , and entity linking , which aims to resolve the ambiguity of entities .",0
20998,"The key assumption relating to these two tasks in the literature is that if an entity is semantically related to the given document , it is relevant to the document , and it is likely to appear in the document .",0
20999,"With this in mind and following past work , we use the cosine similarity between v e and z word as a feature .",0
21000,"Further , as in past entity linking studies , we also use the commonness of the name referring to the entity .",0
21001,"Moreover , we derive a representation based both on entities and words by simply adding z entity and z word 1 :",0
21002,"We then solve the task using a multiclass logistic regression classifier with the computed representation ( i.e. , with z entity or z full ) as features .",0
21003,"In the remainder of this paper , we denote our models based on z entity and z full by NABoE-entity and NABoE - full , respectively .",0
21004,Experimental Setup,0
21005,"In this section , we describe our experimental setup used both in the text classification and the factoid question answering experiments presented below .",0
21006,Pretrained Embeddings,0
21007,We initialized the embeddings of words ( v w ) and entities ( v e ) using pretrained embeddings trained on KB .,0
21008,"To learn embeddings from the KB , we used the method adopted in the open source Wikipedia2 Vec tool .",0
21009,"In particular , we generated an entity - annotated corpus from Wikipedia by treating entity links in Wikipedia articles as entity annotations , and trained skip - gram embeddings of 300 dimensions with negative sampling using the generated corpus as inputs .",0
21010,The learned embeddings place similar words and entities close to one another in a unified vector space .,0
21011,"Here , we used the same version of Wikipedia described in Section 3.1 .",0
21012,Text Classification,0
21013,"To evaluate the effectiveness of our proposed model , we first conducted the text classification task on two standard datasets , namely the 20 Newsgroups ( 20 NG ) and R8 datasets .",0
21014,Setup,0
21015,Our experimental setup described in this section follows that in past work .,0
21016,We address this task as a text classification problem that selects the most relevant answer from the possible answers observed in the dataset .,0
21017,We obtained the dataset proposed in .,0
21018,We only used questions in the history and literature categories .,0
21019,"Furthermore , we excluded questions of which the answers appear fewer than six times in the dataset .",0
21020,"As a result , the number of candidate answers was 303 and 424 in the history and literature categories , respectively .",0
21021,"We used 20 % of questions each for the development set and test sets , and the remaining 60 % for the training set .",0
21022,"As a result , the training , development , and test sets consisted of 1,535 , 511 , and 511 questions for the history category , and 2,524 , 840 , and 840 questions for the literature category .",0
21023,The settings we used to train the model were the same as those in the previous experiment ( see Section 4.1 ) .,0
21024,The model was trained using mini-batch SGD with its learning rate controlled by Adam and its mini-batch size set to 32 .,1
21025,"We used words and entities that were detected three times or more in the dataset , and ignored the other words and entities .",0
21026,The size of the embeddings of words and entities was set to d = 300 .,1
21027,"As in past work , we report the accuracy score , and the score on the development set was used for early stopping . :",0
21028,Accuracy of the proposed and baseline methods for the factoid QA task .,0
21029,Baselines,1
21030,We used the following baseline models :,0
21031,BoW,1
21032,This model is based on a logistic regression classifier with conventional binary BoW features .,1
21033,FTS- BRNN,1
21034,This model is based on a bidirectional RNN with gated recurrent units ( GRU ) .,1
21035,It uses the logistic regression classifier with the features derived by the RNN .,0
21036,NTEE This model is a state - of - the - art model that uses a multi - layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper .,1
21037,"Similar to our previous experiment , we also add SWEM - concat , and the variants of our NABoEentity and NABoE - full models based on Wikifier and TAGME ( see Section 4.2 ) .",0
21038,Note that all the baselines address the task as a text classification problem .,0
21039,provides the results of our models and those of our baselines .,0
21040,"Overall , our models achieved enhanced performance on this task .",0
21041,"In particular , the NABoE - full model successfully outperformed all the baseline models , and the NABoE-entity model achieved competitive performance and outperformed all the baseline models in the literature category .",0
21042,These results clearly highlighted the effectiveness of our model for this task .,0
21043,Results,1
21044,"Relative to the baselines , our models yielded enhanced over all performance on both datasets .",1
21045,The NABoE - full model outperformed all baseline models in terms of both measures on both datasets .,1
21046,"Furthermore , the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset , and the F 1 score on the R8 dataset .",1
21047,"Moreover , our attention mechanism consistently improved the performance .",0
21048,"These results clearly highlighted the effectiveness of our approach , which addresses text classification by using a small number of unambiguous and relevant entities detected by the proposed attention mechanism .",0
21049,"Moreover , the pretrained embeddings improved the performance on both datasets .",0
21050,"Further , the models based on the dictionarybased entity detection ( see Section 2.1 ) generally outperformed the models based on the entity linking systems ( i.e. , Wikifier and TAGME ) .",0
21051,We consider that this is because these entity linking systems failed to detector dis ambiguate entity names that were useful to address the text classification task .,0
21052,"Moreover , our attention mechanism consistently improved the performance for Wikifierand TAGME - based models because the attention mechanism enabled the model to focus on entities that were relevant to the document .",0
21053,Analysis,0
21054,"In this section , we provide a detailed analysis of the performance of our model in terms of conducting the text classification task .",0
21055,"We first provide a comparison of the SWEM - concat , NABoEentity , and NABoE - full models using class - level F 1 scores on both of the datasets ( see ) .",0
21056,"Here , we aim to compare the detailed performance of the word - based model ( SWEM - concat ) , entitybased model ( NABoE-entity ) , and the model based on both words and entities ( NABoE - full ) .",0
21057,"Compared with the SWEM - concat model , the NABoE - full and NABoE-entity models performed more accurately in 23 out of 28 and 17 out of 28 classes , respectively .",0
21058,This result clearly demonstrates the ability of the model to successfully capture strong semantic signals that can only be obtained from entities .,0
21059,"Moreover , we observed that the NABoE-entity model achieved weaker performance especially for the misc. forsale class in the 20 NG dataset and several classes in the R8 dataset .",0
21060,"Regarding the misc. forsale class , because documents in this class contain a wider variety of entities ( i.e. , objects users want to sell ) than other classes , the model failed to capture the effective semantic signals from the entities .",0
21061,"Further , as described in the error analysis provided below , it often appeared to be difficult to distinguish pairs of similar classes in the R8 dataset based only on entities .",0
21062,"Next , we conducted a feature study of the attention mechanism by excluding one feature at a time from the NABoE-entity model .",0
21063,We found both of the features to make an important contribution to the performance .,0
21064,"Furthermore , to investigate the attention mechanism in more detail , we computed the top influential entities in the attention mechanism for each class on the 20 NG and R8 datasets .",0
21065,"In particular , we calculated the number of times each entity obtained the highest attention weight in the test documents in each class and selected the five most frequent ones .",0
21066,presents the results .,0
21067,"Overall , our attention mechanism successfully selected entities that were highly relevant to each class .",0
21068,"For example , Cryptography , Algorithm , Escrow , Considered harmful , and Encryption were selected for the sci.crypt class .",0
21069,"Furthermore , although we did not explicitly perform entity dis ambiguation , the model successfully overcame the ambiguity issues in the entity names and attended to the entities that were relevant to the classes .",0
21070,"Subsequently , we conducted an error analysis by selecting 50 random test documents for which the NABoE-entity model made wrong predictions .",0
21071,"Most of the errors were caused by two pairs of classes : 22 errors were caused by misclassifying documents of acq ( corporate acquisitions ) and those of earn ( corporate earnings ) , and 13 errors were caused by misclassifying documents of interest and those of money - fx .",0
21072,"Furthermore , the model tended to perform poorly if a document contained entities that strongly indicate an incorrect class .",0
21073,"For example , a money - fx document containing the entity interest rate multiple times was classified into the interest class , and a document in the acq class reporting news related to oil companies ( i.e. , ExxonMobil and ZENEX ) was classified into the crude class .",0
21074,Factoid Question Answering,0
21075,"In this section , we address factoid question answering based on a dataset consisting of questions of the quiz bowl trivia quiz game .",0
21076,"Factoid ques- tion answering is one of the common settings of question answering that aims to predict an entity ( e.g. , events , authors , and books ) that is described in a given question .",0
21077,The players of quiz bowl solve questions consisting of sentences that describe an entity .,0
21078,Quiz bowl questions have frequently been used for evaluating neural network - based models in recent studies .,0
21079,This task has a significantly larger number of target classes compared to the task addressed in the previous experiment .,0
21080,Our main aim here is to evaluate the effectiveness of using entities to capture the finer - grained semantics required to perform the task of factoid question answering effectively .,0
21081,Results and Analysis,0
21082,"Furthermore , similar to the previous text classification experiment , the attention mechanism and the pretrained embeddings consistently improved the performance .",0
21083,"Moreover , the models based on dictionary - based entity detection outperformed the models based on the entity linking systems .",0
21084,We also conducted an error analysis using the NABoE-entity model and the test questions in the history category .,0
21085,We found nearly 70 % of the errors to be caused by questions of which the answers were country names .,0
21086,"This is because these questions tended to provide indirect clues ( e.g. , describing a notable person born in the country ) and most entities used in these clues do not directly indicate the answer ( i.e. , country names ) .",0
21087,"Furthermore , our model failed in difficult cases such as predicting Tokugawa shogunate instead of Tokugawa Ieyasu .",0
21088,Related Work,0
21089,KB entities have been conventionally used to model the semantics in texts .,0
21090,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents a document using a bag of entities , namely a sparse vector of which each dimension corresponds to the relevance score of the text to each entity .",0
21091,"This simple method is shown to be effective for various NLP tasks including text classification and information retrieval , Several neural network models that use KB entities to capture the semantics in texts have been proposed .",0
21092,These models typically depend on an additional preprocessing step that extracts the relevant entities from the target texts .,0
21093,"For example , used the Probase conceptualization API for short text classification by retrieving the Probase entities that were relevant to the target text and used them in a model based on also extracted entities using a graph - based linking algorithm and used these entities in a neural network model .",0
21094,A similar approach was adopted in ; they extracted entities from the target text using an entity linking system and simply used the detected entities in a neural network model .,0
21095,"However , un - like these models , our proposed model addresses the task in an end - to - end manner ; i.e. , entities that are relevant to the target text are automatically selected using our neural attention mechanism .",0
21096,"Furthermore , we also used the model proposed by as a baseline in our text classification experiments .",0
21097,"Additionally , our work is also related to studies on entity linking .",0
21098,"Entity linking models can be roughly classified into two groups : local models , which resolve entity names independently using the contextual relevance of the entity given a document , and global models , in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results .",0
21099,Recent state - of - the - art models typically combine both of these models .,0
21100,"However , several studies also showed that the local model alone can achieve results competitive to those of the global and combined models .",0
21101,"In this study , we adopt a simple but effective local model , which uses cosine similarity between the embedding of the target entity and the word - based representation of the document to capture the relevance of an entity given a document .",0
21102,Conclusions,0
21103,"This study proposed NABoE , which is a neural network model that performs text classification using entities in Wikipedia .",0
21104,We combined simple dictionary - based entity detection with a neural attention mechanism to enable the model to focus on a small number of unambiguous and relevant entities in a document .,0
21105,"We achieved state - of - theart results on two important NLP tasks , namely text classification and factoid question answering , which clearly verified the effectiveness of our approach .",0
21106,"As a future task , we intend to more extensively analyze our model and explore its effectiveness for other NLP tasks .",0
21107,"Furthermore , we would also like to test more expressive neural network models for example by integrating global entity coherence information into our neural attention mechanism .",0
21108,title,0
21109,Deep Pyramid Convolutional Neural Networks for Text Categorization,1
21110,abstract,0
21111,This paper proposes a low - complexity word - level deep convolutional neural network ( CNN ) architecture for text categorization that can efficiently represent longrange associations in text .,0
21112,"In the literature , several deep and complex neural networks have been proposed for this task , assuming availability of relatively large amounts of training data .",0
21113,"However , the associated computational complexity increases as the networks go deeper , which poses serious challenges in practical applications .",0
21114,"Moreover , it was shown recently that shallow word - level CNNs are more accurate and much faster than the state - of - the - art very deep nets such as character - level CNN s even in the setting of large training data .",0
21115,"Motivated by these findings , we carefully studied deepening of word - level CNNs to capture global representations of text , and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much .",0
21116,We call it deep pyramid CNN .,0
21117,The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization .,0
21118,Introduction,0
21119,"Text categorization is an important task whose applications include spam detection , sentiment classification , and topic classification .",0
21120,"In recent years , neural networks that can make use of word order have been shown to be effective for text categorization .",0
21121,"While simple and shallow convolutional neural networks ( CNNs ) ) were proposed for this task earlier , more recently , deep and more complex neural networks have also been studied , assuming availability of relatively large amounts of training data ( e.g. , one million documents ) .",0
21122,"Examples are deep character - level CNNs , a complex combination of CNNs and recurrent neural networks ( RNNs ) , and RNNs in a wordsentence hierarchy .",0
21123,A CNN is a feedforward network with convolution layers interleaved with pooling layers .,0
21124,"Essentially , a convolution layer converts to a vector every small patch of data ( either the original data such as text or image or the output of the previous layer ) at every location ( e.g. , 3 - word windows around every word ) , which can be processed in parallel .",0
21125,"By contrast , an RNN has connections that form a cycle .",0
21126,"In its typical application to text , a recurrent unit takes words one by one as well as its own output on the previous word , which is parallel - processing unfriendly .",0
21127,"While both CNNs and RNNs can take advantage of word order , the simple nature and parallel - processing friendliness of CNNs make them attractive particularly when large training data causes computational challenges .",0
21128,There have been several recent studies of CNN for text categorization in the large training data setting .,0
21129,"For example , in , very deep 32 - layer character - level CNNs were shown to outperform deep 9 - layer character - level CNNs of .",0
21130,"However , in , very shallow 1 - layer word - level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of .",0
21131,"Although character - level approaches have merit in not having to deal with millions of distinct words , shallow word - level CNNs turned out to be superior even when used with only a manageable number ( 30 K ) of the most frequent words .",0
21132,This demonstrates the basic fact - knowledge of word leads to a powerful representation .,0
21133,These results motivate us to pursue an effective and efficient design of deep wordlevel CNNs for text categorization .,0
21134,"Note , however , that it is not as simple as merely replacing characters with words in character - level CNNs ; doing so rather degraded accuracy in .",0
21135,We carefully studied deepening of word - level CNNs in the large - data setting and found a deep but low - complexity network architecture with which the best accuracy can be obtained by increasing the depth but not the order of computation time - the total computation time is bounded by a constant .,0
21136,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",1
21137,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",1
21138,The network depth can be treated as a meta-parameter .,1
21139,The computational complexity of this network is bounded to be no more than twice that of one convolution block .,1
21140,"At the same time , as described later , the ' pyramid ' enables efficient discovery of long - range associations in the text ( and so more global information ) , as the network is deepened .",0
21141,"This is why DPCNN can achieve better accuracy than the shallow CNN mentioned above ( hereafter ShallowCNN ) , which can use only short - range associations .",0
21142,"Moreover , DPCNN can be regarded as a deep extension of ShallowCNN , which we proposed in and later tested with large datasets in .",0
21143,We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,1
21144,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",1
21145,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,1
21146,The final pooling layer aggregates internal data for each document into one vector .,1
21147,We use max pooling for all pooling layers .,1
21148,The key features of DPCNN are as follows .,0
21149,"Downsampling without increasing the number of feature maps ( dimensionality of layer output , 250 in ) .",0
21150,Downsampling enables efficient representation of long - range associations ( and so more global information ) in the text .,0
21151,"By keeping the same number of feature maps , every 2 - stride downsampling reduces the per-block computation by half and thus the total computation time is bounded by a constant .",0
21152,Shortcut connections with pre-activation and identity mapping for enabling training of deep networks .,0
21153,Text region embedding enhanced with unsupervised embeddings ( embeddings trained in an unsupervised manner ) for improving accuracy .,0
21154,Network architecture,0
21155,Downsampling with the number of feature maps fixed,0
21156,"After each convolution block , we perform max - pooling with size 3 and stride",0
21157,"2 . That is , the pooling layer produces a new internal representation of a document by taking the component - wise maximum over 3 contiguous internal vectors , representing 3 overlapping text regions , but it does this only for every other possible triplet ( stride 2 ) instead of all the possible triplets ( stride 1 ) .",0
21158,This 2 - stride downsampling reduces the size of the internal representation of each document by half .,0
21159,"A number of models increase the number of feature maps whenever downsampling is performed , causing the total computational complexity to be a function of the depth .",0
21160,"In contrast , we fix the number of feature maps , as we found that increasing the number of feature maps only does harm - increasing computation time substantially without accuracy improvement , as shown later in the experiments .",0
21161,"With the number of feature maps fixed , the computation time for each convolution layer is halved ( as the data size is halved ) whenever 2 - stride downsampling is performed , thus , forming a ' pyramid ' .",0
21162,Computation per layer is halved after every pooling .,0
21163,Computation per layer is halved after every pooling .,0
21164,"Therefore , with DPCNNs , the total computation time is bounded by a constant - twice the computation time of a single block , which makes our deep pyramid networks computationally attractive .",0
21165,"In addition , downsampling with stride 2 essentially doubles the effective coverage ( i.e. , coverage in the original document ) of the convolution kernel ; therefore , after going through downsampling L times , associations among words within a distance in the order of 2 L can be represented .",0
21166,"Thus , deep pyramid CNN is computationally efficient for representing long - range associations and so more global information .",0
21167,Shortcut connections with pre-activation,0
21168,"To enable training of deep networks , we use additive shortcut connections with identity mapping , which can be written as z + f ( z ) where f represents the skipped layers .",0
21169,"In DPCNN , the skipped layers f ( z ) are two convolution layers with pre-activation .",0
21170,"Here , pre-activation refers to activation being done before weighting instead of after as is typically done .",0
21171,"That is , in the convolu-tion layer of DPCNN , W ? ( x ) + b is computed at every location of each document where a column vector x represents a small region ( overlapping with each other ) of input at each location , ? ( ) is a component - wise nonlinear activation , and weights W and biases b ( unique to each layer ) are the parameters to be trained .",0
21172,The number of W 's rows is the number of feature maps ( also called the number of filters ) of this layer .,0
21173,"We set activation ? ( ) to the rectifier ?( x ) = max ( x , 0 ) .",0
21174,"In our implementation , we fixed the number of feature maps to 250 and the kernel size ( the size of the small region covered by x ) to 3 , as shown in .",0
21175,"With pre-activation , it is the results of linear weighting ( W ? ( x ) + b ) that travel through the shortcut , and what is added to them at a ? ( ) is also the results of linear weighting , instead of the results of nonlinear activation (? ( Wx + b ) ) .",0
21176,"Intuitively , such ' linearity ' eases training of deep networks , similar to the role of constant error carousels in LSTM .",0
21177,"We empirically observed that preactivation indeed outperformed ' post - activation ' , which is inline with the image results .",0
21178,No need for dimension matching,0
21179,"Although the shortcut with pre-activation was adopted from the improved ResNet of , our model is simpler than ResNet ) , as all the shortcuts are exactly simple identity mapping ( i.e. , passing data exactly as it is ) without any complication for dimension matching .",0
21180,"When a shortcut meets the ' main street ' , the data from two paths need to have the same dimensionality so that they can be added ; therefore , if a shortcut skips a layer that changes the dimensionality , e.g. , by downsampling or by use of a different number of feature maps , then a shortcut must perform dimension matching .",0
21181,"Dimension matching for increased number of feature maps , in particular , is typically done by projection , introducing more weight parameters to be trained .",0
21182,"We eliminate the complication of dimension matching by not letting any shortcut skip a downsampling layer , and by fixing the number of feature maps throughout the network .",0
21183,"The latter also substantially saves computation time as mentioned above , and we will show later in our experiments that on our tasks , we do not sacrifice anything for such a substantial efficiency gain .",0
21184,Text region embedding,0
21185,A CNN for text categorization typically starts with converting each word in the text to a word vector ( word embedding ) .,0
21186,We take a more general viewpoint as in and consider text region embedding - embedding of a region of text covering one or more words .,0
21187,Basic region embedding,0
21188,We start with the basic setting where there is no unsupervised embedding .,0
21189,"In the region embedding layer we compute Wx + b for each word of a document where input x represents a k -word region ( i.e. , window ) around the word in some straightforward manner , and weights W and bias bare trained with the parameters of other layers .",0
21190,Activation is delayed to the pre-activation of the next layer .,0
21191,"Now let v be the size of vocabulary , and let us consider the following three types of straightforward representation of a k-word region for x : ( 1 ) sequential input : the kv-dimensional concatenation of k one - hot vectors ; ( 2 ) bow input : a v-dimensional bag - of - word ( bow ) vector ; and ( 3 ) bag - of - n- gram input : e.g. , a bag of word uni , bi , and trigrams contained in the region .",0
21192,"Setting the region size k = 1 , they all become word embedding .",0
21193,"A region embedding layer with the sequential input is equivalent to a convolution layer applied to a sequence of one - hot vectors representing a document , and this viewpoint was taken to de-scribe the first layer of ShallowCNN in .",0
21194,"From the region embedding viewpoint , ShallowCNN is DPCNN 's special casein which a region embedding layer is directly followed by the final pooling layer .",0
21195,"A region embedding layer with region size k > 1 seeks to capture more complex concepts than single words in one weight layer , whereas a network with word embedding uses multiple weight layers to do this , e.g. , word embedding followed by a convolution layer .",0
21196,"In general , having fewer layers has a practical advantage of easier optimization .",0
21197,"Beyond that , the optimum input type and the optimum region size can only be determined empirically .",0
21198,"Our preliminary experiments indicated that when used with DPCNN ( but not Shal - lowCNN ) , the sequential input has no advantage over the bow input - comparable accuracy with k times more weight parameters ; therefore , we excluded the sequential input from our experiments",0
21199,2 .,0
21200,"The n-gram input turned out to be prone to overfitting in the supervised setting , likely due to its high representation power , but it is very useful as the input to unsupervised embeddings , which we discuss next .",0
21201,Enhancing region embedding with unsupervised embeddings,0
21202,"In , it was shown that accuracy was substantially improved by extending ShallowCNN with unsupervised embeddings obtained by tvembedding training ( 'tv' stands for two views ) .",0
21203,We found that accuracy of DPCNN can also be improved in this manner .,0
21204,Below we briefly review tv - embedding training and then describe how we use the resulting unsupervised embeddings with DPCNN .,0
21205,The tv-embedding training requires two views .,0
21206,"For text categorization , we define a region of text as view - 1 and its adjacent regions as view - 2 .",0
21207,"Then using unlabeled data , we train a neural network of one hidden layer with an artificial task of predicting view - 2 from view - 1 .",0
21208,"The obtained hidden layer , which is an embedding function that takes view - 1 as input , serves as an unsupervised embedding function in the model for text categorization .",0
21209,"In , we showed theoretical conditions on views and labels under which AG 93 91 unsupervised embeddings obtained this way are useful for classification .",0
21210,"For use with DPCNN , we train several unsupervised embeddings in this manner , which differ from one another in the region size and the vector representations of view - 1 ( input region ) so that we can benefit from diversity .",0
21211,"The region embedding layer of DPCNN computes Wx + u?U W ( u ) z ( u ) + b , where x is the discrete input as in the basic region embedding , and z ( u ) is the output of an unsupervised embedding function indexed by u .",0
21212,We will show below that use of unsupervised embeddings in this way consistently improves the accuracy of DPCNN .,0
21213,Experiments,0
21214,We report the experiments with DPCNNs in comparison with previous models and alternatives .,0
21215,The code is publicly available on the internet .,0
21216,Experimental setup,0
21217,Data and data preprocessing,0
21218,"To facilitate comparisons with previous results , we used the eight datasets compiled by , summarized in .",0
21219,AG and Sogou are news .,0
21220,Dbpedia is an ontology .,0
21221,Yahoo consists of questions and answers from the ' Yahoo ! Answers ' website .,0
21222,"Yelp and Amazon ( ' Ama ' ) are reviews where '.p ' ( polarity ) in the names indicates that labels are binary ( positive / negative ) , and '.f' ( full ) indicates that labels are the number of stars .",0
21223,"Sogou is in Romanized Chinese , and the others are in English .",0
21224,Classes are balanced on all the datasets .,0
21225,Data preprocessing was done as in .,0
21226,"That is , upper-case letters were converted to lower - case letters .",0
21227,"Unlike , variable - sized documents were handled as variable - sized without any shortening or padding ; however , the vocabulary size was limited to 30 K words .",0
21228,"For example , as also mentioned in , the complete vocabulary of the Ama.p training set contains 1.3 M words .",0
21229,"A vocabulary of 30 K words is only a small portion of it , but it covers about 98 % of the text and produced good accuracy as reported below .",0
21230,Training protocol,0
21231,"We held out 10K documents from the training data for use as a validation set on each dataset , and meta-parameter tuning was done based on the performance on the validation set .",0
21232,"To minimize a log loss with softmax , minibatch SGD with momentum 0.9 was conducted for n epochs ( n was fixed to 50 for AG , 30 for Yelp.f / p and Dbpedia , and 15 for the rest ) while the learning rate was set to ?",0
21233,for the first 4 5 n epochs and then 0.1 ?,0
21234,for the rest 3 .,0
21235,The initial learning rate ? was considered to be a meta-parameter .,0
21236,The minibatch size was fixed to 100 .,0
21237,Regularization was done by weight decay with the parameter 0.0001 and by optional dropout with 0.5 applied to the input to the top layer .,0
21238,"In some cases overfitting was observed , and so we performed early stopping , based on the validation performance , after reducing the learning rate to 0.1 ?.",0
21239,Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01 .,0
21240,"The discrete input to the region embedding layer was fixed to the bow input , and the region size was chosen from { 1 , 3 , 5 } , while fixing output dimensionality to 250 ( same as convolution layers ) .",0
21241,Details of unsupervised embedding training,0
21242,"To facilitate comparison with ShallowCNN , we matched our unsupervised embedding setting exactly with that of .",0
21243,"That is , we trained the same four types of tvembeddings , which are embeddings of 5 - and 9 word regions , each of which represents the input regions by either 30 K - dim bow or 200K - dim : Error rates ( % ) on larger datasets in comparison with previous models .",0
21244,The previous results are roughly sorted in the order of error rates ( best to worst ) .,0
21245,"The best results and the second best are shown in bold and italic , respectively .",0
21246,' t v' stands for tv-embeddings .,0
21247,' w2 v ' stands for word2 vec. ' ( w 2 v ) ' in row 7 indicates that the best results among those with and without word2 vec pretraining are shown .,0
21248,Note that ' best ' in rows 4 & 6 - 8 indicates that we are giving an ' unfair ' advantage to these models by choosing the best test error rate among a number of variations presented in the respective papers .,0
21249,Models,0
21250,"[ JZ16 ] : Johnson and Zhang bags of { 1,2,3 } - grams , retaining only the most frequent 30K words or 200K { 1,2,3 } - grams .",0
21251,"Training was done on the labeled data ( disregarding the labels ) , setting the training objectives to the prediction of adjacent regions of the same size as the input region ( i.e. , 5 or 9 ) .",0
21252,Weighted square loss,0
21253,") 2 was minimized where i goes through instances , z represents the target regions by bow , p is the model output , and the weights ?",0
21254,"i , j were set to achieve the negative sampling effect .",0
21255,The dimensionality of unsupervised embeddings was set to 300 unless otherwise specified .,0
21256,Unsupervised embeddings were fixed during the supervised training - no fine - tuning .,0
21257,Results,1
21258,"In the results below , the depth of DPCNN was fixed to 15 unless otherwise specified .",0
21259,Making it deeper did not substantially improve or degrade accuracy .,0
21260,"Note that we count as depth the number of hidden weight layers including the region embedding layer but excluding unsupervised embeddings , therefore , 15 means 7 convolution blocks of 2 layers plus 1 layer for region embedding .",0
21261,Main results,0
21262,Large data results,1
21263,We first report the error rates of our full model ( DPCNN with 15 weight layers plus unsupervised embeddings ) on the larger five datasets .,0
21264,"To put it into perspective , we also show the previous results in the literature .",0
21265,The previous results are roughly sorted in the order of error rates from best to worst .,0
21266,"On all the five datasets , DPCNN outperforms all of the previous results , which validates the effectiveness of our approach .",1
21267,"DPCNN can be regarded as a deep extension of ShallowCNN ( row 2 ) , sharing region embedding enhancement with diverse unsupervised embeddings .",0
21268,"Note that ShallowCNN enhanced with unsupervised embeddings ( row 2 ) was originally proposed in ) as a semi-supervised extension of , and then it was tested on the large datasets in .",0
21269,"The performance improvements of DPCNN over Shallow CNN indicates that the added depth is indeed useful , capturing more global information . 's hierarchical attention network ( row 3 ) consists of RNNs in the word level and the sentence level .",0
21270,It is more complex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation .,0
21271,"Similarly , proposed to use CNN or LSTM to represent each sentence in documents and then use RNNs .",0
21272,Although we do not have direct comparison with reports that their model outperformed and proposed deep character - level CNNs ( row 4 &6 ) .,0
21273,Their models underperform our DPCNN with relatively large differences in spite of their deepness .,0
21274,Our mod - els are word - level and therefore use the knowledge of word boundaries which character - level models have no access to .,0
21275,"While this is arguably not an apple - to - apple comparison , since word boundaries can be obtained for free in many languages , we view our model as much more useful in practice .",0
21276,"Row 7 shows the performance of deep wordlevel CNN from , which was designed to match their character - level models in complexity .",0
21277,It s relatively poor performance shows that it is not easy to design a high - performance deep word - level CNN .,0
21278,Computation time,0
21279,"In , we plot error rates in relation to the computation time - the time spent for categorizing 10K documents using our implementation on a GPU .",0
21280,The right figure is a close - up of x ?,0
21281,"[ 0 , 20 ] of the left figure .",0
21282,It stands out in the left figure that the character - level CNN of is much slower than DPCNNs .,0
21283,"This is partly because it increases the number of feature maps with downsampling ( i.e. , no pyramid ) while it is deeper ( 32 weight layers ) , and partly because it deals with characters - there are more characters than words in each document .",0
21284,DPCNNs are more accurate than Shallow CNNs at the expense of more computation time due to the depth ( 15 layers vs. 1 layer ) .,0
21285,"Nevertheless , their computation time is comparable - the points of both fit in the same range .",0
21286,The efficiency of DPCNNs is due to the exponential decrease of per-layer computation due to downsampling with the number of feature maps being fixed .,0
21287,Comparison with non-pyramid variants,0
21288,"Furthermore , we tested the following two ' nonpyramid ' models for comparison .",0
21289,The first model doubles the number of feature maps at every other downsampling so that per-layer computation is kept approximately constant,0
21290,4 .,0
21291,The second model performs no downsampling .,0
21292,"Otherwise , these two models are the same as DPCNN .",0
21293,We show in the error rates of these two variations ( labeled as ' Increase #feature maps in comparison with DPCNN .,0
21294,"The x - axis is the computation time , measured by the seconds spent for categorizing 10 K documents .",0
21295,"For all types , the models of depth 11 and 15 are shown .",0
21296,"Clearly , DPCNN is more accurate and computes faster than the others .",0
21297,"is on Yelp.f , and we observed the same performance trend on the other four large datasets .",0
21298,Small data results,1
21299,Now we turn to the results on the three smaller datasets in .,0
21300,"Again , the previous models are roughly sorted from best to worst .",0
21301,"For these small datasets , the DPCNN performances with 100 - dim unsupervised embed - dings are shown , which turned out to be as good as those with 300 - dim unsupervised embeddings .",1
21302,One difference from the large dataset results is that the strength of shallow models stands out .,0
21303,"ShallowCNN ( row 2 ) rivals DPCNN ( row 1 ) , and Zhang et al. 's best linear model ( row 3 ) moved up from the worst performer to the third best performer .",1
21304,"The results are inline with the general fact that more complex models require more training data , and with the paucity of training data , simpler models can outperform more complex ones .",0
21305,Empirical studies,0
21306,We present some empirical results to validate the design choices .,0
21307,"For this purpose , the larger five datasets were used to avoid the paucity of training data .",0
21308,"Depth shows error rates of DPCNNs with 3 , 7 , and 15 weight layers ( blue circles from left to right ) .",0
21309,"For comparison , the Shal - lowCNN results ( green ' x ' ) from are also shown .",0
21310,The x - axis represents the computation time ( seconds for categorizing 10K documents on a GPU ) .,0
21311,"For simplicity , the results without unsupervised embeddings are shown for all .",0
21312,The error rate improves as the depth increases .,0
21313,The results confirm the effectiveness of our strategy of deepening the network .,0
21314,Unsupervised embeddings,0
21315,"To study the effectiveness of unsupervised embeddings , we experimented with variations of DPCNN that differ only in whether / how to use unsupervised embeddings ) .",0
21316,"First , we compare DPCNNs with and without unsupervised embeddings .",0
21317,"The model with unsupervised embeddings ( row 1 , copied from for easy comparison ) clearly outperforms the one without them ( row 4 ) , which confirms the effectiveness of the use of unsupervised embeddings .",0
21318,"Second , in the proposed model ( row 1 ) , a region embedding layer receives two types of input , the output of unsupervised embedding functions and the high - dimensional discrete input such as a bow vector .",0
21319,"Row 2 shows the results obtained by using unsupervised embeddings to produce sole input ( i.e. , no discrete vectors provided to the region embedding layer ) .",0
21320,"Degradations of error rates are up to 0.32 % , small but consistent .",0
21321,"Since the discrete input add almost no computation cost due to its sparseness , its use is desirable .",0
21322,"Third , a number of previous studies used unsupervised word embedding to initialize word embedding in neural networks and then fine - tune it as training proceeds ( pretraining ) .",0
21323,The model in row 3 does this with DPCNN using word2vec .,0
21324,"The word2 vec training was done on the training data ( ignoring the labels ) , : Error rates ( % ) of DPCNN variations that differ in use of unsupervised embeddings .",0
21325,The rows are roughly sorted from best to worst .,0
21326,same as tv-embedding training .,0
21327,This model ( row 3 ) underperformed our proposed model ( row 1 ) .,0
21328,We attribute the superiority of the proposed model to its use of richer information than a word embedding .,0
21329,These results support our approach .,0
21330,Conclusion,0
21331,This paper tackled the problem of designing highperformance deep word - level CNNs for text categorization in the large training data setting .,0
21332,"We proposed a deep pyramid CNN model which has low computational complexity , and can efficiently represent long - range associations in text and so more global information .",0
21333,It was shown to outperform the previous best models on six benchmark datasets .,0
21334,title,0
21335,Very Deep Convolutional Networks for Text Classification,1
21336,abstract,0
21337,"The dominant approach for many NLP tasks are recurrent neural networks , in particular LSTMs , and convolutional neural networks .",0
21338,"However , these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state - of - the - art in computer vision .",0
21339,We present a new architecture ( VD - CNN ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations .,0
21340,"We are able to show that the performance of this model increases with the depth : using up to 29 convolutional layers , we report improvements over the state - of the - art on several public text classification tasks .",0
21341,"To the best of our knowledge , this is the first time that very deep convolutional nets have been applied to text processing .",0
21342,Introduction,0
21343,"The goal of natural language processing ( NLP ) is to process text with computers in order to analyze it , to extract information and eventually to represent the same information differently .",0
21344,"We may want to associate categories to parts of the text ( e.g. POS tagging or sentiment analysis ) , structure text differently ( e.g. parsing ) , or convert it to some other form which preserves all or part of the content ( e.g. machine translation , summarization ) .",0
21345,The level of granularity of this processing can range from individual characters to subword units or words up to whole sentences or even paragraphs .,0
21346,"After a couple of pioneer works , , among others ) , the use of neural networks for NLP applications is attracting huge in - terest in the research community and they are systematically applied to all NLP tasks .",0
21347,"However , while the use of ( deep ) neural networks in NLP has shown very good results for many tasks , it seems that they have not yet reached the level to outperform the state - of - the - art by a large margin , as it was observed in computer vision and speech recognition .",0
21348,"Convolutional neural networks , in short Con -vNets , are very successful in computer vision .",0
21349,"In early approaches to computer vision , handcrafted features were used , for instance "" scale - invariant feature transform ( SIFT ) "" , followed by some classifier .",0
21350,The fundamental idea of is to consider feature extraction and classification as one jointly trained task .,0
21351,"This idea has been improved over the years , in particular by using many layers of convolutions and pooling to sequentially extract a hierarchical representation of the input .",0
21352,The best networks are using more than 150 layers as in .,0
21353,Many NLP approaches consider words as basic units .,0
21354,An important step was the introduction of continuous representations of words .,0
21355,These word embeddings are now the state - of - the - art in NLP .,0
21356,"However , it is less clear how we should best represent a sequence of words , e.g. a whole sentence , which has complicated syntactic and semantic relations .",0
21357,"In general , in the same sentence , we maybe faced with local and long - range dependencies .",0
21358,"Currently , the mainstream approach is to consider a sentence as a sequence of tokens ( characters or words ) and to process them with a recurrent neural network ( RNN ) .",0
21359,"Tokens are usually processed in sequential order , from left to right , and the RNN is expected to "" memorize "" the whole sequence in its internal states .",0
21360,The most popular and successful RNN variant are certainly LSTMs ( Hochreiter and Schmid - to name just a few .,0
21361,"However , we argue that LSTMs are generic learning machines for sequence processing which are lacking task - specific structure .",0
21362,We propose the following analogy .,0
21363,"It is well known that a fully connected one hidden layer neural network can in principle learn any realvalued function , but much better results can be obtained with a deep problem - specific architecture which develops hierarchical representations .",0
21364,"By these means , the search space is heavily constrained and efficient solutions can be learned with gradient descent .",0
21365,ConvNets are namely adapted for computer vision because of the compositional structure of an image .,0
21366,"Texts have similar properties : characters combine to form n-grams , stems , words , phrase , sentences etc .",0
21367,"We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .",1
21368,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .",1
21369,"The design of our architecture is inspired by recent progress in computer vision , in particular .",0
21370,This paper is structured as follows .,0
21371,There have been previous attempts to use ConvNets for text processing .,0
21372,We summarize the previous works in the next section and discuss the relations and differences .,0
21373,Our architecture is described in detail in section 3 .,0
21374,"We have evaluated our approach on several sentence classification tasks , initially proposed by .",0
21375,These tasks and our experimental results are detailed in section 4 .,0
21376,The proposed deep convolutional network shows significantly better results than previous ConvNets approach .,0
21377,The paper concludes with a discussion of future research directions for very deep approach in NLP .,0
21378,Related work,0
21379,"There is a large body of research on sentiment analysis , or more generally on sentence classification tasks .",0
21380,"Initial approaches followed the classical two stage scheme of extraction of ( handcrafted ) features , followed by a classification stage .",0
21381,"Typical features include bag - of - words or ngrams , and their TF - IDF .",0
21382,These techniques have been compared with ConvNets by .,0
21383,We use the same corpora for our experiments .,0
21384,"More recently , words or characters , have been projected into a low - dimensional space , and these embeddings are combined to obtain a fixed size representation of the input sentence , which then serves as input for the classifier .",0
21385,The simplest combination is the element - wise mean .,0
21386,This usually performs badly since all notion of token order is disregarded .,0
21387,Another class of approaches are recursive neural networks .,0
21388,"The main idea is to use an external tool , namely a parser , which specifies the order in which the word embeddings are combined .",0
21389,"At each node , the left and right context are combined using weights which are shared for all nodes .",0
21390,The state of the top node is fed to the classifier .,0
21391,"A recurrent neural net-work ( RNN ) could be considered as a special case of a recursive NN : the combination is performed sequentially , usually from left to right .",0
21392,"The last state of the RNN is used as fixed - sized representation of the sentence , or eventually a combination of all the hidden states .",0
21393,First works using convolutional neural networks for NLP appeared in .,0
21394,They have been subsequently applied to sentence classification .,0
21395,We will discuss these techniques in more detail below .,0
21396,"If not otherwise stated , all approaches operate on words which are projected into a high - dimensional space .",0
21397,A rather shallow neural net was proposed in : one convolutional layer ( using multiple widths and filters ) followed by a max pooling layer overtime .,0
21398,The final classifier uses one fully connected layer with drop-out .,0
21399,"Results are reported on six data sets , in particular Stanford Sentiment Treebank ( SST ) .",0
21400,"A similar system was proposed in , but using five convolutional layers .",0
21401,An important difference is also the introduction of multiple temporal k-max pooling layers .,0
21402,"This allows to detect the k most important features in a sentence , independent of their specific position , preserving their relative order .",0
21403,The value of k depends on the length of the sentence and the position of this layer in the network .,0
21404,were the first to perform sentiment analysis entirely at the character level .,0
21405,"Their systems use up to six convolutional layers , followed by three fully connected classification layers .",0
21406,"Convolutional kernels of size 3 and 7 are used , as well as simple max - pooling layers .",0
21407,Another interesting aspect of this paper is the introduction of several large - scale data sets for text classification .,0
21408,We use the same experimental setting ( see section 4.1 ) .,0
21409,The use of character level information was also proposed by : all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information in a shallow architecture .,0
21410,"In parallel to our work , proposed a based hierarchical attention network for document classification that perform an attention first on the sentences in the document , and on the words in the sentence .",0
21411,Their architecture performs very well on datasets whose samples contain multiple sen-tences .,0
21412,"In the computer vision community , the combination of recurrent and convolutional networks in one architecture has also been investigated , with the goal to "" get the best of both worlds "" , e.g. .",0
21413,The same idea was recently applied to sentence classification .,0
21414,A convolutional network with up to five layers is used to learn highlevel features which serve as input for an LSTM .,0
21415,The initial motivation of the authors was to obtain the same performance as with networks which have significantly fewer parameters .,0
21416,They report results very close to those of or even outperform Con-vNets for some data sets .,0
21417,"In summary , we are not aware of any work that uses VGG - like or ResNet - like architecture to go deeper than than six convolutional layers for sentence classification .",0
21418,Deeper networks were not tried or they were reported to not improve performance .,0
21419,"This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks , namely 19 layers , or even up to 152 layers .",0
21420,"In the remainder of this paper , we describe our very deep convolutional architecture and report results on the same corpora than .",0
21421,"We were able to show that performance improves with increased depth , using up to 29 convolutional layers .",0
21422,VDCNN,0
21423,Architecture,0
21424,The over all architecture of our network is shown in .,0
21425,"Our model begins with a look - up table that generates a 2 D tensor of size ( f 0 , s ) that contain the embeddings of the s characters .",0
21426,"sis fixed to 1024 , and f 0 can be seen as the "" RGB "" dimension of the input text .",0
21427,"We first apply one layer of 64 convolutions of size 3 , followed by a stack of temporal "" convolutional blocks "" .",0
21428,"Inspired by the philosophy of VGG and ResNets we apply these two design rules : ( i ) for the same output temporal resolution , the layers have the same number of feature maps , ( ii ) when the temporal resolution is halved , the number of feature maps is doubled .",0
21429,This helps reduce the memory footprint of the network .,0
21430,"The networks contains 3 pooling operations ( halving the tempo - ral resolution each time by 2 ) , resulting in 3 levels of 128 , 256 and 512 feature maps ( see ) .",0
21431,"The output of these convolutional blocks is a tensor of size 512 s d , where s d = s 2 p with p = 3 the number of down - sampling operations .",0
21432,"At this level of the convolutional network , the resulting tensor can be seen as a high - level representation of the input text .",0
21433,"Since we deal with padded input text of fixed size , s dis constant .",0
21434,"However , in the case of variable size input , the convolutional encoder provides a representation of the input text that depends on its initial lengths .",0
21435,"Representations of a text as a set of vectors of variable size can be valuable namely for neural machine translation , in particular when combined with an attention model .",0
21436,"In , temporal convolutions with kernel size 3 and X feature maps are denoted "" 3 , Temp Conv , X "" , fully connected layers which are linear projections ( matrix of size I O ) are denoted "" fc ( I , O ) "" and "" 3 - max pooling , stride 2 "" means temporal maxpooling with kernel size 3 and stride",0
21437,2 .,0
21438,"Most of the previous applications of ConvNets to NLP use an architecture which is rather shallow ( up to 6 convolutional layers ) and combines convolutions of different sizes , e.g. spanning 3 , 5 and 7 tokens .",0
21439,This was motivated by the fact that convolutions extract n-gram features over tokens and that different n-gram lengths are needed to model short - and long - span relations .,0
21440,"In this work , we propose to create instead an architecture which uses many layers of small convolutions ( size 3 ) .",0
21441,"Stacking 4 layers of such convolutions results in a span of 9 tokens , but the network can learn by itself how to best combine these different "" 3 - gram features "" in a deep hierarchical manner .",0
21442,Our architecture can be in fact seen as a temporal adaptation of the VGG network .,0
21443,"We have also investigated the same kind of "" ResNet shortcut "" connections as in , namely identity and 1 1 convolutions ( see ) .",0
21444,"For the classification tasks in this work , the temporal resolution of the output of the convolution blocks is first down - sampled to a fixed dimension using k-max pooling .",0
21445,"By these means , the network extracts the k most important features , independently of the position they appear in the sentence .",0
21446,The 512 k resulting features are transformed into a single vector which is the input to a three layer fully connected classifier with ReLU hidden units and softmax outputs .,0
21447,"The number of output neurons depends on the classification task , the number of hidden units is set to 2048 , and k to 8 in all experiments .",0
21448,"We do not use drop- out with the fully connected layers , but only temporal batch normalization after convolutional layers to regularize our network .",0
21449,Convolutional Block,0
21450,"Each convolutional block ( see ) is a sequence of two convolutional layers , each one followed by a temporal BatchNorm layer and an ReLU activation .",0
21451,"The kernel size of all the temporal convolutions is 3 , with padding such that the temporal resolution is preserved ( or halved in the case of the convolutional pooling with stride 2 , see below ) .",0
21452,Steadily increasing the depth of the network by adding more convolutional layers is feasible thanks to the limited number of parameters of very small convolutional filters in all layers .,0
21453,Different depths of the over all architecture are obtained by varying the number of convolutional blocks in between the pooling layers ( see table 2 ) .,0
21454,Temporal batch normalization applies the same kind of regularization as batch normalization except that the activations in a mini-batch are jointly normalized over temporal ( instead of spatial ) locations .,0
21455,"So , for a mini-batch of size m and feature maps of temporal size s , the sum and the standard deviations related to the BatchNorm algorithm are taken over | B | = m s terms .",0
21456,We explore three types of down - sampling between blocks K i and K i + 1 ( i ) The first convolutional layer of K i + 1 has stride 2 ( ResNet -like ) .,0
21457,( ii ) K i is followed by a k-max pooling layer where k is such that the resolution is halved ) .,0
21458,( iii ) K i is followed by max - pooling with kernel size 3 and stride 2 ( VGG - like ) .,0
21459,All these types of pooling reduce the temporal resolution by a factor 2 .,0
21460,"At the final convolutional layer , the resolution is thus s d .",0
21461,"In this work , we have explored four depths for our networks : 9 , 17 , 29 and 49 , which we define as being the number of convolutional layers .",0
21462,"The depth of a network is obtained by summing the number of blocks with 64 , 128 , 256 and 512 filters , with each block containing two convolutional layers .",0
21463,"In , the network has 2 blocks of each type , resulting in a depth of 2 ( 2 + 2 + 2 + 2 ) = 16 .",0
21464,"Adding the very first convolutional layer , this sums to a depth of 17 convolutional layers .",0
21465,The depth can thus be increased or decreased by adding or removing convolutional blocks with a certain number of filters .,0
21466,"The best configurations we observed for depths 9 , 17 , 29 and 49 are described in .",0
21467,We also give the number of parameters of all convolutional layers .,0
21468,Depth,0
21469,Depth improves performance .,0
21470,"As we increase the network depth to 17 and 29 , the test errors decrease on all data sets , for all types of pooling ( with 2 exceptions for 48 comparisons ) .",0
21471,Going from depth 9 to 17 and 29 for Amazon Full reduces the error rate by 1 % absolute .,0
21472,"Since the test is composed of 650K samples , 6.5 K more test samples have been classified correctly .",0
21473,"These improvements , especially on large data sets , are significant and show that increasing the depth is useful for text processing .",0
21474,"Overall , compared to previous state - of - the - art , our best architecture with depth 29 and max - pooling has a test error of 37.0 compared to 40.43 % .",0
21475,This represents again of 3.43 % absolute accuracy .,0
21476,The significant improvements which we obtain on all data sets compared to Zhang 's convolutional models do not include any data augmentation technique .,0
21477,Max - pooling performs better than other pooling types .,0
21478,"In terms of pooling , we can also see that max - pooling performs best over all , very close to convolutions with stride 2 , but both are significantly superior to k-max pooling .",0
21479,"Both pooling mechanisms perform a max operation which is local and limited to three consecutive tokens , while k-max polling considers the whole sentence at once .",0
21480,"According to our exper-iments , it seems to hurt performance to perform this type of max operation at intermediate layers ( with the exception of the smallest data sets ) .",0
21481,Our models outperform state - of - the - art Con -vNets .,0
21482,"We obtain state - of - the - art results for all data sets , except AG 's news and Sogou news which are the smallest ones .",0
21483,"However , with our very deep architecture , we get closer to the stateof - the - art which are ngrams TF - IDF for these data sets and significantly surpass convolutional models presented in .",0
21484,"As observed in previous work , differences in accuracy between shallow ( TF - IDF ) and deep ( convolutional ) models are more significant on large data sets , but we still perform well on small data sets while getting closer to the non convolutional state - of - the - art results on small data sets .",0
21485,The very deep models even perform as well as ngrams and ngrams - TF - IDF respectively on the sentiment analysis task of Yelp Review Polarity and the ontology classification task of the DBPedia data set .,0
21486,Results of Yang et al .,0
21487,"( only on Yahoo Answers and Amazon Full ) outperform our model on the Yahoo Answers dataset , which is probably linked to the fact that their model is task - specific to datasets whose samples that contain multiple sentences like ( question , answer ) .",0
21488,They use a hierarchical attention mechanism that apply very well to documents ( with multiple sentences ) .,0
21489,Going even deeper degrades accuracy .,0
21490,Shortcut connections help reduce the degradation .,0
21491,"As described in , the gain in accuracy due to the the increase of the depth is limited when using standard ConvNets .",0
21492,"When the depth increases too much , the accuracy of the model gets saturated and starts degrading rapidly .",0
21493,This degradation problem was attributed to the fact that very deep models are harder to optimize .,0
21494,The gradients which are backpropagated through the very deep networks vanish and SGD with momentum is notable to converge to a correct minimum of the loss function .,0
21495,"To overcome this degradation of the model , the ResNet model introduced shortcut connections between convolutional blocks that allow the gradients to flow more easily in the network .",0
21496,We evaluate the impact of shortcut connections by increasing the number of convolutions to 49 layers .,0
21497,We present an adaptation of the ResNet model to the case of temporal convolutions for text ( see ) .,0
21498,shows the evolution of the test errors on the Yelp Review Full data set with or without shortcut connections .,0
21499,"When looking at the column "" without shortcut "" , we observe the same degradation problem as in the original ResNet article : when going from 29 to 49 layers , the test error rate increases from 35.28 to 37.41 ( while the training error goes up from 29.57 to 35.54 ) .",0
21500,"When using shortcut connections , we observe improved results when the network has 49 layers : both the training and test errors go down and the network is less prone to underfitting than it was without shortcut connections .",0
21501,"While shortcut connections give better results when the network is very deep ( 49 layers ) , we were notable to reach state - of - the - art results with them .",0
21502,We plan to further explore adaptations of residual networks to temporal convolutions as we think this a milestone for going deeper in NLP .,0
21503,"Residual units better adapted to the text processing task may help for training even deeper models for text processing , and is left for future research .",0
21504,Exploring these models on text classification tasks with more classes sounds promising .,0
21505,Note that one of the most important difference between the classification tasks discussed in this work and ImageNet is that the latter deals with 1000 classes and thus much more information is back - propagated to the network through the gra- dients .,0
21506,Exploring the impact of the depth of temporal convolutional models on categorization tasks with hundreds or thousands of classes would bean interesting challenge and is left for future research .,0
21507,Experimental evaluation,0
21508,Tasks and data,0
21509,"In the computer vision community , the availability of large data sets for object detection and image classification has fueled the development of new architectures .",0
21510,"In particular , this made it possible to compare many different architectures and to show the benefit of very deep convolutional networks .",0
21511,"We present our results on eight freely available large - scale data sets introduced by which cover several classification tasks such as sentiment analysis , topic classification or news categorization ( see ) .",0
21512,"The number of training examples varies from 120 k up to 3.6 M , and the number of classes is comprised between 2 and 14 .",0
21513,This is considerably lower than in computer vision ( e.g. 1 000 classes for ImageNet ) .,0
21514,for a detailed description .,0
21515,This has the consequence that each example induces less gradient information which may make it harder to train large architectures .,0
21516,"It should be also noted that some of the tasks are very ambiguous , in particular sentiment analysis for which it is difficult to clearly associate fine grained labels .",0
21517,There are equal numbers of examples in each class for both training and test sets .,0
21518,The reader is referred to for more details on the construction of the data sets .,0
21519,summarizes the best published results on these corpora we are aware of .,0
21520,"We do not use "" Thesaurus data augmentation "" or any other preprocessing , except lower - casing .",0
21521,"Nevertheless , we still outperform the best convolutional neural networks of for all data sets .",0
21522,The main goal of our work is to show that it is possible and beneficial to train very deep convolutional networks as text encoders .,0
21523,Data augmentation may improve our results even further .,0
21524,We will investigate this in future research .,0
21525,Common model settings,0
21526,The following settings have been used in all our experiments .,0
21527,They were found to be best in initial experiments .,0
21528,"Following , all processing is done at the character level which is the atomic representation of a sentence , same as pixels for images .",0
21529,"The dictionary consists of the following characters "" abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'"" / | # $ % & *' +=<>( ) [ ]{} "" plus a special padding , space and unknown token which add up to a total of 69 tokens .",1
21530,"The input text is padded to a fixed size of 1014 , larger text are truncated .",1
21531,The character embedding is of size 16 .,1
21532,"Training is performed with SGD , using a mini-batch of size 128 , an initial learning rate of 0.01 and momentum of 0.9 .",1
21533,We follow the same training procedure as in .,0
21534,We initialize our convolutional layers following .,0
21535,"One epoch took from 24 minutes to 2h45 for depth 9 , and from 50 minutes to 7h ( on the largest datasets ) for depth 29 .",0
21536,It took between 10 to 15 epoches to converge .,0
21537,The implementation is done using Torch 7 .,1
21538,All experiments are performed on a single NVidia K40 GPU .,1
21539,"Unlike previous research on the use of ConvNets for text processing , we use temporal batch norm without dropout .",1
21540,Experimental results,0
21541,"In this section , we evaluate several configurations of our model , namely three different depths and three different pooling types ( see Section 3 ) .",0
21542,"Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with small temporal convolution filters with different types of pooling , which shows that a significant improvement on the state - of - the - art configurations can be achieved on text classification tasks by pushing the depth to 29 convolutional layers .",0
21543,"Our deep architecture works well on big data sets in particular , even for small depths .",1
21544,"shows the test errors for depths 9 , 17 and 29 and for each type of pooling : convolution with stride 2 , k- max pooling and temporal max - pooling .",0
21545,"For the smallest depth we use ( 9 convolutional layers ) , we see that our model already performs better than Zhang 's convolutional baselines ( which includes 6 convolutional layers and has a different architecture ) on the biggest data sets :",1
21546,"Yelp Full , Yahoo Answers and Amazon Full and Polarity .",0
21547,The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples . :,1
21548,Best published results from previous work .,0
21549,best results use a Thesaurus data augmentation technique ( marked with an * ) . 's hierarchical methods is particularly adapted to datasets whose samples contain multiple sentences .,0
21550,"We also observe that for a small depth , temporal max - pooling works best on all data sets .",1
21551,Conclusion,0
21552,We have presented a new architecture for NLP which follows two design principles :,0
21553,"1 ) operate at the lowest atomic representation of text , i.e. characters , and 2 ) use a deep stack of local operations , i.e. convolutions and max - pooling of size 3 , to learn a high - level hierarchical representation of a sentence .",0
21554,This architecture has been evaluated on eight freely available large - scale data sets and we were able to show that increasing the depth up to 29 convolutional layers steadily improves performance .,0
21555,Our models are much deeper than previously published convolutional neural networks and they outperform those approaches on all data sets .,0
21556,"To the best of our knowledge , this is the first time that the "" benefit of depths "" was shown for convolutional neural networks in NLP .",0
21557,"Eventhough text follows human - defined rules and images can be seen as raw signals of our environment , images and small texts have similar properties .",0
21558,Texts are also compositional for many languages .,0
21559,"Characters combine to form n-grams , stems , words , phrase , sentences etc .",0
21560,These similar properties make the comparison between computer vision and natural language processing very profitable and we believe future research should invest into making text processing models deeper .,0
21561,Our work is a first attempt towards this goal .,0
21562,"In this paper , we focus on the use of very deep convolutional neural networks for sentence classification tasks .",0
21563,"Applying similar ideas to other sequence processing tasks , in particular neural machine translation is left for future research .",0
21564,It needs to be investigated whether these also benefit from having deeper convolutional encoders .,0
21565,title,0
21566,A Corpus for Multilingual Document Classification in Eight Languages,1
21567,abstract,0
21568,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,1
21569,Several approaches have been proposed in the literature and the current best practice is to evaluate them on a subset of the Reuters Corpus Volume,0
21570,2 .,0
21571,"However , this subset covers only few languages ( English , German , French and Spanish ) and almost all published works focus on the the transfer between English and German .",0
21572,"In addition , we have observed that the class prior distributions differ significantly between the languages .",0
21573,We argue that this complicates the evaluation of the multilinguality .,0
21574,"In this paper , we propose a new subset of the Reuters corpus with balanced class priors for eight languages .",0
21575,"By adding Italian , Russian , Japanese and Chinese , we cover languages which are very different with respect to syntax , morphology , etc .",0
21576,We provide strong baselines for all language transfer directions using multilingual word and sentence embeddings respectively .,0
21577,"Our goal is to offer a freely available framework to evaluate cross - lingual document classification , and we hope to foster by these means , research in this important area .",0
21578,Introduction,0
21579,There are many tasks in natural language processing which require the classification of sentences or longer paragraphs into a set of predefined categories .,0
21580,"Typical applications are for instance topic identification ( e.g. sports , news , . . . ) or product reviews ( positive or negative ) .",0
21581,There is a large body of research on approaches for document classification .,0
21582,An important aspect to compare these different approaches is the availability of high quality corpora to train and evaluate them .,0
21583,"Unfortunately , most of these evaluation tasks focus on the English language only , while there is an ever increasing need to perform document classification in many other languages .",0
21584,"One could of course collect and label training data for other languages , but this would be costly and time consuming .",0
21585,"An interesting alternative is "" crosslingual document classification "" .",0
21586,The underlying idea is to use a representation of the words or whole documents which is independent of the language .,0
21587,"By these means , a classifier trained on one language can be transferred to a different one , without the need of resources in that transfer language .",0
21588,"Ideally , the performance obtained by crosslingual transfer should be as close as possible to training the entire system on language specific resources .",0
21589,Such a task was first proposed by using the Reuters Corpus Volume,0
21590,2 .,0
21591,"The aim was to first train a classifier on English and then to transfer it to German , and vice versa .",0
21592,An extension to the transfer between English and French and Spanish respectively was proposed by .,0
21593,"However , only few comparative results are available for these transfer directions .",0
21594,The contributions of this work are as follows .,0
21595,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .",1
21596,"For each language , we define a train , development and test corpus .",1
21597,"We also provide strong reference results for all transfer directions between the eight languages , e.g. not limited to the transfer between a foreign language and English .",0
21598,"We compare two approaches , based either on multilingual word or sentence embeddings respectively .",0
21599,"By these means , we hope to define a clear evaluation environment for highly multilingual document classification .",0
21600,Corpus description,0
21601,"The Reuters Corpus Volume 2 , in short RCV2 1 , is a multilingual corpus with a collection of 487,000 news stories .",0
21602,"Each news story was manually classified into four hierarchical groups : CCAT ( Corporate / Industrial ) , ECAT ( Economics ) , GCAT ( Government / Social ) and MCAT ( Markets ) .",0
21603,Topic codes were assigned to capture the major subject of the news story .,0
21604,"The entire corpus covers thirteen languages , i.e. Dutch , French , German , Chinese , Japanese , Russian , Portuguese , Spanish , Latin American Spanish , Italian , Danish , Norwegian , and Swedish , written by local reporters in each language .",0
21605,The news stories are not parallel .,0
21606,"Single - label stories , i.e. those labeled with only one topic out of the four top categories , are often used for evaluations .",0
21607,"However , the class distributions vary significantly across all the thirteen languages ( see ) .",0
21608,"Therefore , using random samples to extract evaluation corpora may lead to very imbalanced test sets , i.e. undesired and misleading variability among the languages when the main focus is to evaluate cross - lingual transfer .",0
21609,Cross - lingual document classification,0
21610,A subset of the English and German sections of RCV2 was defined by to evaluate crosslingual document classification .,0
21611,This subset was used in several follow - up works and many comparative results are available for the transfer between German and English .,0
21612,extended the use of RCV2 for cross - lingual document classification to the French and Spanish language ( transfer from and to English ) .,0
21613,An analysis of these evaluation corpora has shown that the class prior distributions vary significantly between the classes ( see ) .,0
21614,"For German and English , more than 80 % of the ex- amples in the test set belong to the classes GCAT and MCAT and at most 2 % to the class CCAT .",0
21615,These class prior distributions are very different for French and Spanish : the class CCAT is quite frequent with 21 % and 15 % of the French and Spanish test set respectively .,0
21616,"One may of course argue that variability in the class prior distribution is typical for real - world problems , but this shifts the focus from a high quality cross - lingual transfer to "" tricks "" for how to best handle the class imbalance .",0
21617,"Indeed , in previous research the transfer between English and German achieves accuracies higher than 90 % , while the performance is below 80 % for EN / FR or even 70 % EN / ES .",0
21618,We have seen experimental evidence that these important differences are likely to be caused by the discrepancy in the class priors of the test sets .,0
21619,Multilingual document classification,0
21620,"In this work , we propose a new evaluation framework for highly multilingual document classification which significantly extends the current state .",0
21621,"We continue to use Reuters Corpus Volume 2 , but based on the above mentioned limitations of the current subset of RCV2 , we propose new tasks for cross - lingual document classification .",0
21622,The design choices are as follow :,0
21623,Uniform class coverage : we sample from RCV2 the same number of examples for each class and language ;,0
21624,"Split the data into train , development and test corpus : for each languages , we provide training data of different sizes ( 1 k , 2 k , 5 k and 10 k stories ) , a development ( 1 k ) and a test corpus ( 4 k ) ;",0
21625,Support more languages :,0
21626,German,0
21627,Most works in the literature use only 1 000 examples to train the document classifier .,0
21628,"To invest the impact of more training data , we also provide training corpora of 2 000 , 5 000 and 10 000 documents .",0
21629,The development corpus for each language is composed of 1 000 and the test set of 4 000 documents respectively .,0
21630,uniform class distributions .,0
21631,An important aspect of this work is to provide a framework to study and evaluate cross - lingual document classification for many language pairs .,0
21632,"In that spirit , we will name this corpus "" Multilingual Document Classification Corpus "" , abbreviated as MLDoc .",0
21633,The full Reuters Corpus Volume 2 has a special license and we can not distribute it ourselves .,0
21634,"Instead , we provide tools to extract all the subsets of MLDoc at https://github.com/facebookresearch/MLDoc.",0
21635,Baseline results,0
21636,"In this section , we provide comparative results on our new Multilingual Document Classification Corpus .",0
21637,Since the initial work by many alternative approaches to cross -lingual document classification have been developed .,0
21638,We will encourage the respective authors to evaluate their systems on MLDoc .,0
21639,We believe that a large variety of transfer language pairs will give valuable insights on the performance of the various approaches .,0
21640,"In this paper , we propose initial strong baselines which represent two complementary directions of research : one based on the aggregation of multilingual word embeddings , and another one , which directly learns multilingual sentence representations .",0
21641,Details on each approach are given in section 3.1 . and 3.2 . respectively .,0
21642,"In contrast to previous works on cross -lingual document classification with RVC2 , we explore training the classifier on all languages and transfer it to all others , ie .",0
21643,we do not limit our study to the transfer between English and a foreign language .,0
21644,"One can envision several ways to define cross - lingual document classification , in function of the resources which are used in the source and transfer language ( see ) .",0
21645,"The first scheme assumes that we have no resources in the transfer language at all , neither labeled nor unlabeled .",0
21646,"We will name this case "" zero - shot cross - lingual document classification "" .",0
21647,"To simplify the presentation , we will assume that we transfer from English to German .",0
21648,"Once the best performing model is selected , it is applied to the transfer language , eg. the German test set .",0
21649,"Since no resources of the transfer language are used , the same system can be applied to many different transfer languages .",0
21650,This type of cross - lingual document classification needs a very strong multilingual representation since no knowledge on the target language was used during the development of the classifier .,0
21651,"In a second class of cross - lingual document classification , we may aim in improving the transfer performance by using a limited amount of resources in the target language .",0
21652,In the framework of the proposed MLDoc we will use the development corpus of target language for model selection .,0
21653,"We will name this method "" targeted cross - lingual document classification "" since the system is tailored to one particular transfer language .",0
21654,It is unlikely that this system will perform well on other languages than the ones used for training or model selection .,0
21655,"If the goal is to build one document classification system for many languages , it maybe interesting to use already several languages during training and model selection .",0
21656,"To allow a fair comparison , we will assume that these multilingual resources have the same size than the ones used for zero - shot or targeted cross - language document classification , e.g. a training set composed of five languages with 200 examples each .",0
21657,This type of training is not a cross - lingual approach anymore .,0
21658,"Consequently , we will refer to this method as "" joint multilingual document classification "" .",0
21659,Multilingual word representations,0
21660,"Several works have been proposed to learn multilingual word embeddings , which are then combined to perform cross - lingual document classifications .",0
21661,These word embeddings are trained on either word alignments or sentencealigned parallel corpora .,0
21662,"To provide reproducible benchmark results , we use MultiCCA word embeddings published by .",0
21663,There are multiple ways to combine these word embeddings for classification .,0
21664,"We train a simple one - layer convolutional neural network ( CNN ) on top of the word embeddings , which has shown to perform well on text classification tasks regardless of training data size .",0
21665,"Specifically , convolutional filters are applied to windows of word embeddings , with a max - over - time pooling on top of them .",0
21666,We freeze the multilingual word embeddings while only training the classifier .,0
21667,"Hyper- parameters such as convolutional output dimension , window sizes are done by grid search over the Dev set of the same language as the train set .",0
21668,Multilingual sentence representations,0
21669,A second direction of research is to directly learn multilingual sentence representations .,0
21670,"In this paper , we evaluate a recently proposed technique to learn joint multilingual sentence representations .",0
21671,The underlying idea is to use multiple sequence encoders and decoders and to train them with aligned corpora from the machine translation community .,0
21672,"The goal is that all encoders share the same sentence representation , i.e. we map all languages into one common space .",0
21673,A detailed description of Each entry corresponds to a specifically optimized system .,0
21674,this approach can be found in .,0
21675,"We have developed two versions of the system : one trained on the Europarl corpus to cover the languages English , German , French , Spanish and Italian , and another one trained on the United Nations corpus which allows to learn a joint sentence embedding for English , French , Spanish , Russian and Chinese .",0
21676,We use a one hidden - layer MLP as classifier .,0
21677,"For comparison , we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross - lingual document classification : we are able to outperform the current state - of - the - art in three out of six transfer directions .",0
21678,Zero - shot cross - lingual document classification,1
21679,The classification accuracy for zero - shot transfer on the test set of our Multilingual Document Classification Corpus are summarized in .,0
21680,The classifiers based on the MultiCCA embeddings perform very well on the development corpus ( accuracies close or exceeding 90 % ) .,1
21681,"The system trained on English also achieves excellent results when transfered to a different languages , it scores best for three out of seven languages ( DE , IT and ZH ) .",1
21682,3,0
21683,"However , the transfer accuracies are quite low when training the classifiers on other languages than English , in particular for Russian , Chinese and Japanese .",1
21684,The systems using multilingual sentence embeddings seem to be over all more robust and less language specific .,1
21685,"They score best for four out of seven languages ( EN , ES , FR and RU ) .",0
21686,Training on German or French actually leads to better transfer performance than training on English .,1
21687,Crosslingual transfer between very different languages like Chinese and Russian also achieves remarkable results .,1
21688,Targeted cross - lingual document classification,0
21689,The classification accuracy for targeted transfer are summarized in .,0
21690,"Due to space constraints , we provide only the results for multilingual sentence embeddings and five target languages .",0
21691,"Not surprisingly , targeting the classi -",0
21692,We exclude Japanese from the comparison since we do not have joint sentence embeddings for that language yet .,0
21693,"fier to the transfer language can lead to important improvements , in particular when training on Italian .",0
21694,Joint multilingual document classification,1
21695,The classification accuracies for joint multilingual training are given in .,0
21696,We use a multilingual train and Dev corpus composed of 200 examples of each of the five languages .,0
21697,One could argue that the data collection and annotation cost for such a corpus would be the same than producing a corpus of the same size in one language only .,0
21698,"This leads to important improvement for all languages , in comparison to zero - shot or targeted transfer learning .",1
21699,Conclusion,0
21700,We have defined a new evaluation framework for crosslingual document classification in eight languages .,0
21701,"This corpus largely extends previous corpora which were also based on the Reuters Corpus Volume 2 , but mainly considered the transfer between English and German .",0
21702,"We also provide detailed baseline results using two competitive approaches ( multilingual word and sentence embeddings , respectively ) , for cross - lingual document classification between all eight languages .",0
21703,This new evaluation framework is freely available at https://github.com/facebookresearch/MLDoc.,0
21704,title,0
21705,Rethinking Complex Neural Network Architectures for Document Classification,1
21706,abstract,0
21707,"Neural network models for many NLP tasks have grown increasingly complex in recent years , making training and deployment more difficult .",0
21708,"A number of recent papers have questioned the necessity of such architectures and found that well - executed , simpler models are quite effective .",0
21709,"We show that this is also the case for document classification : in a large - scale reproducibility study of several recent neural models , we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",0
21710,"Surprisingly , our simple model is able to achieve these results without attention mechanisms .",0
21711,"While these regularization techniques , borrowed from language modeling , are not novel , to our knowledge we are the first to apply them in this context .",0
21712,Our work provides an opensource platform and the foundation for future work in document classification .,0
21713,Introduction,0
21714,Recent developments in neural architectures for a wide range of NLP tasks can be characterized as a drive towards increasingly complex network components and modeling techniques .,0
21715,"Worryingly , these new models are accompanied by smaller and smaller improvements in effectiveness on standard benchmark datasets , which leads us to wonder if observed improvements are "" real "" .",0
21716,"There is , however , ample evidence to the contrary .",0
21717,To provide a few examples : report that standard LSTM architectures outperform more recent models when properly tuned .,0
21718,"show that sequence transduction using encoder - decoder networks with attention mechanisms work just as well with the attention module only , making most of the complex * Equal contribution .",0
21719,neural machinery unnecessary .,0
21720,show that simple RNN - and CNN - based models yield accuracies rivaling far more complex architectures in simple question answering over knowledge graphs .,0
21721,"Perhaps most damning are the indictments of , who lament the lack of empirical rigor in our field and cite even more examples where improvements can be attributed to far more mundane reasons ( e.g. , hyperparameter tuning ) or are simply noise .",0
21722,"concur with these sentiments , adding that authors often use fancy mathematics to obfuscate or to impress ( reviewers ) rather than to clarify .",0
21723,"Complex architectures are more difficult to train , more sensitive to hyperparameters , and brittle with respect to domains with different data characteristics - thus both exacerbating the "" crisis of reproducibility "" and making it difficult for practitioners to deploy networks that tackle real - world problems in production environments .",0
21724,"Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .",1
21725,"Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",1
21726,"As the closest comparison point , we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms .",0
21727,"While these regularization techniques , borrowed from language modeling , are not novel , we are to our knowledge the first to apply them in this context .",0
21728,Our work provides an opensource platform and the foundation for future work in document classification .,0
21729,Background and Related Work,0
21730,Document Classification,0
21731,"Over the last few years , deep neural networks have achieved the state of the art in document classification .",0
21732,"One popular model , hierarchical attention network ( HAN ) , uses word - and sentence - level attention in classifying documents .",0
21733,"Although this model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence - level discourse modeling , one wonders if such complex architectures are really necessary , especially given the size of training data available today .",0
21734,"An important variant of document classification is the multi-label , multi-class case .",0
21735,"develop XML - CNNs for multi-label text classification , basing the architecture on with increased filter sizes and an additional fully - connected layer .",0
21736,They also incorporate dynamic adaptive max - pooling instead of the vanilla max - pooling overtime in KimCNN .,0
21737,"The paper compares with CNN - based approaches for the multi-label task , but only reports precision and disregards recall .",0
21738,instead adopts encoder - decoder sequence generation models ( SGMs ) for generating multiple labels for each document .,0
21739,"Similar to our critique of HAN , we opine against the high complexity of these multi-label approaches .",0
21740,Regularizing RNNs,0
21741,There have been attempts to extend dropout from feedforward neural networks to recurrent ones .,0
21742,"Unfortunately , direct application of dropout on the hidden units of an RNN empirically harms its ability to retain longterm information .",0
21743,"Recently , however , successfully apply dropout - like techniques to regularize RNNs for language modeling , achieving competitive word - level perplexity on multiple datasets .",0
21744,"Inspired by this development , we adopt two of their regularization techniques , embedding dropout and weight - dropped LSTMs , to our task of document classification .",0
21745,Weight - dropped LSTM .,0
21746,"LSTMs comprise eight total input-hidden and hidden - hidden weight matrices ; in weight dropping , regularize the four hidden - hidden matrices with DropConnect .",0
21747,"The operation is applied only once per sequence , using the same dropout mask across multiple timesteps .",0
21748,"Conveniently , this allows practitioners to use fast , out - of the - box LSTM implementations without affecting the RNN formulation or training performance .",0
21749,Embedding Dropout .,0
21750,"Introduced in and successfully employed for neural language modeling , embedding dropout performs dropout on entire word embeddings , effectively removing some of the words at each training iteration .",0
21751,"As a result , the technique conditions the model to be robust against missing input ; for document classification , this discourages the model from relying on a small set of input words for prediction .",0
21752,BiLSTM,0
21753,Model,0
21754,We design our model to be minimalistic :,0
21755,"First , we feed the word embeddings w 1:n of a document to a single - layer BiLSTM , extracting concatenated forward and backward word - level context vectors h 1:n = hf 1:n ? h b 1:n .",0
21756,"Subsequently , we max - pool h 1:n across time to yield document vector d-see , labels a-f .",0
21757,"Finally , we feed d to a sigmoid or a softmax layer over the labels , depending on if the task type is multi-label or single - label classification ( label g ) .",0
21758,"Contrary to prior art , our approach refrains from attention , hierarchical structure , and sequence generation , each of which increases model complexity .",0
21759,"For one , hierarchical structure requires sentence - level tokenization and multiple RNNs .",0
21760,"For another , sequence generation uses an encoderdecoder architecture , reducing computational parallelism .",0
21761,"All three methods add depth to the model ; our approach instead uses a single - layer BiLSTM with trivial max - pooling and concatena - tion operations , which makes for both simple implementation and resource - efficient inference .",0
21762,Experimental Setup,0
21763,"We conduct a large - scale reproducibility study involving HAN , XML - CNN , KimCNN , and SGM .",1
21764,"These are compared to our proposed model , referred to as LSTM reg , as well as an ablated variant without regularization , denoted LSTM base .",0
21765,"The implementation of our model as well as fromscratch reimplementations of all the comparison models ( except for SGM ) are provided in our toolkit called Hedwig , which we make publicly available to serve as the foundation for future work .",0
21766,1,0
21767,"In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .",1
21768,"The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .",1
21769,Both of these methods use word - level tf - idf vectors of the documents as features .,0
21770,"All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .",1
21771,We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .,1
21772,Datasets,0
21773,"We evaluate our models on the following four datasets : Reuters - 21578 , arXiv Abstract Paper dataset ( AAPD ) , IMDB , and Yelp 2014 .",0
21774,"Reuters and AAPD are multi-label datasets , whereas IMDB and Yelp are single - label ones .",0
21775,"For IMDB and Yelp , we use random sampling to split the dataset such that 80 % is used for training , 10 % for validation , and 10 % for test .",0
21776,"We use the standard ModApte splits for the Reuters dataset , and author - defined splits for AAPD .",0
21777,We summarize the statistics of these datasets in .,0
21778,"Unfortunately , there is little consensus within the natural language processing community for choosing the splits of IMDB and Yelp 2014 .",0
21779,"Furthermore , they are often unreported in modeling papers , hence preventing direct comparison with past results .",0
21780,"We are notable to find the exact splits use ; for consistency , we use the same proportion the authors report , but of course this yields different samples in each split .",0
21781,"For the multi-label datasets , we report the wellknown micro-averaged F 1 score , which is the class - weighted harmonic mean between recall and precision .",0
21782,"For the single - label datasets , we compare the models using accuracy .",0
21783,Training and Hyperparameters,0
21784,"To ensure a fair comparison , we tune the hyperparameters for all baseline models .",0
21785,"For HAN , we use a batch size of 32 across all the datasets , with a learning rate of 0.01 for Reuters and 0.001 for the rest .",0
21786,"To train XML - CNN , we select a dynamic pooling window length of eight , a learning rate of 0.001 , and 128 output channels , with batch sizes of 32 and 64 for single - label and multilabel datasets , respectively .",0
21787,"For KimCNN , we use a batch size of 64 with a learning rate of 0.01 .",0
21788,"For training SGM on Reuters , we use the source code provided by the authors 2 and follow the same hyperparameters in their paper .",0
21789,"For the LR and SVM models , we use the default set of hyperparameters in Scikit - learn .",0
21790,"For LSTM reg and LSTM base , we use the Adam optimizer with a learning rate of 0.01 on Reuters and 0.001 on the rest of the datasets , using batch sizes of 32 and 64 for multi-label and single - label tasks , respectively .",0
21791,"For LSTM reg , we also apply temporal averaging ( TA ) : as shown in , TA reduces both generalization error and stochastic noise in recent parameter estimates from stochastic approximation .",0
21792,We set the default TA exponential smoothing coefficient of ? EMA to 0.99 .,0
21793,"We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .",0
21794,"We also regularize the input-hidden and hidden - hidden Bi - LSTM connections using embedding dropout and weight dropping , respectively , with dropout rates of 0.1 and 0.2 .",0
21795,"For our optimization objective , we use crossentropy and binary cross - entropy loss for singlelabel and multi-label tasks , respectively .",0
21796,"On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .",0
21797,"We train all neural models for 30 epochs with five random seeds , reporting the mean validation set scores and their corresponding test set results .",0
21798,Toward Robust Baselines .,0
21799,"Recently , reproducibility is becoming a growing concern for the NLP community .",0
21800,"Indeed , very few of the papers that we consider in this study report validation set results , let alone run on multiple seeds .",0
21801,"In order to address these issues , we report scores on both validation and test sets for our reimplementations ; doing so is good practice , since it reinforces the validity of the experimental results and claims .",0
21802,We also provide the standard deviation of the scores across different seeds to demonstrate the stability of our results .,0
21803,This is inline with previous papers that emphasize reporting variance for robustness against potentially spurious conclusions .,0
21804,Results and Discussion,0
21805,We report the mean and standard deviation ( SD ) of the F 1 scores and accuracy for all five runs in Table,0
21806,2 .,0
21807,"For HAN and KimCNN , we include results from the original papers to validate our reimplementation .",0
21808,We fail to replicate the reported results of SGM on AAPD using the authors ' codebase and data splits .,0
21809,3,0
21810,"As a result , we simply copy the value reported in in , row 8 , which represents their maximum F 1 score .",0
21811,"To verify the correctness of our HAN and KimCNN reimplementations , we compare the differences in F 1 and accuracy on the appropriate datasets .",0
21812,We attribute the small differences to using different dataset splits ( see Section 4.1 ) and reporting mean values .,0
21813,Baseline Comparison .,0
21814,"We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .",1
21815,This highlights the efficacy of proper regularization and optimization techniques for the task .,0
21816,"We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .",1
21817,A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .,1
21818,"However , in the interest of robustness , we report the mean value , as mentioned in Section 4.2 .",0
21819,"We also find the accuracy of LSTM reg and our reimplemented version of HAN on Yelp 2014 to be almost two points lower than the copied result of HAN ( rows 6 , 7 , and 10 ) from .",0
21820,"On the other hand , both of the models surpass the original result by nearly two points for the IMDB dataset .",0
21821,We can not rule out that these disparities are caused by the absence of any widely - accepted splits for evaluation on Yelp 2014 and IMDB ( as opposed to model or implementation differences ) .,0
21822,"Interestingly , the non-neural LR and SVM baselines perform remarkably well .",1
21823,"On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .",1
21824,"On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .",1
21825,"Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .",1
21826,Conclusions and Future Work,0
21827,"In this paper , we question the complexity of existing neural network architectures for document classification .",0
21828,"To demonstrate the effectiveness of proper regularization and optimization , we apply embedding dropout , weight dropping , and temporal averaging when training a simple BiLSTM model , establishing either competitive or state - of the - art results on multiple datasets .",0
21829,One potential extension of this work is to conduct a comprehensive ablation study to determine the relative contribution of each of the regularization and optimization techniques .,0
21830,"Furthermore , it would be interesting to compare these techniques to the recent line of research in deep language representation models , such as Embeddings from Language Models ( ELMo ; and pre-trained transformers .",0
21831,"Finally , the examined regularization and optimization methods deserve exploration in other NLP tasks as well .",0
21832,title,0
21833,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,1
21834,abstract,0
21835,Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting .,0
21836,"However , both methods require making small perturbations to numerous entries of the input vector , which is inappropriate for sparse high - dimensional inputs such as one - hot word representations .",0
21837,We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself .,0
21838,The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks .,0
21839,"We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training , the model is less prone to overfitting .",0
21840,INTRODUCTION,0
21841,Adversarial examples are examples thatare created by making small perturbations to the input designed to significantly increase the loss incurred by a machine learning model .,0
21842,"Several models , including state of the art convolutional neural networks , lack the ability to classify adversarial examples correctly , sometimes even when the adversarial perturbation is constrained to be so small that a human observer can not perceive it .",0
21843,Adversarial training is the process of training a model to correctly classify both unmodified examples and adversarial examples .,0
21844,"It improves not only robustness to adversarial examples , but also generalization performance for original examples .",0
21845,"Adversarial training requires the use of labels when training models that use a supervised cost , because the label appears in the cost function that the adversarial perturbation is designed to maximize .",0
21846,Virtual adversarial training extends the idea of adversarial training to the semi-supervised regime and unlabeled examples .,0
21847,"This is done by regularizing the model so that given an example , the model will produce the same output distribution as it produces on an adversarial perturbation of that example .",0
21848,Virtual adversarial training achieves good generalization performance for both supervised and semi-supervised learning tasks .,0
21849,Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,1
21850,"In this work , we extend these techniques to text classification tasks and sequence models .",1
21851,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,1
21852,"For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",1
21853,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .",1
21854,Traditional adversarial and virtual adversarial training can be interpreted both as a regularization strategy and as defense against an adversary who can supply malicious inputs .,0
21855,"Since the perturbed embedding does not map to any word and the adversary presumably does not have access to the word embedding layer , our proposed training strategy is no longer intended as a defense against an adversary .",0
21856,We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function .,1
21857,"We show that our approach with neural language model unsupervised pretraining as proposed by achieves state of the art performance for multiple semi-supervised text classification tasks , including sentiment classification and topic classification .",0
21858,"We emphasize that optimization of only one additional hyperparameter ? , the norm constraint limiting the size of the adversarial perturbations , achieved such state of the art performance .",0
21859,These results strongly encourage the use of our proposed method for other text classification tasks .,0
21860,We believe that text classification is an ideal setting for semi-supervised learning because there are abundant unlabeled corpora for semi-supervised learning algorithms to leverage .,0
21861,This work is the first work we know of to use adversarial and virtual adversarial training to improve a text or RNN model .,0
21862,We also analyzed the trained models to qualitatively characterize the effect of adversarial and virtual adversarial training .,0
21863,We found that adversarial and virtual adversarial training improved word embeddings over the baseline methods .,0
21864,MODEL,0
21865,"We denote a sequence of T words as {w ( t ) |t = 1 , . . . , T } , and a corresponding target as y .",0
21866,"To transform a discrete word input to a continuous vector , we define the word embedding matrix V ?",0
21867,R ( K+1 ) D where K is the number of words in the vocabulary and each row v k corresponds to the word embedding of the i - th word .,0
21868,"Note that the ( K + 1 ) - th word embedding is used as an embedding of an ' end of sequence ( eos ) ' token , v eos .",0
21869,"As a text classification model , we used a simple LSTM - based neural network model , shown in .",0
21870,"At time step t , the input is the discrete word w ( t ) , and the corresponding word embedding is v ( t ) .",0
21871,We additionally tried the bidirectional v eos,0
21872,The model with perturbed embeddings .,0
21873,LSTM architecture since this is used by the current state of the art method .,0
21874,"For constructing the bidirectional LSTM model for text classification , we add an additional LSTM on the reversed sequence to the unidirectional LSTM model described in .",0
21875,The model then predicts the label on the concatenated LSTM outputs of both ends of the sequence .,0
21876,"In adversarial and virtual adversarial training , we train the classifier to be robust to perturbations of the embeddings , shown in .",0
21877,These perturbations are described in detail in Section 3 .,0
21878,"At present , it is sufficient to understand that the perturbations are of bounded norm .",0
21879,The model could trivially learn to make the perturbations insignificant by learning embeddings with very large norm .,0
21880,"To prevent this pathological solution , when we apply adversarial and virtual adversarial training to the model we defined above , we replace the embeddings v k with normalized embeddingsv k , defined as : v",0
21881,"where f i is the frequency of the i - th word , calculated within all training examples .",0
21882,ADVERSARIAL AND VIRTUAL ADVERSARIAL TRAINING,0
21883,"Adversarial training is a novel regularization method for classifiers to improve robustness to small , approximately worst case perturbations .",0
21884,Let us denote x as the input and ?,0
21885,as the parameters of a classifier .,0
21886,"When applied to a classifier , adversarial training adds the following term to the cost function :",0
21887,where r is a perturbation on the input and ?,0
21888,is a constant set to the current parameters of a classifier .,0
21889,The use of the constant copy ?,0
21890,rather than ?,0
21891,indicates that the backpropagation algorithm should not be used to propagate gradients through the adversarial example construction process .,0
21892,"At each step of training , we identify the worst case perturbations r adv against the current model p ( y | x ; ? ) in Eq.",0
21893,"( 2 ) , and train the model to be robust to such perturbations through minimizing Eq.",0
21894,( 2 ) with respect to ?.,0
21895,"However , we can not calculate this value exactly in general , because exact minimization with respect tor is intractable for many interesting models such as neural networks .",0
21896,proposed to approximate this value by linearizing log p ( y | x ;? ) around x .,0
21897,With a linear approximation and a L 2 norm constraint in Eq.,0
21898,"( 2 ) , the resulting adversarial perturbation is",0
21899,This perturbation can be easily computed using backpropagation in neural networks .,0
21900,Virtual adversarial training ) is a regularization method closely related to adversarial training .,0
21901,The additional cost introduced by virtual adversarial training is the following :,0
21902,"where r v-adv = arg max r , r ??",0
21903,where KL [ p||q ] denotes the KL divergence between distributions p and q.,0
21904,By minimizing Eq.,0
21905,"( 3 ) , a classifier is trained to be smooth .",0
21906,This can be considered as making the classifier resistant to perturbations in directions to which it is most sensitive on the current model p ( y | x ; ? ) .,0
21907,Virtual adversarial loss Eq.,0
21908,( 3 ) requires only the input x and does not require the actual label y while adversarial loss defined in Eq .,0
21909,( 2 ) requires the label y .,0
21910,This makes it possible to apply virtual adversarial training to semi-supervised learning .,0
21911,"Although we also in general can not analytically calculate the virtual adversarial loss , proposed to calculate the approximated Eq.",0
21912,( 3 ) efficiently with backpropagation .,0
21913,"As described in Sec. 2 , in our work , we apply the adversarial perturbation to word embeddings , rather than directly to the input .",0
21914,"To define adversarial perturbation on the word embeddings , let us denote a concatenation of a sequence of ( normalized ) word embedding vectors",0
21915,"ass , and the model conditional probability of y given s as p ( y | s ; ? ) where ?",0
21916,are model parameters .,0
21917,Then we define the adversarial perturbation r adv on s as :,0
21918,"To be robust to the adversarial perturbation defined in Eq. ( 5 ) , we define the adversarial loss by",0
21919,"log p (y n | s n + r adv , n ; ? )",0
21920,where N is the number of labeled examples .,0
21921,"In our experiments , adversarial training refers to minimizing the negative log - likelihood plus L adv with stochastic gradient descent .",0
21922,"In virtual adversarial training on our text classification model , at each training step , we calculate the below approximated virtual adversarial perturbation :",0
21923,where dis a T D-dimensional small random vector .,0
21924,This approximation corresponds to a 2 ndorder Taylor expansion and a single iteration of the power method on Eq.,0
21925,( 3 ) as in previous work .,0
21926,Then the virtual adversarial loss is defined as :,0
21927,where N ?,0
21928,is the number of both labeled and unlabeled examples .,0
21929,See for a recent review of adversarial training methods .,0
21930,EXPERIMENTAL SETTINGS,0
21931,All experiments used TensorFlow on GPUs .,1
21932,Code will be available at https://github.com/tensorflow/models/tree/master/adversarial_text.,1
21933,"To compare our method with other text classification methods , we tested on 5 different text datasets .",0
21934,We summarize information about each dataset in ) and we conducted a single topic classification task on the second level topics .,0
21935,"We used the same division into training , test and unlabeled sets as .",0
21936,"Regarding pre-processing , we treated any punctuation as spaces .",0
21937,"We converted all words to lower - case on the Rotten Tomatoes , DBpedia , and RCV1 datasets .",0
21938,We removed words which appear in only one document on all datasets .,0
21939,"On RCV1 , we also removed words in the English stop - words list provided by Lewis et al. , we initialized the word embedding matrix and LSTM weights with a pre-trained recurrent language model ) that was trained on 1 http://ai.stanford.edu/~amaas/data/sentiment/",0
21940,2 http://riejohnson.com/cnn_data.html,0
21941,3,0
21942,"There are some duplicated reviews in the original Elec dataset , and we used the dataset with removal of the duplicated reviews , provided by , thus there are slightly fewer examples shown in than the ones in previous works , with batch size 256 , an initial learning rate of 0.001 , and a 0.9999 learning rate exponential decay factor at each training step .",0
21943,"We trained for 100,000 steps .",1
21944,We applied gradient clipping with norm set to 1.0 on all the parameters except word embeddings .,1
21945,"To reduce runtime on GPU , we used truncated backpropagation up to 400 words from each end of the sequence .",0
21946,"For regularization of the recurrent language model , we applied dropout on the word embedding layer with 0.5 dropout rate .",1
21947,"For the bidirectional LSTM model , we used 512 hidden units LSTM for both the standard order and reversed order sequences , and we used 256 dimensional word embeddings which are shared with both of the LSTMs .",1
21948,The other hyperparameters are the same as for the unidirectional LSTM .,0
21949,"We tested the bidirectional LSTM model on IMDB , Elec and RCV because there are relatively long sentences in the datasets .",0
21950,Pretraining with a recurrent language model was very effective on classification performance on all the datasets we tested on and so our results in Section 5 are with this pretraining .,0
21951,TRAINING CLASSIFICATION MODELS,0
21952,"After pre-training , we trained the text classification model shown in with adversarial and virtual adversarial training as described in Section 3 .",0
21953,"Between the softmax layer for the target y and the final output of the LSTM , we added a hidden layer , which has dimension 30 on IMDB , Elec and Rotten Tomatoes , and 128 on DBpedia and RCV1 .",0
21954,The activation function on the hidden layer was ReLU .,0
21955,"For optimization , we again used the Adam optimizer , with 0.0005 initial learning rate 0.9998 exponential decay .",0
21956,"Batch sizes are 64 on IMDB , Elec , RCV1 , and 128 on DBpedia .",0
21957,"For the Rotten Tomatoes dataset , for each step , we take a batch of size 64 for calculating the loss of the negative log -likelihood and adversarial training , and 512 for calculating the loss of virtual adversarial training .",0
21958,"Also for Rotten Tomatoes , we used texts with lengths T less than 25 in the unlabeled dataset .",0
21959,"We iterated 10,000 training stepson all datasets except IMDB and DBpedia , for which we used 15,000 and 20,000 training steps respectively .",0
21960,We again applied gradient clipping with the norm as 1.0 on all the parameters except the word embedding .,0
21961,"We also used truncated backpropagation up to 400 words , and also generated the adversarial and virtual adversarial perturbation up to 400 words from each end of the sequence .",0
21962,"We found the bidirectional LSTM to converge more slowly , so we iterated for 15,000 training steps when training the bidirectional LSTM classification model .",0
21963,"For each dataset , we divided the original training set into training set and validation set , and we roughly optimized some hyperparameters shared with all of the methods ; ( model architecture , batchsize , training steps ) with the validation performance of the base model with embedding dropout .",0
21964,"For each method , we optimized two scalar hyperparameters with the validation set .",0
21965,These were the dropout rate on the embeddings and the norm constraint ? of adversarial and virtual adversarial training .,0
21966,"Note that for adversarial and virtual adversarial training , we generate the perturbation after applying embedding dropout , which we found performed the best .",0
21967,We did not do early stopping with these methods .,0
21968,The method with only pretraining and embedding dropout is used as the baseline ( referred to as Baseline in each table ) .,0
21969,"shows the learning curves on the IMDB test set with the baseline method ( only embedding dropout and pretraining ) , adversarial training , and virtual adversarial training .",0
21970,We can see in 2 a that adversarial and virtual adversarial training achieved lower negative log likelihood than the baseline .,0
21971,"Furthermore , virtual adversarial training , which can utilize unlabeled data , maintained this low negative log - likelihood while the other methods began to overfit later in training .",0
21972,"Regarding adversarial and virtual adversarial loss in and 2 c , we can see the same tendency as for negative log likelihood ; virtual adversarial training was able to keep these values lower than other methods .",0
21973,"Because adversarial training operates only on the labeled subset of the training data , it eventually overfits even the task of resisting adversarial perturbations .",0
21974,A common misconception is that adversarial training is equivalent to training on noisy examples .,0
21975,"Noise is actually afar weaker regularizer than adversarial perturbations because , in high dimensional input spaces , an average noise vector is approximately orthogonal to the cost gradient .",0
21976,Adversarial perturbations are explicitly chosen to consistently increase the cost .,0
21977,"To demonstrate the superiority of adversarial training over the addition of noise , we include control experiments which replaced adversarial perturbations with random perturbations from a multivariate Gaussian with scaled norm , on each embedding in the sequence .",0
21978,"In , ' Random perturbation with labeled examples ' is the method in which we replace r adv with random perturbations , and ' Random perturbation with labeled and unlabeled examples ' is the method in which we replace r v-adv with random perturbations .",0
21979,Every adversarial training method outperformed every random perturbation method .,0
21980,RESULTS,0
21981,TEST PERFORMANCE ON IMDB DATASET AND MODEL ANALYSIS,0
21982,"To visualize the effect of adversarial and virtual adversarial training on embeddings , we examined embeddings trained using each method .",0
21983,shows the 10 top nearest neighbors to ' good ' and ' bad ' with trained embeddings .,0
21984,"The baseline and random methods are both strongly influenced by the grammatical structure of language , due to the language model pretraining step , but are not strongly influenced by the semantics of the text classification task .",0
21985,"For example , ' bad ' appears in the list of nearest neighbors to ' good ' on the baseline and the random perturbation method .",0
21986,"Both ' bad ' and ' good ' are adjectives that can modify the same set of nouns , so it is reasonable for a language model to assign them similar embeddings , but this clearly does not convey much information about the actual meaning of the words .",0
21987,"Adversarial training ensures that the meaning of a sentence can not be inverted via a small change , so these words with similar grammatical role but different meaning become separated .",0
21988,"When using adversarial and virtual adversarial training , ' bad ' no longer appears in the 10 top nearest neighbors to ' good ' .",0
21989,"' bad ' falls to the 19th nearest neighbor for adversarial training and 21st nearest neighbor for virtual adversarial training , with cosine distances of 0.463 and 0.464 , respectively .",0
21990,"For the baseline and random perturbation method , the cosine distances were 0.361 and 0.377 , respectively .",0
21991,"In the other direction , the nearest neighbors to ' bad ' included ' good ' as the 4th nearest neighbor for the baseline method and random perturbation method .",0
21992,"For both adversarial methods , ' good ' drops to the 36th nearest neighbor of ' bad ' .",0
21993,We also investigated the 15 nearest neighbors to ' great ' and its cosine distances with the trained embeddings .,0
21994,We saw that cosine distance on adversarial and virtual adversarial training ( 0.159-0.331 ) were much smaller than ones on the baseline and random perturbation method ( 0.244-0.399 ) .,1
21995,9.99 % NBSVM- bigrams 8.78 % Paragraph Vectors 7.42 % SA - LSTM 7.24 % One - hot bi -LSTM * 5.94 % The much weaker positive word ' good ' also moved from the 3rd nearest neighbor to the 15th after virtual adversarial training .,0
21996,shows the test performance on the Elec and RCV1 datasets .,1
21997,"We can see our proposed method improved test performance on the baseline method and achieved state of the art performance on both datasets , even though the state of the art method uses a combination of CNN and bidirectional LSTM models .",1
21998,Our unidirectional LSTM model improves on the state of the art method and our method with a bidirectional LSTM further improves results on RCV1 .,1
21999,"The reason why the bidirectional models have better performance on the RCV1 dataset would be that , on the RCV1 dataset , there are some very long sentences compared with the other datasets , and the bidirectional model could better handle such long sentences with the shorter dependencies from the reverse order sentences .",0
22000,shows test performance on the Rotten Tomatoes dataset .,1
22001,"Adversarial training was able to improve over the baseline method , and with both adversarial and virtual adversarial cost , achieved almost the same performance as the current state of the art method .",1
22002,However the test performance of only virtual adversarial training was worse than the baseline .,1
22003,We speculate that this is because the Rotten Tomatoes dataset has very few labeled sentences and the labeled sentences are very short .,0
22004,Virtual Adversarial ( on bidirectional LSTM ) 5.55 % 6.71 % Adversarial + Virtual Adversarial ( on bidirectional LSTM ) 5.45 % 6.68 %,0
22005,"TEST PERFORMANCE ON ELEC , RCV1 AND ROTTEN TOMATOES DATASET",0
22006,Transductive SVM 16.41 % 10.77 % NBLM ( Na? ve Bayes logisitic regression model ) 8.11 % 13.97 % One - hot CNN * 6.27 % 7.71 % One - hot CNN 5.87 % 7.15 % One - hot bi-LSTM 5.55 % 8.52 %,0
22007,"In this case , the virtual adversarial loss on unlabeled examples overwhelmed the supervised loss , so the model prioritized being robust to perturbation rather than obtaining the correct answer .",0
22008,NBSVM - bigrams 20.6 % CNN * 18.5 % AdaSent * 16.9 % SA - LSTM 16.7 % shows the test performance of each method on DBpedia .,0
22009,The ' Random perturbation ' is the same method as the ' Random perturbation with labeled examples ' explained in Section 5.1 .,0
22010,"Note that DBpedia has only labeled examples , as we explained in Section 4 , so this task is purely supervised learning .",0
22011,"We can see that the baseline method has already achieved nearly the current state of the art performance , and our proposed method improves from the baseline method .",0
22012,PERFORMANCE ON THE DBPEDIA PURELY SUPERVISED CLASSIFICATION TASK,0
22013,RELATED WORKS,0
22014,Dropout ) is a regularization method widely used for many domains including text .,0
22015,"There are some previous works adding random noise to the input and hidden layer during training , to prevent overfitting ( e.g. ) .",0
22016,"However , in our experiments and in previous works , training with adversarial and virtual adversarial perturbations outperformed the method with random perturbations .",0
22017,"For semi-supervised learning with neural networks , a common approach , especially in the image domain , is to train a generative model whose latent features maybe used as features for classification ( e.g. ) .",0
22018,These models now achieve state of the art Bag - of - words 3.57 % Large - CNN ( character - level ) 1.73 % SA - LSTM ( word - level ) 1.41 % N-grams TFIDF 1.31 % SA - LSTM ( character - level ) 1.19 % Word CNN 0.84 % performance on the image domain .,0
22019,"However , these methods require numerous additional hyperparameters with generative models , and the conditions under which the generative model will provide good supervised learning performance are poorly understood .",0
22020,"By comparison , adversarial and virtual adversarial training requires only one hyperparameter , and has a straightforward interpretation as robust optimization .",0
22021,"Adversarial and virtual adversarial training resemble some semi-supervised or transductive SVM approaches in that both families of methods push the decision boundary far from training examples ( or in the case of transductive SVMs , test examples ) .",0
22022,"However , adversarial training methods insist on margins on the input space , while SVMs insist on margins on the feature space defined by the kernel function .",0
22023,This property allows adversarial training methods to achieve the models with a more flexible function on the space where the margins are imposed .,0
22024,"In our experiments ( , 4 ) and , adversarial and virtual adversarial training achieve better performance than SVM based methods .",0
22025,There has also been semi-supervised approaches applied to text classification with both CNNs and RNNs .,0
22026,These approaches utilize ' view - embeddings ' which use the window around a word to generate its embedding .,0
22027,"When these are used as a pretrained model for the classification model , they are found to improve generalization performance .",0
22028,These methods and our method are complementary as we showed that our method improved from a recurrent pretrained language model .,0
22029,CONCLUSION,0
22030,"In our experiments , we found that adversarial and virtual adversarial training have good regularization performance in sequence models on text classification tasks .",0
22031,"On all datasets , our proposed method exceeded or was on par with the state of the art performance .",0
22032,We also found that adversarial and virtual adversarial training improved not only classification performance but also the quality of word embeddings .,0
22033,"These results suggest that our proposed method is promising for other text domain tasks , such as machine translation , learning distributed representations of words or paragraphs and question answering tasks .",0
22034,"Our approach could also be used for other general sequential tasks , such as for video or speech .",0
22035,title,0
22036,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,1
22037,abstract,0
22038,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",1
22039,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,0
22040,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",0
22041,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",0
22042,We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,0
22043,The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data .,0
22044,"The results indicate that on this task , embeddings of text regions , which can convey complex concepts , are more useful than embeddings of single words in isolation .",0
22045,We report performances exceeding the previous best results on four benchmark datasets .,0
22046,Introduction,0
22047,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",0
22048,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",0
22049,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",0
22050,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",0
22051,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",0
22052,"In its simplest form , onehot CNN works as follows .",0
22053,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",0
22054,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,0
22055,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",0
22056,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) ,",0
22057,"where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",0
22058,"It is simple and fast to compute , and considering its simplicity , the method works surprisingly well if the region size is appropriately set .",0
22059,"However , there are also potential shortcomings .",0
22060,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",0
22061,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",0
22062,JZ15 proposed variations to alleviate these issues .,0
22063,"For example , a bow - input variation allows x above to be a bow vector of the region .",0
22064,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",0
22065,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",1
22066,LSTM ) is a recurrent neural network .,0
22067,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ?",0
22068,"1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",0
22069,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,1
22070,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",1
22071,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",0
22072,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",1
22073,Our findings are threefold .,0
22074,"First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",0
22075,"Second , accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input .",0
22076,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",0
22077,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",0
22078,"Overall , our results show that for text categorization , embeddings of text regions , which can convey higher - level concepts than single words in isolation , are useful , and that useful region embeddings can be learned without going through word embedding learning .",0
22079,We report performances exceeding the previous best results on four benchmark datasets .,0
22080,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,1
22081,Preliminary,0
22082,"On text , LSTM has been used for labeling or generating words .",0
22083,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",0
22084,"Unlike these studies , this work as well as JZ15 focuses on classifying general full - length documents without any special linguistic knowledge .",0
22085,"Similarly , DL15 applied LSTM to categorizing general full - length documents .",0
22086,"Therefore , our empirical comparisons will focus on DL15 and JZ15 , both of which reported new state of the art results .",0
22087,"Let us first introduce the general LSTM formulation , and then briefly describe DL15 's model as it illustrates the challenges in using LSTMs for this task .",0
22088,LSTM,0
22089,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. ,",0
22090,where denotes element - wise multiplication and ?,0
22091,"is an element - wise squash function to make the gating values in [ 0 , 1 ] .",0
22092,We fix ? to sigmoid .,0
22093,x t ?,0
22094,"Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",0
22095,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ?",0
22096,R q for all types .,0
22097,"The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .",0
22098,The forget gate ft is for resetting the memory cells .,0
22099,The input gate it and output gate o t control the input and output of the memory cells .,0
22100,Word - vector LSTM ( wv - LSTM ) [ DL15 ] DL15 's application of LSTM to text categorization is straightforward .,0
22101,"As illustrated in , for each document , the output of the LSTM layer is the output of the last time step ( corresponding to the last word of the document ) , which represents the whole document ( document embedding ) .",0
22102,"Like many other studies of LSTM on text , words are first converted to low - dimensional dense word vectors via a word embedding layer ; therefore , we call it word - vector LSTM or wv - LSTM .",0
22103,DL15 observed that wv - LSTM underperformed linear predictors and its training was unstable .,0
22104,This was attributed to the fact that documents are long .,0
22105,"In addition , we found that training and testing of wv - LSTM is time / resource consuming .",0
22106,"To put it into perspective , using a GPU , one epoch of wv - LSTM training takes nearly 20 times longer than that of one - hot CNN training even though it achieves poorer accuracy ( the first two rows of ) .",0
22107,"This is due to the sequential nature of LSTM , i.e. , computation at time t requires the output of time t ? 1 , whereas modern computation depends on parallelization for speedup .",0
22108,"Documents in a mini-batch can be processed in parallel , but the variability of document lengths reduces the degree of parallelization 1 .",0
22109,It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective ( predicting the next word ) or autoencoder objective ( memorizing the document ) .,0
22110,Supervised LSTM for text categorization,0
22111,"Within the framework of ' region embedding + pooling ' for text categorization , we seek effective and efficient use of LSTM as an alternative region embedding method .",0
22112,"This section focuses on an end - to - end supervised setting so that there is no additional data ( e.g. , unlabeled data ) or additional algorithm ( e.g. , for learning a word embedding ) .",0
22113,Our general strategy is to simplify the model as much as possible .,0
22114,"We start with elimination of the word embedding layer so that one - hot vectors are directly fed to LSTM , which we call one - hot LSTM in short .",0
22115,Elimination of the word embedding layer,0
22116,Facts : A word embedding is a linear operation that can be written as Vx t with x t being a one - hot vector and columns of V being word vectors .,0
22117,"Therefore , by replacing the LSTM weights W ( ) with W ( ) V and removing the word embedding layer , a word - vector LSTM can be turned into a one - hot LSTM without changing the model behavior .",0
22118,"Thus , word - vector LSTM is not more expressive than one - hot LSTM ; rather , a merit , if any , of training with a word embedding layer would be through imposing restrictions ( e.g. , a low - rank V makes a less expressive model ) to achieve good prior / regularization effects .",0
22119,"In the end - to - end supervised setting , a word embedding matrix V would need to be initialized randomly and trained as part of the model .",0
22120,"In the preliminary experiments under our framework , we were unable to improve accuracy over one - hot LSTM by inclusion of such a randomly initialized word embedding layer ; i.e. , random vectors failed to provide good prior effects .",0
22121,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",0
22122,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",0
22123,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",0
22124,"Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .",0
22125,More simplifications,0
22126,We introduce four more useful modifications to wv - LSTM that lead to higher accuracy or faster training .,0
22127,Pooling : simplifying sub - problems,0
22128,Our framework of ' region embedding + pooling ' has a simplification effect as follows .,0
22129,"In wv - LSTM , the sub-problem that LSTM needs to solve is to represent the entire document by one vector ( document embedding ) .",0
22130,We make this easy by changing it to detecting regions of text ( of arbitrary sizes ) thatare relevant to the task and representing them by vectors ( region embedding ) .,0
22131,"As illustrated in , we let the LSTM layer emit vectors ht at each time step , and let pooling aggregate them into a document vector .",0
22132,"With wv - LSTM , LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10 K words away .",0
22133,The task of our LSTM is easier as it is allowed to forget old things via the forget gate and can focus on representing the concepts conveyed by smaller segments such as phrases or sentences .,0
22134,A related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding .,0
22135,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .",0
22136,Chopping for speeding up training,0
22137,"In addition to simplifying the sub-problem , pooling has the merit of enabling faster training via chopping .",0
22138,"Since we set the goal of LSTM to embedding text regions instead of documents , it is no longer crucial to go through the document from the beginning to the end sequentially .",0
22139,"At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .",0
22140,( Note that this is done only in the LSTM layer and pooling is done over the entire document . ),0
22141,We perform testing without chopping .,0
22142,"That is , we train LSTM with approximations of sequences for speedup and test with real sequences for better accuracy .",0
22143,"There is a risk of chopping important phrases ( e.g. , "" do n't | like it "" ) , and this can be easily avoided by having segments slightly overlap .",0
22144,"However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .",0
22145,Removing the input / output gates,0
22146,"We found that when LSTM is followed by pooling , the presence of input and output gates typically does not improve accuracy , while removing them nearly halves the time and memory required for training and testing .",0
22147,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",0
22148,"Without the input and output gates , the LSTM formulation can be simplified to :",0
22149,( 2 ),0
22150,This is equivalent to fixing it and o t to all ones .,0
22151,"It is in spirit similar to Gated Recurrent Units but simpler , having fewer gates .",0
22152,Bidirectional LSTM for better accuracy,0
22153,The changes from wv - LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html,0
22154,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,0
22155,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",0
22156,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",0
22157,"shows how much accuracy and / or training speed can be improved by elimination of the word embedding layer , pooling , chopping , removing the input / output gates , and adding the backward LSTM .",0
22158,Experiments ( supervised ),1
22159,"We used four datasets : IMDB , Elec , RCV1 ( second - level topics ) , and 20 - newsgroup ( 20 NG ) 3 , to facilitate direct comparison with JZ15 and DL15 .",0
22160,The first three were used in JZ15 .,0
22161,IMDB and 20 NG were used in DL15 .,0
22162,The datasets are summarized in .,0
22163,The data was converted to lower - case letters .,0
22164,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",0
22165,"Data. "" avg "" / "" max "" : the average / maximum length of documents ( #words ) of the training / test data .",0
22166,"IMDB and Elec are for sentiment classification ( positive vs. negative ) of movie reviews and Amazon electronics product reviews , respectively .",0
22167,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .",0
22168,zero mean and standard deviation 0.01 .,0
22169,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,1
22170,"Hyper parameters such as learning rates were chosen based on the performance on the development data , which was a held - out portion of the training data , and training was redone using all the training data with the chosen parameters .",0
22171,"We used the same pooling method as used in JZ15 , which parameterizes the number of pooling regions so that pooling is done fork non-overlapping regions of equal size , and the resulting k vectors are concatenated to make one vector per document .",0
22172,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",0
22173,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",1
22174,Now we review the non -LSTM baseline methods .,1
22175,The last row of shows the best one - hot CNN results within the constraints above .,0
22176,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .",0
22177,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",1
22178,"However , on RCV1 , it underperforms both .",0
22179,We conjecture that this is because strict word order is not very useful on RCV1 .,0
22180,This point can also be observed in the SVM and CNN performances .,0
22181,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",1
22182,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",0
22183,This is presumably because the former can more easily cover variability of expressions indicative of topics .,0
22184,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",0
22185,More on one - hot CNN vs. one - hot LSTM LSTM can embed regions of variable ( and possibly large ) sizes whereas CNN requires the region size to be fixed .,0
22186,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,0
22187,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",0
22188,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,0
22189,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",0
22190,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",0
22191,This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,0
22192,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",1
22193,Comparison with the previous best results on 20 NG,0
22194,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",1
22195,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",1
22196,"The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",0
22197,Semi-supervised LSTM,0
22198,"To exploit unlabeled data as an additional resource , we use a non-linear extension of two - view feature learning , whose linear version appeared in our earlier work .",0
22199,This was used in JZ15 b to learn from unlabeled data a region embedding embodied by a convolution layer .,0
22200,In this work we use it to learn a region embedding embodied by a one - hot LSTM .,0
22201,Let us start with a brief review of non-linear two - view feature learning .,0
22202,Two - view embedding ( tv-embedding ) [ JZ15 b ],0
22203,A rough sketch is as follows .,0
22204,Consider two views of the input .,0
22205,An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view .,0
22206,"If the two views and the labels ( classification targets ) are related to one another only through some hidden states , then the tv-embedded view is as good as the original view for the purpose of classification .",0
22207,Such an embedding is useful provided that its dimensionality is much lower than the original view .,0
22208,JZ15 b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding ( embodied by a convolution layer ) on unlabeled data .,0
22209,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .",0
22210,"we consider the following two views : the words we have already seen in the document ( view - 1 ) , and the next few words ( view - 2 ) .",0
22211,The task of tv-embedding learning is to predict view - 2 based on view - 1 .,0
22212,"We train one - hot LSTMs in both directions , as in , on unlabeled data .",0
22213,"For this purpose , we use the input and output gates as well as the forget gate as we found them to be useful .",0
22214,Learning LSTM tv-embeddings,0
22215,The theory of tv-embedding says that the region embeddings obtained in this way are useful for the task of interest if the two views are related to each other through the concepts relevant to the task .,0
22216,"To reduce undesirable relations between the views such as syntactic relations , JZ15 b performed vocabulary control to remove function words from ( and only from ) the vocabulary of the target view , which we found useful also for LSTM .",0
22217,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,0
22218,) .,0
22219,"x j t is the output of a tv-embedding ( an LSTM trained with unlabeled data ) indexed by j at time step t , and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in .",0
22220,"Although it is possible to fine - tune the tv-embeddings with labeled data , for simplicity and faster training , we fixed them in our experiments .",0
22221,Combining LSTM tv-embeddings and CNN tv-embeddings,0
22222,"It is easy to see that the set S above can be expanded with any tv-embeddings , not only those in the form of LSTM ( LSTM tv-embeddings ) but also with the tv-embeddings in the form of convolution layers ( CNN tv-embeddings ) such as those obtained in JZ15 b .",0
22223,"Similarly , it is possible to use LSTM tv-embeddings to produce additional input to CNN .",0
22224,"While both LSTM tv-embeddings and CNN tv-embeddings are region embeddings , their formulations are very different from each other ; therefore , we expect that they complement each other and bring further performance improvements when combined .",0
22225,We will empirically confirm these conjectures in the experiments below .,0
22226,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,0
22227,6.66 6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,0
22228,"6.81 6.57 7.97 our framework , which uses unlabeled data to produce additional input to LSTM instead of pre-training .",0
22229,Semi-supervised experiments,1
22230,"We used IMDB , Elec , and RCV1 for our semi-supervised experiments ; 20 NG was excluded due to the absence of standard unlabeled data .",0
22231,summarizes the unlabeled data .,0
22232,"To experiment with LSTM tv-embeddings , we trained two LSTMs ( forward and backward ) with 100 units each on unlabeled data .",0
22233,The training objective was to predict the next k words where k was set to 20 for RCV1 and 5 for others .,0
22234,"Similar to JZ15 b , we minimized weighted square",0
22235,") 2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ?",0
22236,"i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",0
22237,Other details followed the supervised experiments .,0
22238,"Our semi-supervised one - hot bidirectional LSTM with pooling ( oh - 2 LSTM p ) in row # 4 of used the two LSTM tv-embeddings trained on unlabeled data as described above , to produce additional input to one - hot LSTMs in two directions ( 500 units each ) .",0
22239,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .",0
22240,We review the semi-supervised performance of wv - LSTMs ) .,0
22241,"In DL15 the model consisted of a word embedding layer of 512 dimensions , an LSTM layer with 1024 units , and 30 hidden units on top of the LSTM layer ; the word embedding layer and the LSTM were pre-trained with unlabeled data and were fine - tuned with labeled data ; pre-training used either the language model objective or autoencoder objective .",0
22242,"The error rate on IMDB is from DL15 , and those on Elec and RCV1 are our best effort to perform pre-training with the language model objective .",0
22243,"We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .",0
22244,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",1
22245,"Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",0
22246,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",0
22247,The word vectors were optionally updated ( finetuned ) during training .,0
22248,Two types of word vectors were tested .,0
22249,The Google News word vectors were trained by word2vec on a huge ( 100 billion - word ) news corpus and are provided publicly .,0
22250,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",1
22251,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",0
22252,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .",0
22253,"We attribute the superiority of the models with tv-embeddings to the fact that they learn , from unlabeled data , embeddings of text regions , which can convey higher - level concepts than single words in isolation .",0
22254,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",1
22255,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,1
22256,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",1
22257,"As discussed earlier , we attribute the superiority of one - hot CNN on RCV1 to its unique way of representing parts of documents via bow input .",0
22258,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,0
22259,In Section 3.3 we noted that LSTM tv-embeddings and CNN tv-embeddings can be naturally combined .,0
22260,We experimented with this idea in the following two settings ..,0
22261,Comparison with previous best results .,0
22262,Error rates ( % ) .,0
22263,""" U "" : Was unlabeled data used ?",0
22264,""" Co - tr. optimized "" : co-training using oh - CNN as a base learner with parameters ( e.g. , when to stop ) optimized on the test data ; it demonstrates the difficulty of exploiting unlabeled data on these tasks .",0
22265,"In one setting , oh - 2 LSTMp takes additional input from five embeddings : two LSTM tv-embeddings used in and three CNN tv-embeddings from JZ15 b obtained by three distinct combinations of training objectives and input representations , which are publicly provided .",0
22266,"These CNN tv-embeddings were trained to be applied to text regions of size k at every location taking bow input , where k is 5 on IMDB / Elec and 20 on RCV1 .",0
22267,"We connect each of the CNN tv-embeddings to an LSTM by aligning the centers of the regions of the former with the LSTM time steps ; e.g. , the CNN tv-embedding result on the first five words is passed to the LSTM at the time step on the third word .",0
22268,"In the second setting , we trained one - hot CNN with these five types of tv-embeddings by replacing ( 1 ) max ( 0 , Wx + b ) by max ( 0 , Wx + j W ( j ) x j + b ) where x j is the output of the j - th tv-embedding with the same alignment as above .",0
22269,Rows 3 - 4 of show the results of these two types of models .,0
22270,"For comparison , we also show the results of the LSTM with LSTM tv-embeddings only ( row# 1 ) and the CNN with CNN tv-embeddings only ( row # 2 ) .",0
22271,"To see the effects of combination , compare row# 3 with row# 1 , and compare row # 4 with row # 2 .",0
22272,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .",0
22273,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .",0
22274,Comparison with the previous best results,0
22275,The previous best results in the literature are shown in Table 7 .,0
22276,"More results of previous semi-supervised models can be found in JZ15b , all of which clearly underperform the semi-supervised one - hot CNN of .",0
22277,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .",0
22278,"Many more of the previous results on IMDB can be found in , all of which are over 10 % except for 8.78 by bi-gram NBSVM .",0
22279,7.42 by paragraph vectors ) and 6.51 by JZ15 b were considered to be large improvements .,0
22280,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .",0
22281,Conclusion,0
22282,"Within the general framework of ' region embedding + pooling ' for text categorization , we explored region embeddings via one - hot LSTM .",0
22283,"The region embedding of onehot LSTM rivaled or outperformed that of the state - of - the art one - hot CNN , proving its effectiveness .",0
22284,We also found that the models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM .,0
22285,"The best results were obtained by combining the two types of region embedding trained on unlabeled data , suggesting that their strengths are complementary .",0
22286,"As a result , we reported substantial improvements over the previous best results on benchmark datasets .",0
22287,"At a high level , our results indicate the following .",0
22288,"First , on this task , embeddings of text regions , which can convey higher - level concepts , are more useful than embeddings of single words in isolation .",0
22289,"Second , useful region embeddings can be learned by working with one - hot vectors directly , either on labeled data or unlabeled data .",0
22290,"Finally , a promising future direction might be to seek , under this framework , new region embedding methods with complementary benefits .",0
22291,title,0
22292,Character - level Convolutional Networks for Text Classification,1
22293,*,0
22294,abstract,0
22295,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,0
22296,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,0
22297,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",0
22298,There are also related works that use character - level features for language processing .,0
22299,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",0
22300,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",0
22301,Improvements for part - of - speech tagging and information retrieval were observed .,0
22302,Introduction,0
22303,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",0
22304,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,0
22305,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",0
22306,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",0
22307,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",0
22308,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",1
22309,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,0
22310,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",0
22311,An extensive set of comparisons is offered with traditional models and other deep learning models .,0
22312,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,0
22313,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",0
22314,These approaches have been proven to be competitive to traditional models .,0
22315,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,0
22316,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",0
22317,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,0
22318,Character - level Convolutional Networks,0
22319,"In this section , we introduce the design of character - level ConvNets for text classification .",0
22320,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",0
22321,Key Modules,0
22322,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",0
22323,Suppose we have a discrete input function g ( x ) ?,0
22324,"[ 1 , l ] ?",0
22325,Rand a discrete kernel function,0
22326,where c = k ? d + 1 is an offset constant .,0
22327,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",0
22328,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",0
22329,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,0
22330,One key module that helped us to train deeper models is temporal max - pooling .,0
22331,It is the 1 - D version of the max - pooling module used in computer vision .,0
22332,Given a discrete input function g ( x ) ?,0
22333,"[ 1 , l ] ?",0
22334,"R , the max - pooling function h ( y ) ?",0
22335,"[ 1 , ( l ? k) / d + 1 ] ?",0
22336,R of g ( x ) is defined as,0
22337,where c = k ? d + 1 is an offset constant .,0
22338,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",0
22339,The analysis by might shed some light on this .,0
22340,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",0
22341,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",0
22342,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,0
22343,This number will later be detailed for each dataset sparately .,0
22344,The implementation is done using Torch 7 .,0
22345,Character quantization,0
22346,Our models accept a sequence of encoded characters as input .,0
22347,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",0
22348,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",0
22349,"Any character exceeding length l 0 is ignored , and any characters thatare not in the alphabet including blank characters are quantized as all - zero vectors .",0
22350,"The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output , making it easy for fully connected layers to associate weights with the latest reading .",0
22351,Later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower - case letters .,0
22352,Model Design,0
22353,We designed 2 ConvNets - one large and one small .,0
22354,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,0
22355,gives an illustration .,0
22356,Some Text,0
22357,Convolutions,0
22358,Max - pooling Length Feature Quantization ...,0
22359,Conv. and Pool. layers,0
22360,Fully - connected :,0
22361,Illustration of our model,0
22362,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",0
22363,It seems that 1014 characters could already capture most of the texts of interest .,0
22364,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,0
22365,They have dropout probability of 0.5 .,0
22366,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",0
22367,We initialize the weights using a Gaussian distribution .,0
22368,"The mean and standard deviation used for initializing the large model is ( 0 , 0.02 ) and small model ( 0 , 0.05 ) . :",0
22369,Fully - connected layers used in our experiments .,0
22370,The number of output units for the last layer is determined by the problem .,0
22371,"For example , for a 10 - class classification problem it will be 10 .",0
22372,Depends on the problem,0
22373,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",0
22374,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",0
22375,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,0
22376,Layer Output Units Large Output Units,0
22377,Data Augmentation using Thesaurus,0
22378,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,0
22379,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,0
22380,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",0
22381,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",0
22382,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",0
22383,"We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .",0
22384,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",0
22385,"To decide on how many words to replace , we extract all replaceable words from the given text and randomly chooser of them to be replaced .",0
22386,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,0
22387,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,0
22388,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",0
22389,We will report the results using this new data augmentation technique with p = 0.5 and q = 0.5 .,0
22390,Comparison Models,0
22391,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .",0
22392,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",0
22393,Traditional Methods,1
22394,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,0
22395,The classifier used is a multinomial logistic regression in all these models .,0
22396,Bag - of - words and its TFIDF .,1
22397,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",1
22398,"For the normal bag - of - words , we use the counts of each word as the features .",0
22399,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",0
22400,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,0
22401,The features are normalized by dividing the largest feature value .,0
22402,Bag - of - ngrams and its TFIDF .,1
22403,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",1
22404,The feature values are computed the same way as in the bag - of - words model .,0
22405,Bag - of - means on word embedding .,1
22406,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",1
22407,We take into consideration all the words that appeared more than 5 times in the training subset .,0
22408,The dimension of the embedding is 300 .,0
22409,The bag - of - means features are computed the same way as in the bag - of - words model .,0
22410,The number of means is 5000 .,0
22411,Deep Learning Methods,1
22412,Recently deep learning methods have started to be applied to text classification .,0
22413,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .",0
22414,Word - based ConvNets .,1
22415,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",0
22416,We offer comparisons with both using the pretrained word2vec embedding and using lookup tables .,0
22417,"The embedding size is 300 in both cases , in the same way as our bagof - means model .",0
22418,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",0
22419,Experiments using a thesaurus for data augmentation are also conducted .,0
22420,LSTM LSTM LSTM ... : long - short term memory,0
22421,Long - short term memory .,1
22422,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",0
22423,"The LSTM model used in our case is word - based , using pretrained word2vec embedding of size 300 as in previous models .",0
22424,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .",0
22425,The output dimension is 512 .,0
22426,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",0
22427,We also used gradient clipping in which the gradient norm is limited to 5 . gives an illustration .,0
22428,Mean,0
22429,Choice of Alphabet,0
22430,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",0
22431,We report experiments on this choice and observed that it usually ( but not always ) gives worse results when such distinction is made .,0
22432,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",0
22433,Large - scale Datasets and Results,0
22434,"Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",0
22435,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",0
22436,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",0
22437,is a summary .,0
22438,Sogou news corpus .,0
22439,"This dataset is a combination of the Sogo u CA and Sogo uCS news corpora , containing in total 2,909,551 news articles in various topic channels .",0
22440,"We then labeled each piece of news using its URL , by manually classifying the their domain names .",0
22441,This gives us a large corpus of news articles labeled with their categories .,0
22442,There area large number categories but most of them contain only few articles .,0
22443,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",0
22444,"The number of training samples selected for each class is 90,000 and testing 12,000 .",0
22445,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",0
22446,The models for English can then be applied to this dataset without change .,0
22447,The fields used are title and content . :,0
22448,Testing errors of all the models .,0
22449,Numbers are in percentage .,0
22450,""" Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",0
22451,""" w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",0
22452,DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .,0
22453,The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014 .,0
22454,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",0
22455,The fields we used for this dataset contain title and abstract of each Wikipedia article .,0
22456,Yelp reviews .,0
22457,The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .,0
22458,"This dataset contains 1,569,264 samples that have review texts .",0
22459,"Two classification tasks are constructed from this dataset - one predicting full number of stars the user has given , and the other predicting a polarity label by considering stars 1 and 2 negative , and 3 and 4 positive .",0
22460,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",0
22461,Yahoo!,0
22462,Answers dataset .,0
22463,We obtained Yahoo!,0
22464,Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo !,0
22465,Webscope program .,0
22466,"The corpus contains 4,483,032 questions and their answers .",0
22467,We constructed a topic classification dataset from this corpus using 10 largest main categories .,0
22468,"Each class contains 140,000 training samples and 5,000 testing samples .",0
22469,"The fields we used include question title , question content and best answer .",0
22470,Amazon reviews .,0
22471,"We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",0
22472,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",0
22473,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",0
22474,The fields used are review title and review content .,0
22475,lists all the testing errors we obtained from these datasets for all the applicable models .,0
22476,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",0
22477,We labeled the best result in blue and worse result in red .,0
22478,Figure 3 : Relative errors with comparison models,0
22479,"To understand the results in table 4 further , we offer some empirical analysis in this section .",0
22480,"To facilitate our analysis , we present the relative errors in with respect to comparison models .",0
22481,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",0
22482,All ConvNets in the figure are the large models with thesaurus augmentation respectively .,0
22483,Character - level ConvNet is an effective method .,0
22484,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,1
22485,This is a strong indication that language could also bethought of as a signal no different from any other kind .,0
22486,shows 12 random first - layer patches learnt by one of our character - level ConvNets for DBPedia dataset .,0
22487,Dataset size forms a dichotomy between traditional and ConvNets models .,0
22488,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,0
22489,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",1
22490,Conv Nets may work well for user - generated data .,1
22491,User- generated data vary in the degree of how well the texts are curated .,0
22492,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",0
22493,Answers .,0
22494,"Plots comparing word - based deep models ( figures 3 c , 3 d and 3 e ) show that character - level ConvNets work better for less curated user - generated texts .",0
22495,This property suggests that ConvNets may have better applicability to real - world scenarios .,0
22496,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",0
22497,Choice of alphabet makes a difference .,1
22498,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,0
22499,"For million - scale datasets , it seems that not making such distinction usually works better .",0
22500,"One possible explanation is that there is a regularization effect , but this is to be validated .",0
22501,Semantics of tasks may not matter .,1
22502,Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,0
22503,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,0
22504,Bag - of - means is a misuse of word2vec .,0
22505,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,0
22506,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",0
22507,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",0
22508,There is no free lunch .,0
22509,Our experiments once again verifies that there is not a single machine learning model that can work for all kinds of datasets .,0
22510,The factors discussed in this section could all play a role in deciding which method is the best for some specific application .,0
22511,Conclusion and Outlook,0
22512,This article offers an empirical study on character - level convolutional networks for text classification .,0
22513,We compared with a large number of traditional and deep learning models using several largescale datasets .,0
22514,"On one hand , analysis shows that character - level ConvNet is an effective method .",0
22515,"On the other hand , how well our model performs in comparisons depends on many factors , such as dataset size , whether the texts are curated and choice of alphabet .",0
22516,"In the future , we hope to apply character - level ConvNets for a broader range of language processing tasks especially when structured outputs are needed .",0
22517,title,0
22518,HDLTex : Hierarchical Deep Learning for Text Classification,1
22519,abstract,0
22520,"Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",0
22521,"Central to these information processing methods is document classification , which has become an important application for supervised learning .",1
22522,Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,0
22523,This is because along with growth in the number of documents has come an increase in the number of categories .,0
22524,This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification .,0
22525,Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,1
22526,HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy .,0
22527,I. INTRODUCTION,0
22528,Each year scientific researchers produce a massive number of documents .,0
22529,"In 2014 the 28,100 active , scholarly , peerreviewed , English - language journals published about 2.5 million articles , and there is evidence that the rate of growth in both new journals and publications is accelerating .",0
22530,The volume of these documents has made automatic organization and classification an essential element for the advancement of basic and applied research .,0
22531,"Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees , nave Bayes , support vector machines ( SVM ) , neural nets , and ensemble methods .",0
22532,Classification trees and nave Bayes approaches provide good interpretability but tend to be less accurate than the other methods .,0
22533,"However , automatic classification has become increasingly challenging over the last several years due to growth in corpus sizes and the number of fields and sub-fields .",0
22534,Areas of research that were little known only five years ago have now become areas of high growth and interest .,0
22535,"This growth in sub-fields has occurred across a range of disciplines including biology ( e.g. , CRISPR - CA9 ) , material science ( e.g. , chemical programming ) , and health sciences ( e.g. , precision medicine ) .",0
22536,This growth in sub-fields means that it is important to not just label a document by specialized area but to also organize it within its over all field and the accompanying sub-field .,0
22537,This is hierarchical classification .,0
22538,"Although many existing approaches to document classification can quickly identify the over all area of a document , few of them can rapidly organize documents into the correct subfields or areas of specialization .",0
22539,"Further , the combination of top - level fields and all sub-fields presents current document classification approaches with a combinatorially increasing number of class labels that they can not handle .",0
22540,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,1
22541,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,1
22542,"This paper reports our experiments with HDLTex , which exhibits improved accuracy over traditional document classification methods .",0
22543,II .,0
22544,RELATED WORK,0
22545,"Document classification is necessary to organize documents for retrieval , analysis , curation , and annotation .",0
22546,Researchers have studied and developed a variety of methods for document classification .,0
22547,Work in the information retrieval community has focused on search engine fundamentals such as indexing and dictionaries thatare considered core technologies in this field .,0
22548,"Considerable work has built on these foundational methods to provide improvements through feedback and query reformulation , .",0
22549,More recent work has employed methods from data mining and machine learning .,0
22550,Among the most accurate of these techniques is the support vector machine ( SVM ) - .,0
22551,SVMs use kernel functions to find separating hyperplanes in high - dimensional spaces .,0
22552,"Other kernel methods used for information retrieval include string kernels such as the spectrum kernel and the mismatch kernel , which are widely used with DNA and RNA sequence data .",0
22553,SVM and related methods are difficult to interpret .,0
22554,"For this reason many information retrieval systems use decision trees and nave Bayes , methods .",0
22555,"These methods are easier to understand and , as such , can support query reformulation , but they lack accuracy .",0
22556,Some recent work has investigated topic modeling to provide similar interpretations as nave Bayes methods but with improved accuracy .,0
22557,This paper uses newer methods of machine learning for document classification taken from deep learning .,0
22558,"Deep learning is an efficient version of neural networks that can perform unsupervised , supervised , and semi-supervised learning .",0
22559,"Deep learning has been extensively used for image processing , but many recent studies have applied deep learning in other domains such as text and data mining .",0
22560,The basic architecture in a neural network is a fully connected network of nonlinear processing nodes organized as layers .,0
22561,"The first layer is the input layer , the final layer is the output layer , and all other layers are hidden .",0
22562,"In this paper , we will refer to these fully connected networks as Deep Neural Networks ( DNN ) .",0
22563,Convolutional Neural Networks ( CNNs ) are modeled after the architecture of the visual cortex where neurons are not fully connected but are spatially distinct .,0
22564,CNNs provide excellent results in generalizing the classification of objects in images .,0
22565,More recent work has used CNNs for text mining .,0
22566,"In research closely related to the work in this paper , Zhang et al.",0
22567,used CNNs for text classification with character - level features provided by a fully connected DNN .,0
22568,"Regardless of the application , CNNs require large training sets .",0
22569,Another fundamental deep learning architecture used in this paper is the Recurrent Neural Network ( RNN ) .,0
22570,RNNs connect the output of a layer back to its input .,0
22571,This architecture is particularly important for learning time - dependent structures to include words or characters in text .,0
22572,"Deep learning for hierarchical classification is not new with this paper , although the specific architectures , the comparative analyses , and the application to document classification are new .",0
22573,"Salakhutdinov , used deep learning to hierarchically categorize images .",0
22574,At the top level the images are labeled as animals or vehicles .,0
22575,The next level then classifies the kind of animal or vehicle .,0
22576,This paper describes the use of deep learning approaches to create a hierarchical document classification approach .,0
22577,These deep learning methods have the promise of providing greater accuracy than SVM and related methods .,0
22578,Deep learning methods also provide flexible architectures that we have used to produce hierarchical classifications .,0
22579,The hierarchical classification our methods produce is not only highly accurate but also enables greater understanding of the resulting classification by showing where the document sits within afield or area of study .,0
22580,III .,0
22581,BASELINE TECHNIQUES,0
22582,This paper compares fifteen methods for performing document classification .,0
22583,"Six of these methods are baselines since they are used for traditional , non-hierarchical document classification .",0
22584,"Of the six baseline methods three are widely used for document classification : term - weighted support vector machines , multi-word support vector machines , and nave Bayes classification ( NBC ) .",0
22585,The other three are newer deep learning methods that form the basis for our implementation of a new approach for hierarchical document classification .,0
22586,These deep learning methods are described in Section V .,0
22587,A. Support Vector Machines ( SVMs ),0
22588,"Vapnik and Chervonenkis introduced the SVM in 1963 , , and in 1992 Boser et al. introduced a nonlinear version to address more complex classification problems .",0
22589,"The key idea of the nonlinear SVM is the generating kernel shown in Equation 1 , followed by Equations 2 and 3 :",0
22590,Multi - Class SVM : Text classification using string kernels within SVMs has been successful in many research projects .,0
22591,"The original SVM solves a binary classification problem ; however , since document classification often involves several classes , the binary SVM requires an extension .",0
22592,"In general , the multi-class SVM ( MSVM ) solves the following optimization :",0
22593,"where k indicates number of classes , ?",0
22594,"i are slack variables , and w is the learning parameter .",0
22595,"To solve the MSVM we construct a decision function of all k classes at once , .",0
22596,One approach to MSVM is to use binary SVM to compare each of the k ( k ?,0
22597,"1 ) pairwise classification labels , where k is the number of labels or classes .",0
22598,"Yet another technique for MSVM is one - versus - all , where the two classes are one of the k labels versus all of the other k ?",0
22599,1 labels .,0
22600,Stacking Support Vector Machines ( SVM ) :,0
22601,We use Stacking SVMs as another baseline method for comparison with HDL - Tex .,0
22602,"The stacking SVM provides an ensemble of individual SVM classifiers and generally produces more accurate results than single - SVM models , .",0
22603,B. Nave Bayes classification,0
22604,"Nave Bayes is a simple supervised learning technique often used for information retrieval due to its speed and interpretability , .",0
22605,"Suppose the number of documents is n and each document has the label c , c ? {c 1 , c 2 , ... , ck } , where k is the number of labels .",0
22606,"Nave Bayes calculates where d is the document , resulting in",0
22607,The nave Bayes document classifier used for this study uses word - level classification .,0
22608,Let ?,0
22609,"j be the parameter for word j , then",0
22610,IV .,0
22611,FEATURE EXTRACTION,0
22612,Documents enter our hierarchical models via features extracted from the text .,0
22613,We employed different feature extraction approaches for the deep learning architectures we built .,0
22614,"For CNN and RNN , we used the text vector - space models using 100 dimensions as described in Glove .",0
22615,"A vector-space model is a mathematical mapping of the word space , defined as",0
22616,"where l j is the length of the document j , and w i , j is the Glove word embedding vectorization of word i in document j.",0
22617,"For DNN , we used count - based and term frequency - inverse document frequency ( tf - idf ) for feature extraction .",0
22618,"This approach uses counts for N - grams , which are sequences of N words , .",0
22619,"For example , the text "" In this paper we introduced this technique "" is composed of the following Ngrams :",0
22620,"Feature count ( 1 ) : { ( In , 1 ) , ( this , 2 ) , ( paper , 1 ) , ( we , 1 ) , ( introduced , 1 ) , ( technique , 1 ) } Feature count ( 2 ) : { ( In , 1 ) , ( this , 2 ) , ( paper , 1 ) , ( we , 1 ) , ( introduced , 1 ) , ( technique , 1 ) , ( In this , 1 ) , ( This paper , 1 ) , ( paper we , 1 ) , ...}",0
22621,Where the counts are indexed by the maximum N - grams .,0
22622,So Feature count ( 2 ) includes both 1 - grams and 2 - grams .,0
22623,The resulting DNN feature space is,0
22624,"where f is the feature space of document j for n-grams of size n , n ? { 0 , 1 , ... , N } , and x is determined byword or ngram counts .",0
22625,Our algorithm is able to use N-grams for features within deep learning models .,0
22626,V .,0
22627,DEEP LEARNING NEURAL NETWORKS,0
22628,The methods used in this paper extend the concepts of deep learning neural networks to the hierarchical document classification problem .,0
22629,Deep learning neural networks provide efficient computational models using combinations of nonlinear processing elements organized in layers .,0
22630,"This organization of simple elements allows the total network to generalize ( i.e. , predict correctly on new data ) .",0
22631,"In the research described here , we used several different deep learning techniques and combinations of these techniques to create hierarchical document classifiers .",0
22632,"The following subsections provide an overview of the three deep learning architectures we used : Deep Neural Networks ( DNN ) , Recurrent Neural Networks ( RNN ) , and Convolutional Neural Networks ( CNN ) .",0
22633,A. Deep Neural Networks ( DNN ),0
22634,In the DNN architecture each layer only receives input from the previous layer and outputs to the next layer .,0
22635,The layers are fully connected .,0
22636,The input layer consists of the text features ( see IV ) and the output layer has anode for each classification label or only one node if it is a binary classification .,0
22637,This architecture is the baseline DNN .,0
22638,Additional details on this architecture can be found in . This paper extends this baseline architecture to allow hierarchical classification .,0
22639,shows this new architecture .,0
22640,The DNN for the first level of classification ( on the left side in ) is the same as the baseline DNN .,0
22641,The second level classification in the hierarchy consists of a DNN trained for the domain output in the first hierarchical level .,0
22642,Each second level in the DNN is connected to the output of the first level .,0
22643,"For example , if the output of the first model is labeled computer science then the DNN in the next hierarchical level ( e.g. , ? 1 in ) is trained only with all computer science documents .",0
22644,"So while the first hierarchical level DNN is trained with all documents , each DNN in the next level of the document hierarchy is trained only with the documents for the specified domain .",0
22645,LSTM / GRU,0
22646,The DNNs in this study are trained with the standard backpropagation algorithm using both sigmoid ( Equation 11 ) and ReLU ( Equation 12 ) as activation functions .,0
22647,The output layer uses softmax ( Equation 13 ) .,0
22648,"? j ? { 1 , . . . , K}",0
22649,"Given a set of example pairs ( x , y ) , x ?",0
22650,"X , y ?",0
22651,Y the goal is to learn from the input and target spaces using hidden layers .,0
22652,"In text classification , the input is generated by vectorization of text ( see Section IV ) .",0
22653,B. Recurrent Neural Networks ( RNN ),0
22654,The second deep learning neural network architecture we use is RNN .,0
22655,In RNN the output from a layer of nodes can reenter as input to that layer .,0
22656,This approach has advantages for text processing .,0
22657,The general RNN formulation is given in Equation 14 where x t is the state at time t and u t refers to the input at step t.,0
22658,We use weights to reformulate Equation 14 as shown in below :,0
22659,"In Equation , W rec is the recurrent matrix weight , W in are the input weights , b is the bias , and ?",0
22660,is an element - wise function .,0
22661,Again we have modified the basic architecture for use in hierarchical classification .,0
22662,shows this extended RNN architecture .,0
22663,"Several problems ( e.g. , vanishing and exploding gradients ) arise in RNNs when the error of the gradient descent algorithm is back - propagated through the network .",0
22664,"To deal with these problems , long short - term memory ( LSTM ) is a special type of RNN that preserves long - term dependencies in a more effective way compared with the basic RNN .",0
22665,This is particularly effective at mitigating the vanishing gradient problem .,0
22666,shows the basic cell of an LSTM model .,0
22667,"Although LSTM has a chain - like structure similar to RNN , LSTM uses multiple gates to regulate the amount of information allowed into each node state .",0
22668,A step - by - step explanation the LSTM cell and it s gates is provided below :,0
22669,1 ) Input Gate :,0
22670,2 ) Candid Memory Cell Value :,0
22671,3 ) Forget Gate Activation :,0
22672,4 ) New Memory Cell Value :,0
22673,5 ) Output Gate Values :,0
22674,"In the above description , b is a bias vector , Wis a weight matrix , and x t is the input to the memory cell at time t.",0
22675,"The i , c , f and o indices refer to input , cell memory , forget and output gates , respectively .",0
22676,shows the structure of these gates with a graphical representation .,0
22677,An RNN can be biased when later words are more influential than the earlier ones .,0
22678,To overcome this bias convolutional neural network ( CNN ) models ( discussed in Section V - C ) include a max - pooling layer to determine discriminative phrases in text .,0
22679,A gated recurrent unit ( GRU ) is a gating mechanism for RNNs that was introduced in 2014 .,0
22680,"GRU is a simplified variant of the LSTM architecture , but there are differences as follows :",0
22681,"GRUs contain two gates , they do not possess internal memory ( the C t?1 in , and a second non-linearity is not applied ( tanh in ) .",0
22682,C. Convolutional Neural Networks ( CNN ),0
22683,The final deep learning approach we developed for hierarchical document classification is the convolutional neural network ( CNN ) .,0
22684,"Although originally built for image processing , as discussed in Section II , CNNs have also been effectively used for text classification .",0
22685,The basic convolutional layer in a CNN connects to a small subset of the inputs usually of size 3 3 . Similarly the next convolutional layer connects to only a subset of its preceding layer .,0
22686,"In this way these convolution layers , called feature maps , can be stacked to provide multiple filters on the input .",0
22687,"To reduce computational complexity , CNNs use pooling to reduce the size of the output from one stack of layers to the next in the network .",0
22688,Different pooling techniques are used to reduce outputs while preserving important features .,0
22689,The most common pooling method is max - pooling where the maximum element is selected in the pooling window .,0
22690,"In order to feed the pooled output from stacked featured maps to the next layer , the maps are flattened into one column .",0
22691,The final layers in a CNN are typically fully connected .,0
22692,In general during the back - propagation step of a CNN not only the weights are adjusted but also the feature detector filters .,0
22693,A potential problem of CNNs used for text is the number of channels or size of the feature space .,0
22694,"This might be very large ( e.g. , 50 K words ) for text , but for images this is less of a problem ( e.g. , only 3 channels of RGB ) .",0
22695,D. Hierarchical Deep Learning,0
22696,The primary contribution of this research is hierarchical classification of documents .,0
22697,"A traditional multi-class classification technique can work well for a limited number classes , but performance drops with increasing number of classes , as is present in hierarchically organized documents .",0
22698,"In our hierarchical deep learning model we solve this problem by creating architectures that specialize deep learning approaches for their level of the document hierarchy ( e.g. , see ) .",0
22699,The structure of our Hierarchical Deep Learning for Text ( HDLTex ) architecture for each deep learning model is as follows :,0
22700,DNN : 8 hidden layers with 1024 cells in each hidden layer .,0
22701,"RNN : GRU and LSTM are used in this implementation , 100 cells with GRU with two hidden layers .",0
22702,"CNN : Filter sizes of { 3 , 4 , 5 , 6 , 7 } and max - pool of 5 , layer sizes of { 128 , 128 , 128 } with max pooling of { 5 , 5 , 35 } , the CNN contains 8 hidden layers .",0
22703,All models used the following parameters :,0
22704,"Batch Size = 128 , learning parameters = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 , = 1e 08 , decay = 0.0 , Dropout=0.5 ( DNN ) and Dropout = 0.25 ( CNN and RNN ) .",0
22705,E. Evaluation,0
22706,We used the following cost function for the deep learning models :,0
22707,"where is the number of levels , k indicates number of classes for each level , and ?",0
22708,refers to the number of classes in the child 's level of the hierarchical model .,0
22709,F. Optimization,0
22710,We used two types of stochastic gradient optimization for the deep learning models in this paper : RMSProp and Adam .,0
22711,These are described below .,0
22712,RMSProp Optimizer :,0
22713,The basic stochastic gradient descent ( SGD ) is shown below :,0
22714,"For these equations , ?",0
22715,"is the learning parameter , ?",0
22716,"is the learning rate , and J ( ? , x i , y i ) is the objective or cost function .",0
22717,The history of updates is defined by ? ?,0
22718,"( 0 , 1 ) .",0
22719,"To update parameters , SGD uses a momentum term on a rescaled gradient , which is shown in Equation .",0
22720,"This approach to the optimization does not perform bias correction , which is a problem for a sparse gradient .",0
22721,Adam Optimizer :,0
22722,"Adam is another stochastic gradient optimizer , which averages over only the first two moments of the gradient , v and m , as shown below :",0
22723,where,0
22724,"In these equations , mt and v tare the first and second moments , respectively .",0
22725,Both are estimated asm t = mt,0
22726,"This approach can handle the non-stationarity of the objective function as can RMSProp , but Adam can also overcome the sparse gradient issue that is a drawback in RMSProp .",0
22727,VI .,0
22728,RESULTS,0
22729,A.,0
22730,Data,0
22731,Our document collection had 134 labels as shown in Table I .,0
22732,"The target value has two levels , k 0 ? { 1 , .. , 7 } which are k 0 ? {",0
22733,"Computer Science , Electrical Engineering , Psychology , Mechanical Engineering , Civil Engineering , Medical Science , biochemistry } and children levels of the labels , k , which contain { 17 , , 9 } specific topics belonging to k 0 , respectively .",0
22734,"To train and test the baseline methods described in Section III and the new hierarchical document classification methods described in Section V , we collected data and meta-data on 46 , 985 published papers available from the Web Of Science , .",0
22735,To automate collection we used Selenium with Chorome Driver for the Chrome web browser .,0
22736,To extract the data from the site we used Beautiful Soup .,0
22737,"We specifically extracted the abstract , domain , and keywords of this set of published papers .",0
22738,The text in the abstract is the input for classification while the domain name provides the label for the top level of the hierarchy .,0
22739,The keywords provide the descriptors for the next level in the classification hierarchy .,0
22740,We divided the data set into three parts as shown in .,0
22741,Data set W OS ?,0
22742,"46985 is the full data set with 46,985 documents , and data sets W OS ? 11967 and W OS ? 5736 are subsets of this full data set with the number of training and testing documents shown as well as the number of labels or classes in each of the two levels .",0
22743,"For dataset W OS ? 11967 , each of the seven level - 1 classes has five sub -classes .",0
22744,"For data set W OS ? 5736 , two of the three higher - level classes have four sub - classes and the last high - level class has three subclasses .",0
22745,We removed all special characters from all three data sets before training and testing .,0
22746,B. Hardware and Implementation,0
22747,The following results were obtained using a combination of central processing units ( CPUs ) and graphical processing units ( GPUs ) .,1
22748,"The processing was done on a Xeon E5 ? 2640 ( 2.6 GHz ) with 32 cores and 64GB memory , and the GPU cards were N vidia Quadro K620 and N vidia Tesla K20c .",1
22749,"We implemented our approaches in Python using the Compute Unified Device Architecture ( CUDA ) , which is a parallel computing platform and Application Programming Interface ( API ) model created by N vidia .",1
22750,"We also used Keras and Tensor Flow libraries for creating the neural networks , .",1
22751,shows the results from our experiments .,0
22752,"The baseline tests compare three conventional document classification approaches ( nave Bayes and two versions of SVM ) and stacking SVM with three deep learning approaches ( DNN , RNN , and CNN ) .",1
22753,In this set of tests the RNN outperforms the others for all three W OS data sets .,1
22754,CNN performs secondbest for three data sets .,1
22755,SVM with term weighting is third for the first two sets while the multi-word approach of is in third place for the third data set .,1
22756,The third data set is the smallest of the three and has the fewest labels so the differences among the three best performers are not large .,0
22757,These results show that over all performance improvement for general document classification is obtainable with deep learning approaches compared to traditional methods .,0
22758,"Overall , nave Bayes does much worse than the other methods throughout these tests .",1
22759,"As for the tests of classifying these documents within a hierarchy , the HDLTex approaches with stacked , deep learning architectures clearly provide superior performance .",1
22760,"For data set W OS ? 11967 , the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level .",1
22761,"This gives accuracies of 94 % for the first level , 92 % for the second level and 86 % over all .",1
22762,This is significantly better than all of the others except for the combination of CNN and DNN .,0
22763,For data set W OS ? 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2 .,1
22764,"The closest scores to this are obtained by CNN and RNN in levels 1 and 2 , respectively .",0
22765,C. Empirical Results,0
22766,Finally the simpler data set W OS ?,0
22767,"5736 has a winner in CNN at level 1 and CNN at level 2 , but there is little difference between these scores and those obtained by two other HDLTex architectures : DNN with CNN and RNN with CNN .",0
22768,VII .,0
22769,CONCLUSIONS AND FUTURE WORK,0
22770,"Document classification is an important problem to address , given the growing size of scientific literature and other document sets .",0
22771,"When documents are organized hierarchically , multi-class approaches are difficult to apply using traditional supervised learning methods .",0
22772,"This paper introduces a new approach to hierarchical document classification , HDLTex , that combines multiple deep learning approaches to produce hierarchical classifications .",0
22773,Testing on a data set of documents obtained from the Web of Science shows that combinations of RNN at the higher level and DNN or CNN at the lower level produced accuracies consistently higher than those obtainable by conventional approaches using nave Bayes or SVM .,0
22774,These results show that deep learning methods can provide improvements for document classification and that they provide flexibility to classify documents within a hierarchy .,0
22775,"Hence , they provide extensions over current methods for document classification that only consider the multi -class problem .",0
22776,The methods described here can improved in multiple ways .,0
22777,Additional training and testing with other hierarchically structured document data sets will continue to identify architectures that work best for these problems .,0
22778,"Also , it is possible to extend the hierarchy to more than two levels to capture more of the complexity in the hierarchical classification .",0
22779,"For example , if keywords are treated as ordered then the hierarchy continues down multiple levels .",0
22780,"HDLTex can also be applied to unlabeled documents , such as those found in news or other media outlets .",0
22781,title,0
22782,Squeezed Very Deep Convolutional Neural Networks for Text Classification,1
22783,abstract,0
22784,"Most of the research in convolutional neural networks has focused on increasing network depth to improve accuracy , resulting in a massive number of parameters which restricts the trained network to platforms with memory and processing constraints .",0
22785,We propose to modify the structure of the Very Deep Convolutional Neural Networks ( VDCNN ) model to fit mobile platforms constraints and keep performance .,0
22786,"In this paper , we evaluate the impact of Temporal Depthwise Separable Convolutions and Global Average Pooling in the network parameters , storage size , and latency .",0
22787,"The squeezed model ( SVDCNN ) is between 10x and 20x smaller , depending on the network depth , maintaining a maximum size of 6 MB .",0
22788,"Regarding accuracy , the network experiences a loss between 0.4 % and 1.3 % and obtains lower latencies compared to the baseline model .",0
22789,I. INTRODUCTION,0
22790,The general trend in deep learning approaches has been developing models with increasing layers .,0
22791,"Deeper neural networks have achieved high - quality results in different tasks such as image classification , detection , and segmentation .",0
22792,Deep models can also learn hierarchical feature representations from images .,0
22793,"In the Natural Language Processing ( NLP ) field , the belief that compositional models can also be used to textrelated tasks is more recent .",0
22794,The increasing availability of text data motivates research for models able to improve accuracy in different language tasks .,0
22795,"Following the image classification Convolutional Neural Network ( CNN ) tendency , the research in text classification has placed effort into developing deeper networks .",0
22796,The first CNN based approach for text was a shallow network with one layer .,0
22797,"Following this work , deeper architectures were proposed , .",0
22798,Conneau et al. were the first to propose Very Deep Convolutional Neural Networks ( VDCNN ) applied to text classification .,0
22799,VDCNN accuracy increases with depth .,0
22800,The approach with 29 layers is the state - of - the - art accuracy of CNNs for text classification .,0
22801,* Authors contributed equally and are both first writers .,0
22802,"However , regardless of making networks deeper to improve accuracy , little effort has been made to build text classification models to constrained resources .",0
22803,"It is a very different scenario compared to image approaches , where size and speed constrained models have been proposed , .",0
22804,"In real - world applications , size and speed are constraints to an efficient mobile and embedded deployment of deep models .",0
22805,"Several relevant real - world applications depend on text classification tasks such as sentiment analysis , recommendation and opinion mining .",0
22806,The appeal for these applications combined with the boost in mobile devices usage motivates the need for research in restrained text classification models .,0
22807,"Concerning mobile development , there are numerous benefits to developing smaller models .",0
22808,Some of the most relevant are requiring fewer data transferring while updating the client model and increasing usability by diminishing the inference time .,0
22809,Such advantages would boost the usage of deep neural models in text - based applications for embedded platforms .,0
22810,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .",1
22811,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,1
22812,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",1
22813,Section II provides an overview of the state - of - the - art in CNNs for text classification .,0
22814,Section III presents the VDCNN model .,0
22815,Section IV explains the proposed model SVDCNN and the subsequent impact in the total number of parameters of the network .,0
22816,Section,0
22817,V details how we perform experiments .,0
22818,"Section VI analyses the results and lastly , Section VII , presents conclusions and direction for future works .",0
22819,II .,0
22820,RELATED WORK,0
22821,CNNs were originally designed for Computer Vision with the aim of considering feature extraction and classification as one task .,0
22822,"Although CNNs are very successful in image classification tasks , its use in text classification is relatively new and has some peculiarities .",0
22823,"Contrasting with traditional image bi-dimensional representations , texts are onedimensionally represented .",0
22824,"Due to this property , the convolutions are designed as temporal convolutions .",0
22825,"Furthermore , it is necessary to generate a numerical representation from the text so the network can be trained using this representation .",0
22826,"This representation , namely embeddings , is usually obtained through the application of a lookup table , generated from a given dictionary .",0
22827,An early approach for text classification tasks consisted of a shallow neural network working on the word level and using only one convolutional layer .,0
22828,The author reported results in smaller datasets .,0
22829,"Later ,",0
22830,"Zhang et al. proposed the first CNN performing on a character level ( Char - CNN ) , which allowed them to train up to 6 convolutional layers , followed by three fully connected classification layers .",0
22831,"Char - CNN uses convolutional kernels of size 3 and 7 , as well as simple maxpooling layers .",0
22832,"proposed the Very Deep CNN ( VD - CNN ) also on a character level , presenting improvements compared to Char - CNN .",0
22833,have shown that text classification accuracy increases when the proposed model becomes deeper .,0
22834,VDCNN uses only small kernel convolutions and pooling operations .,0
22835,"The proposed architecture relies on the VGG and ResNet philosophy , : The number of feature maps and the temporal resolution is modeled so that their product is constant .",0
22836,This approach makes it easier to control the memory footprint of the network .,0
22837,Both Zhang and Conneau et al .,0
22838,"CNNs utilized standard convolutional blocks and fully connected layers to combine convolution information , .",0
22839,This architecture choice increases the number of parameters and storage size of the models .,0
22840,"However , size and speed was not the focus of those works .",0
22841,"The idea of developing smaller and more efficient CNNs without losing representative accuracy is a less explored research direction in NLP , but it has already been a trend for computer vision applications , , .",0
22842,Most approaches consist in compressing pre-trained networks or training small networks directly .,0
22843,A recent tendency in deep models is replacing standard convolutional blocks with Depthwise Separable Convolutions ( DSCs ) .,0
22844,The purpose is to reduce the number of parameters and consequently the model size .,0
22845,"DSCs were initially introduced in and since then have been successfully applied to image classification and , , machine translation to reduce the computation in convolutional blocks .",0
22846,Another approach is the use of a Global Average Pooling ( GAP ) layer at the output of the network to replace fully connected layers .,0
22847,"This approach has become a standard architectural decision for newer CNNs , .",0
22848,III .,0
22849,VDCNN MODEL FOR TEXT CLASSIFICATION,0
22850,"The VDCNN is a modular architecture for text classification tasks developed to offer different depth levels ( 9 , 17 , 29 and 49 ) .",0
22851,presents the architecture for depth,0
22852,"9 . The network begins with a lookup table , which generates the embeddings for the input text and stores them in a 2D tensor of size ( f 0 , s ) .",0
22853,"The number of input characters ( s ) is fixed to 1,024 while the embedding dimension ( f 0 ) is 16 .",0
22854,The embedding dimension can be seen as the number of RGB channels of an image .,0
22855,"The following layer ( 3 , Temp Convolution , 64 ) applies 64 temporal convolutions of kernel size 3 , so the output tensor has size 64 * s .",0
22856,It s primary function is to fit the lookup table output with the modular network segment input composed by convolutional blocks .,0
22857,"Each aforenamed block is a sequence of two temporal convolutional layers , each one accompanied by a temporal batch normalization layer and a ReLU activation .",0
22858,"Besides , the different network depths are obtained varying the number of convolutional blocks .",0
22859,"As a convention , the depth of a network is given as its total number of convolutions .",0
22860,"For instance , the architecture of depth 17 has two convolutional blocks of each level of feature maps , which results in 4 convolutional layers for each level ( see ) .",0
22861,"Considering the first convolutional layer of the network , we obtain the depth 2 * ( 2+2+2+2 ) +1 = 17 . The different depth architectures provided by VDCNN model are summarized in .",0
22862,The following rule is employed to minimize the network 's memory footprint :,0
22863,"Before each convolutional block doubling the number of feature maps , a pooling layer halves the temporal dimension .",0
22864,"This strategy is inspired by the VGG and ResNets philosophy and results in three levels of feature maps : 128 , 256 and 512 ( see ) .",0
22865,"Additionally , the VDCNN network also contains shortcut connections for each convolutional blocks implemented through the usage of 1 1 convolutions .",0
22866,"Lastly , for the classification task , the k most valuable features ( k = 8 ) are extracted using k-max pooling , generating a one-dimensional vector which supplies three fully connected layers with ReLU hidden units and softmax outputs .",0
22867,"The number of hidden units is 2,048 , and they do not use dropout but rather batch normalization after convolutional layers perform the network regularization .",0
22868,IV .,0
22869,SVDCNN MODEL FOR TEXT CLASSIFICATION,0
22870,The primary objective is reducing the number of parameters so that the resulting network has a significative lower storage size .,0
22871,We first propose to modify the convolutional blocks of VDCNN model by the usage of Temporal Depthwise Separable Convolutions ( TDSCs ) .,0
22872,"Next , we reduce the number of fully connected layers using the Global Average Pooling ( GAP ) technique .",0
22873,The resulting proposed architecture is called Squeezed Very Deep Convolutional Neural Networks ( SVD - CNN ) .,0
22874,a) Temporal Depthwise Separable Convolutions ( TD - SCs ) :,0
22875,The use of TDSCs over standard convolutions allowed reducing the number of parameters without relevant accuracy loss .,0
22876,TDSCs work decompounding the standard convolution into two parts : Depthwise and Pointwise .,0
22877,The first one is responsible for applying a convolutional filter to each channel of the input at a time .,0
22878,"For an image input , one possibility of channels are the RGB components , whereas in a text input the dimensions of the embedding can be used instead .",0
22879,"For both cases mentioned above , the result is one feature map by channel .",0
22880,The second convolution unifies the generated feature maps successively applying 1x1 convolutions so that the target amount of feature maps can be achieved .,0
22881,TDSCs are DSCs which work with one - dimensional convolutions .,0
22882,"Although DSCs hold verified results in image classification networks , the usage of its temporal version for text related tasks is less explored .",0
22883,presents the architecture of a temporal standard convolution while presents the TDSC .,0
22884,"For a more formal definition , let P tsc be the number of parameters of a temporal standard convolution , where In and Out are the numbers of Input and Output channels respectively , and D k is the kernel size :",0
22885,"Alternatively , a TDSC achieves fewer parameters ( P tdsc ) :",0
22886,"In the VDCNN model , one convolutional block is composed of two temporal standard convolutional layers .",0
22887,The first one doubles the number of feature maps while the second keeps the same value received as input .,0
22888,"Besides , each convolutional layer is followed by a Batch Normalization and a ReLU layers .",0
22889,"In our model , we proposed changing the temporal standard convolutions by TDSCs .",0
22890,presents the standard convolutional block on the left and the proposed convolutional block using TDSC on the right .,0
22891,"The pattern used in the figure for the convolutional layers is the following : "" Kernel Size , Conv type , Output Feature Maps "" ; as a brief example consider "" 3x1 , Temporal Conv , 256 "" , which means a Temporal Convolution with kernel size 3 and 256 feature maps as output .",0
22892,"From Equation 1 , we have the number of parameters of the original convolutional block ( P convblock ) as follows :",0
22893,"Moreover , from equation 2 , the number of parameters of the proposed convolutional block ( P convblock ? tdsc ) that uses TDSC being :",0
22894,"P convblock?tdsc = In * 3 + In * Out + Out * 3 + Out * Out ( 4 ) For illustration , following the same characteristics of , consider that the number of input channels",0
22895,In is equal to 128 and the number of output channels,0
22896,Out is equal to 256 .,0
22897,"Our proposed approach accumulates a total of 99,456 parameters .",0
22898,"In contrast , there are 294,912 parameters in the original convolutional block .",0
22899,The use of TDSC yields a reduction of 66.28 % in the network size .,0
22900,"Lastly , since each standard temporal convolution turns into two ( Depthwise and Pointwise ) , the number of convolutions per convolutional block has doubled .",0
22901,"Nevertheless , these two convolutions work as one because it is not possible to use them separately keeping the same propose .",0
22902,"In this way , we count them as one layer in the network depth .",0
22903,"This decision holds the provided depth architectures the same as the VDCNN model summarized in , contributing to a proper comparison between the models .",0
22904,b) Global Average Pooling ( GAP ) :,0
22905,The VDCNN model uses a k-max pooling layer ( k = 8 ) followed by three fully connected ( FC ) layers to perform the classification task ) .,0
22906,"Although this approach is the traditional architecture choice for text classification CNNs , it introduces a significant number of parameter in the network .",0
22907,"The resulting number of the FC layers parameters ( P f c ) aforementioned is presented below , for a problem with four target classes :",0
22908,"Instead of maintaining these fully connected layers , we directly aggregate the output of the last convolutional block through the usage of an average pooling layer .",0
22909,"This method , known as Global Average Pooling , contributes substantially to the parameters reduction without degrading the network accuracy significantly .",0
22910,The number of resulting feature maps given by the average pooling layer was the same as the original k-max pooling layer ( k = h = 8 ) .,0
22911,presents this proposed modification .,0
22912,The number of parameters obtained by the usage of GAP ( P gap ) is revealed as follows :,0
22913,"Our proposed approach accumulates a total of 16,384 parameters .",0
22914,"In contrast , there are 12,591,104 parameters in the original classification method .",0
22915,The use of GAP yields a reduction of 99.86 % .,0
22916,V .,0
22917,EXPERIMENTS,0
22918,The experiment goal is to investigate the impact of modifying the convolutional block of VDCNN to TDSCs and using GAP instead of the original fully connected layers .,0
22919,"We evaluate Char - CNN , VDCNN , and SVDCNN according to the number of parameters , storage size , inference time and accuracy .",0
22920,"The source code of the proposed model is available in the GitHub repository SVDCNN The original VDCNN paper reported the number of parameters of the convolutional layers , in which we reproduce in this article .",0
22921,"For SVDCNN and Char - CNN , we calculated the abovementioned number from the network architecture implemented in PyTorch .",1
22922,"As for the FC layer 's parameters , the number is obtained as the summation of the product of the input and output size of each FC layer for each CNN .",0
22923,Considering the network parameters,0
22924,"P and assuming that one float number on Cuda environment takes 4 bytes , we can calculate the network storage in megabytes , for all the models , as follows :",0
22925,"Regarding the inference time , its average and standard deviation were calculated as the time to predict one instance of the AG's News dataset throughout 1,000 repetitions .",0
22926,"The SVDCNN experimental settings are similar to the original VDCNN paper , using the same dictionary and the same embedding size of 16 .",1
22927,"The training is also performed with SGD , utilizing size batch of 64 , with a maximum of 100 epochs .",1
22928,"We use an initial learning rate of 0.01 , a momentum of 0.9 and a weight decay of 0.001 .",1
22929,All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i 7 4770s CPU .,1
22930,The model 's performance is evaluated on three large - scale public datasets also used by Zhang et al. in the introduction of Char - CNN and VDCNN models .,0
22931,"presents the details of the utilized datasets : AG 's News , Yelp Polarity and Yelp Full .",0
22932,"presents the number of parameters , storage size , and accuracy for the SVDCNN , VDCNN , and Char - CNN in all datasets .",0
22933,The use of TDSCs promoted a significant reduction in convolutional parameters compared to VDCNN .,0
22934,"For the most in - depth network evaluated , which contains 29 convolutional layers ( depth 29 ) , the number of parameters of these convolutional layers had a reduction of 66.08 % , from 4.6 to 1.56 million parameters .",0
22935,"This quantity is slightly larger than the one obtained from the Char - CNN , 1.40 million parameters , but this network has only six convolutional layers ( depth 6 ) .",0
22936,VI .,0
22937,RESULTS,0
22938,The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks .,1
22939,"Considering a dataset with four target classes , and comparing SVDCNN with VDCNN , the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters , representing a reduction of 99.84 % .",1
22940,"Following with the same comparison , but to Char - CNN , the proposed model is 99.82 % smaller , 0.02 against 11.36 million of FC parameters .",1
22941,The reduction of the total parameters impacts directly on the storage size of the networks .,0
22942,"While our most in - depth model ( 29 ) occupies only 6 MB , VDCNN with the same depth occupies 64. 16 MB of storage .",1
22943,"Likewise , Char- CNN ( which has depth 6 ) occupies 43.25 MB .",0
22944,This reduction is a significant result because many embedded platforms have several memory constraints .,0
22945,"For example , FPGAs often have less than 10 MB of on - chip memory and no off - chip memory or storage .",0
22946,"Regarding accuracy results , usually , a model with such parameter reduction should present some loss of accuracy in comparison to the original model .",1
22947,"Nevertheless , the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3 % , which is pretty modest considering the parameters and storage size reduction aforementioned .",1
22948,"In , it is possible to see the accuracy scores obtained by the compared models .",0
22949,Another two fundamental results obtained are a),0
22950,The base property of VDCNN model is preserved on its squeezed model : the performance still increasing up with the depth and b),0
22951,"The performance evaluated for the most extensive dataset , i.e. , Yelp Review ( 62.30 % ) , still overcomes the accuracy of the Char - CNN model ( 62.05 % ) .",0
22952,Deep learning processing architecture has the property of being high parallelizable ; it is expected smaller latencies when performing inferences in hardware with high parallelization power .,0
22953,"Despite this property , the model ability to use all hardware parallel potential available also depends on the network architecture .",0
22954,"The more parameters per layers , the more parallelizable a model tends to be , while the increase of the depth gets the opposite result .",0
22955,"Another natural comprehension fact is if a model has few parameters , there exists less content to be processed , and then we have a faster inference time .",0
22956,"Concerning mobile devices , the presence of dedicated hardware for deep learning is not entirely feasible .",0
22957,"This hardware usually requires more energy and dissipates more heat , two undesirable features for a mobile platform .",0
22958,"Therefore , obtaining fewer inference times , even out of environments with high parallelization capabilities , is a pretty desirable characteristic for a model designed to work on mobile platforms .",0
22959,"The latency ratio between CPU and GPU inference times indicates how undependable of dedicated hardware a model is , with higher values meaning more independence .",0
22960,The inference times obtained for the three models compared are available in .,0
22961,"As explained in Section IV a ) , each convolutional layer of the convolutional blocks was substituted by two convolutions .",0
22962,"This change could impact the inference time negatively , but the significant parameter reduction allows the SVDCNN to obtain better results than the VDCNN model .",0
22963,"The CPU inference time obtained by the proposed model was smaller than the base model for the depth 9 ( 25.88 ms against 29 , 13 ms ) and depth 17 ( 47.80 ms against 48.05 ms ) ,",0
22964,VII .,0
22965,CONCLUSION,0
22966,"In this paper , we presented a squeezed version of the VDCNN model considering the number of parameters and size .",0
22967,The new model proprieties became it feasible for mobile platforms .,0
22968,"To achieve this goal , we analyzed the impact of including Temporal Depthwise Separable Convolutions and a Global Average Pooling layer in a very deep convolutional neural network for text classification .",0
22969,"The SVDCNN model reduces about 92.45 % the number of parameters and storage size while presents an inference time ratio ( CPU / GPU ) , 31.94 % higher .",0
22970,"For future works , we plan to evaluate other techniques able to reduce storage size , such as model compression .",0
22971,"Moreover , the model accuracy over even more massive datasets will be evaluated as well as the efficiency of its depth 49 configuration .",0
22972,title,0
22973,Joint Embedding of Words and Labels for Text Classification,1
22974,abstract,0
22975,"Word embeddings are effective intermediate representations for capturing semantic regularities between words , when learning the representations of text sequences .",0
22976,We propose to view text classification as a label - word joint embedding problem : each label is embedded in the same space with the word vectors .,0
22977,We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels .,0
22978,"The attention is learned on a training set of labeled samples to ensure that , given a text sequence , the relevant words are weighted higher than the irrelevant ones .",0
22979,"Our method maintains the interpretability of word embeddings , and enjoys a built - in ability to leverage alternative sources of information , in addition to input text sequences .",0
22980,"Extensive results on the several large text datasets show that the proposed framework outperforms the state - of - the - art methods by a large margin , in terms of both accuracy and speed .",0
22981,U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2,0
22982,J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q,0
22983,Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q,0
22984,Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e,0
22985,Introduction,0
22986,Text classification is a fundamental problem in natural language processing ( NLP ) .,0
22987,The task is to annotate a given text sequence with one ( or multiple ) class label ( s ) describing its textual content .,0
22988,A key intermediate step is the text representation .,0
22989,"Traditional methods represent text with hand - crafted features , such as sparse lexical features ( e.g. , n-grams ) .",0
22990,"Recently , neural models have been employed to learn text representations , including convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) based on long short - term memory ( LSTM ) .",0
22991,"To further increase the representation flexibility of such models , attention mechanisms have been introduced as an integral part of models employed for text classification .",0
22992,"The attention module is trained to capture the dependencies that make significant contributions to the task , regardless of the distance between the elements in the sequence .",0
22993,It can thus provide complementary information to the distance - aware dependencies modeled by RNN / CNN .,0
22994,The increasing representation power of the attention mechanism comes with increased model complexity .,0
22995,"Alternatively , several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings .",0
22996,"Particularly , quantitatively show that the word - embeddings - based text classification tasks can have the similar level of difficulty regardless of the employed models , using the concept of intrinsic dimension .",0
22997,"Thus , simple models are preferred .",0
22998,"As the basic building blocks in neural - based NLP , word embeddings capture the similarities / regularities between words .",0
22999,"This idea has been extended to compute embeddings that capture the semantics of word sequences ( e.g. , phrases , sentences , paragraphs and documents ) .",0
23000,"These representations are built upon various types of compositions of word vectors , ranging from simple averaging to sophisticated architectures .",0
23001,"Further , they suggest that simple models are efficient and interpretable , and have the poten - ar Xiv : 1805.04174v1 [ cs. CL ] 10 May 2018 tial to outperform sophisticated deep neural models .",0
23002,"It is therefore desirable to leverage the best of both lines of works : learning text representations to capture the dependencies that make significant contributions to the task , while maintaining low computational cost .",0
23003,"For the task of text classification , labels play a central role of the final performance .",0
23004,A natural question to ask is how we can directly use label information in constructing the text - sequence representations .,0
23005,Our Contribution,0
23006,"Our primary contribution is therefore to propose such a solution by making use of the label embedding framework , and propose the Label - Embedding Attentive Model ( LEAM ) to improve text classification .",1
23007,"While there is an abundant literature in the NLP community on word embeddings ( how to describe a word ) for text representations , much less work has been devoted in comparison to label embeddings ( how to describe a class ) .",0
23008,"The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .",1
23009,"Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .",1
23010,"( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .",1
23011,"( iii ) Our attention mechanism ( derived from the text - label compatibility ) has fewer parameters and less computation than related methods , and thus is much cheaper in both training and testing , compared with sophisticated deep attention models .",0
23012,"( iv ) We perform extensive experiments on several text - classification tasks , demonstrating the effectiveness of our label - embedding attentive model , providing state - of - the - art results on benchmark datasets .",0
23013,( v ) We further apply LEAM to predict the medical codes from clinical text .,0
23014,"As an interesting by - product , our attentive model can highlight the informative key words for prediction , which in practice can reduce a doctor 's burden on reading clinical notes .",0
23015,Related Work,0
23016,Label embedding has been shown to be effective in various domains and tasks .,0
23017,"In computer vision , there has been avast amount of research on leveraging label embeddings for image classification , multimodal learning between images and text , and text recognition in images .",0
23018,"It is particularly successful on the task of zero - shot learning , where the label correlation captured in the embedding space can improve the prediction when some classes are unseen .",0
23019,"In NLP , labels embedding for text classification has been studied in the context of heterogeneous networks in and multitask learning in , respectively .",0
23020,"To the authors ' knowledge , there is little research on investigating the effectiveness of label embeddings to design efficient attention models , and how to joint embedding of words and labels to make full use of label information for text classification has not been studied previously , representing a contribution of this paper .",0
23021,"For text representation , the currently bestperforming models usually consist of an encoder and a decoder connected through an attention mechanism , with successful applications to sentiment classification , sentence pair modeling and sentence summarization .",0
23022,"Based on this success , more advanced attention models have been developed , including hierarchical attention networks , attention over attention , and multi-step attention .",0
23023,"The idea of attention is motivated by the observation that different words in the same context are differentially informative , and the same word maybe differentially important in a different context .",0
23024,"The realization of "" context "" varies in different applications and model architectures .",0
23025,"Typically , the context is chosen as the target task , and the attention is computed over the hidden layers of a CNN / RNN .",0
23026,"Our attention model is directly builtin the joint embedding space of words and labels , and the context is specified by the label embedding .",0
23027,"Several recent works have demonstrated that sim - ple attention architectures can alone achieve stateof - the - art performance with less computational time , dispensing with recurrence and convolutions entirely .",0
23028,"Our work is in the same direction , sharing the similar spirit of retaining model simplicity and interpretability .",0
23029,"The major difference is that the aforementioned work focused on self attention , which applies attention to each pair of word tokens from the text sequences .",0
23030,"In this paper , we investigate the attention between words and labels , which is more directly related to the target task .",0
23031,"Furthermore , the proposed LEAM has much less model parameters .",0
23032,Preliminaries,0
23033,"Throughout this paper , we denote vectors as bold , lower - case letters , and matrices as bold , uppercase letters .",0
23034,We use for element - wise division when applied to vectors or matrices .,0
23035,"We use for function composition , and ?",0
23036,p for the set of one hot vectors in dimension p.,0
23037,Given a training set S =,0
23038,"{ ( X n , y n ) } N n=1 of pair - wise data , where X ?",0
23039,"X is the text sequence , and y ?",0
23040,Y is its corresponding label .,0
23041,"Specifically , y is a one hot vector in single - label problem and a binary vector in multi-label problem , as defined later in Section 4.1 .",0
23042,Our goal for text classification is to learn a function f : X ?,0
23043,Y by minimizing an empirical risk of the form :,0
23044,where ? : Y Y ?,0
23045,"R measures the loss incurred from predicting f ( X ) when the true label is y , where f belongs to the functional space F .",0
23046,"In the evaluation stage , we shall use the 0 / 1 loss as a target loss : ? ( y , z ) = 0 if y = z , and 1 otherwise .",0
23047,"In the training stage , we consider surrogate losses commonly used for structured prediction in different problem setups ( see Section 4.1 for details on the surrogate losses used in this paper ) .",0
23048,"More specifically , an input sequence X of length L is composed of word tokens : X = {x 1 , , x L }.",0
23049,Each token x l is a one hot vector in the space ?,0
23050,"D , where Dis the dictionary size .",0
23051,Performing learning in ?,0
23052,Dis computationally expensive and difficult .,0
23053,"An elegant framework in NLP , initially proposed in , allows to concisely perform learning by mapping the words into an embedding space .",0
23054,"The framework relies on so called word embedding : ? D ? RP , where P is the dimensionality of the embedding space .",0
23055,"Therefore , the text sequence X is represented via the respective word embedding for each token : V = {v 1 , , v L } , where v l ?",0
23056,RP .,0
23057,"A typical text classification method proceeds in three steps , endto - end , by considering a function decomposition f = f 0 f 1 f 2 as shown in ( a ) :",0
23058,f 0 : X ?,0
23059,"V , the text sequence is represented as its word - embedding form V , which is a matrix of P L.",0
23060,"f 1 : V ? z , a compositional function f 1 aggregates word embeddings into a fixed - length vector representation z .",0
23061,"f 2 : z ? y , a classifier f 2 annotates the text representation z with a label .",0
23062,"A vast amount of work has been devoted to devising the proper functions f 0 and f 1 , i.e. , how to represent a word or a word sequence , respectively .",0
23063,The success of NLP largely depends on the effectiveness of word embeddings inf 0 .,0
23064,"They are often pre-trained offline on large corpus , then refined jointly via f 1 and f 2 for task - specific representations .",0
23065,"Furthermore , the design off 1 can be broadly cast into two categories .",0
23066,"The popular deep learning models consider the mapping as a "" black box , "" and have employed sophisticated CNN / RNN architectures to achieve state - of - theart performance .",0
23067,"On the contrary , recent studies show that simple manipulation of the word embeddings , e.g. , mean or max - pooling , can also provide surprisingly excellent performance .",0
23068,"Nevertheless , these methods only leverage the information from the input text sequence .",0
23069,Label - Embedding Attentive Model,0
23070,Model,0
23071,Yahoo DBPedia AGNews Yelp P. Yelp F. Bag-of- words 68.90 96.60 88.80 92.20 58.00 Small word CNN 69.98 98.15 89.13 94.46 58.59 Large word CNN 70.94 98.28 91.45 95.11 59.48 LSTM 70 73.53 98.42 92.24 93.76 61.11 fast Text 72.30 98.60 92.50 95.70 63.90 HAN,0
23072,75.80 ----Bi-BloSAN,0
23073,76 . The results are shown in .,0
23074,Testing accuracy,0
23075,Simple compositional methods indeed achieve comparable performance as the sophisticated deep CNN / RNN models .,0
23076,"On the other hand , deep hierarchical attention model can improve the pure CNN / RNN models .",0
23077,The recently proposed self - attention network generally yield higher accuracy than previous methods .,0
23078,All approaches are better than traditional bag - of - words method .,0
23079,"Our proposed LEAM outperforms the state - of - the - art methods on two largest datasets , i.e. , Yahoo and DBPedia .",0
23080,"On other datasets , LEAM ranks the 2nd or 3rd best , which are similar to top 1 method in term of the accuracy .",0
23081,"This is probably due to two reasons : ( i ) the number of classes on these datasets is smaller , and ( ii ) there is no explicit corresponding word embedding available for the label embedding initialization during learning .",0
23082,The potential of label embedding may not be fully exploited .,0
23083,"As the ablation study , we replace the nonlinear compatibility to the linear one in ( 2 ) .",0
23084,The degraded performance demonstrates the necessity of spatial dependency and nonlinearity in constructing the attentions .,0
23085,"Nevertheless , we argue LEAM is favorable for text classification , by comparing the model size and time cost , as well as convergence speed in .",0
23086,The time cost is reported as the wall - clock time for 1000 iterations .,0
23087,"LEAM maintains the simplicity and low cost of SWEM , compared with other models .",0
23088,"LEAM uses much less model parameters , and converges significantly faster than Bi - BloSAN .",0
23089,"We also compare the performance when only a partial dataset is labeled , the results are shown in ( b ) .",0
23090,LEAM consistently outperforms other methods with different proportion of labeled data .,0
23091,Hyper-parameter,0
23092,"Our method has an additional hyperparameter , the window sizer to define the length of "" phase "" to construct the attention .",0
23093,"Larger r captures long term dependency , while smaller r enforces the local dependency .",0
23094,We study its impact in ( c ) .,0
23095,"The topic classification tasks generally requires a larger r , while sentiment classification tasks allows relatively smaller r.",0
23096,One may safely chooser around 50 if not finetuning .,0
23097,We report the optimal results in .,0
23098,Joint Embeddings of Words and Labels,0
23099,"We propose to embed both the words and the labels into a joint space i.e. , ? D ? RP and Y ?",0
23100,RP .,0
23101,"The label embeddings are C = [ c 1 , , c K ] , where K is the number of classes .",0
23102,A simple way to measure the compatibility of label - word pairs is via the cosine similarity,0
23103,where ?,0
23104,"is the normalization matrix of size KL , with each element obtained as the multiplication of 2 norms of the c - th label embedding and l - th word embedding : ?",0
23105,kl = ck v l .,0
23106,"To further capture the relative spatial information among consecutive words ( i.e. , phrases 1 ) and introduce non-linearity in the compatibility measure , we consider a generalization of ( 2 ) .",0
23107,"Specifically , for a text phase of length 2 r + 1 centered at l , the local matrix block G l?r:l+r in G measures the label - to - token compatibility for the "" label - phrase "" pairs .",0
23108,"To learn a higher - level compatibility stigmatization u l between the l - th phrase and all labels , we have :",0
23109,where W 1 ? R 2 r+ 1 and b 1 ?,0
23110,"R K are parameters to be learned , and u l ?",0
23111,R K .,0
23112,The largest compatibility value of the l - th phrase wrt the labels is collected :,0
23113,"Together , m is a vector of length L.",0
23114,The compatibility / attention score for the entire text sequence is :,0
23115,where the l - th element of SoftMax is,0
23116,.,0
23117,"The text sequence representation can be simply obtained via averaging the word embeddings , weighted by label - based attention score :",0
23118,Relation to Predictive Text Embeddings Predictive Text Embeddings ( PTE ) is the first method to leverage label embeddings to improve the learned word embeddings .,0
23119,We discuss three major differences between PTE and our LEAM : ( i ) The general settings are different .,0
23120,"PTE casts the text representation through heterogeneous networks , while we consider text representation through an attention model .",0
23121,"( ii ) In PTE , the text representation z is the averaging of word embeddings .",0
23122,"In LEAM , z is weighted averaging of word embeddings through the proposed labelattentive score in ( 6 ) .",0
23123,( iii ) PTE only considers the linear interaction between individual words and labels .,0
23124,LEAM greatly improves the performance by considering nonlinear interaction between phrase and labels .,0
23125,"Specifically , we note that the text embedding in PTE is similar with a very special case of LEAM , when our window sizer = 1 and attention score ?",0
23126,is uniform .,0
23127,"As shown later in of the experimental results , LEAM can be significantly better than the PTE variant .",0
23128,Training Objective,0
23129,The proposed joint embedding framework is applicable to various text classification tasks .,0
23130,We consider two setups in this paper .,0
23131,"For a learned text sequence representation z = f 1 f 0 ( X ) , we jointly optimize f = f 0 f 1 f 2 over F , where f 2 is defined according to the specific tasks :",0
23132,"Single - label problem : categorizes each text instance to precisely one of K classes , y ?",0
23133,"where CE ( , ) is the cross entropy between two probability vectors , and f 2 ( z n ) = SoftMax ( z n ) , with z n = W 2 z n + b 2 and W 2 ?",0
23134,"R KP , b 2 ?",0
23135,R K are trainable parameters .,0
23136,"Multi-label problem : categorizes each text instance to a set of K target labels {y k ? ? 2 |k = 1 , , K} ; there is no constraint on how many of the classes the instance can be assigned to , and",0
23137,"where f 2 ( z nk ) = 1 1 + exp ( z nk ) , and z nk is the kth column of z n .",0
23138,"To summarize , the model parameters ? = {V , C , W 1 , b 1 , W 2 , b 2 }.",0
23139,"They are trained endto - end during learning . { W 1 , b 1 } and { W 2 , b 2 } are weights inf 1 and f 2 , respectively , which are treated as standard neural networks .",0
23140,"For the joint embeddings { V , C} inf 0 , the pre-trained word embeddings are used as initialization if available .",0
23141,Learning & Testing with LEAM,0
23142,Learning and Regularization,0
23143,The quality of the jointly learned embeddings are key to the model performance and interpretability .,0
23144,"Ideally , we hope that each label embedding acts as the "" anchor "" points for each classes : closer to the word / sequence representations thatare in the same classes , while farther from those in different classes .",0
23145,"To best achieve this property , we consider to regularize the learned label embeddings ck to be on its corresponding manifold .",0
23146,This is imposed by the fact ck should be easily classified as the correct label y k :,0
23147,where f 2 is specficied according to the problem in either or .,0
23148,"This regularization is used as a penalty in the main training objective in or , and the default weighting hyperparameter is set as 1 .",0
23149,It will lead to meaningful interpretability of learned label embeddings as shown in the experiments .,0
23150,"Interestingly in text classification , the class itself is often described as a set of E words {e i , i = 1 , , E}.",0
23151,"These words are considered as the most representative description of each class , and highly distinguishing between different classes .",0
23152,"For example , the Yahoo !",0
23153,"Answers Topic dataset contains ten classes , most of which have two words to precisely describe its class - specific features , such as "" Computers & Internet "" , "" Business & Finance "" as well as "" Politics & Government "" etc .",0
23154,We consider to use each label 's corresponding pre-trained word embeddings as the initialization of the label embeddings .,0
23155,"For the datasets without representative class descriptions , one may initialize the label embeddings as random samples drawn from a standard Gaussian distribution .",0
23156,Testing Both the learned word and label embeddings are available in the testing stage .,0
23157,We clarify that the label embeddings C of all class candidates Y are considered as the input in the testing stage ; one should distinguish this from the use of groundtruth label yin prediction .,0
23158,"For a text sequence X , one may feed it through the proposed pipeline for prediction : ( i ) f 1 : harvesting the word embeddings V , ( ii ) f 2 : V interacts with C to obtain G , pooled as ? , which further attends V to derive z , and ( iii ) f 3 : assigning labels based on the tasks .",0
23159,"To speedup testing , one may store G offline , and avoid its online computational cost .",0
23160,Parameters,0
23161,Complexity,0
23162,"We compare CNN , LSTM , Simple Word Embeddings - based Models ( SWEM ) and our LEAM wrt the parameters and computational speed .",0
23163,"For the CNN , we assume the same size m for all filters .",0
23164,"Specifically , h represents the dimension of the hidden units in the LSTM or the number of filters in the CNN ; R denotes the number of blocks in the Bi - Blo SAN ; P denotes the final sequence representation dimension .",0
23165,"Similar to , we examine the number of compositional parameters , computational complexity and sequential steps of the four methods .",0
23166,"As shown in , both the CNN and LSTM have a large number of compositional parameters .",0
23167,"Since K m , h , the number of parameters in our models is much smaller than for the CNN and LSTM models .",0
23168,"For the computational complexity , our model is almost same order as the most simple SWEM model , and is smaller than the CNN or LSTM by a factor of mh / K or h/K.",0
23169,Experimental Results,0
23170,Setup We use 300 - dimensional Glo Ve word embeddings as initialization for word embeddings and label embeddings in our model .,1
23171,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",1
23172,The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task .,1
23173,"We train our model 's parameters with the Adam Optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 , and a minibatch size of 100 .",1
23174,"Dropout regularization is employed on the final MLP layer , with dropout rate 0.5 .",1
23175,The model is implemented using Tensorflow and is trained on GPU Titan X.,1
23176,The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM :,1
23177,"Summary statistics of five datasets , including the number of classes , number of training samples and number of testing samples .",0
23178,Classification on Benchmark Datasets,0
23179,We test our model on the same five standard benchmark datasets as in .,0
23180,"The summary statistics of the data are shown in , with content specified below :",0
23181,"AGNews : Topic classification over four categories of Internet news articles composed of titles plus description classified into : World , Entertainment , Sports and Business .",0
23182,Yelp Review Full :,0
23183,"The dataset is obtained from the Yelp Dataset Challenge in 2015 , the task is sentiment classification of polarity star labels ranging from 1 to 5 .",0
23184,Yelp Review Polarity :,0
23185,"The same set of text reviews from Yelp Dataset Challenge in 2015 , except that a coarser sentiment definition is considered : 1 and 2 are negative , and 4 and 5 as positive .",0
23186,DBPedia :,0
23187,Ontology classification over fourteen non-overlapping classes picked from DBpedia 2014 ( Wikipedia ) .,0
23188,Yahoo! Answers Topic : Topic classification over ten largest main categories from Yahoo !,0
23189,"Answers Comprehensive Questions and Answers version 1.0 , including question title , question content and best answer .",0
23190,"We compare with a variety of methods , including ( i ) the bag - of - words in ; ( ii ) sophisticated deep CNN / RNN models : large / small word CNN , LSTM reported in and deep CNN ( 29 layer ) ; ( iii ) simple compositional methods : fastText and simple word embedding models ( SWEM ) ; ( iv ) deep attention models : hierarchical attention network ( HAN ) ( Yang et al. ,",0
23191,Representational Ability,0
23192,Label embeddings are highly meaningful,0
23193,"To provide insight into the meaningfulness of the learned representations , in we visualize the correlation between label embeddings and document embeddings based on the Yahoo dateset .",0
23194,"First , we compute the averaged document embeddings per class : z k = 1 text manifold for class k.",0
23195,"Ideally , the perfect label embedding ck should be the representative anchor point for class k.",0
23196,"We compute the cosine similarity betweenz k and ck across all the classes , shown in ( a ) .",0
23197,"The rows are averaged per-class document embeddings , while columns are label embeddings .",0
23198,"Therefore , the on - diagonal elements measure how representative the learned label embeddings are to describe its own classes , while off - diagonal elements reflect how distinctive the label embeddings are to be separated from other classes .",0
23199,The high on - diagonal elements and low off - diagonal elements in indicate the superb ability of the label representations learned from LEAM .,0
23200,"Further , since both the document and label embeddings live in the same high - dimensional space , we use t- SNE ( Maaten and Hinton , 2008 ) to visualize them on a 2D map in .",0
23201,"Each color represents a different class , the point clouds are document embeddings , and the label embeddings are the large dots with black circles .",0
23202,"As can be seen , each label embedding falls into the inter-nal region of the respective manifold , which again demonstrate the strong representative power of label embeddings .",0
23203,Interpretability of attention,0
23204,Our attention score ?,0
23205,can be used to highlight the most informative words wrt the downstream prediction task .,0
23206,We visualize two examples in for the Yahoo dataset .,0
23207,The darker yellow means more important words .,0
23208,"The 1st text sequence is on the topic of "" Sports "" , and the 2nd text sequence is "" Entertainment "" .",0
23209,The attention score can correctly detect the key words with proper scores .,0
23210,Applications to Clinical Text,0
23211,"To demonstrate the practical value of label embeddings , we apply LEAM for a real healthcare scenario : medical code prediction on the Electronic Health Records dataset .",0
23212,"A given patient may have multiple diagnoses , and thus multi-label learning is required .",0
23213,Specifically 0.876 0.907 0.576 0.625 0.620 C- Mem NN 0.833 --- 0.42 Attentive LSTM - 0.900 - 0.532 - CAML 0.875 0.909 0.532 0.614 0.609 LEAM 0.881 0.912 0.540 0.619 0.612 : Quantitative results for doctor - notes multi-label classification task .,0
23214,contains text and structured records from a hospital intensive care unit .,0
23215,"Each record includes a variety of narrative notes describing a patients stay , including diagnoses and procedures .",0
23216,"They are accompanied by a set of metadata codes from the International Classification of Diseases ( ICD ) , which present a standardized way of indicating diagnoses / procedures .",0
23217,"To compare with previous work , we follow , and preprocess a dataset consisting of the most common 50 labels .",0
23218,"It results in 8,067 documents for training , 1,574 for validation , and 1,730 for testing .",0
23219,Results,0
23220,"We compare against the three baselines : a logistic regression model with bag - ofwords , a bidirectional gated recurrent unit ( Bi - GRU ) and a single - layer 1 D convolutional network .",1
23221,"We also compare with three recent methods for multi-label classification of clinical text , including Condensed Memory Networks ( C - MemNN ) , Attentive LSTM and Convolutional Attention ( CAML ) .",1
23222,"To quantify the prediction performance , we follow to consider the micro-averaged and macro-averaged F1 and area under the ROC curve ( AUC ) , as well as the precision at n ( P@n ) .",0
23223,"Micro - averaged values are calculated by treating each ( text , code ) pair as a separate prediction .",0
23224,Macro - averaged values are calculated by averaging metrics computed per-label .,0
23225,P@n is the fraction of then highestscored labels thatare present in the ground truth .,0
23226,The results are shown in .,0
23227,"LEAM provides the best AUC score , and better F1 and P@5 values than all methods except CNN .",1
23228,"CNN consistently outperforms the basic Bi - GRU architecture , and the logistic regression baseline performs worse than all deep learning architectures .",1
23229,We emphasize that the learned attention can be very useful to reduce a doctor 's reading burden .,0
23230,"As shown in , the health related words are highlighted .",0
23231,Conclusions,0
23232,"In this work , we first investigate label embeddings for text representations , and propose the label - embedding attentive models .",0
23233,"It embeds the words and labels in the same joint space , and measures the compatibility of word - label pairs to attend the document representations .",0
23234,The learning framework is tested on several large standard datasets and a real clinical text application .,0
23235,"Compared with the previous methods , our LEAM algorithm requires much lower computational cost , and achieves better if not comparable performance relative to the state - of - the - art .",0
23236,The learned attention is highly interpretable : highlighting the most informative words in the text sequence for the downstream classification task .,0
23237,title,0
23238,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,1
23239,abstract,0
23240,The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages .,0
23241,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",1
23242,"However , even if the language problem was resolved , models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures .",0
23243,"We consider the setting of semi-supervised cross - lingual understanding , where labeled data is available in a source language ( English ) , but only unlabeled data is available in the target language .",0
23244,We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .,1
23245,We show that addressing the domain gap is crucial .,0
23246,We improve over strong baselines and achieve a new state - of - the - art for cross - lingual document classification .,0
23247,*,0
23248,Work completed at Facebook AI 1 ar Xiv :1909.07009v2 [ cs.CL ],0
23249,INTRODUCTION,0
23250,Recent advances in Natural Language Processing have enabled us to train high - accuracy systems for many language tasks .,0
23251,"However , training an accurate system still requires a large amount of training data .",0
23252,It is inefficient to collect data for a new task and it is virtually impossible to annotate a separate data set for each language .,0
23253,"To go beyond English and a few popular languages , we need methods that can learn from data in one language and apply it to others .",0
23254,Cross - Lingual,0
23255,Understanding ( XLU ) has emerged as afield concerned with learning models on data in one language and applying it to others .,0
23256,"Much of the work in XLU focuses on the zero - shot setting , which assumes that labeled data is available in one source language ( usually English ) and not in any of the target languages in which the model is evaluated .",0
23257,The labeled data can be used to train a high quality model in the source language .,0
23258,One then relies on general domain parallel corpora and monolingual corpora to learn to ' transfer ' from the source language to the target language .,0
23259,Transfer methods can explicitly rely on machine translation models built from such parallel corpora .,0
23260,"Alternatively , one can use such corpora to learn language universal representations to produce features to train a model in one language , which one can directly apply to other languages .",0
23261,"Such representations can be in the form of cross - lingual word embeddings , contextual word embeddings , or sentence embeddings ; ; ) .",0
23262,"Using such techniques , recent work has demonstrated reasonable zero - shot performance for crosslingual document classification ) and natural language inference ) .",0
23263,"What we have so far described is a simplified view of XLU , which focuses solely on the problem of aligning languages .",0
23264,"This view assumes that , if we had access to a perfect translation system , and translated our source training data into the target language , the resulting model would perform as well as if we had collected a similarly sized labeled dataset directly in our target language .",0
23265,Existing work in XLU to date also works under this assumption .,0
23266,"However , in real world applications , we must also bridge the domain gap across different languages , as well as the language gap .",0
23267,"No task is ever identical in two languages , even if we group them under the same label , e.g. ' news document classification ' or ' product reviews ' .",0
23268,A Chinese customer might express sentiment differently than his American counterpart .,0
23269,Or French news might simply cover different topics than English news .,0
23270,"As a result , any approach which ignores this domain drift will fall short of native in - language performance in real world XLU .",0
23271,"In this paper , we propose to jointly tackle both language and domain transfer .",0
23272,"We consider the semi-supervised XLU setting , wherein addition to labeled data in a source language , we have access to unlabeled data in the target language .",0
23273,"Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",0
23274,"In particular , we focus on two approaches for domain adaptation .",1
23275,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,1
23276,Such methods have been shown to improve over general purpose pre-trained models such as BERT in the weakly supervised setting ; ) .,0
23277,"The second method is unsupervised data augmentation ( UDA ) ) , where synthetic paraphrases are generated from the unlabeled corpus , and the model is trained on a label consistency loss .",1
23278,"While both of these techniques were proposed previously , in both cases it is non-trivial to extend them to the cross - lingual setting .",0
23279,"For instance when performing data augmentation , one could generate paraphrases in either the source or the target language or both .",0
23280,We experiment with various approaches and provide guidelines with ablation studies .,0
23281,"Furthermore , we find that the value of additional labeled data in the source language is limited due to the train - test discrepancy of XLU tasks .",0
23282,We propose to alleviate this issue by using self - training technique to do the domain adaptation from the source language into the target language .,0
23283,"By combining these methods , we are able to reduce error rates by an average 44 % over a strong XLM baseline , setting a new state - of - the - art for cross - lingual document classification .",0
23284,RELATED WORK,0
23285,CROSS - LINGUAL UNDERSTANDING,0
23286,Cross - lingual document classification was first introduced in .,0
23287,"The subsequent work proposes the cross - lingual sentiment classification datasets , and have extended this to the news domain .",0
23288,"Cross - lingual understanding has also been applied to other NLP tasks , with datasets available in dependency parsing , natural language inference ( XNLI ) and question answering ) .",0
23289,Cross - lingual methods gained popularity with the advent of cross - lingual word embeddings ) .,0
23290,"Since then , many methods have been proposed to better align the word embedding spaces of different languages ( see for a survey ) .",0
23291,"Recently , more sophisticated extensions have been proposed based on seq2seq training of cross - lingual sentence embeddings ; ) and contextual word embeddings pre-trained on masked language modeling , notably multilingual BERT ) and the cross - lingual language model ( XLM ) of .",0
23292,"We use XLM as our baseline representation in all experiments , as it 's the current state - of - the - art on the commonly used XNLI benchmark for cross - lingual understanding .",0
23293,DOMAIN ADAPTATION,0
23294,"Domain adaptation , closely related to transfer learning , has a rich history in machine learning and natural language processing ( Pan & Yang ( 2009 ) ) .",0
23295,Such methods have long been applied to document classification tasks .,0
23296,Domain adaptation for NLP is intimately related to transfer learning and semi-supervised learning ) .,0
23297,Transfer learning has made tremendous advances recently due to the success of pre-training representations using language modeling as a source task ; ) .,0
23298,"While such representations trained on large amounts of general domain text have been shown to transfer well generally , performance still suffers when the target domain is sufficiently different than what the models were pre-trained on .",0
23299,"In such cases , it is known that further pre-training the language model on in - domain text is helpful ; ) .",0
23300,"It is natural to use unsupervised domain data for this task , when available ; ) .",0
23301,The study of weakly supervised learning in language processing is relatively new .,0
23302,"Most recently , has introduced an unsupervised data augmentation ( UDA ) technique to demonstrate improvements in the few - shot learning setting .",0
23303,"Here , we extend this technique to facilitate cross - lingual and cross-domain transfer .",0
23304,FRAMEWORK,0
23305,"In this section , we formally define the problem discussed in this paper and describe the proposed approach in detail .",0
23306,PROBLEM FORMULATION,0
23307,"In vanilla zero - shot cross - lingual document classification , it is assumed that we have available a labeled dataset in a source language ( English in our case ) , which we can denote by L src = { ( x , y) |x ? P src ( x ) } , where P src ( x ) is the prior distribution of task data in the source language .",0
23308,It is assumed that no data is available for the task in the target language .,0
23309,"General purpose parallel and monolingual resources are used to train a cross-lingual classifier on the labeled source data , which is then applied to the target language data at test time .",0
23310,"In this work , we also assume access to a large unlabeled corpus in the target language , U tgt = { x |x ? P tgt ( x ) } , which is usually the casein practical applications .",0
23311,We aim to utilize this domain - specific unlabeled data to learn the best classifier for the data from the target language .,0
23312,"We refer to this setting as semi-supervised XLU , although we 're still in the zero - shot setting , in that no labeled data is used in the target language .",0
23313,BASELINE APPROACHES,0
23314,"There are two standard ways to transfer knowledge across the languages in the vanilla zero - shot setting : ( 1 ) using a translation system to translate the labeled samples , such as translate - train and translate - test methods , and ( 2 ) learning a multilingual embedding system to obtain a language irrelevant representations of the data .",0
23315,"In this paper , we adopt the second approach as the basic model , and utilize the XLM model ) as our base model , which has been pre-trained by large - scale parallel and monolingual data from various languages .",0
23316,"Because XLM is a multilingual embedding system , a baseline is obtained by fine - tuning XLM with the labeled set L src ( x ) and directly applying the resulting model to the target language .",0
23317,"In the experiments section , we also discuss the combination of the XLM and the translation based approaches .",0
23318,SEMI - SUPERVISED XLU,0
23319,"As argued in Introduction , even with a perfect translation or multilingual embedding system , we still face the domain - mismatch problem .",0
23320,This mismatch may limit the generalization ability of the model during testing time .,0
23321,"To fully adapt the classifier to the target distribution , we explore the following approaches , each of which leverages unlabeled data in the target language in different ways .",0
23322,Masked Language,0
23323,Model pre-training BERT and its derivations ( such as XLM ) are trained on general domain corpora .,0
23324,It is standard practice to further pre-train to adapt to a particular domain when data is available .,0
23325,This technique can lead to improved performance for the target domain .,0
23326,We refer to this approach as the MLM pre-training .,0
23327,"However , in the cross - lingual setting , fine - tuning the XLM model in the target language can make the model degenerate in the source language , decreasing its ability to transfer across languages .",0
23328,"Therefore , in this case , we take care to use this method in combination with the translate - train method , which translates all labeled samples into the target language .",0
23329,Unsupervised Data Augmentation,0
23330,"The second approach is utilizing the state - of - the - art semisupervised learning technique , Unsupervised Data Augmentation ( UDA ) algorithm , to leverage the unlabeled data .",0
23331,"The objective function of UDA can be written as ,",0
23332,wherex is an augmented sample generated by a predefined augmentation functionx = q ( x ) .,0
23333,"The augmentation function can be a paraphrase generation model , or a noising heuristic .",0
23334,"Here , we use a machine translation system for this purpose .",0
23335,The UDA loss enforces the classifier to produce label consistent predictions for pairs of original and augmented samples .,0
23336,"In the cross - lingual setting , there are multiple ways of generating augmented samples using translation .",0
23337,One could translate samples from the target language into the source language and use this crosslingual pair as the augmented sample .,0
23338,"Alternatively , one could translate back into the target language and use only target - language augmented samples .",0
23339,We find that the latter works best .,0
23340,It is also possible to do data augmentation using source domain unlabeled data .,0
23341,The results of these comparisons are included in out detailed ablation study in the experiments section .,0
23342,"Alleviating the Train - Test Discrepancy of the UDA Method With the UDA algorithm , the classifier is able to learn some prior information on the target domain , however it still suffers from the train - test discrepancy .",0
23343,"During the testing phase , our goal is to maximize the classifier performance on the target language , which can be written as ,",0
23344,where y is the ground - truth label of the sample x .,0
23345,"Upon observing the training objective of the UDA method , Eq.",0
23346,"( 1 ) , one can see that the data x that feed to model in the training phrase is sampled from three domains : ( 1 ) the source domain P src ( x ) , ( 2 ) the target domain P tgt ( x ) and the augmented sample domain P aug ( x ) .",0
23347,"On the other hand , the testing phrase only processes data from the target domain P tgt ( x ) .",0
23348,"The source and target domain are mismatched , due to differences in language as argued earlier .",0
23349,"Furthermore , the augmented domain , although generated from the target domain , can also be mismatched , due to artifacts introduced by the translation system .",0
23350,"This can be especially problematic , since the UDA method needs diversity in the augmented samples to perform well , which trades off against their quality .",0
23351,We propose to apply the self - training technique to tackle this problem .,0
23352,"We first train a classifier based on the UDA algorithm and denote it as f * ( x ) , which is the teacher model used to score the unlabeled data U tgt from the target domain .",0
23353,"Then we fine - tune a new XLM model using the soft classification loss function with the pseudo - labeled data , which is written as ,",0
23354,"Follwing this process , we obtain a new classifier trained only based on the target domain , which does not suffer from the train - test mismatch problem .",0
23355,We show that this process provides better generalization ability compared to the teacher model .,0
23356,A process diagram of the final model is presented in figure 1 .,0
23357,EXPERIMENTS,0
23358,"In this section , we present a comprehensive study on two benchmark tasks , cross - lingual sentiment classification , and cross -lingual news classification .",0
23359,DATASETS,0
23360,Sentiment Classification,0
23361,"In this task , we test the proposed framework on a sentiment classification benchmark in three target languages , i.e. French , German and Chinese .",0
23362,"The French , German and English data come from the benchmark cross - lingual Amazon reviews dataset , which we denote as amazon - fr , amazon - de and amazon - en .",0
23363,"We merge training and testing samples from all product categories in one language , which leads to 6000 training samples .",0
23364,"However , for the purpose of facilitating fair comparison with previous work , we also provide results for specific categories .",0
23365,"In addition , we use 54K unlabeled samples from amazon - fr and 310K unlabeled samples from amazon - de .",0
23366,"For Chinese , we use the Chinese Amazon ( amazon - cn ) and Dianping datasets .",0
23367,Dianping is a business review website similar to Yelp .,0
23368,"The training data for amazon - cn is amazon - en , and for dianping it is the Yelp dataset .",0
23369,"In these two cases , the size of the training sample is 2000 .",0
23370,"For both amazon - cn and dianping datasets , we have 4M unlabeled examples .",0
23371,"Because the number of the unlabeled set is very large , we randomly sample 10 % for the UDA algorithm .",0
23372,News Classification,0
23373,We use the MLDoc dataset for this task .,0
23374,The MLdoc dataset is a subset of RCV2 multilingual news dataset .,0
23375,"It has 4 categories , i.e. Corporate / Industrial , Economics , Government / Social and Markets , and each category has 250 training samples .",0
23376,We use the rest of the news documents in RCV2 dataset as the unlabeled data .,0
23377,"The number of unlabeled samples for each language ranges from 5 K to 20K , which is relatively smaller compared to the sentiment classification task .",0
23378,"Because the XLM model is pre-trained on 15 languages , we ignore languages which are not supported by XLM in the above benchmark datasets .",0
23379,"The pre-processing scripts for the above datasets , augmented samples and experiment settings needed to reproduce results are released in the Github repo 1 .",0
23380,MASKED LANGUAGE MODEL PRE - TRAINING STRATEGY,0
23381,"As introduced in section 3.3 , we apply MLM pre-training on the unlabeled data corpus to obtain a domain - specific XLM , denoted as XLM ft in the following sections .",0
23382,The pre-training strategies for the two tasks are slightly different .,0
23383,"In the sentiment classification task , because the size of the unlabeled corpus in each target domain is large enough , we fine - tune an XLM with MLM loss for each target domain respectively .",0
23384,"In contrast , we do not have enough unlabeled data in each language in the MLDoc dataset , therefore we integrate unlabeled data from all languages as the training corpus .",0
23385,As a result the XLM ft still preserves its language universality in this task .,0
23386,MAIN RESULTS,0
23387,We compare the follwing models :,0
23388,Fine-tune ( Ft ) : Fine - tuning the pre-trained model with the source - domain training set .,1
23389,"In the case of XLM ft , the training set is translated into the target language .",0
23390,Fine - tune with UDA ( UDA ) :,1
23391,This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function ( Eq. ) .,1
23392,Self - training based on the UDA model ( UDA + Self ) :,1
23393,"We first train the Ft model and UDA model , and choose the better one as the teacher model .",1
23394,"The teacher model is used to train a new XLM student using only unlabeled data U tgt in the target domain , as described above .",1
23395,We report the results of applying these three methods on both the original XLM model and the XLM ft model .,0
23396,"In order to keep the notation simple , we use parenthesis after the method name to indicate which basic model was used , such as UDA ( XLM ft ) .",0
23397,The details about the implementation and hyper - parameter tuning are included in Appendix A.1 .,0
23398,The results for the cross - lingual sentiment classification task are summarized in table,0
23399,1 .,0
23400,"As our experiment setting on the cross -lingual amazon dataset is different from previous publications , in order to provide a fair comparison with previous works , we summarize the results of the standard category - wise setting in table",0
23401,3 .,0
23402,The results for cross - lingual news classification is included in table,0
23403,"2 . The last column "" Unlabeled "" in these tables indicates whether this method utilizes the unlabeled data .",0
23404,"For the monolingual baselines , the models are trained with labeled data from the target domain .",0
23405,The size of the labeled set is the same as the English training set used for cross - lingual experiments .,0
23406,We can summarize our findings as follows :,0
23407,"Looking at Ft ( XLM ) results , it is clear that without the help of unlabeled data from the target domain , there still exists a substantial gap between the model performance of the cross -lingual settings and the monolingual baselines , even when using state - of - the - art pre-trained cross -lingual representations .",1
23408,Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data .,1
23409,"In the sentiment classification task , where the unlabeled data size is larger , Ft ( XLM ft ) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method .",1
23410,"On the other hand , the MLM method is relatively more resource intensive and takes longer to converge ( see Appendix A.5 ) .",1
23411,"In contrast , in the MLdoc dataset , when the size of the unlabeled samples is limited , the UDA method is more helpful .",1
23412,The combination of both methods - as in the UDA ( XLM ft ) model - consistently outperforms either method alone .,0
23413,"In this case the additional improvement provided by the UDA algorithm is smaller , but still consistent .",0
23414,"In the sentiment classification task , we observe the self - training technique consistently improves over its teacher model .",1
23415,It offers best results in both XLM and XLM ft based classifiers .,1
23416,The results demonstrate that self - training process is able to alleviate the train - test distribution mismatch problem and provide better generalization ability .,0
23417,"In the MLdoc dataset , self - training also achieves the best results over all , however the gains are less clear .",1
23418,We hypothesize that this technique is not as useful without enough number of unlabeled samples .,0
23419,"Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .",1
23420,"Furthermore , our framework reaches new state - of - the - art results , improving over vanilla XLM baselines by 44 % on average .",1
23421,"Furthermore , we provide an additional baseline , which only uses English samples to perform semisupervised learning , whose details are in Appendix A.2 .",0
23422,The experment results show that it lags behind the ones using unlabeled data from the target domain .,0
23423,This observation also justifies the importance of information from the target domain in the XLU task .,0
23424,LABELED DATA IN THE SOURCE LANGUAGE HAS LIMITED VALUE,0
23425,"In this section , we provide evidence for the train - test domain discrepancy in the context of the UDA method , by showing that adding more labeled data in the source language does not improve target task accuracy after a certain point .",0
23426,plots the target model performance vs. the number of labeled training samples in the cross - lingual and monolingual settings respectively .,0
23427,The figures are based on the UDA ( XLM ) method with 6 runs in the Yelp - Dianping cross - lingual setting .,0
23428,The dot is the average accuracy and the filling area contains one standard derivation .,0
23429,"We observe that , in the cross - lingual setting , the model performance peaks at around 10 k training samples per category , and becomes worse with the larger training set .",0
23430,"In contrast , the performance of the model improves consistently with more labeled data in the monolingual setting .",0
23431,This suggests that more training data from the source domain could harm model generalization ability in the target domain with UDA approach in the cross - lingual setting .,0
23432,"In order to alleviate this issue , we : Error rates for the sentiment classification task byproduct category .",0
23433,The pre-XLM sota results are provided by .,0
23434,"propose to utilize the self - training technique , which abandons the data from the source domain and the augmentation domain , to maximize its performance in the target domain .",0
23435,ABLATION STUDY : AUGMENTATION STRATEGIES,0
23436,"Next , we explore different augmentation strategies and their influence on the final performance .",0
23437,"As stated in section 3.3 , the augmentation strategy used in the main experiment is that we first translate the samples into English and translate them back to its original language .",0
23438,"We refer to this strategy as augmenting "" from target domain to target domain "" and abbreviate it as t2t .",0
23439,We also explore two additional augmentation strategies :,0
23440,"( 1 ) First , we do not translate the samples back to the target language and directly use English samples as the augmented samples , denoted as t2s .",0
23441,"Naturally , the parallel samples in two languages have the same sentiment information and different input format which are suitable to be used as the augmentation sample pairs for the multilingual system such as XLM .",0
23442,The second approach is to leverage unlabeled data from other language domains .,0
23443,"Here , we attempt to use the English unlabeled data .",0
23444,We translate them into the target language as the augmented samples .,0
23445,This strategy is denoted as s 2t. :,0
23446,Error rates when using different augmentation strategies and their combinations .,0
23447,"Results for sentiment classification shown on the left , and news document classification on the right .",0
23448,summarizes the performance of the proposed augmentation strategies and their combinations with the UDA ( XLM ) method in the sentiment classification and the UDA ( XLM ft ) in the news classification settings .,0
23449,"From the results , we conclude that t2t is the best performing approach , as it 's the best matched to the target domain .",0
23450,"Leveraging the unlabeled data from other domains does not offer consistent improvement , however can provide additional value in isolated cases .",1
23451,"We include additional ablations regarding translation system in the appendix , including the application of translate - train method in our experiments ( section A.3 ) and effects of hyper - parameters ( section A.4 ) .",0
23452,CONCLUSION,0
23453,"In this paper , we tackled the domain mismatch challenge in cross - lingual document classification - an important , yet often overlooked problem in cross -lingual understanding .",0
23454,"We provided evidence for the existence and importance of this problem , even when utilizing strong pre-trained cross -lingual representations .",0
23455,"We proposed a framework combining cross - lingual transfer techniques with three domain adaptation methods ; unsupervised data augmentation , masked language model pre-training and self - training , which can leverage unlabeled data in the target language to moderate the domain gap .",0
23456,"Our results show that by removing the domain discrepancy , we can close the performance gap between crosslingual transfer and monolingual baselines almost completely for the document classification task .",0
23457,We are also able to improve the state - of - the - art in this area by a large margin .,0
23458,"While document classification is by no means the most challenging task for XLU , we believe the strong gains that we demonstrated will encourage the community to pay more attention to domain",0
23459,The experiments in this paper are based on the PyTorch and Pytext package .,0
23460,"We use the Adam ( Kingma & Ba , 2014 ) as the optimizer .",0
23461,"For all experiments , we grid search the learning rate in the set { 5 10 ?6 , 1 10 ?5 , 2 10 ?5 }.",0
23462,"When using UDA method , we also try the three different annealing strategies introduced in the UDA paper , and the ?",0
23463,in is always set as 1 .,0
23464,The batch size in the Ft and UDA + Self method is 128 .,0
23465,"In the UDA method , the batch size is 16 for the labeled data and 80 for the unlabeled data .",0
23466,"Due to the limitation of the GPU memory , in all experiments , we set the length of samples as 256 , and cut the input tokens exceeding this length .",0
23467,"Finally , we report the results with the best hyper - parameters .",0
23468,"As for the augmentation process , we sweep the temperature which controls the diversity of beam search in translation .",0
23469,"The best temperature for "" en - de , en - fr , en -es "" and "" en-ru "" are 1.0 and 0.6 , the sampling space is the whole vocabulary .",0
23470,"In the "" en-zh "" setting , the temperature is 1.0 and the sampling space is the top 100 tokens in the vocabulary .",0
23471,"We note that this uses the Facebook production translation models , and results could vary when other translation systems are applied .",0
23472,"For reproducibility , we will release the augmented datasets that we generated .",0
23473,A.2 ABLATION STUDY : THE BASELINE WITH ENGLISH UNLABELED DATA,0
23474,"Here , we provide a baseline only using English samples to perform semi-supervised learning .",0
23475,"More specifically , we first train the model with English unlabeled data and augmented samples , then tests it on different target domains .",0
23476,This approach is similar to the traditional translate - test method .,0
23477,"This method offers a baseline , which merely increases the size of data but without providing the target domain information .",0
23478,"During the test phrasing , we experiment with two input strategies .",0
23479,"One is using the original test samples , and another is translating the samples into English .",0
23480,"We report the results ) of the UDA ( XLM ) method with two input strategies and compare them with the main results , which uses the unlabeled data from the target domain .",0
23481,"First , we observe that the performance of using original and translated samples is similar .",0
23482,"Second , compared with Ft ( XLM ) baselines in section 4.3 , utilizing the unlabeled data from the English domain is slightly better than only training with labeled data , but it still lags behind the performance of using the unlabeled data from the target domain . :",0
23483,The first part is the baseline results using the English unlabeled data .,0
23484,"The second part is the results using the unlabeled data from the target domain , which are copied from the section 4.3 .",0
23485,A.3 ABLATION STUDY : TRANSLATE - TRAIN,0
23486,"As discussed earlier , fine - tuning XLM on the target language would depreciate the multilingual ability of the model .",0
23487,We apply the translate - train method to tackle this problem .,0
23488,"In order to understand the influence of this strategy when using the proposed framework , we perform an ablation study .",0
23489,We test 3 input strategies : ( 1 ) English : use the original English data as training data .,0
23490,"( 2 ) tr-train : use the translate - train strategy , which translate the training data into the target language .",0
23491,( 3 ) both : we combine the and as the training data .,0
23492,"We report the results of the UDA ( XLM ) method in the sentiment classification tasks and UDA ( XLM ft ) method in the news classification tasks in Given a translation system , we use the sample decoding strategy to translate the sample .",0
23493,The sample space is the entire vocabulary space .,0
23494,We tune the temperature of of the softmax distribution .,0
23495,"As discussed in , this controls the trade - off between quality and diversity .",0
23496,"When = 0 , the sampling reduces to the greedy search and produce the best quality samples .",0
23497,"When = 1 , the sampling produces diverse outputs but loses some semantic information .",0
23498,The table A.4 illustrates the influence of value to the final performance in the English - to - French and English - to - German settings .,0
23499,The results show that temperature has a significant influence on the final performance .,0
23500,"However , because the quality of translation systems for different language pairs are not the same , their best temperature also varies .",0
23501,"In the appendix A.1 , we include the best temperature values for the translation systems used in this paper . A.5 COMPUTATION TIME OF UDA AND MLM PRETRAINING",0
23502,"From the main results in section 4.3 , we can see that MLM pre-training can offer better improvements over the UDA method .",0
23503,"However , it is also more resource intensive , since MLM pre-training is a token level task with a large output space , which leads to more computationally intensive updates and also takes longer to converge .",0
23504,"In our experiments , we used NVIDIA V100 - 16G GPUs to train all models .",0
23505,"8 GPUs were used to train Ft and UDA methods , and 32 GPUs to perform MLM pretraining .",0
23506,"In the "" amazonen - > amazonfr "" setting , for example , the unlabeled set contains 50K unlabeled samples and 8 M tokens after BPE tokenization .",0
23507,The Ft method takes 3.2 GPU hours to converge .,0
23508,"The UDA method training takes 16.8 GPU hours , excluding the time it takes to generate augmented samples ( which we handle as part of data pre-processing ) .",0
23509,MLM pre-training takes upwards of 500 GPU hours to converge .,0
23510,This is another factor which should betaken into account .,0
23511,B MONOLINGUAL DOMAIN ADAPTATION,0
23512,"As further evidence that our method addresses the domain mismatch , we apply out framework to the monolingual cross - domain document classification problem .",0
23513,"We again focus on sentiment classification where data comes from two different domains , product reviews ( amazon - en , amazon - cn ) and business reviews ( Yelp and Dianping ) .",0
23514,"We train and test on the same language , only transferring across domains .",0
23515,"We consider the two domain - pairs , amazonen - yelp and amazoncn - dianping .",0
23516,The results are illustrated in table,0
23517,8 . Conclusions are similar to the cross - domain setting ( section 4.3 ) :,0
23518,"There exists a clear gap between the cross - domain and in - domain results of the Ft method , even when using strong pre-trained representations .",0
23519,"By leveraging the unlabeled data from the target domain , we can significantly boost the model performance .",0
23520,Best results are achieved with our combined approach and almost completely matches the indomain baselines .,0
23521,title,0
23522,Explicit Interaction Model towards Text Classification,1
23523,abstract,0
23524,Text classification is one of the fundamental tasks in natural language processing .,0
23525,"Recently , deep neural networks have achieved promising performance in the text classification task compared to shallow models .",0
23526,"Despite of the significance of deep models , they ignore the fine - grained ( matching signals between words and classes ) classification clues since their classifications mainly rely on the text - level representations .",0
23527,"To address this problem , we introduce the interaction mechanism to incorporate word - level matching signals into the text classification task .",0
23528,"In particular , we design a novel framework , EXplicit interAction Model ( dubbed as EXAM ) , equipped with the interaction mechanism .",0
23529,We justified the proposed approach on several benchmark datasets including both multilabel and multi-class text classification tasks .,0
23530,Extensive experimental results demonstrate the superiority of the proposed method .,0
23531,"As a byproduct , we have released the codes and parameter settings to facilitate other researches .",0
23532,Introduction,0
23533,"Text classification is one of the fundamental tasks in natural language processing , targeting at classifying apiece of text content into one or multiple categories .",0
23534,"According to the number of desired categories , text classification can be divided into two groups , namely , multi-label ( multiple categories ) and multi-class ( unique category ) .",0
23535,"For instance , classifying an article into different topics ( e.g. , machine learning or data mining ) falls into the former one since an article could be under several topics simultaneously .",0
23536,"By contrast , classifying a comment of a movie into its corresponding rating level lies into the multi -class group .",0
23537,"Both multi-label and multi-class text classifications have been widely applied in many fields like sentimental analysis , topic tagging , and document classification .",0
23538,Feature engineering dominates the performance of traditional shallow text classification methods for a very longtime .,0
23539,"Various rule - based and statistical features like bag - of - words and N - grams ) are designed to describe the text , and fed into the shallow machine learning models such as Linear Regression and Support Vector Machine to make the judgment .",0
23540,Traditional solutions suffer from two defects :,0
23541,"1 ) High labor intensity for the manually crafted features , and 2 ) data sparsity ( a N -grams could occur only several times in a given dataset ) .",0
23542,"Recently , owing to the ability of tackling the aforementioned problems , deep neural networks ; Liu , Qiu , and Huang 2016 ; Grave et al. ) have become the promising solutions for the text classification .",0
23543,"Deep neural networks typically learn a word - level representation for the input text , which is usually a matrix with each row / column as an embedding of a word in the text .",0
23544,"They then compress the word - level representation into a text - level representation ( vector ) with aggregation operations ( e.g. , pooling ) .",0
23545,"Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .",0
23546,"Note that these solutions are also called encoding - based methods , since they encode the textual content into a latent vector representation .",0
23547,"Although great success has been achieved , these deep neural network based solutions naturally ignore the finegrained classification clues ( i.e. , matching signals between words and classes ) , since their classifications are based on text - level representations .",0
23548,"As shown in , the classification ( i.e. , FC ) layer of these solutions matches the text - level representation with class representations via a dotproduct operation .",0
23549,"Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .",0
23550,"As such , the probability of the text belonging to a class is largely determined by their over all matching score regardless of word - level matching signals , which would provide explicit signals for classification ( e.g. , missile strongly indicates the topic of military ) .",0
23551,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .",1
23552,The key idea behind the interaction mechanism is to explicitly calculate the matching scores between the words and classes .,0
23553,"From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .",0
23554,"By taking the interaction matrix as a text representation , the later classification layer could incorporate fine - grained word level signals for the finer classification rather than simply making the text - level matching .",0
23555,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .",1
23556,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .",1
23557,The word - level encoder projects the textual contents into the word - level representations .,1
23558,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",1
23559,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .",1
23560,We justify our proposed EXAM model over both the multi-label and multi-class text classifications .,0
23561,"Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed method , surpassing the corresponding state - of - the - art methods remarkably .",0
23562,"In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .",0
23563,We justify the proposed EXAM model over both multilabel and multi-class text classifications .,0
23564,Extensive experimental results demonstrate the effectiveness of the proposed method .,0
23565,We release the implementation of our method ( including some baselines ) and the involved parameter settings to facilitate later researchers 1 .,0
23566,Preliminaries,0
23567,"In this section , we introduce two widely - used word - level encoders :",0
23568,"Gated Recurrent Units notations in this paper , we use bold capital letters ( e.g. , X ) and bold lowercase letters ( e.g. , x ) to denote matrices and vectors , respectively .",0
23569,"We employ non-bold letters ( e.g. , x ) to represent scalars , and Greek letters ( e.g. , ? ) as parameters .",0
23570,"X i, : is used to refer the i - th row of the matrix X , X :, j to represent the j - th column vector and X i , j to denote the element in the i - th row and j- th column .",0
23571,Gated Recurrent Units,0
23572,"Owing to the ability of capturing the sequential dependencies and being easily optimized ( i.e. , avoid the gradient vanishing and explosion problems ) , Gated Recurrent Units ( GRU ) becomes a widely used word - level encoder .",0
23573,"Typically , a GRU generates word - level representations in two phases :",0
23574,"1 ) mapping each word in the text into an embedding ( a real - valued vector ) , and 2 ) projecting the sequence of word embeddings into a sequence of hidden representations , which encodes the sequential dependencies .",0
23575,Word embedding .,0
23576,Word embedding is a general method to map a word from one hot vector to a low dimensional and real - valued vector .,0
23577,"With enough data , word embedding can capture high - level representations of words .",0
23578,Hidden representation .,0
23579,"Given an embedding feature sequence E = [ E 1 , : , E 2 , : , , E n, : ] , GRU will compute a vector H i , : at the i - th time - step for each E i , : , and H i , : is defined as :",0
23580,"where Mr and M z are trainable parameters in the GRU , and ?",0
23581,"and tanh are sigmoid and tanh activation functions , respectively .",0
23582,"The sequence of hidden representations H = [ H 1 , : , , H n, : ] is denoted as the word - level representation of the input text .",0
23583,Region Embedding,0
23584,"Although word embedding is a good representation for the word , it can only compute the feature vector for the single word .",0
23585,Qiao et al. proposed region embedding to learn and utilize task - specific distributed representations of Ngrams .,0
23586,"In the region embedding layer , the representation of a word has two parts , the embedding of the word itself and a weighting matrix to interact with the local context .",0
23587,"For the word w i , the first part e w i is learned by an embedding matrix E ?",0
23588,R kv and the second part,0
23589,"where v is the size of the vocabulary , 2 s + 1 the region size and k the embedding size .",0
23590,"And then , each column in K w i is used to interact with the context word in the corresponding relative position of w i to get the contextaware pt w i +t for each word w i +t in the region .",0
23591,Formally it is computed by the following function : where denotes element - wise multiply .,0
23592,"And the final representation r i , s of the middle word w i is computed as follows :",0
23593,Model Problem Formulation,0
23594,Multi - Class Classification .,0
23595,"In this task , we should categorize each text instance to precisely one of c classes .",0
23596,"Suppose that we have a data set D = {d i , l i } N , where d i denotes the text and the one - hot vector l i ?",0
23597,"R c represents the label ford i , our goal is to learn a neural network N to classify the text .",0
23598,Multi- Label Classification .,0
23599,"In this task , each text instance belongs to a set of c target labels .",0
23600,"Formally , suppose that we have a dataset D = {d i , l i } N i = 1 , where d i denotes the text and the multi -hot vector l i represents the label for the text d i .",0
23601,Our goal is to learn a neural network N to classify the text .,0
23602,Model Overview,0
23603,"Motivated by the limitation of encoding - based models for text classification , which is lacking the fine - grained classification clue , we propose a novel framework , named EXplicit interAction Model ( EXAM ) , leveraging the interaction mechanism to incorporate word - level matching signals .",0
23604,"As can be seen from , EXAM mainly contains three components :",0
23605,A word - level encoder to project the input text d i into a word - level representation H. An interaction layer to compute the interaction signals between the words and classes .,0
23606,An aggregation layer to aggregate the interaction signals for each class and make the final predictions .,0
23607,"Considering that word - level encoders are well investigated in previous studies ( as mentioned in the Section 2 ) , and the target of this work is to learn the fine - grained classification signals , we only elaborate the interaction layer and aggregation layer in the following subsections .",0
23608,Interaction Layer,0
23609,"Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference ( Wang and Jiang 2016 b ) and retrieve - based chatbot .",0
23610,"The key idea of interaction mechanism is to use the interaction features between the small units ( e.g. , words in the textual contents ) to infer fine - grained clues whether two contents are matching .",0
23611,"Inspired by the success of methods equipped with interaction mechanism over encodebased methods in matching the textual contents , we introduce the interaction mechanism into the task of matching textual contents with their classes ( i.e. , text classification ) .",0
23612,"Specifically , we devise an interaction layer which aims to compute the matching score between the word and class .",0
23613,"Different from conventional interaction layer , where the wordlevel representations of both source and target are extracted with encoders like GRU , here we first project classes into real - valued latent representations .",0
23614,"In other words , we employ a trainable representation matrix T ?",0
23615,"R ck to encode classes ( each row represents a class ) , where c denotes the amount of classes and k is the embedding size equals to that of words .",0
23616,"We then adopt dot product as the interaction function to estimate the matching score between the target word t and class s , of which the formulation is ,",0
23617,where H ?,0
23618,"R nk denotes word - level representation of the text , extracted by the encoder with n denoting the length of the text .",0
23619,"In this way , we can compute the interaction matrix I ?",0
23620,R cn by following :,0
23621,"Note that we reject more complex interaction functions like element - wise multiply ( Gong , Luo , and and cosine similarity ( Wang , Hamza , and Florian 2017 ) for the consideration of efficiency .",0
23622,Aggregation Layer,0
23623,"This layer is devised to aggregate the interaction features for each class s into a logits o s i , which denotes the matching score between class sand the input text d i .",0
23624,"The aggregation layer can be implemented in different ways such as CNN ( Gong , Luo , and and LSTM ( Wang , Hamza , and Florian 2017 ) .",0
23625,"However , to keep the simplicity and efficiency of EXAM , here we only use a MLP with two FC layers , where ReLU is employed as the activation function of the first layer .",0
23626,"Formally , the MLP aggregates the interaction features I s , : for class s , and compute it s associated logits as following :",0
23627,where W 1 and W 2 are trainable parameters and b is the bias in the first layer .,0
23628,"We then normalize the logits oi = [ o 1 i , , o c i ] into probabilities pi .",0
23629,"Note that we follow previous work and employ softmax and sigmoid for multi-class and multi-label classifications , respectively .",0
23630,Loss Function,0
23631,"Similar to previous studies , in the multi-class text classification , we use cross entorpy loss as our loss function :",0
23632,"Following previous researchers ( Grave et al. ) , we choose binary classification loss as our loss function for the multi-label one :",0
23633,Generalized Encoding - Based Model,0
23634,"In this section , we elaborate how the encoding - based model can be interpreted as a special case of our EXAM framework .",0
23635,"As FastText is the most popular model for text classification and has been investigated extensively in the literature , being able to recover it allows EXAM to mimic a large family of text classification models .",0
23636,FastText contains three layers :,0
23637,"1 ) an embedding layer to get the word - level representation H t , : for the word t , 2 ) an average pooling layer to get the text - level representation f ?",0
23638,"R 1 k , and 3 ) a FC layer to get the final logits p ?",0
23639,"R 1 c , where k denotes the embedding size and c means the number of classes .",0
23640,Note that we omit the subscript of the document ID for conciseness .,0
23641,"Formally , it computes the logits p s of s-th class as follows :",0
23642,where W ?,0
23643,R kc and b ?,0
23644,"R 1c are the trainable parameters in the last FC layer , and n denotes the length of the text .",0
23645,The Eqn. ( 9 ) has an equivalent form as following :,0
23646,"It is worth noting that H t , : W :,s is exactly the interaction feature between word t and class s .",0
23647,"Therefore , the FastText is a special case of EXAM with an average pooling as the aggregation layer .",0
23648,"In EXAM , we use a non-linear MLP to be the aggregation layer , and it will generalize FastText to a non-linear setting which might be more expressive than the original one .",0
23649,Experiments,1
23650,Multi - Class Classification,1
23651,Datasets,0
23652,"We used publicly available benchmark datasets from ( Zhang , Zhao , and LeCun 2015 ) to evaluate EXAM .",0
23653,"There are in total 6 text classification datasets , corresponding to sentiment analysis , news classification , question - answer and ontology extraction tasks , respectively .",0
23654,shows the descriptive statistics of datasets used in our experiments .,0
23655,Stanford tokenizer is used to tokenize the text and all words are converted to lowercase .,0
23656,"We used padding to handle the various lengths of the text , and different maximum lengths are set for each dataset , respectively .",0
23657,"If the length of the text is less than the corresponding predefined value , we padded it with zero ; otherwise we truncated the original text .",0
23658,"To guarantee a fair comparison , the same evaluation protocol of ( Zhang , Zhao , and LeCun 2015 ) is employed .",0
23659,We split 10 % samples from the training set as the validation set to perform early stop for our models .,0
23660,Hyperparameters,0
23661,"For the multi -class task , we chose region embedding as the Encoder in EXAM .",1
23662,The region size is 7 and embedding size is 128 .,1
23663,We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .,1
23664,"As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .",1
23665,Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .,1
23666,Baselines,1
23667,"To demonstrate the effectiveness of our proposed EXAM , we compared it with several state - of - the - art baselines .",0
23668,The baselines are mainly in three variants :,1
23669,1 ) models based on feature engineering ;,1
23670,"2 ) Char - based deep models , and 3 ) Word - based deep models .",1
23671,"The first category uses the feature from the text to conduct the classification , and we reported the results from BoW Overall Performance We compared our EXAM to several state - of - the - art baselines with respect to accuracy .",0
23672,All results are summarized in .,0
23673,Four points are observed as following :,0
23674,Models based on feature engineering get the worst results on all the five datasets compared to the other methods .,1
23675,The main reason is that the feature engineering can not take full advantage of the supervision from the training set and it also suffers from the data sparsity .,0
23676,Char - based models get the highest over all scores on the two Amazon datasets .,1
23677,"There are possibly two reasons , 1 ) compared to the word - based models , char - based models enrich the supervision from characters and the characters are combined to form N-grams , stems , words and phrase which are helpful in the sentimental classification .",0
23678,"2 ) The two Amazon datasets contain millions of training samples , perfectly fitting the deep residual architecture for the VDCNN .",0
23679,"For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .",0
23680,Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .,1
23681,"The main reason is that the three tasks like news classification conduct categorization mainly via key words , and the wordbased models are able to directly use the word embedding without combining the characters .",0
23682,"For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .",1
23683,"It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .",1
23684,"For the Yah.A. , EXAM improves the best performance by 1.1 % .",1
23685,"Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .",0
23686,Component - wise Evaluation,0
23687,We studied the variant of our model to further investigate the effectiveness of the interaction layer and aggregation layer .,0
23688,We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities .,0
23689,EXAM,0
23690,"Encoder does not consider the interaction features between the classes and words , so it will automatically be degenerated into the Encoding - Based model .",0
23691,"We reported the results of the two models on all the datasets at , and it is clear to see that EXAM Encoder is not a patch on the original EXAM , verifying the effectiveness of interaction mechanism .",0
23692,We also drew the convergence lines for EXAM and the EXAM Encoder for the datasets .,0
23693,"From the , where the red lines represent EXAM and the blue is EXAM Encoder , we observed that EXAM converges faster than EXAM Encoder with respect to all the datasets .",0
23694,"Therefore , the interaction brings not only performance improvement but also faster convergence .",0
23695,The possible reason is that a non-linear aggregation layer introduces more parameters to fit the interaction features compared to the average pooling layer as mentioned in Section 4 .,0
23696,Multi - Label Classification,1
23697,Datasets,0
23698,"We conducted experiments on two different multi-label text classification datasets , named KanShan - Cup dataset 2 ( a benchmark ) and Zhihu dataset 3 , respectively .",0
23699,KanShan - Cup dataset .,0
23700,"This dataset is released by a competition of tagging topics for questions ( multi- label classification ) posted in the largest Chinese community question answering platform , Zhihu .",0
23701,"The dataset contains 3,000,000 questions and 1,999 topics ( classes ) , where one question may belong to one to five topics .",0
23702,"For questions with more than 30 words , we kept the last 30 words , otherwise , we padded zeros .",0
23703,"We separated the dataset into training , validation , and testing with 2,800,000 , 20,000 , and 180,000 questions , respectively .",0
23704,Zhihu dataset .,0
23705,"Considering the user privacy and data security , KanShan - Cup does not provide the original texts of the questions and topics , but uses numbered codes and numbered segmented words to represent text messages .",0
23706,"Therefore , it is inconvenient for researchers to perform analyses like visualization and case study .",0
23707,"To solve this problem , we constructed a dataset named Zhihu dataset .",0
23708,"We chose the top 1,999 frequent topics from Zhihu and crawled all the questions relevant to these topics .",0
23709,"Finally , we acquired 3,300,000 questions , with less than 5 topics for each question .",0
23710,"We adopted 3,000,000 samples as the training set , 30,000 samples as validation and 300,000 samples as testing .",0
23711,Baselines,0
23712,We applied the following models as baselines to evaluate the effectiveness of EXAM .,0
23713,Hyperparameters,0
23714,We implemented the baseline models and EXAM by MXNet .,1
23715,"We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .",1
23716,"We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .",1
23717,The accumulated MLP has 60 hidden units .,1
23718,We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .,1
23719,The validation set is applied for early - stopping to avoid overfitting .,1
23720,All hyperparameters are chosen empirically .,0
23721,Metrics,0
23722,We used the following metrics to evaluate the performance of our model and baseline models .,0
23723,"Precision : Different from the traditional precision metric ( Precision@ 5 ) which is set as the fraction of the relevant topic tags among the five returned tags , we utilized weighted precision to encourage the relevant topic tags to be ranked higher in the returned list .",0
23724,"Formally , the Precision is computed as following , P recision = pos ? { 1 , 2 , 3 , 4 , 5 }",0
23725,P recision@pos log ( pos + 1 ) .:,0
23726,The visualization of interaction features of EXAM .,0
23727,"Recall@5 : Recall is the fraction of relevant topic tags that have been retrieved over the total amount of five relevant topic tags , high recall means that the model returns most of the relevant topic tags . F 1 : F 1 is the harmonic average of the precision and recall , we computed it as following ,",0
23728,"Performance Comparison gives the performance of our model and baselines over two different datasets with respect to Precision , Recall@5 and F 1 .",0
23729,We observed the following from the :,0
23730,Word - based models are better than char - based models in Kanshan - Cup dataset .,1
23731,That maybe because in Chinese the words can offer more supervisions than characters and the question tagging task needs more word supervision .,0
23732,"For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .",0
23733,Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .,1
23734,"Different from the traditional models which encode the whole text into a vector , in EXAM , the representations of classes firstly interact with words to get more fine - grained features as shown in .",0
23735,The results suggest that word - level interaction features are relatively more important than global text - level representations in this task .,0
23736,Interaction Visualization,0
23737,"To illustrate the effectiveness of explicit interaction , we visualized an interaction feature I of the question "" Second - hand TIDDA 1.6 T Mannual gear has gotten some problems , please everybody help me to solve it ? "" .",0
23738,This question has 5 topics :,0
23739,"Car ,",0
23740,"Second - hand Car , Motor Dom , Autocar Conversation and Autocar Service .",0
23741,EXAM only misclassified the last topic .,0
23742,"In , we observed that when classifying different topics , the interaction features are different .",0
23743,"The topics "" Car "" and "" Second - hand Car "" pay much attention to the words like "" Second - hand TIIDA "" and the other topic like "" Autocar Conversation "" focuses more on "" got some problems "" .",0
23744,The results clearly signify that the interaction feature between the word and class is well - learned and highly meaningful .,0
23745,Related Work,0
23746,Text Classification,0
23747,Existing researches on text classification can be categorized into two groups : feature - based and deep neural models .,0
23748,The former focuses on handcraft features and uses machine learning algorithms as the classifier .,0
23749,Bag - of - words ( Wallach 2006 ) is a very efficient way to conduct the feature engineering .,0
23750,SVM and Naive Bayes are constantly the classifier .,0
23751,"The latter , deep neural models , taking advantage of neural networks to accomplish the model learning from data , have become the promising solution for the text classification .",0
23752,"For instance , Iyyer et al.",0
23753,proposed Deep Averaging Networks ( DAN ) and Grave et al.,0
23754,"proposed the FastText , and both are simple but efficient .",0
23755,"To get the temporal features between the words in the text , some models like TextCNN and Char - CNN ( Zhang , Zhao , and LeCun 2015 ) exploit the convolutional neural network , and there are also some models based on Recurrent Neural Network ( RNN ) .",0
23756,"Recently , Johnson et al. investigated the residual architecture and built a model called VD - CNN and Qiao et al. proposed a new method of region embedding for the text classification .",0
23757,"However , as mentioned in the Introduction , all these methods are text - level models while EXAM conducts the matching at the word level .",0
23758,Interaction Mechanism Interaction Mechanism is widely used in Natural Language Sentence Matching ( NLSM ) .,0
23759,The key idea of interaction mechanism is to use the interaction features between the small units ( like words in sentence ) to make the matching .,0
23760,"Wang et al. proposed a "" matching - aggregation "" framework to perform the interaction in Natural Language Inference .",0
23761,"Following this work , Parikh et al. integrated the attention mechanism into this framework , called Decomposable Attention Model .",0
23762,Then Wang et al.,0
23763,discussed different interaction functions in Text Matching .,0
23764,Yu et al. adopted tree - LSTM to get different level units to perform the interaction .,0
23765,Gong et al. proposed a densely interactive inference network to use DenseNet to aggregate dense interaction features .,0
23766,Our work is different from them since they mainly apply this mechanism in text matching instead of the classification .,0
23767,Conclusion,0
23768,"In this work , we present a novel framework named EXAM which employs the interaction mechanism to explicitly compute the word - level interaction signals for the text classification .",0
23769,We apply the proposed EXAM on multi-class and multi-label text classifications .,0
23770,Experiments over several benchmark datasets verify the effectiveness of our proposed mechanism .,0
23771,"In the future , we plan to investigate the effect of different interaction functions in the interaction mechanism .",0
23772,"Besides , we are interested in extend EXAM by introducing more complex aggregation layers like ResNet or DenseNet .",0
23773,title,0
23774,Recurrent Neural Network Grammars,1
23775,abstract,0
23776,"This is modified version of a paper originally published at NAACL 2016 that contains a corrigendum at the end , with improved results after fixing an implementation bug in the RNNG composition function .",0
23777,"We introduce recurrent neural network grammars , probabilistic models of sentences with explicit phrase structure .",0
23778,We explain efficient inference procedures that allow application to both parsing and language modeling .,0
23779,Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state - of - the - art sequential RNNs in English and Chinese 1 .,0
23780,Introduction,0
23781,Sequential recurrent neural networks ( RNNs ) are remarkably effective models of natural language .,0
23782,"In the last few years , language model results that substantially improve over long - established state - of the - art baselines have been obtained using RNNs as well as in various conditional language modeling tasks such as machine translation , image caption generation , and dialogue generation .",0
23783,"Despite these impressive results , sequential models area priori inappropriate models of natural language , since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order .",0
23784,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",1
23785,"RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .",1
23786,The foundation of this work is a top - down variant of transition - based parsing ( 3 ) .,0
23787,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",1
23788,"While several transition - based neural models of syntactic generation exist , these have relied on structure building operations based on parsing actions in shift - reduce and leftcorner parsers which operate in a largely bottomup fashion .",0
23789,"While this construction is appealing because inference is relatively straightforward , it limits the use of top - down grammar information , which is helpful for generation .",0
23790,"2 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top - down ( i.e. , rootto - terminal ) syntactic information ( 4 ) .",0
23791,"The top - down transition set that RNNGs are based on lends itself to discriminative modeling as well , where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures .",0
23792,"Similar to previously published discriminative bottomup transition - based parsers , greedy prediction with our model yields a linear -",0
23793,"The left - corner parsers used by incorporate limited top - down information , but a complete path from the root of the tree to a terminal is not generally present when a terminal is generated .",0
23794,Refer to for an example .,0
23795,"time deterministic parser ( provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed ) ; however , our algorithm generates arbitrary tree structures directly , without the binarization required by shift - reduce parsers .",0
23796,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .",1
23797,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,1
23798,Experiments show that RNNGs are effective for both language modeling and parsing ( 6 ) .,0
23799,Our generative model obtains ( i ) the best - known parsing results using a single supervised generative model and ( ii ) better perplexities in language modeling than state - of - the - art sequential LSTM language models .,0
23800,Surprisingly - although inline with previous parsing results showing the effectiveness of generative models ) parsing with the generative model obtains significantly better results than parsing with the discriminative model .,0
23801,RNN Grammars,0
23802,"Formally , an RNNG is a triple ( N , ? , ? ) consisting of a finite set of nonterminal symbols ( N ) , a finite set of terminal symbols ( ? ) such that N ? ? = ? , and a collection of neural network parameters ?.",0
23803,It does not explicitly define rules since these are implicitly characterized by ?.,0
23804,"The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition - based algorithm , which is outlined in the next section .",0
23805,"In the section after that , the semantics of the parameters thatare used to turn this into a stochastic algorithm that generates pairs of trees and strings are discussed .",0
23806,Top - down Parsing and Generation,0
23807,RNNGs are based on a top - down generation algorithm that relies on a stack data structure of partially completed syntactic constituents .,0
23808,"To emphasize the similarity of our algorithm to more familiar bottom - up shift - reduce recognition algorithms , we first present the parsing ( rather than generation ) version of our algorithm ( 3.1 ) and then present modifications to turn it into a generator ( 3.2 ) .",0
23809,Parser Transitions,0
23810,The parsing algorithm transforms a sequence of words x into a parse tree y using two data structures ( a stack and an input buffer ) .,0
23811,"As with the bottomup algorithm of , our algorithm begins with the stack ( S ) empty and the complete sequence of words in the input buffer ( B ) .",0
23812,"The buffer contains unprocessed terminal symbols , and the stack contains terminal symbols , "" open "" nonterminal symbols , and completed constituents .",0
23813,"At each timestep , one of the following three classes of operations ( ) is selected by a classifier , based on the current contents on the stack and buffer :",0
23814,"NT ( X ) introduces an "" open nonterminal "" X onto the top of the stack .",0
23815,"Open nonterminals are written as a nonterminal symbol preceded by an open parenthesis , e.g. , "" ( VP "" , and they represent a nonterminal whose child nodes have not yet been fully constructed .",0
23816,"Open nonterminals are "" closed "" to form complete constituents by subsequent REDUCE operations .",0
23817,"SHIFT removes the terminal symbol x from the front of the input buffer , and pushes it onto the top of the stack .",0
23818,"REDUCE repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered , and then this open NT is popped and used as the label of a new constituent that has the popped subtrees as its children .",0
23819,This new completed constituent is pushed onto the stack as a single composite item .,0
23820,A single REDUCE operation can thus create constituents with an unbounded number of children .,0
23821,The parsing algorithm terminates when there is a single completed constituent on the stack and the buffer is empty .,0
23822,shows an example parse using our transition set .,0
23823,"Note that in this paper we do not model preterminal symbols ( i.e. , part - ofspeech tags ) and our examples therefore do not include them .",0
23824,3,0
23825,Our transition set is closely related to the operations used in Earley 's algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN .,0
23826,"It is likewise closely related to the "" linearized "" parse trees proposed by and to the top - down , left - to - right decompositions of trees used in previous generative parsing and language modeling work .",0
23827,"A further connection is to LL ( * ) parsing which uses an unbounded lookahead ( compactly represented by a DFA ) to distinguish between parse alternatives in a top - down parser ; however , our parser uses an RNN encoding of the lookahead rather than a DFA .",0
23828,Constraints on parser transitions .,0
23829,"To guarantee that only well - formed phrase - structure trees are produced by the parser , we impose the following constraints on the transitions that can be applied at each step which area function of the parser state ( B , S , n ) where n is the number of open nonterminals on the stack :",0
23830,The NT ( X ) operation can only be applied if B is not empty and n < 100 .,0
23831,4 The SHIFT operation can only be applied if B is not empty and n ?,0
23832,1 . The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol .,0
23833,The REDUCE operation can only be applied if n ?,0
23834,2 or if the buffer is empty .,0
23835,"To designate the set of valid parser transitions , we write AD ( B , S , n ) .",0
23836,quires no special handling .,0
23837,"However , leaving them out reduces the number of transitions by O ( n ) and also reduces the number of action types , both of which reduce the runtime .",0
23838,"Furthermore , standard parsing evaluation scores do not depend on preterminal prediction accuracy .",0
23839,"4 Since our parser allows unary nonterminal productions , there are an infinite number of valid trees for finite - length sentences .",0
23840,The n < 100 constraint prevents the classifier from misbehaving and generating excessively large numbers of nonterminals .,0
23841,Similar constraints have been proposed to deal with the analogous problem in bottom - up shift - reduce parsers .,0
23842,Generator Transitions,0
23843,The parsing algorithm that maps from sequences of words to parse trees can be adapted with minor changes to produce an algorithm that stochastically generates trees and terminal symbols .,0
23844,"Two changes are required : ( i ) there is no input buffer of unprocessed words , rather there is an output buffer ( T ) , and ( ii ) instead of a SHIFT operation there are operations which generate terminal symbol x ? ?",0
23845,and add it to the top of the stack and the output buffer .,0
23846,At each timestep an action is stochastically selected according to a conditional distribution that depends on the current contents of Sand T .,0
23847,The algorithm terminates when a single completed constituent remains on the stack .,0
23848,shows an example generation sequence .,0
23849,Constraints on generator transitions .,0
23850,The generation algorithm also requires slightly modified constraints .,0
23851,These are :,0
23852,The GEN ( x ) operation can only be applied if n ?,0
23853,1 . The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n ?,0
23854,1 .,0
23855,"To designate the set of valid generator transitions , we write A G ( T , S , n ) .",0
23856,"This transition set generates trees using nearly the same structure building actions and stack configurations as the "" top - down PDA "" construction proposed by , albeit without the restriction that the trees be in Chomsky normal form .",0
23857,Transition Sequences from Trees,0
23858,"Any parse tree can be converted to a sequence of transitions via a depth - first , left - to - right traversal of a parse tree .",0
23859,"Since there is a unique depth - first , leftro - right traversal of a tree , there is exactly one transition sequence of each tree .",0
23860,"For a tree y and a sequence of symbols x , we write a ( x , y) to indicate the corresponding sequence of generation transitions , and b ( x , y) to indicate the parser transitions .",0
23861,Runtime Analysis,0
23862,"A detailed analysis of the algorithmic properties of our top - down parser is beyond the scope of this paper ; however , we briefly state several facts .",0
23863,"As - is a terminal symbol , X is a nonterminal symbol , and each ?",0
23864,is a completed subtree .,0
23865,"The top of the stack is to the right , and the buffer is consumed from left to right .",0
23866,Elements on the stack and buffer are delimited by a vertical bar ( | ) .,0
23867,Input :,0
23868,The hungry cat meows .,0
23869,Stack,0
23870,"Buffer Action suming the availability of constant time push and pop operations , the runtime is linear in the number of the nodes in the parse tree that is generated by the parser / generator ( intuitively , this is true since although an individual REDUCE operation may require applying a number of pops that is linear in the number of input symbols , the total number of pop operations across an entire parse / generation run will also be linear ) .",0
23871,"Since there is noway to bound the number of output nodes in a parse tree as a function of the number of input words , stating the runtime complexity of the parsing algorithm as a function of the input size requires further assumptions .",0
23872,"Assuming our fixed constraint on maximum depth , it is linear .",0
23873,Comparison to Other Models,0
23874,Our generation algorithm algorithm differs from previous stack - based parsing / generation algorithms in two ways .,0
23875,"First , it constructs rooted tree structures top down ( rather than bottom up ) , and second , the transition operators are capable of directly generating arbitrary tree structures rather than , e.g. , assuming binarized trees , as is the casein much prior work that has used transition - based algorithms to produce phrase - structure trees .",0
23876,Generative Model,0
23877,RNNGs use the generator transition set just presented to define a joint distribution on syntax trees ( y ) and words ( x ) .,0
23878,"This distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step ( u t ) ; i.e. ,",0
23879,and where action - specific embeddings r a and bias vector bare parameters in ?.,0
23880,"The representation of the algorithm state at time t , u t , is computed by combining the representation of the generator 's three data structures : the output buffer ( T t ) , represented by an embedding o t , the stack ( S t ) , represented by an embedding st , and the history of actions ( a <t ) taken by the generator , represented by an embedding ht ,",0
23881,where W and care parameters .,0
23882,Refer to for an illustration of the architecture .,0
23883,"The output buffer , stack , and history are sequences that grow unboundedly , and to obtain representations of them we use recurrent neural networks to "" encode "" their contents .",0
23884,"Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet , it is straightforward to apply a standard RNN encoding architecture .",0
23885,The stack ( S ) is more complicated for two reasons .,0
23886,"First , the elements of the stack are more complicated objects than symbols from a discrete alphabet : open nonterminals , terminals , and full trees , are all present on the stack .",0
23887,"Second , it is manipulated using both push and pop operations .",0
23888,"To efficiently obtain representations of S under push and pop operations , we use stack LSTMs .",0
23889,"To represent complex parse trees , we define a new syntactic composition function that recursively defines representations of trees .",0
23890,Syntactic Composition Function,0
23891,"When a REDUCE operation is executed , the parser pops a sequence of completed subtrees and / or tokens ( together with their vector embeddings ) from the stack and makes them children of the most recent open nonterminal on the stack , "" completing "" the constituent .",0
23892,"To compute an embedding of this new subtree , we use a composition function based on bidirectional LSTMs , which is illustrated in . The first vector read by the LSTM in both the forward and reverse directions is an embedding of the label on the constituent being constructed ( in the figure , NP ) .",0
23893,This is followed by the embeddings of the child subtrees ( or tokens ) in forward or reverse order .,0
23894,"Intuitively , this order serves to "" notify "" each LSTM what sort of head it should be looking for as it processes the child node embeddings .",0
23895,"The final state of the forward and reverse LSTMs are concatenated , passed through an affine transformation and a tanh nonlinearity to become the subtree embedding .",0
23896,5,0
23897,"Because each of the child node embeddings ( u , v , w in ) is computed similarly ( if it corresponds to an internal node ) , this composition function is a kind of recursive neural network .",0
23898,Word Generation,0
23899,"To reduce the size of A G ( S , T , n ) , word generation is broken into two parts .",0
23900,"First , the decision to generate is made ( by predicting GEN as an action ) , and then choosing the word , conditional on the current parser state .",0
23901,"To further reduce the computational complexity of modeling the generation of a word , we use a class - factored softmax .",0
23902,"By using |? | classes for a vocabulary of size |?| , this prediction",0
23903,We found the many previously proposed syntactic composition functions inadequate for our purposes .,0
23904,"First , we must contend with an unbounded number of children , and many previously proposed functions are limited to binary branching nodes .",0
23905,"Second , those that could deal with n-ary nodes made poor use of nonterminal information , which is crucial for our task .",0
23906,step runs in time O ( |? | ) rather than the O ( |? | ) of the full - vocabulary softmax .,0
23907,"To obtain clusters , we use the greedy agglomerative clustering algorithm of .",0
23908,Training,0
23909,The parameters in the model are learned to maximize the likelihood of a corpus of trees .,0
23910,Discriminative Parsing Model,0
23911,A discriminative parsing model can be obtained by replacing the embedding of T tat each time step with an embedding of the input buffer B t .,0
23912,"To train this model , the conditional likelihood of each sequence of actions given the input string is maximized .",0
23913,6,0
23914,Inference via,0
23915,Importance Sampling,0
23916,"Our generative model p ( x , y) defines a joint distribution on trees ( y ) and sequences of words ( x ) .",0
23917,"To evaluate this as a language model , it is necessary to compute the marginal probability p ( x ) = y ? Y ( x ) p ( x , y ) .",0
23918,"And , to evaluate the model as a parser , we need to be able to find the MAP parse tree , i.e. , the tree y ?",0
23919,"Y (x ) that maximizes p ( x , y ) .",0
23920,"However , because of the unbounded dependencies across the sequence of parsing actions in our model , exactly solving either of these inference problems is intractable .",0
23921,"To obtain estimates of these , we use a variant of importance sampling .",0
23922,"Our importance sampling algorithm uses a conditional proposal distribution q ( y | x ) with the following properties : ( i ) p ( x , y ) > 0 =? q (y | x ) > 0 ; ( ii ) samples y ? q (y | x ) can be obtained efficiently ; and ( iii ) the conditional probabilities q ( y | x ) of these samples are known .",0
23923,"While many such distributions are available , the discriminatively trained variant of our parser ( 4.4 ) fulfills these requirements : sequences of actions can be sampled using a simple ancestral sampling approach , and , since parse trees and action sequences exist in a one - to - one relationship , the product of the action probabilities is the conditional probability of the parse tree under q .",0
23924,We therefore use our discriminative parser as our proposal distribution .,0
23925,"Importance sampling uses importance weights , which we define as w ( x , y ) = p ( x , y ) / q ( y | x ) , to compute this estimate .",0
23926,"Under this definition , we can derive the estimator as follows :",0
23927,"We now replace this expectation with its Monte Carlo estimate as follows , using N samples from q:",0
23928,"To obtain an estimate of the MAP tree ? , we choose the sampled tree with the highest probability under the joint model p ( x , y ) .",0
23929,Experiments,0
23930,We present results of our two models both on parsing ( discriminative and generative ) and as a language model ( generative only ) in English and Chinese .,0
23931,Model and training parameters .,0
23932,"For the discriminative model , we used hidden dimensions of 128 and 2 - layer LSTMs ( larger numbers of dimensions reduced validation set performance ) .",1
23933,"For the generative model , we used 256 dimensions and 2 layer LSTMs .",1
23934,"For both models , we tuned the dropout rate to maximize validation set likelihood , obtaining optimal rates of 0.2 ( discriminative ) and 0.3 ( generative ) .",1
23935,"For the sequential LSTM baseline for the language model , we also found an optimal dropout rate of 0.3 .",1
23936,For training we used stochastic gradient descent with a learning rate of 0.1 .,1
23937,All parameters were initialized according to recommendations given by .,0
23938,English parsing results .,0
23939,Chinese parsing results .,0
23940,Chinese parsing results were obtained with the same methodology as in English and show the same pattern .,0
23941,Language model results .,0
23942,"We report held - out perword perplexities of three language models , both sequential and syntactic .",0
23943,Log probabilities are normalized by the number of words ( excluding the stop,0
23944,"The value ? = 0.8 was chosen based on the diversity of the samples generated on the development set . symbol ) , inverted , and exponentiated to yield the perplexity .",0
23945,Results are summarized in .,0
23946,Discussion,0
23947,It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model .,0
23948,"This is the result of ( i ) relaxing conventional independence assumptions ( e.g. , context - freeness ) and ( ii ) inferring continuous representations of symbols alongside non-linear models of their syntactic relationships .",0
23949,The most significant question that remains is why the discriminative model - which has more information available to it than the generative model - performs worse than the generative model .,0
23950,"This pattern has been observed before in neural parsing by , who hypothesized that larger , unstructured conditioning contexts are harder to learn from , and provide opportunities to overfit .",0
23951,"Our discriminative model conditions on the entire history , stack , and buffer , while our generative model only accesses the history and stack .",0
23952,The fully discriminative model of was able to obtain results similar to those of our generative model ( albeit using much larger training sets obtained through semisupervision ) but similar results to those of our discriminative parser using the same data .,0
23953,"In light of their results , we believe Henderson 's hypothesis is correct , and that generative models should be considered as a more statistically efficient method for learning neural networks from small data .",0
23954,Related Work,0
23955,Our language model combines work from two modeling traditions : ( i ) recurrent neural network language models and ( ii ) syntactic language modeling .,0
23956,Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left - to - right language model .,0
23957,Syntactic language models jointly generate a syntactic structure and a sequence of words .,0
23958,"There is an extensive literature here , but one strand of work has emphasized a bottom - up generation of the tree , using variants of shift - reduce parser actions to define the probability space .",0
23959,The neural - network - based model of is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize generative parsing based on a left - corner model .,0
23960,Dependency - only language models have also been explored .,0
23961,Modeling generation top - down as a rooted branching process that recursively rewrites nonterminals has been explored by and .,0
23962,"Of particular note is the work of Charniak , which uses random forests and hand - engineered features over the entire syntactic derivation history to make decisions over the next action to take .",0
23963,The neural networks we use to model sentences are structured according to the syntax of the sentence being generated .,0
23964,"Syntactically structured neural architectures have been explored in a number of applications , including discriminative parsing , sentiment analysis , and sentence representation .",0
23965,"However , these models have been , without exception , discriminative ; this is the first work to use syntactically structured neural models to generate language .",0
23966,Earlier work has demonstrated that sequential RNNs have the capacity to recognize contextfree ( and beyond ) languages .,0
23967,"In contrast , our work maybe understood as away of incorporating a context - free inductive bias into the model structure .",0
23968,Outlook,0
23969,"RNNGs can be combined with a particle filter inference scheme ( rather than the importance sampling method based on a discriminative parser , 5 ) to produce a left - to - right marginalization algorithm that runs in expected linear time .",0
23970,"Thus , they could be used in applications that require language models .",0
23971,A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input .,0
23972,Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time .,0
23973,"A third consideration regarding how RNNGs , human sentence processing takes place in a left - toright , incremental order .",0
23974,"While an RNNG is not a processing model ( it is a grammar ) , the fact that it is left - to - right opens up several possibilities for developing new sentence processing models based on an explicit grammars , similar to the processing model of .",0
23975,"Finally , although we considered only the supervised learning scenario , RNNGs are joint models that could be trained without trees , for example , using expectation maximization .",0
23976,Conclusion,0
23977,"We introduced recurrent neural network grammars , a probabilistic model of phrase - structure trees that can be trained generatively and used as a language model or a parser , and a corresponding discriminative model that can be used as a parser .",0
23978,"Apart from out - of - vocabulary preprocessing , the approach requires no feature design or transformations to treebank data .",0
23979,"The generative model outperforms every previously published parser built on a single supervised generative model in English , and a bit behind the best - reported generative model in Chinese .",0
23980,"As language models , RNNGs outperform the best single - sentence language models .",0
23981,Corrigendum to Recurrent Neural Network Grammars Abstract,0
23982,"Due to an implentation bug in the RNNG 's recursive composition function , the results reported in did not correspond to the model as it was presented .",0
23983,This corrigendum describes the buggy implementation and reports results with a corrected implementation .,0
23984,"After correction , on the PTB 23 and CTB 5.1 test sets , respectively , the generative model achieves language modeling perplexities of 105.2 and 148.5 , and phrase - structure parsing F1 of 93.3 and 86.9 , a new state of the art in phrase - structure parsing for both languages .",0
23985,RNNG Composition Function and Implementation Error,0
23986,The composition function reduces a completed constituent into a single vector representation using a bidirectional LSTM ) over embeddings of the constituent 's children as well as an embedding of the resulting nonterminal symbol type .,0
23987,"The implementation error ) composed the constituent ( NP the hungry cat ) by reading the sequence "" NP the hungry NP "" , that is , it discarded the rightmost child of every constituent and replaced it with a second copy of the constituent 's nonterminal symbol .",0
23988,This error occurs for every constituent and means crucial information is not properly propagated upwards in the tree .,0
23989,Results after Correction,0
23990,The implementation error affected both the generative and discriminative RNNGs .,0
23991,"We summarize corrected English phrase - structure PTB 23 parsing result in , Chinese ( CTB 5.1 271-300 ) in ( achieving the the best reported result on both datasets ) , and English and Chinese language modeling perplexities in .",0
23992,"The considerable improvement in parsing accuracy indicates that 11 The discriminative model can only be used for parsing and not for language modeling , since it only models p ( y | x ) .",0
23993,properly composing the constituent and propagating information upwards is crucial .,0
23994,"Despite slightly higher language modeling perplexity on PTB 23 , the fixed RNNG still outperforms a highly optimized sequential LSTM baseline . :",0
23995,PTB and CTB language modeling results including results with the buggy composition function implementation ( indicated by ) and with the correct implementation .,0
23996,title,0
23997,Constituency Parsing with a Self - Attentive Encoder,1
23998,abstract,0
23999,We demonstrate that replacing an LSTM encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .,0
24000,"The use of attention makes explicit the manner in which information is propagated between different locations in the sentence , which we use to both analyze our model and propose potential improvements .",0
24001,"For example , we find that separating positional and content information in the encoder can lead to improved parsing accuracy .",0
24002,"Additionally , we evaluate different approaches for lexical representation .",0
24003,"Our parser achieves new state - of the - art results for single models trained on the Penn Treebank : 93.55 F1 without the use of any external data , and 95.13 F1 when using pre-trained word representations .",0
24004,Our parser also outperforms the previous best - published accuracy figures on 8 of the 9 languages in the SPMRL dataset .,0
24005,Introduction,0
24006,"In recent years , neural network approaches have led to improvements in constituency parsing .",0
24007,"Many of these parsers can broadly be characterized as following an encoder - decoder design : an encoder reads the input sentence and summarizes it into a vector or set of vectors ( e.g. one for each word or span in the sentence ) , and then a decoder uses these vector summaries to incrementally buildup a labeled parse tree .",0
24008,"In contrast to the large variety of decoder architectures investigated in recent work , the encoders in recent parsers have predominantly been built using recurrent neural networks ( RNNs ) , and in particular Long Short - Term Memory networks ( LSTMs ) .",0
24009,RNNs have largely replaced approaches such as the fixed - window - size feed - forward networks of in part due to their ability to capture global context .,0
24010,"However , RNNs are not the only architecture capable of summarizing large global contexts : recent work by presented a new state - of - the - art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism .",0
24011,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .",1
24012,"In Section 2 of this paper , we describe the architecture and present our finding that self - attention can outperform an LSTM - based approach .",0
24013,"A neural attention mechanism makes explicit the manner in which information is transferred between different locations in the sentence , which we can use to study the relative importance of different kinds of context to the parsing task .",0
24014,"Different locations in the sentence can attend to each other based on their positions , but also based on their contents ( i.e. based on the words at or around those positions ) .",0
24015,"In Section 3 we present our find - ing that when our parser learns to make an implicit trade - off between these two types of attention , it predominantly makes use of position - based attention , and show that explicitly factoring the two types of attention can noticeably improve parsing accuracy .",0
24016,"In Section 4 , we study our model 's use of attention and reaffirm the conventional wisdom that sentence - wide global context is important for parsing decisions .",0
24017,"Like in most neural parsers , we find morphological ( or at least sub - word ) features to be important to achieving good results , particularly on unseen words or inflections .",0
24018,"In Section 5.1 , we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes / suffixes can outperform using part - of - speech tags from an external system .",0
24019,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .",1
24020,"In Section 5.2 , we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus .",0
24021,We find that using the deep contextualized representations proposed by can boost parsing accuracy .,0
24022,"Our parser achieves 93.55 F1 on the Penn Treebank WSJ test set when not using external word representations , outperforming all previous singlesystem constituency parsers trained only on the WSJ training set .",0
24023,"The addition of pre-trained word representations following increases parsing accuracy to 95.13 F1 , a new stateof - the - art for this dataset .",0
24024,Our model also outperforms previous best published results on 8 of the 9 languages in the SPMRL 2013 / 2014 shared tasks .,0
24025,Code and trained English models are publicly available .,0
24026,1,0
24027,Base Model,0
24028,"Our parser follows an encoder - decoder architecture , as shown in .",0
24029,"The decoder , described in Section 2.1 , is borrowed from the chart parser of with additional modifications from .",0
24030,"Their parser is architecturally streamlined yet achieves the highest performance among discriminative single - system parsers trained on WSJ data only , which is why we selected it as the starting point for our experiments with encoder variations .",0
24031,"Sections 2.2 and 2.3 de-scribe the base version of our encoder , where the self - attentive architecture described in Section 2.2 is adapted from .",0
24032,Tree Scores and Chart Decoder,0
24033,"Our parser assigns a real - valued score s ( T ) to each tree T , which decomposes as",0
24034,"Here s ( i , j , l ) is a real - valued score for a constituent that is located between fencepost positions i and j in a sentence and has label l .",0
24035,"To handle unary chains , the set of labels includes a collapsed entry for each unary chain in the training set .",0
24036,The model handles n-ary trees by binarizing them and introducing a dummy label ?,0
24037,"to nodes created during binarization , with the property that ? i , j : s ( i , j , ? ) = 0 . Enforcing that scores associated with the dummy labels are always zero ensures that Here ?",0
24038,"is the Hamming loss on labeled spans , and the tree corresponding to the most -violated constraint can be found using a slight modification of the inference algorithm used at test time .",0
24039,"For further details , see .",0
24040,"The remainder of this paper concerns itself with the functional form of s ( i , j , l ) , which is calculated using a neural network for all l = ?.",0
24041,Context - Aware Word Representations,0
24042,"The encoder portion of our model is split into two parts : a word - based portion that assigns a contextaware vector representation y t to each position tin the sentence ( described in this section ) , and a chart portion that combines the vectors y t to generate span scores s ( i , j , l) ( Section 2.3 ) .",0
24043,The architecture for generating the vectors y t is adapted from . :,0
24044,"An overview of our encoder , which produces a context - aware summary vector for each word in the sentence .",0
24045,The multi-headed attention mechanism is the only means by which information may propagate between different positions in the sentence .,0
24046,Multi - Head Attention,0
24047,"The encoder takes as input a sequence of word embeddings [ w 1 , w 2 , . . . , w T ] , where the first and last embeddings are of special start and stop tokens .",0
24048,All word embeddings are learned jointly with other parts of the model .,0
24049,"To better generalize to words thatare not seen during training , the encoder also receives a sequence of part - of - speech tag embeddings [ m 1 , m 2 , . . . , m T ] based on the output of an external tagger ( alternative lexical representations are discussed in Section 5 ) .",0
24050,"Additionally , the encoder stores a learned table of position embeddings , where every number i ? 1 , 2 , . . . ( up to some maximum sentence length ) is associated with a vector pi .",0
24051,"All embeddings have the same dimensionality , which we call d model , and are added together at the input of the encoder :",0
24052,"The vectors [ z 1 , z 2 , . . . , z T ] are transformed by a stack of 8 identical layers , as shown in .",0
24053,Each layer consists of two stacked sublayers : a multi-headed attention mechanism and a positionwise feed - forward sublayer .,0
24054,"The output of each sublayer given an input x is LayerNorm ( x + SubLayer ( x ) ) ,",0
24055,i.e. each sublayer is followed by a residual connection and a Layer Normalization step .,0
24056,"As a result , all sublayer outputs , including final outputs y t , are of size d model .",0
24057,Self - Attention,0
24058,"The first sublayer in each of our 8 layers is a multi-headed self - attention mechanism , which is the only means by which information may propagate between positions in the sentence .",0
24059,"The input An input x t is split into three vectors that participate in the attention mechanism : a query qt , a key kt , and a value v t .",0
24060,"The query qt is compared with all keys to form a probability distribution p ( t ? ) , which is then used to retrieve an average valuev t .",0
24061,"to the attention mechanism is a T d model matrix X , where each row vector x t corresponds to word tin the sentence .",0
24062,"We first consider a single attention head , as illustrated in .",0
24063,"Learned parameter matrices W Q , W K , and WV are used to map an input x t to three vectors : a query qt = W Q x t , a key kt = W K x t , and a value v t = WV x t .",0
24064,"Query and key vectors have the same number of dimensions , which we call d k .",0
24065,The probability that word i attends to word j is then calculated as p ( i ? j ) ? exp (,0
24066,.,0
24067,"The values v j for all words that have been attended to are aggregated to form an average valuev i = j p ( i ? j ) v j , which is projected back to size d model using a learned matrix W O .",0
24068,"In matrix form , the behavior of a single attention head is :",0
24069,"Rather than using a single head , our model sums together the outputs from multiple heads :",0
24070,Each of the 8 heads has its own trainable parameters W,0
24071,O .,0
24072,This allows a word to gather information from up to 8 remote locations in the sentence at each attention sublayer .,0
24073,Position - Wise Feed - Forward Sublayer,0
24074,We use the same form as :,0
24075,"Here relu denotes the Rectified Linear Unit nonlinearity , and distinct sets of learned parameters are used at each of the 8 instances of the feedforward sublayer in our model .",0
24076,"The input and output dimensions are the same because of the use of residual connections throughout the model , but we can vary the number of parameters by adjusting the size of the intermediate vector that the nonlinearity is applied to .",0
24077,Span Scores,0
24078,"The outputs y t from the word - based encoder portion described in the previous section are combined to form span scores s ( i , j , ) following the method of .",0
24079,"Concretely ,",0
24080,] combines summary vectors for relevant positions in the sentence .,0
24081,"A span endpoint to the right of the word potentially requires different information from the endpoint to the left , so a word at a position k is associated with two annotation vectors ( y k by splitting in half 2 the outputs y k from Section 2.2 .",0
24082,We also introduce a Layer Normalization step to match the use of Layer Normalization throughout our model .,0
24083,Results,0
24084,The model presented above achieves a score of 92.67 F1 on the Penn Treebank WSJ development set .,0
24085,Details regarding hyperparameter choice and optimizer settings are presented in Appendix A.,0
24086,"For comparison , a model that uses the same decode procedure with an LSTM - based encoder achieves a development set score of 92.24 .",0
24087,"These results demonstrate that an RNN - based encoder is not required for building a 2 To avoid an adverse interaction with material described in Section 3 , when a vector y k is split in half the even coordinates contribute to ?",0
24088,y k and the odd coordinates contribute to ?,0
24089,y k .,0
24090,"good parser ; in fact , self - attention can achieve better results .",0
24091,Content vs. Position Attention,0
24092,"To examine the relative utilization of contentbased vs. position - based attention in our architecture , we perturb a trained model at test - time by selectively zeroing out the contribution of either the content or the position component to any attention mechanism .",0
24093,This can be done independently at different layers ; the results of this experiment are shown in .,0
24094,"We can see that our model learns to use a combination of the two attention types , with positionbased attention being the most important .",0
24095,"We also see that content - based attention is more useful at later layers in the network , which is consistent with the idea that the initial layers of our model act similarly to a dilated convolutional network while the upper layers have a greater balance between the two attention types .",0
24096,Attention,0
24097,Content,0
24098,Position F1,0
24099,Analysis of our Model,0
24100,"The defining feature of our encoder is the use of self - attention , which is the only mechanism for transfer of information between different locations throughout a sentence .",0
24101,The attention is further factored into types : content - based attention and position - based attention .,0
24102,"In this section , we analyze the manner in which our model uses this attention mechanism to make its predictions .",0
24103,Windowed Attention,0
24104,We can also examine our model 's use of longdistance context information by applying window - :,0
24105,Development - set F1 scores when attention is constrained to not exceed a particular distance in the sentence at test time only .,0
24106,"In the relaxed setting , the first and last two tokens of the sentence can attend to any word and be attended to by any word , to allow for sentence - wide pooling of information .",0
24107,ing to the attention mechanism .,0
24108,We begin by taking our trained model and windowing the attention mechanism at test - time only .,0
24109,"As shown in Table 2 , strict windowing yields poor results : even a window of size 40 causes a loss in parsing accuracy compared to the original model .",0
24110,"When we began to investigate how the model makes use of long - distance attention , we immediately found that there are particular attention heads at some layers in our model that almost always attend to the start token .",0
24111,"This suggests that the start token is being used as the location for some sentence - wide pooling / processing , or perhaps as a dummy target location when ahead fails to find the particular phenomenon that it 's learned to search for .",0
24112,"In light of this observation , we introduce a relaxed variation on the windowing scheme , where the start token , first word , last word , and stop token can participate in all possible uses of attention , but pairs of other words in the sentence can only attend to each other if they are within a given window .",0
24113,We include three other positions in addition to the start token to do our best to cover possible locations for global pooling by our model .,0
24114,Results for relaxed windowing at test - time only are also shown in .,0
24115,"Even when we allow global processing to take place at designated locations such as the start token , our model is able to make use of long - distance dependencies at up to length 40 .",0
24116,"Next , we examine whether the parser 's use of long - distance dependencies is actually essential to performing the task by retraining our model subject to windowing .",0
24117,To evaluate the role of global : Development - set F1 scores when attention is constrained to not exceed a particular distance in the sentence during training and at test time .,0
24118,"In the relaxed setting , the first and last two tokens of the sentence can attend to any word and be attended to by any word , to allow for sentencewide pooling of information .",0
24119,"computation , we consider both strict and relaxed windowing .",0
24120,"In principle we could have replaced relaxed windowing at training time with explicit provisions for global computation , but for analysis purposes we choose to minimize departures from our original architecture .",0
24121,"The results , shown in , demonstrate that long - distance dependencies continue to be essential for achieving maximum parsing accuracy using our model .",0
24122,"Note that when a window of size 10 was imposed at training time , this was per-layer and the series of 8 layers actually had an effective context size of around 80 - which was still insufficient to recover the performance of our full parser ( with either approach to windowing ) .",0
24123,"The sideby - side comparison of strict and relaxed windowing shows that the ability to pool global information , using the designated locations thatare always available in the relaxed scheme , consistently translates to accuracy gains but is insufficient to compensate for small window sizes .",0
24124,"This suggests that not only must the information signal from longdistance tokens be available in principle , but that it also helps to have this information be directly accessible without an intermediate bottleneck .",0
24125,Lexical Models,0
24126,"The models described in previous sections all rely on pretagged input sentences , where the tags are predicted using the Stanford tagger .",0
24127,We use the same pretagged dataset as .,0
24128,"In this section we explore two alternative classes of lexical models : those that use no external systems or data of any kind , as well as word vectors thatare pretrained in an unsupervised manner . :",0
24129,"Development - set F1 scores for different approaches to handling morphology , with and without the addition of learned word embeddings .",0
24130,Models with Subword Features,0
24131,"If tag embeddings are removed from our model and only word embeddings remain ( where word embeddings are learned jointly with other model parameters ) , performance suffers by around 1 F1 .",0
24132,"To restore performance without introducing any dependencies on an external system , we explore incorporating lexical features directly into our model .",0
24133,The results for different approaches we describe in this section are shown in .,0
24134,We first evaluate an approach ( CHARLSTM ) that independently runs a bidirectional LSTM over the characters in each word and uses the LSTM outputs in place of part - of - speech tag embeddings .,0
24135,We find that this approach performs better than using predicted part - of - speech tags .,0
24136,"We can further remove the word embeddings ( leaving the character LSTMs only ) , which does not seem to hurt and can actually help increase parsing accuracy .",0
24137,Next we examine the importance of recurrent connections by constructing and evaluating a simpler alternative .,0
24138,"Our approach ( CHARCONCAT ) is inspired by , who found it effective to replace words with frequently - occurring suffixes , and the observation that our original tag embeddings are rather high - dimensional .",0
24139,"To represent a word , we extract its first 8 letters and last 8 letters , embed each letter , and concatenate the results .",0
24140,"If we use 32 - dimensional embeddings , the 16 letters can be packed into a 512 - dimensional vector - the same size as the inputs to our model .",0
24141,"This size for the inputs in our model was chosen to simplify the use of residual connections ( by matching vector dimensions ) , even though the inputs themselves could have been encoded in a smaller vector .",0
24142,This allows us to directly replace tag embeddings with the 16 - letter prefix / suffix concatenation .,0
24143,"For short words , embeddings of a padding token are inserted as needed .",0
24144,"Words longer than 16 letters are represented in a lossy manner by this concatenative approach , but we hypothesize that prefix / suffix information is enough for our task .",0
24145,We find this simple scheme remarkably effective : it is able to outperform pretagging and can operate even in the absence of word embeddings .,0
24146,"However , its performance is ultimately not quite as good as using a character LSTM .",0
24147,"Given the effectiveness of the self - attentive encoder at the sentence level , it is aesthetically appealing to consider it as a sub-word architecture as well .",0
24148,"However , it was empirically much slower , did not parallelize better than a character - level LSTM ( because words tend to be short ) , and initial results underperformed the LSTM .",0
24149,"One explanation is that in a lexical model , one only wants to compute a single vector per word , whereas the self - attentive architecture is better adapted for producing context - aware summaries at multiple positions in a sequence .",0
24150,External Embeddings,0
24151,"Next , we consider a version of our model that uses external embeddings .",0
24152,Recent work by has achieved state - of - the - art performance across a range of NLP tasks by augmenting existing models with a new technique for word representation called ELMo ( Embeddings from Language Models ) .,0
24153,Their approach is able to capture both subword information and contextual clues : the embeddings are produced by a network that takes characters as input and then uses an LSTM to capture contextual information when producing a vector representation for each word in a sentence .,0
24154,"We evaluate a version of our model that uses ELMo as the sole lexical representation , using publicly available ELMo weights .",0
24155,"These pre-trained word representations are 1024 dimensional , whereas all of our factored models thus far have 512 - dimensional content representations ; we found that the most effective way to address this mismatch is to project the ELMo vectors to the required dimensionality using a learned weight matrix .",0
24156,"With the addition of contextualized word representations , we hypothesized that a full 8 layers of self - attention would no longer be necessary .",0
24157,This proved true in practice : our best development set result of 95.21 F1 was obtained with a 4 - layer encoder .,0
24158,6 Results,1
24159,English ( WSJ ),1
24160,The development set scores of the parser variations presented in previous sections are summarized in .,0
24161,Our best - performing parser used a factored self - attentive encoder over ELMo word representations .,0
24162,The results of evaluating our model on the test set are shown in .,0
24163,"The test score of 93.55 F1 for our CharLSTM parser exceeds the previous best numbers for single - system parsers trained on the Penn Treebank ( without the use of any external data , such as pre-trained word embeddings ) .",1
24164,"When our parser is augmented with ELMo word representations , it achieves a new state - of - the - art score of 95.13 F1 on the WSJ test set .",1
24165,Our WSJ - only parser took 18 hours to train using a single Tesla K80 GPU and can parse the,0
24166,Multilingual ( SPMRL ),1
24167,We tested our model 's ability to generalize across languages by training it on the nine languages represented in the SPMRL 2013 / 2014 shared tasks .,0
24168,"To verify that our lexical representations can function for morphologicallyrich languages and smaller treebanks , we restricted ourselves to running a subset of the exact models that we evaluated on English .",0
24169,"In particular , we evaluated the model that uses a character - level LSTM , with and without the addition of learned word embeddings .",0
24170,We did not evaluate ELMo in the multilingual setting because pre-trained ELMo weights were only available for English .,0
24171,"Hyperparameters were unchanged compared to the English model with the exception of the learning rate , which we adjusted for some of the smaller datasets in the SPMRL task ( see Appendix A ) .",0
24172,Results are shown in .,0
24173,"Development set results show that the addition of word embeddings to a model that uses a character LSTM has a mixed effect : it improves performance for some languages , but hurts for others .",1
24174,"For each language , we selected the trained model that performed better on the development set and evaluated it on the test set .",0
24175,"On 8 of the 9 languages , our test set result exceeds the previous best - published numbers from any system we are aware of .",1
24176,"The exception is Swedish , where the model of continues to be state - of - the - art despite a number of approaches proposed in the intervening years that have achieved better performance on other languages .",0
24177,"We note that their model uses ensembling ( via product grammars ) and a reranking step , whereas our model was only evaluated in the single - system condition .",0
24178,Conclusion,0
24179,"In this paper , we show that the choice of encoder can have a substantial effect on parser performance .",0
24180,"In particular , we demonstrate state - of - theart parsing results with a novel encoder based on factored self - attention .",0
24181,"The gains we see come not only from incorporating more information ( such as subword features or externally - trained word representations ) , but also from structuring the architecture to separate different kinds of information from each other .",0
24182,Our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks .,0
24183,A Training Details,0
24184,A.1 Model Hyperparameters,0
24185,The hyperparameters for our model are shown in .,0
24186,Hyperparameters were tuned on the development set for English .,0
24187,A.2 Optimizer Parameters,0
24188,Our model was trained using Adam with a batch size of 250 sentences .,0
24189,"For the first 160 batches ( equal to 1 epoch for English ) , the learning rate was increased linearly from 0 up to the base learning rate shown in .",0
24190,Development - set performance was evaluated four times per epoch ; if it did not improve for 5 epochs in a row the learning rate was halved .,0
24191,The iterate that performed best on the development set was taken as the output of the training procedure .,0
24192,"To ensure stability of the optimizer , we found it important to use a large batch size , to warm up the learning rate overtime ( similar to ) , and to pick an appropriate learning rate .",0
24193,A.3 Position Embeddings,0
24194,All variations of our model use learned position embeddings .,0
24195,Our attempts to use the sinusoidal position embeddings proposed by consistently performed worse than using learned embeddings .,0
24196,Language Base Learning Rate,0
24197,English 0.0008 Hebrew 0.002 Polish 0.0015 Swedish 0.002,0
24198,All others 0.0008 : Learning rates .,0
24199,title,0
24200,What Do Recurrent Neural Network Grammars Learn About Syntax ?,1
24201,abstract,0
24202,Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language .,1
24203,They show state - of the - art language modeling and parsing performance .,0
24204,"We investigate what information they learn , from a linguistic perspective , through various ablations to the model and the data , and by augmenting the model with an attention mechanism ( GA - RNNG ) to enable closer inspection .",0
24205,We find that explicit modeling of composition is crucial for achieving the best performance .,0
24206,"Through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand - crafted head rules , albeit with some important differences ) .",0
24207,"By training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .",0
24208,Introduction,0
24209,"In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .",0
24210,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .",1
24211,Fitting a probabilistic model to data has often been understood as away to test or confirm some aspect of a theory .,0
24212,"We talk about a model 's assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it "" discovers "" from the data .",0
24213,"In some sense , such models can bethought of as mini-scientists .",0
24214,"Neural networks , including RNNGs , are capable of representing larger classes of hypotheses than traditional probabilistic models , giving them more freedom to explore .",0
24215,"Unfortunately , they tend to be bad mini-scientists , because their parameters are difficult for human scientists to interpret .",0
24216,RNNGs are striking because they obtain stateof - the - art parsing and language modeling performance .,0
24217,"Their relative lack of independence assumptions , while still incorporating a degree of linguistically - motivated prior knowledge , affords the model considerable freedom to derive its own insights about syntax .",0
24218,"If they are mini-scientists , the discoveries they make should be of particular interest as propositions about syntax ( at least for the particular genre and dialect of the data ) .",0
24219,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,1
24220,We begin with an ablation study to discover the importance of the composition function in 3 .,1
24221,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",1
24222,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .",1
24223,"Our key findings are that lexical heads play an important role in representing most phrase types ( although compositions of multiple salient heads are not infrequent , especially 1 RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context - free grammars , but more than models that parse by transducing word sequences to linearized parse trees represented as strings .",0
24224,"Inductive bias is necessary for learning ; we believe the important question is not "" how little can a model getaway with ? "" but rather the benefit of different forms of inductive bias as data vary .",0
24225,for conjunctions ) and that nonterminal labels provide little additional information .,0
24226,"As a by-product of our investigation , a variant of the RNNG without ensembling achieved the best reported supervised phrase - structure parsing ( 93.6 F 1 ; English PTB ) and , through conversion , dependency parsing ( 95.8 UAS , 94.6 LAS ; PTB SD ) .",0
24227,The code and pretrained models to replicate our results are publicly available 2 .,0
24228,Recurrent Neural Network Grammars,0
24229,An RNNG defines a joint probability distribution over string terminals and phrase - structure nonterminals .,0
24230,3,0
24231,"Formally , the RNNG is defined by a triple N , ? , ? , where N denotes the set of nonterminal symbols ( NP , VP , etc. ) , ?",0
24232,"the set of all terminal symbols ( we assume that N ? ? = ? ) , and ?",0
24233,the set of all model parameters .,0
24234,"Unlike previous works that rely on hand - crafted rules to compose more fine - grained phrase representations , the RNNG implicitly parameterizes the information passed through compositions of phrases ( in ? and the neural network architecture ) , hence weakening the strong independence assumptions in classical probabilistic context - free grammars .",0
24235,"The RNNG is based on an abstract state machine like those used in transition - based parsing , with its algorithmic state consisting of a stack of partially completed constituents , a buffer of already - generated terminal symbols , and a list of past actions .",0
24236,"To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .",0
24237,"Given y , there is one such sequence ( easily identified ) , which we call the oracle , a = a 1 , . . . , an used during supervised training .",0
24238,The RNNG uses three different actions :,0
24239,"introduces an open nonterminal symbol onto the stack , e.g. , "" ( NP "" ; GEN ( x ) , where x ? ? , generates a terminal symbol and places it on the stack and buffer ; and REDUCE indicates a constituent is now complete .",0
24240,The elements of the stack that comprise the current constituent ( going back to the last 2 https://github.com/clab/rnng/tree/,0
24241,master / interpreting-rnng also defined a conditional version of the RNNG that can be used only for parsing ; here we focus on the generative version since it is more flexible and ( rather surprisingly ) even learns better estimates of p ( y | x ) . :,0
24242,"The RNNG consists of a stack , buffer of generated words , and list of past actions that lead to the current configuration .",0
24243,"Each component is embedded with LSTMs , and the parser state summary u t is used as top - layer features to predict a softmax over all feasible actions .",0
24244,This figure is due to .,0
24245,"open nonterminal ) are popped , a composition function is executed , yielding a composed representation that is pushed onto the stack .",0
24246,"At each timestep , the model encodes the stack , buffer , and past actions , with a separate LSTM for each component as features to define a distribution over the next action to take ( conditioned on the full algorithmic state ) .",0
24247,The over all architecture is illustrated in ; examples of full action sequences can be found in .,0
24248,"A key element of the RNNG is the composition function , which reduces a completed constituent into a single element on the stack .",0
24249,This function computes a vector representation of the new constituent ; it also uses an LSTM ( here a bidirectional one ) .,0
24250,"This composition function , which we consider in greater depth in 3 , is illustrated in . : RNNG composition function on each REDUCE operation ; the network on the right models the structure on the left .",0
24251,"Since the RNNG is a generative model , it attempts to maximize p ( x , y ) , the joint distribution of strings and trees , defined as p ( x , y ) = p ( a ) = n t =1 p ( a t | a 1 , . . . , a t?1 ) .",0
24252,"In other words , p ( x , y ) is defined as a product of local probabilities , conditioned on all past actions .",0
24253,"The joint probability estimate p ( x , y) can be used for both phrase - structure parsing ( finding arg max y p ( y | x ) ) and language modeling ( finding p ( x ) by marginalizing over the set of possible parses for x ) .",0
24254,Both inference problems can be solved using an importance sampling procedure .,0
24255,We report all RNNG performance based on the corrigendum to .,0
24256,Composition is Key,0
24257,"Given the same data , under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than ( respectively ) the models of and Choe and Charniak ( 2016 ) that represent y as a "" linearized "" sequence of symbols and parentheses without explicitly capturing the tree structure , or even constraining they to be a well - formed tree ( see ) .",0
24258,"directly predict the sequence of nonterminals , "" shifts "" ( which consume a terminal symbol ) , and parentheses from left to right , conditional on the input terminal sequence x , while Choe and Charniak ( 2016 ) used a sequential LSTM language model on the same linearized trees to create a generative variant of the model .",0
24259,The generative model is used to re-rank parse candidates .,0
24260,"The results in suggest that the RNNG 's explicit composition function , which and Choe and Charniak ( 2016 ) must learn implicitly , plays a crucial role in the RNNG 's generalization success .",0
24261,"Beyond this , Choe and Charniak 's generative variant of is another instance where generative models trained on the PTB outperform discriminative models .",0
24262,Model,0
24263,Ablated RNNGs,0
24264,"On close inspection , it is clear that the RNNG 's three data structures - stack , buffer , and action history - are redundant .",0
24265,"For example , the action history and buffer contents completely determine the structure of the stack at every timestep .",0
24266,"Every generated word goes onto the stack , too ; and some past words will be composed into larger structures , but through the composition function , they are all still "" available "" to the network that predicts the next action .",0
24267,"Similarly , the past actions are redundant with the stack .",0
24268,"Despite this redundancy , only the stack incorporates the composition function .",0
24269,"Since each of the ablated models is sufficient to encode all necessary partial tree information , the primary difference is that ablations with the stack use explicit composition , to which we can therefore attribute most of the performance difference .",0
24270,"We conjecture that the stack - the component that makes use of the composition function - is critical to the RNNG 's performance , and that the buffer and action history are not .",0
24271,"In transitionbased parsers built on expert - crafted features , the most recent words and actions are useful if they are salient , although neural representation learners can automatically learn what information should be salient .",0
24272,"To test this conjecture , we train ablated RN - NGs that lack each of the three data structures ( action history , buffer , stack ) , as well as one that lacks both the action history and buffer .",0
24273,5,0
24274,"If our conjecture is correct , performance should degrade most without the stack , and the stack alone should perform competitively .",0
24275,Experimental settings .,0
24276,"We perform our experiments on the English PTB corpus , with 02 - 21 for training , 24 for validation , and 23 for test ; no additional data were used for training .",0
24277,We fol - low the same hyperparameters as the generative model proposed in .,0
24278,The generative model did not use any pretrained word embeddings or POS tags ; a discriminative variant of the standard RNNG was used to obtain tree samples for the generative model .,0
24279,All further experiments use the same settings and hyperparameters unless otherwise noted .,0
24280,Experimental results .,0
24281,"We trained each ablation from scratch , and compared these models on three tasks : English phrase - structure parsing ( labeled F 1 ) , ; dependency parsing , , by converting parse output to Stanford dependencies",0
24282,"( De using the tool by ; and language modeling , GA - RNNG 93.5 :",0
24283,Phrase - structure parsing performance on PTB 23 . indicates systems that use additional unparsed data ( semisupervised ) .,0
24284,The GA - RNNG results will be discussed in 4 .,0
24285,Discussion .,0
24286,"The RNNG with only a stack is the strongest of the ablations , and it even outperforms the "" full "" RNNG with all three data structures .",1
24287,Ablating the stack gives the worst among the new results .,1
24288,This strongly supports the importance of the composition function : a proper REDUCE operation that transforms a constituent 's parts and nonterminal label into a single explicit ( vector ) representation is helpful to performance .,0
24289,"It is noteworthy that the stack alone is stronger than the original RNNG , which - in principlecan learn to disregard the buffer and action his - tory .",0
24290,"Since the stack maintains syntactically "" recent "" information near its top , we conjecture that the learner is overfitting to spurious predictors in the buffer and action history that explain the training data but do not generalize well .",0
24291,"A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .",1
24292,"Indeed , modeling syntax without explicit composition ( the stackablated RNNG ) provides little benefit over a sequential LSTM language model .",1
24293,We remark that the stack - only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models .,1
24294,Gated Attention RNNG,1
24295,"Having established that the composition function is key to RNNG performance ( 3 ) , we now seek to understand the nature of the composed phrasal representations thatare learned .",0
24296,"Like most neural networks , interpreting the composition function 's behavior is challenging .",0
24297,"Fortunately , linguistic theories offer a number of hypotheses about the nature of representations of phrases that can provide a conceptual scaffolding to understand them .",0
24298,Linguistic Hypotheses,0
24299,We consider two theories about phrasal representation .,0
24300,The first is that phrasal representations are strongly determined by a privileged lexical head .,0
24301,"Augmenting grammars with lexical head information has along history in parsing , starting with the models of , and theories of syntax such as the "" bare phrase structure "" hypothesis of the Minimalist Program posit that phrases are represented purely by single lexical heads .",0
24302,Proposals for multiple headed phrases ( to deal with tricky cases like conjunction ) likewise exist .,0
24303,Do the phrasal representations learned by RN - NGs depend on individual lexical heads or multiple heads ?,0
24304,Or do the representations combine all children without any salient head ?,0
24305,Related to the question about the role of heads in phrasal representation is the question of whether phrase - internal material wholly determines the representation of a phrase ( an endocentric representation ) or whether nonterminal relabeling of a constitutent introduces new information ( exocentric representations ) .,0
24306,"To illustrate the contrast , an endocentric representation is representing a noun phrase with a noun category , whereas S ?",0
24307,NP VP exocentrically introduces a new syntactic category that is neither NP nor VP .,0
24308,Gated Attention Composition,0
24309,"To investigate what the stack - only RNNG learns about headedness ( and later endocentricity ) , we propose a variant of the composition function that makes use of an explicit attention mechanism and a sigmoid gate with multiplicative interactions , henceforth called GA - RNNG .",0
24310,"At every REDUCE operation , the GA - RNNG assigns an "" attention weight "" to each of its children ( between 0 and 1 such that the total weight off all children sums to 1 ) , and the parent phrase is represented by the combination of a sum of each child 's representation scaled by its attention weight and its nonterminal type .",0
24311,"Our weighted sum is more expressive than traditional head rules , however , because it allows attention to be divided among multiple constituents .",0
24312,"Head rules , con-versely , are analogous to giving all attention to one constituent , the one containing the lexical head .",0
24313,We now formally define the GA - RNNG 's composition function .,0
24314,"Recall that u t is the concatenation of the vector representations of the RNNG 's data structures , used to assign probabilities to each of the actions available at timestep t ( see , the layer before the softmax at the top ) .",0
24315,"For simplicity , we drop the timestep index here .",0
24316,"Let o nt denote the vector embedding ( learned ) of the nonterminal being constructed , for the purpose of computing attention weights .",0
24317,"Now let c 1 , c 2 , . . . denote the sequence of vector embeddings for the constituents of the new phrase .",0
24318,The length of these vectors is defined by the dimensionality of the bidirectional LSTM used in the original composition function .,0
24319,We use semicolon ( ; ) to denote vector concatenation operations .,0
24320,The attention vector is given by :,0
24321,"Note that the length of a is the same as the number of constituents , and that this vector sums to one due to the softmax .",0
24322,It divides a single unit of attention among the constituents .,0
24323,"Next , note that the constituent source vector m = [ c 1 ; c 2 ; ] a is a convex combination of the child - constituents , weighted by attention .",0
24324,We will combine this with another embedding of the nonterminal denoted as t nt ( separate from o nt ) using a sigmoid gating mechanism :,0
24325,"Note that the value of the gate is bounded between [ 0 , 1 ] in each dimension .",0
24326,"The new phrase 's final representation uses element - wise multiplication ( ) with respect to both t nt and m , a process reminiscent of the LSTM "" forget "" gate :",0
24327,(,0
24328,The intuition is that the composed representation should incorporate both nonterminal information and information about the constituents ( through weighted sum and attention mechanism ) .,0
24329,The gate g modulates the interaction between them to account for varying importance between the two in different contexts .,0
24330,Experimental results .,0
24331,We include this model 's performance in Tables 2 - 4 ( last row in all tables ) .,0
24332,"It is clear that the model outperforms the baseline RNNG with all three structures present and achieves competitive performance with the strongest , stack - only , RNNG variant .",1
24333,Headedness in Phrases,1
24334,We now exploit the attention mechanism to probe what the RNNG learns about headedness on the WSJ 23 test set ( unseen before by the model ) .,0
24335,The Heads that GA - RNNG Learns,0
24336,The attention weight vectors tell us which constituents are most important to a phrase 's vector representation in the stack .,0
24337,"Here , we inspect the attention vectors to investigate whether the model learns to center its attention around a single , or by extension a few , salient elements , which would confirm the presence of headedness in GA - RNNG .",0
24338,"First , we consider several major nonterminal categories , and estimate the average perplexity of the attention vectors .",0
24339,"The average perplexity can be interpreted as the average number of "" choices "" for each nonterminal category ; this value is only computed for the cases where the number of components in the composed phrase is at least two ( otherwise the attention weight would be trivially 1 ) .",0
24340,"The minimum possible value for the perplexity is 1 , indicating a full attention weight around one component and zero everywhere else .",0
24341,"( in blue ) shows much less than 2 average "" choices "" across nonterminal categories , which also holds true for all other categories not shown .",0
24342,For comparison we also report the average perplexity of the uniform distribution for the same nonterminal categories ; this represents the highest entropy cases where there is no headedness at all by assigning the same attention weight to each constituent ( e.g. attention weights of 0.25 each for phrases with four constituents ) .,0
24343,"It is clear that the learned attention weights have much lower perplexity than the uniform distribution baseline , indicating that the learned attention weights are quite peaked around certain components .",0
24344,"This implies that phrases ' vectors tend to resemble the vector of one salient constituent , but not exclusively , as the perplexity for most categories is still not close to one .",0
24345,"Next , we consider the how attention is distributed for the major nonterminal categories in , where the first five rows of each category represent compositions with highest entropy , and the next five rows are qualitatively analyzed .",0
24346,The high - entropy cases where the attention is most divided represent more complex phrases with conjunctions or more than one plausible head .,0
24347,NPs .,0
24348,"In most simple noun phrases ( representative samples in rows 6 - 7 of Table 5 ) , the model pays the most attention to the rightmost noun and assigns near - zero attention on determiners and possessive determiners , while also paying nontrivial attention weights to the adjectives .",0
24349,"This finding matches existing head rules and our intuition that nouns head noun phrases , and that adjectives are more important than determiners .",0
24350,We analyze the case where the noun phrase contains a conjunction in the last three rows of .,0
24351,The syntax of conjunction is a long - standing source of controversy in syntactic analysis .,0
24352,"Our model suggests that several representational strategies are used , when coordinating single nouns , both the first noun ( 8 ) and the last noun ( 9 ) maybe selected .",0
24353,"However , in the case of conjunctions of multiple noun phrases ( as opposed to multiple single - word nouns ) , the model consistently picks the conjunction as the head .",0
24354,"All of these representational strategies have been argued for individually on linguistic grounds , and since we see all of them present , RNNGs face the same confusion that linguists do .",0
24355,VPs .,0
24356,"The attention weights on simple verb phrases ( e.g. , "" VP ? V NP "" , 9 ) are peaked around the noun phrase instead of the verb .",0
24357,This implies that the verb phrase would look most similar to the noun under it and contradicts existing head rules that unanimously put the verb as the head of the verb phrase .,0
24358,"Another interesting finding is that the model pays attention to polarity information , where negations are almost always assigned nontrivial attention weights .",0
24359,"7 Furthermore , we find that the model attends to the conjunction terminal in conjunctions of verb phrases ( e.g. , "" VP ? VP and VP "" , 10 ) , reinforcing the similar finding for conjunction of noun phrases .",0
24360,PPs .,0
24361,"In almost all cases , the model attends to the preposition terminal instead of the noun phrases or complete clauses under it , regardless of the type of preposition .",0
24362,"Even when the preposi-tional phrase is only used to make a connection between two noun phrases ( e.g. , "" PP ? NP after NP "" , 10 ) , the prepositional connector is still considered the most salient element .",0
24363,"This is less consistent with the Collins and Stanford head rules , where prepositions are assigned a lower priority when composing PPs , although more consistent with the Johansson head rule .",0
24364,Comparison to Existing Head Rules,0
24365,"To better measure the overlap between the attention vectors and existing head rules , we converted the trees in PTB 23 into a dependency representation using the attention weights .",0
24366,"In this case , the attention weight functions as a "" dynamic "" head rule , where all other constituents within the same composed phrase are considered to modify the constituent with the highest attention weight , repeated recursively .",0
24367,"The head of the composed representation for "" S "" at the top of the tree is attached to a special root symbol and functions as the head of the sentence .",0
24368,We measure the overlap between the resulting tree and conversion results of the same trees using the and Stanford dependencies ( De head rules .,0
24369,"Results are evaluated using the standard evaluation script ( excluding punctuation ) in terms of UAS , since the attention weights do not provide labels .",0
24370,Results .,0
24371,The model has a higher overlap with the conversion using Collins head rules ( 49.8 UAS ) rather than the Stanford head rules ( 40.4 UAS ) .,1
24372,"We attribute this large gap to the fact that the Stanford head rules incorporate more semantic considerations , while the RNNG is a purely syntactic model .",0
24373,"In general , the attention - based tree output has a high error rate ( ? 90 % ) when the dependent is a verb , since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb , as discussed above .",1
24374,"The conversion accuracy is better for nouns ( ? 50 % error ) , and much better for determiners ( 30 % ) and particles ( 6 % ) with respect to the Collins head rules .",1
24375,Discussion .,0
24376,"GA - RNNG has the power to infer head rules , and to a large extent , it does .",0
24377,"It follows some conventions thatare established in one or more previous head rule sets ( e.g. , prepositions head prepositional phrases , nouns head noun phrases ) , but attends more to a verb phrase 's nominal constituents than the verb .",0
24378,"It is important to note that this is not the by -product of learning a specific model for parsing ; the training objective is joint likelihood , which is not a proxy loss for parsing performance .",0
24379,These decisions were selected because they make the data maximally likely ( though admittedly only locally maximally likely ) .,0
24380,We leave deeper consideration of this noun - centered verb phrase hypothesis to future work .,0
24381,The Role of Nonterminal Labels,1
24382,"Emboldened by our finding that GA - RNNGs learn a notion of headedness , we next explore whether heads are sufficient to create representations of phrases ( in line with an endocentric theory of phrasal representation ) or whether extra nonterminal information is necessary .",0
24383,"If the endocentric hypothesis is true ( that is , the representation of a phrase is built from within depending on its components but independent of explicit category labels ) , then the nonterminal types should be easily inferred given the endocentrically - composed representation , and that ablating the nonterminal information would not make much difference in performance .",0
24384,"Specifically , we train a GA - RNNG on unlabeled trees ( only bracketings without nonterminal types ) , denoted U - GA - RNNG .",0
24385,This idea has been explored in research on methods for learning syntax with less complete annotation .,0
24386,"A key finding from was that , given bracketing structure , simple dimensionality reduction techniques could reveal conventional nonterminal categories with high accuracy ; also showed that latent variables can be used to recover fine - grained nonterminal categories .",0
24387,We therefore expect that the vector embeddings of the constituents that the U - GA - RNNG correctly recovers ( on test data ) will capture categories similar to those in the Penn Treebank .,0
24388,Experiments .,0
24389,"Using the same hyperparameters and the PTB dataset , we first consider unlabeled F 1 parsing accuracy .",0
24390,"On test data ( with the usual split ) , the GA - RNNG achieves 94.2 % , while the U - GA - RNNG achieves 93.5 % .",1
24391,This result suggests that nonterminal category labels add a relatively small amount of information compared to purely endocentric representations .,0
24392,Visualization .,0
24393,"If endocentricity is largely sufficient to account for the behavior of phrases , where do our robust intuitions for syntactic category types come from ?",0
24394,We use t- SNE ( van der to visualize composed phrase vectors from the U - GA - RNNG model applied to the unseen test data .,0
24395,"shows that the U - GA - RNNG tends to recover nonterminal categories as encoded in the PTB , even when trained without them .",0
24396,8,0
24397,"These results suggest nonterminal types can be inferred from the purely endocentric compositional process to a certain extent , and that the phrase clusters found by the U - GA - RNNG largely overlap with nonterminal categories .",0
24398,Analysis of PP and SBAR .,0
24399,indicates a certain degree of overlap between SBAR ( red ) and PP ( yellow ) .,0
24400,"As both categories are interesting from the linguistic perspective and quite similar , we visualize the learned phrase vectors of 40 randomly selected SBARs and PPs from the test set ( using U - GA - RNNG ) , illustrated in .",0
24401,"First , we can see that phrase representations for PPs and SBARs depend lesson the nonterminal categories 9 and more on the connector .",0
24402,"For instance , the model learns to cluster phrases that start with words that can be either prepositions or complementizers ( e.g. , for , at , to , under , by ) , regardless of whether the true nonterminal labels are PPs or SBARs .",0
24403,"This suggests that SBARs that start with "" prepositional "" words are similar to PPs from the model 's perspective .",0
24404,"Second , the model learns to disregard the word that , as "" SBAR ? that S "" and "" SBAR ?",0
24405,"S "" are close together .",0
24406,"This finding is intuitive , as complementizer that is often optional ( Jaeger , 2010 ) , unlike prepositional words that might describe relative time and location .",0
24407,"Third , certain categories of PPs and SBARs form their own separate clusters , such as those that involve the words because and of .",0
24408,We attribute these distinctions to the fact that these words convey different meanings than many prepositional words ; the word of indicates possession while because indicates cause - and - effect relationship .,0
24409,"These examples show that , to a certain extent , the GA - RNNG is able to learn non-trivial semantic information , even when trained on a fairly small amount of syntactic data .",0
24410,Related Work,0
24411,The problem of understanding neural network models in NLP has been previously studied for sequential RNNs .,0
24412,"showed that sequence - tosequence neural translation models capture a certain degree of syntactic knowledge of the source language , such as voice ( active or passive ) and tense information , as a by-product of the translation objective .",0
24413,"Our experiment on the importance of composition function was motivated by and , who achieved competitive parsing accuracy without explicit composition .",0
24414,"In another work , investigated the importance of recursive tree structures ( as opposed to linear recurrent models ) in four different tasks , including sentiment and semantic relation classification .",0
24415,"Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long - range relations , such as semantic relationship classification , with no conclusive advantage for sentiment classification and discourse parsing .",0
24416,Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state - of the - art parsing performance .,0
24417,"Extensive prior work on phrase - structure parsing typically employs the probabilistic context - free grammar formalism , with lexicalized and nonterminal augmentations .",0
24418,The conjecture that fine - grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies .,0
24419,"In a similar work , Sangati and Zuidema ( 2009 ) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase - structure trees in an unsupervised manner .",0
24420,"In contrast , we used neural attention to obtain the "" head rules "" in the GA - RNNG ; the whole model is trained end - to - end to maximize the log probability of the correct action given the history .",0
24421,"Unlike prior work , GA - RNNG allows the attention weight to be divided among phrase constituents , essentially propagating ( weighted ) headedness information from multiple components .",0
24422,Conclusion,0
24423,"We probe what recurrent neural network grammars learn about syntax , through ablation scenarios and a novel variant with a gated attention mechanism on the composition function .",0
24424,"The composition function , a key differentiator between the RNNG and other neural models of syntax , is crucial for good performance .",0
24425,"Using the attention vectors we discover that the model is learning something similar to heads , although the attention vectors are not completely peaked around a single component .",0
24426,We show some cases where the attention vector is divided and measure the relationship with existing head rules .,0
24427,"RNNGs without access to nonterminal information during training are used to support the hypothesis that phrasal representations are largely endocentric , and a visualization of representations shows that traditional nonterminal categories fallout naturally from the composed phrase vectors .",0
24428,"This confirms previous conjectures that bracketing annotation does most of the work of syntax , with nonterminal categories easily discoverable given bracketing .",0
24429,title,0
24430,An Empirical Study of Building a Strong Baseline for Constituency Parsing,1
24431,abstract,0
24432,This paper investigates the construction of a strong baseline based on general purpose sequence - to - sequence models for constituency parsing .,0
24433,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequenceto - sequence model achieves the current top - notch parsers ' performance without requiring explicit task - specific knowledge or architecture of constituent parsing .",0
24434,Introduction,0
24435,"Sequence - to - sequence ( Seq2seq ) models have successfully improved many well - studied NLP tasks , especially for natural language generation ( NLG ) tasks , such as machine translation ( MT ) and abstractive summarization .",0
24436,Seq2seq models have also been applied to constituency parsing and provided a fairly good result .,0
24437,"However one obvious , intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees , Thus , models that directly model them , such as RNNG , are an intuitively more promising approach .",0
24438,"In fact , RNNG and its extensions provide the current stateof - the - art performance .",0
24439,Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing .,0
24440,"After the first proposal of an Seq2seq constituency parser , many task - independent techniques have been developed , mainly in the NLG research area .",0
24441,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,1
24442,Our motivation is basically identical to that described in .,0
24443,A strong baseline is crucial for reporting reliable experimental results .,0
24444,It offers a fair evaluation of promising new techniques if they solve new issues or simply resolve issues that have already been addressed by current generic technology .,0
24445,"More specifically , it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the outputs of strong Seq2seq models and task - specific models , e.g. , RNNG .",0
24446,"The contributions of this paper are summarized as follows : ( 1 ) a strong baseline for constituency parsing based on general purpose Seq2seq models 1 , ( 2 ) an empirical investigation of several generic techniques that can ( or can not ) contribute to improve the parser performance , ( 3 ) empirical evidence that Seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information .",0
24447,Constituency Parsing by Seq2seq,0
24448,Our starting point is an RNN - based Seq2seq model with an attention mechanism that was applied to constituency parsing .,0
24449,"We omit detailed descriptions due to space limitations , but note that our model architecture is identical to the one introduced in",0
24450,A key trick for applying Seq2seq models to constituency parsing is the linearization of parse 1 Our code and experimental configurations for reproducing our experiments are publicly available : https://github.com/nttcslab-nlp/strong,0
24451,s 2s baseline parser,0
24452,"2 More specifically , our Seq2seq model follows the one implemented in seq2seq - attn ( https://github.com/harvardnlp/seq2seq-attn ) , which is the alpha-version of the Open NMT tool ( http://opennmt.net ) .",0
24453,Original input,0
24454,John has a dog .,0
24455,Output : S-exp.,0
24456,( S ( NP NNP ) ( VP VBZ ( NP DT NN ) ) . ) Linearized form ( S ( NP NNP ) NP ( VP VBZ ( NP DT NN ) NP ) VP . ) S w/ POS normalized ( S ( NP XX ) NP ( VP XX ( NP XX XX ) NP ) VP . ) S : Examples of linearization and POS - tag normalization trees .,0
24457,"Roughly speaking , a linearized parse tree consists of open , close bracketing and POS - tags that correspond to a given input raw sentence .",0
24458,"Since a one - to - one mapping exists between a parse tree and it s linearized form ( if the linearized form is a valid tree ) , we can recover parse trees from the predicted linearized parse tree .",0
24459,also introduced the part - of - speech ( POS ) tag normalization technique .,0
24460,"They substituted each POS tag in a linearized parse tree to a single XX - tag 3 , which allows Seq2seq models to achieve a more competitive performance range than the current state - of the - art parses 4 . shows an example of a parse tree to which linearization and POS - tag normalization was applied .",0
24461,Task - independent,0
24462,Extensions,0
24463,This section describes several generic techniques that improve Seq2seq performance 5 . lists the notations used in this paper for a convenient reference .,0
24464,Subword as input features,0
24465,Applying subword decomposition has recently become a leading technique in NMT literature .,0
24466,Its primary advantage is a significant reduction of the serious out - of - vocabulary ( OOV ) problem .,0
24467,We incorporated subword information as an additional feature of the original input words .,0
24468,A similar usage of subword features was previously proposed in .,0
24469,"Formally , the encoder embedding vector at encoder position i , namely , e i , is calculated as follows :",0
24470,"We did not substitute POS - tags for punctuation symbols such as "" . "" , and "" , "" .",0
24471,4 Several recently developed neural - based constituency parsers ignore POS tags since they are not evaluated in the standard evaluation metric of constituency parsing ( Bracketing F- measure ) .,0
24472,Figure in the supplementary material shows the brief sketch of the method explained in the following section .,0
24473,D : dimension of the embeddings H : dimension of the hidden states i : index of the ( token ) position in input sentence j : index of the ( token ) position in output linearized format of parse tree V ( e ) :,0
24474,vocabulary of word for input ( encoder ) side V ( s ) :,0
24475,"vocabulary of subword for input ( encoder ) side E : encoder embedding matrix for V ( e ) , where E ? R D|V ( e ) | F : encoder embedding matrix for V ( s ) , where F ? R D|V ( s ) | wi : i - th word ( token ) in the input sentence , wi ? V ( e ) x k : one - hot vector representation of the k - th word in V ( e ) s k : one - hot vector representation of the k - th subword in V ( s ) u : encoder embedding vector of unknown token ?( ) :",0
24476,function that returns the index of given word in the vocabulary V ( e ) ?( ) : function that returns a set of indices in the subword vocabulary V ( s ) generated from the given word .,0
24477,"e.g. , k ? ? ( wi ) ei : encoder embedding vector at position i in encoder V ( d ) :",0
24478,vocabulary of output with POS - tag normalization V ( q ) :,0
24479,vocabulary of output without POS - tag normalization,0
24480,zj : final hidden vector calculated at the decoder position j oj : final decoder output scores at decoder position j qj : output scores of auxiliary task at decoder position j b : additional bias term in the decoder output layer for mask pj : vector format of output probability at decoder position j A : number of models for ensembling C : number of candidates generating for LM - reranking :,0
24481,List of notations used in this paper .,0
24482,where k = ? ( w i ) .,0
24483,"Note that the second term of RHS indicates our additional subword features , and the first represents the standard word embedding extraction procedure .",0
24484,"Among several choices , we used the byte - pair encoding ( BPE ) approach proposed in applying 1,000 merge operations 6 .",0
24485,Unknown token embedding as a bias,0
24486,"We generally replace rare words , e.g. , those appearing less than five times in the training data , with unknown tokens in the Seq2seq approach .",0
24487,"However , we suspect that embedding vectors , which correspond to unknown tokens , can not be trained well for the following reasons : ( 1 ) the occurrence of unknown tokens remains relatively small in the training data since they are obvious replacements for rare words , and Seq2seq is relatively ineffective for training infrequent words .",0
24488,"Based on these observations , we utilize the unknown embedding as a bias term b of linear layer ( W x + b ) when obtaining every encoder embeddings for overcoming infrequent word problem .",0
24489,"Then , we modify Eq. 2 as follows :",0
24490,"Note that if w i is unknown token , then Eq. 2 becomes e i = 2 u + k ??( w i ) ( F s k + u ) .",0
24491,Multi - task learning,0
24492,Several papers on the Seq2seq approach have reported that the multi-task learning extension often improves the task performance if we can find effective auxiliary tasks related to the target task .,0
24493,"From this general knowledge , we re-consider jointly estimating POS - tags by incorporating the linearized forms without the POS - tag normalization as an auxiliary task .",0
24494,"In detail , the linearized forms with and without the POS - tag normalization are independently and simultaneously estimated as o j and q j , respectively , in the decoder output layer by following equation :",0
24495,Output length controlling,0
24496,"As described in , not all the outputs ( predicted linearized parse trees ) obtained from the Seq2seq parser are valid ( well - formed ) as a parse tree .",0
24497,"Toward guaranteeing that every output is a valid tree , we introduce a simple extension of the method for controlling the Seq2seq output length .",0
24498,"First , we introduce an additional bias term bin the decoder output layer to prevent the selection of certain output words :",0
24499,"If we set a large negative value at the m - th element in b , namely b m ? ?? , then the m - th element in p j becomes approximately 0 , namely p j , m ? 0 , regardless of the value of the k - th element in o j .",0
24500,We refer to this operation to set value ??,0
24501,in b as a mask .,0
24502,"Since this naive masking approach is harmless to GPU - friendly processing , we can still exploit GPU parallelization .",0
24503,"We set b to always mask the EOS - tag and change b when at least one of the following conditions is satisfied : ( 1 ) if the number of open and closed brackets generated so far is the same , then we mask the XX - tags ( or the POS - tags ) and all the closed brackets .",0
24504,"( 2 ) if the number of predicted XX - tags ( or POS - tags ) is equivalent to that of the words in a given input sentence , then we mask the XX - tags ( or all the POS - tags ) and all the open brackets .",0
24505,"If both conditions and are satisfied , then the decoding process is finished .",0
24506,The additional cost for controlling the mask is to count the number of XX - tags and the open and closed brackets so far generated in the decoding process .,0
24507,The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer .,0
24508,"In constituency parsing , several systems also incorporate pre-trained word embeddings , such as ; .",0
24509,"To maintain as much reproducibility of our experiments as possible , we simply applied publicly available pre-trained word embeddings , i.e. , glove.840B .300 d",0
24510,"7 , as initial values of the encoder embedding layer .",0
24511,Model ensemble,0
24512,Ensembling several independently trained models together significantly improves many NLP tasks .,0
24513,"In the ensembling process , we predict the output tokens using the arithmetic mean of predicted probabilities computed by each model :",0
24514,where p ( a ) j represents the probability distribution at position j predicted by the a - th model .,0
24515,Language model ( LM ) reranking,0
24516,Choe and Charniak ( 2016 ) demonstrated that reranking the predicted parser output candidates with an RNN language model ( LM ) significantly improves performance .,0
24517,We refer to this reranking process as LM-rerank .,0
24518,"Following their success , we also trained RNN - LMs on the PTB dataset with their published preprocessing code 8 to reproduce the experiments in Choe and Charniak ( 2016 ) for our LM-rerank .",0
24519,"We selected the current stateof - the - art LM ) 9 as our LMreranker , which is a much stronger LM than was used in Choe and Charniak ( 2016 ) .",0
24520,Experiments,0
24521,"Our experiments used the English Penn Treebank data , which are the most widely used benchmark data in the literature .",0
24522,"We used the standard split of training ( Sec.02 - 21 ) , development ( Sec.22 ) , and test data ( Sec.23 ) and strictly followed the instructions for the evaluation settings explained in .",0
24523,"For data pre-processing , all the parse trees were transformed into linearized forms , which include standard UNK replacement for OOV words and POS - tag normalization by XX - tags .",0
24524,"As explained in , we did not apply any parse tree binarization or special unary treatment , which were used as common techniques in the literature .",0
24525,One drawback of Seq2seq approach is that it seems sensitive to initialization .,0
24526,Comparing only with a single result for each setting may produce inaccurate conclusions .,0
24527,"Therefore , we should evaluate the performances over several trials to improve the evaluation reliability .",0
24528,"The baseline Seq2seq models , ( a ) and ( f ) , produced the malformed parse trees .",0
24529,We postprocessed such malformed parse trees by simple rules introduced in .,0
24530,"On the other hand , we confirmed that all the results applying the technique explained in Sec. 3.4 produced no malformed parse trees .",0
24531,Ensembling and Reranking : shows the results of our models with model ensembling and LM- reranking .,0
24532,"For ensemble , we randomly selected eight of the ten Seq2seq models reported in .",0
24533,"For LM - reranking , we first generated 80 candidates by the above eight ensemble models and selected the best parse tree for each input in terms of the LM- reranker .",0
24534,"The results in were taken from a single - shot evaluation , unlike the averages often independent runs in .",0
24535,Hyper - parameter selection :,0
24536,We empirically investigated the impact of the hyper - parameter selections .,0
24537,shows the results .,0
24538,The following observations appear informative for building strong baseline systems :,0
24539,( 1 ) Smaller mini-batch size M and gradient clipping G provided the better performance .,1
24540,"Such settings lead to slower and longer training , but higher performance .",0
24541,"( 2 ) Larger layer size , hidden state dimension , and beam size have little impact on the performance ; our setting , L = 2 , H = 200 , and B = 5 looks adequate in terms of speed / performance trade - off .",1
24542,Input unit selection :,0
24543,"As often demonstrated in the NMT literature , using subword split as input token unit instead of standard tokenized word unit has potential to improve the performance .",1
24544,( e ) shows the results of utilizing subword splits .,0
24545,"Clearly , 8 K and 16K subword splits as input token units significantly degraded the performance .",0
24546,"It seems that the numbers of XX - tags in output and tokens in input should keep consistent for better performance since Seq2seq models look to somehow learn such relationship , and used it during the decoding .",0
24547,"Thus , using subword information as features is one promising approach for leveraging subword information into constituency parsing .",1
24548,lists the reported constituency parsing scores on PTB that were recently published in the literature .,0
24549,We split the results into three categories .,0
24550,"The first category ( top row ) contains the results of the methods that were trained only from the pre-defined training data ( PTB Sec.02 - 21 ) , without any additional resources .",0
24551,"The second category ( middle row ) consists of the results of methods that were trained from the pre-defined PTB training data as well as those listed in the top row , but incorporating word embeddings obtained from a large - scale external corpus to initialize the encoder embedding layer .",0
24552,"The third category ( bottom row ) shows the performance of the methods that were trained using high - confidence , auto - parsed trees in addition to the pre-defined PTB training data .",0
24553,Comparison to current top systems,0
24554,Our Seq2seq approach successfully achieved the competitive level as the current top - notch methods : RNNG and its variants .,1
24555,"Note here that , as described in , RNNG uses Berkeley parser 's mapping rules for effectively handling singleton words in the training corpus .",0
24556,"In contrast , we demonstrated that Seq2seq models have enough power to achieve a competitive stateof - the - art performance without leveraging such task - dependent knowledge .",0
24557,"Moreover , they need no explicit information of parse tree structures , transition states , stacks , ( Stanford or Berkeley ) mapping rules , or external silver training data during the model training except general purpose word embeddings as initial values .",0
24558,These observations from our experiments imply that recently developed Seq2seq models have enough ability to implicitly learn parsing structures from linearized parse trees .,0
24559,Our results argue that Seq2seq models can be a strong baseline for constituency parsing .,0
24560,Conclusion,0
24561,This paper investigated how well general purpose Seq2seq models can achieve the higher performance of constituency parsing as a strong baseline method .,0
24562,"We incorporated several generic techniques to enhance Seq2seq models , such as incorporating subword features , and output length controlling .",0
24563,"We experimentally demonstrated that by applying ensemble and LM - reranking techniques , a general purpose Seq2seq model achieved almost the same performance level as the state - of - the - art constituency parser without any task - specific or explicit tree structure information .",0
24564,title,0
24565,In- Order Transition - based Constituent Parsing,1
24566,abstract,0
24567,Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing .,1
24568,"The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree , where bottom - up strategies and top - down strategies take post-order and pre-order traversal over trees , respectively .",0
24569,"Bottom - up parsers benefit from rich features from readily built partial parses , but lack lookahead guidance in the parsing process ; top - down parsers benefit from non-local guidance for local decisions , but rely on a strong encoder over the input to predict a constituent hierarchy before its construction .",0
24570,"To mitigate both issues , we propose a novel parsing system based on in - order traversal over syntactic trees , designing a set of transition actions to find a compromise between bottom - up constituent information and top - down lookahead information .",0
24571,"Based on stack - LSTM , our psycholinguistically motivated constituent parsing system achieves 91.8 F 1 on the WSJ benchmark .",0
24572,"Furthermore , the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking , which are the best results on the WSJ benchmark .",0
24573,Introduction,0
24574,Transition - based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences .,0
24575,"There are two popular transition - based constituent parsing systems , namely bottom - up parsing and top - down parsing .",0
24576,The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree .,0
24577,The process of bottom - up parsing can be regarded as post -order traversal over a constituent tree .,0
24578,"For example , given the sentence in , a bottom - up shift - reduce parser takes the action sequence in ( a ) 1 to build the output , where the word sequence "" The little boy "" is first read , and then an NP recognized for the word sequence .",0
24579,"After the system reads the verb "" likes "" and its subsequent NP , a VP is recognized .",0
24580,The full order of recognition for the tree nodes is 3 ? 4 ? 5 ? 2 ? 7 ? 9 ? 10 ? 8 ? 6 ? 11 ? 1 .,0
24581,"When making local decisions , rich information is available from readily built partial trees , which contributes to local dis ambiguation .",0
24582,"However , there is lack of top - down guidance from lookahead information , which can be useful .",0
24583,"In addition , binarization must be applied to trees , as shown in , to ensure a constant number of actions , and to take advantage of lexical head information .",0
24584,"However , such binarization requires a set of language - specific rules , which hampers adaptation of parsing to other languages .",0
24585,"On the other hand , the process of top - down parsing can be regarded as pre-order traversal over a tree .",0
24586,"Given the sentence in shift - reduce parser takes the action sequence in ( b ) to build the output , where an S is first made and then an NP is generated .",0
24587,"After that , the system makes a decision to read the word sequence "" The little boy "" to complete the NP .",0
24588,The full order of recognition for the tree nodes is 1 ? 2 ? 3 ? 4 ? 5 ? 6 ? 7 ? 8 ? 9 ? 10 ? 11 .,0
24589,The top - down lookahead guidance contributes to non-local dis ambiguation .,0
24590,"However , it is difficult to generate a constituent before its sub constituents have been realized , since no explicit features can be extracted from their subtree structures .",0
24591,"Thanks to the use of recurrent neural networks , which make it possible to represent a sentence globally before syntactic tree construction , seminal work of neural top - down parsing directly generates bracketed constituent trees using sequence - to - sequence models . ( c ) in - order system :",0
24592,Action sequences of three types of transition constituent parsing system .,0
24593,"Details of the action system are introduced in Section 2.1 , Section 2.2 and Section 3 , respectively .",0
24594,transition - based parsing .,0
24595,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .",1
24596,The process of the proposed constituent parsing can be regarded as in - order traversal over a tree .,0
24597,"Given the sentence in , the system takes the action sequence in ( c ) to build the output .",0
24598,"The system reads the word "" The "" and then projects an NP , which is based on bottom - up evidence .",0
24599,"After this , based on the projected NP , the system reads the word sequence "" little boy "" , with top - down guidance from NP .",0
24600,"Similarly , based on the completed constituent "" ( NP The little boy ) "" , the system projects an S , with the bottom - up evidence .",0
24601,"With the Sand the word "" likes "" , the system projects an VP , which can serve as top - down guidance .",0
24602,"The full order of recognition for the tree nodes is 3 ? 2 ? 4 ? 5 ? 1 ? 7 ? 6 ? 9 ? 8 ? 10 ? 11 . Compared to post -order traversal , in - order traversal can potentially resolve non-local ambiguity better by top - down guidance .",0
24603,"Compared to pre-order traversal , in - order traversal can potentially resolve local ambiguity better by bottom - up evidence .",0
24604,"Furthermore , in - order traversal is psycholinguistically motivated .",0
24605,"Empirically , a human reader comprehends sentences by giving lookahead guesses for constituents .",0
24606,"For example , when reading a word "" likes "" , a human reader could guess that it could be a start of a constituent VP , instead of waiting to read the object "" red tomatoes "" , which is the procedure of a bottom - up system .",0
24607,We compare our system with the two baseline systems ( i.e. a top - down system and a bottomup system ) under the same neural transition - based framework of .,0
24608,"Our final models outperform both of the bottom - up and top - down transition - based constituent parsing by achieving a 91.8 F 1 in English and a 86.1 F 1 in Chinese for greedy fully - supervised parsing , respectively .",0
24609,"Furthermore , our final model obtains a 93.6 F 1 with supervised reranking ( Choe and Charniak , 2016 ) and a 94.2 F 1 with semi-supervised reranking , achieving the state - of - the - art results on constituent parsing on the English benchmark .",0
24610,"By converting to Stanford dependencies , our final model achieves the state - of the - art results on dependency parsing by obtaining a 96.2 % UAS and a 95.2 % LAS .",0
24611,"To our knowledge , we are the first to systematically compare top - down and bottom - up constituent parsing under the same neural framework .",0
24612,We release our code at https://github.com/LeonCrashCode/InOrderParser .,1
24613,Transition - based constituent parsing,0
24614,"Transition - based constituent parsing takes a leftto - right scan of the input sentence , where a stack is used to maintain partially constructed phrasestructures , while the input words are stored in a buffer .",0
24615,"Formally , a state is defined as [ ? , i , f ] , where ?",0
24616,"is the stack , i is the front index of the buffer , and f is a boolean value showing that the parsing is finished .",0
24617,"At each step , a transition action is applied to consume an input word or construct a new phrasestructure .",0
24618,Different parsing systems employ their own sets of actions .,0
24619,Bottom - up system,0
24620,We take the bottom - up system of as our bottom - up baseline .,0
24621,"Given a state , the set of transition actions are :",0
24622,"SHIFT : pop the front word from the buffer , and push it onto the stack .",0
24623,"REDUCE - L /R - X : pop the top two constituents off the stack , combine them into a new constituent with label X , and push the new constituent onto the stack .",0
24624,"UNARY - X : pop the top constituent off the stack , raise it to a new constituent with label X , and push the new constituent onto the stack .",0
24625,FINISH : pop the root node off the stack and end parsing .,0
24626,The bottom - up parser can be summarized as the deductive system in ( a ) .,0
24627,"Given the sentence with the binarized syntactic tree in ( b ) , the sequence of actions SHIFT , SHIFT , SHIFT , REDUCE - R - NP , REDUCE - R - NP , SHIFT , SHIFT , SHIFT , REDUCE - R - NP , REDUCE - L - VP , SHIFT , REDUCE - L-S , REDUCE - R - S and FINISH , can be used to construct its constituent tree .",0
24628,Top - down system,0
24629,We take the top - down system of as our top - down baseline .,0
24630,"Given a state , the set of transition actions are :",0
24631,"SHIFT : pop the front word from the buffer , and push it onto the stack .",0
24632,NT - X : open a nonterminal with label X on top of the stack .,0
24633,"REDUCE : repeatedly pop completed subtrees or terminal symbols from the stack until an open nonterminal is encountered , and then this open NT is popped and used as the label of a new constituent that has the popped subtrees as",0
24634,( c ) in - order system its children .,0
24635,This new completed constituent is pushed onto the stack as a single composite item .,0
24636,The deduction system for the process is shown in ( b ),0
24637,"2 . Given the sentence in , the sequence of actions NT - S , NT - NP , SHIFT , SHIFT , SHIFT , REDUCE , NT - VP , SHIFT , NT - NP , SHIFT , SHIFT , REDUCE , REDUCE , SHIFT and REDUCE , can be used to construct its constituent tree .",0
24638,3 In - order system,0
24639,We propose a novel in - order system for transitionbased constituent parsing .,0
24640,"Similar to the bottom - up and top - down systems , the in - order system maintains a stack and a buffer for representing a state .",0
24641,The set of transition actions are defined as :,0
24642,"SHIFT : pop the front word from the buffer , and push it onto the stack .",0
24643,PJ - X : project a nonterminal with label X on top of the stack .,0
24644,"REDUCE : repeatedly pop completed subtrees or terminal symbols from the stack until a projected nonterminal encountered , and then this projected nonterminal is popped and used as the label of a new constituent .",0
24645,"Furthermore , one more item on the top of stack is popped and inserted as the leftmost child of the new constituent .",0
24646,The popped subtrees are inserted as the rest of the children .,0
24647,This new completed constituent is pushed onto the stack as a single composite item .,0
24648,FINISH : pop the root node off the stack and end parsing .,0
24649,The deduction system for the process is shown in ( c ) .,0
24650,"Given the sentence in , the sequence of actions SHIFT , PJ - NP , SHIFT , SHIFT , REDUCE , PJ - S , SHIFT , PJ - VP , SHIFT , PJ - NP , SHIFT , REDUCE , REDUCE , SHIFT , REDUCE , FIN - ISH can be used to construct its constituent tree .",0
24651,Variants,0
24652,"The in - order system can be generalized into variants by modifying k , the number of leftmost nodes traced before the parent node .",0
24653,"For example , given the tree "" ( S ab c d ) "" , the traversal is "" a Sb c d "" if k = 1 while the traversal is "" a b Sc d "" if k = 2 .",0
24654,We name each variant with a certain k value as k - in - order systems .,0
24655,"In this paper , we only investigate the in - order system with k = 1 , the 1 - inorder system .",0
24656,"Note that the top - down parser can be regarded as a special case of a generalized version of the in - order parser with k = 0 , and the bottom - up parser can be regarded as a special case with k = ?.",0
24657,Neural parsing model,0
24658,"We employ the stack - LSTM parsing model of for the three types of transition - based parsing systems in Section 2.1 , 2.2 and 3 , respectively , where a stack - LSTM is used to represent the stack , a stack - LSTM is used to represent the buffer , and a vanilla LSTM is used to represent the action history , as shown in .",0
24659,Word representation,0
24660,"We follow , representing each word using three different types of embeddings , including pretrained word embedding , e w i , which is not fine - tuned during the training of the parser , randomly initialized embeddings e w i , which is finetuned , and the randomly initialized part - of - speech embeddings , which is fine - tuned .",0
24661,"The three embeddings are concatenated , and then fed to nonlinear layer to derive the final word embedding :",0
24662,"where W input and b input are model parameters , w i and pi denote the form and the POS tag of the ith input word , respectively , and f is an nonlinear function .",0
24663,"In this paper , we use ReLu for f . is for binarized trees , where "" NP - r* "" means that "" little boy "" is a non-completed noun phrase with head "" boy "" .",0
24664,Stack representation,0
24665,"We employ a bidirectional LSTM as the composition function to represent constituents on stack 3 . where e nt is the representation of a non-terminal , s j , j ?",0
24666,"[ 0 , m ] is the jth child node , and m is the number of the child nodes .",0
24667,"For bottom - up parsing , we make use of the head information in the composition function by requiring the order that the head node is always before the non-head node in the bidirectional LSTM , as shown in where sh and so is the representation of the head and the non-head node , respectively .",0
24668,Greedy action classification,0
24669,"Given a sentence w 0 , w 1 , ... , w n?1 , where w i is the ith word , and n is the length of the sentence , our parser makes local action classification decisions incrementally .",0
24670,"For the kth parsing state like [ s j , ... , s 1 , s 0 , i , false ] , the probability distribution of the current action p is :",0
24671,"where W and bare model parameters , the representation of stack information h stk is :",0
24672,the representation of buffer information h buf is :,0
24673,"x is the word representation , and the representation of action history h ah is :",0
24674,"h ah = LSTM [e act k ?1 , e act k?2 , ... , e act 0 ] , where e act k?1 is the representation of action in the k - 1th parsing state .",0
24675,Training,0
24676,"Our models are trained to minimize a cross - entropy loss objective with an l 2 regularization term , defined by",0
24677,where ?,0
24678,"is the set of parameters , pa ij is the probability of the jth action in the ith training example given by the model and ?",0
24679,is a regularization hyperparameter (? = 10 ?6 ) . We use stochastic gradient descent with a 0.1 initialized learning rate with a 0.05 learning rate decay .,0
24680,Experiments,0
24681,Data,0
24682,"We empirically compare our bottom - up , top - down and in - order parsers .",0
24683,The experiments are carried out on both English and Chinese .,0
24684,"For English data , we use the standard benchmark of WSJ sections in PTB , where the Sections 2 - 21 are taken for training data , Section 22 for development data and Section 23 for testing both dependency parsing and constituency parsing .",0
24685,We adopt the pretrained English word embeddings generated on the AFP portion of English Gigaword .,0
24686,"For Chinese data , we use Version 5.1 of the Penn Chinese Treebank ( CTB ) .",0
24687,"We use articles 001-270 and 440-1151 for training , articles 301-325 for system development , and articles 271-300 for final performance evaluation .",0
24688,We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus .,0
24689,"The POS tags in both the English data and the Chinese data are automatically assigned the same as the work of , using Stanford tagger .",0
24690,We follow the work of Choe and and adopt the AFP portion of the English Gigaword as the extra resources for the semi-supervised reranking .,0
24691,Settings,0
24692,Hyper-parameters,0
24693,"For both English and Chinese experiments , we use the same hyper - parameters as the work of without further optimization , as shown in .",0
24694,Reranking experiments,1
24695,Following the same reranking setting of shows the development results of the three parsing systems .,0
24696,The bottom - up system performs slightly better than the top - down system .,1
24697,The inorder system outperforms both the bottom - up and the top - down system .,1
24698,shows the parsing results on the English test dataset .,0
24699,"We find that the bottom - up parser and the top - down parser have similar results under the greedy setting , and the in - order parser outperforms both of them .",1
24700,"Also , with supervised reranking , the in - order parser achieves the best results .",0
24701,English constituent results,1
24702,"We compare our models with previous work , as shown in Table 4 .",0
24703,"With the fully - supervise setting 5 , the inorder parser outperforms the state - of - the - art discrete parser , the state - of - the - art neural parsers",1
24704,"Here , we only consider the work of a single model .",0
24705,Development experiments,0
24706,Results,0
24707,Model,0
24708,Chinese dependency results,1
24709,"As shown in , by converting the results to dependencies 6 , our final model achieves the best results among transitionbased parsing , and obtains comparable results to the state - of - the - art graph - based models . 85.5 84.0 87.7 86.2",1
24710,UAS LAS,0
24711,Analysis,0
24712,"We analyze the results of Section 23 in WSJ given by our model ( i.e. in - order parser ) and two baseline models ( i.e. the bottom - up parser and the top - down parser ) against the sentence length , the span length and the constituent type , respectively .",0
24713,shows the F 1 scores of the three parsers on sentences of different lengths .,0
24714,"Compared to the topdown parser , the bottom - up parser performs better on the short sentences with the length falling in the range .",0
24715,"This is likely because the bottomup parser takes advantages of rich local features from partially - built trees , which are useful for parsing short sentences .",0
24716,"However , these local structures are can be insufficient for parsing long sentences due to error propagation .",0
24717,"On the other hand , the top - down parser obtains better results on long sentences with the length falling in the range .",0
24718,"This is because , as the length of sentences increase , lookahead features become rich and they could be correctly represented by the LSTM , which is beneficial for parsing non-local structures .",0
24719,"We find that the in - order parser performs the best for both short and long sentences , showing the advantages of integrating bottom - up and top - down information .",0
24720,shows the F 1 scores of the three parsers on spans of different lengths .,0
24721,The trend of performances of the two baseline parsers are similar .,0
24722,"Compared to the baseline parsers , the in - order parser obtains significant improvement on long spans .",0
24723,"Linguistically , it is because the in - order traversal , ( over a tree ) allows constituent types of spans to be correctly projected based on the information of the beginning ( leftmost nodes ) of the spans .",0
24724,"Then the projected constituents constrain long span construction , which is different from the top - down parser , generating constituent types of spans without trace of the spans .",0
24725,shows the F 1 scores of the three parsers on frequent constituent types .,0
24726,"The bottom - up parser performs better than the top - down parser on con-stituent types including NP , S , SBAR , QP .",0
24727,"We find that the prediction of these constituent types requires , explicitly , modeling of bottom - up structures .",0
24728,Influence of sentence length,0
24729,Influence of span length,0
24730,Influence of constituent type,0
24731,"In other words , bottom - up information is necessary for us to know if the span can be a noun phrase ( NP ) or sentence ( S ) , for example .",0
24732,"On the other hand , the top - down parser has better performance on WHNP , which is likely because a WHNP starts with a certain question word , making the prediction easy without bottom - up information .",0
24733,"The in - order parser performs the best on all constituent types , demonstrating that the in - order parser can benefit from both bottom - up and top - down information .",0
24734,Examples,0
24735,"We give output examples from the test set to qualitatively compare the performances of the three parsers using the fully - supervised model without reranking , as shown in .",0
24736,"For example , given the Sentence # 2006 , the bottom - up and the in - order parsers give both correct results .",0
24737,"However , the top - down parser makes an incorrect decision to generate an S , leading to subsequent incorrect decisions on VP to complete S. Sentence pattern ambiguity allows topdown guidance to over-parsing the sentence by recognizing the word "" Plans "" as a verb , while more bottom - up information is useful for the local dis ambiguation .",0
24738,"Given the Sentence # 308 , the bottom - up parser prefers construction of local constituents such as "" once producers and customers "" , ignoring the possible clause SBAR , however , which is captured by the in - order parser .",0
24739,"The parser projects a constituent SBAR from the word "" stick "" and continues to complete the clause , showing that top - down lookahead information is necessary for non-local dis ambiguation .",0
24740,"The in - order parser gives the correct output for the Sentence # 2066 and the Sentence # 308 , showing that it can benefit from bottom - up and top - down information .",0
24741,... whether the new posted prices will stick once producers and customers start to haggle .,0
24742,Gold ... ( VP will ( VP stick ( SBAR once ( S ( NP producers and customers ) ( VP start ( S ... ) ) ) ) ) ),0
24743,... Top- down ... ( VP will ( VP stick ( SBAR once ( S ( NP producers and customers ) ( VP start ( S ... ) ) ) ) ) ) ... Bottom - up ... ( VP will ( VP stick ( NP once producers and customers ) ) ),0
24744,... ),0
24745,... In-order ... ( VP will ( VP stick ( SBAR once ( S ( NP producers and customers ) ( VP start ( S ... ) ) ) ) ) ),0
24746,...,0
24747,Sent # 1715,0
24748,This has both made investors uneasy and the corporations more vulnerable .,0
24749,Gold ( S ( NP This ) ( VP has ( VP both made ( S ( S investors uneasy ) and ( S the corporations ... ) ) ) ) .) Top- down ( S ( S ( NP This ) ( VP has ( S ( NP both ) ( VP made investors uneasy ) ) ) ) and ( S the corporations ... ) .) Bottom - up ( S ( S ( NP This ) ( VP has ( S both ( VP made investors uneasy ) ) ) ) and ( S the corporations ... ) .),0
24750,In-order ( S ( NP This ) ( VP has both ( VP made ( S ( S investors uneasy ) and ( S the corporations ... ) ) ) ) . ) :,0
24751,Output examples of the three parsers on the English test set .,0
24752,Incorrect constituents are marked in red .,0
24753,"In the Sentence # 1715 , there are coordinated objects such as "" investors uneasy "" and "" the corporations more vulnerable "" .",0
24754,All of the three parsers can recognize coordination .,0
24755,"However , the top - down and the bottom - up parsers incorrectly recognize the "" This has both made investors uneasy "" as a complete sentence .",0
24756,"The top - down parser incorrectly generates S , marked in red , at a early stage , leaving no choice but to follow this incorrect non-terminal .",0
24757,The bottom - up parser without lookahead information makes incorrect local decisions .,0
24758,"By contrast , the in - order parser reads the word "" and "" and projects a non-terminal S for coordination after completing "" ( S investors uneasy ) "" .",0
24759,"On the other hand , the inorder parser is confused by projecting for the word "" made "" or the word "" both "" into an VP , which we think could be addressed by using a in - order system variant with k =2 described in Section 3 .",0
24760,Related work,0
24761,Our work is related to left corner parsing .,0
24762,"Rosenkrantz and formalize this in automata theory , which have appeared frequently in the compiler literature .",0
24763,apply the strategy into parsing .,0
24764,Typical works investigate the transformation of syntactic trees based on left - corner rules .,0
24765,"In contrast , we propose a novel general transition - based in - order constituent parsing system .",0
24766,"Neural networks have achieved the state - of - the - art for parsing under various grammar formalisms , including dependency , constituent , and CCG parsing .",0
24767,Seminal work employs transition - based methods .,0
24768,This method has been extended by investigating more complex representations of configurations for constituent parsing .,0
24769,"employ stack - LSTM onto the top - down system , which is the same as our topdown parser .",0
24770,employ tree - LSTM to model the complex representation in the stack in bottom - up system .,0
24771,We are the first to investigate in - order traversal by designing a novel transition - based system under the same neural structure model framework .,0
24772,Conclusion,0
24773,"We proposed a novel psycho-linguistically motivated constituent parsing system based on the inorder traversal over syntactic trees , aiming to find a compromise between bottom - up constituent information and top - down lookahead information .",0
24774,"On the standard WSJ benchmark , our in - order system outperforms bottom - up parsing on a non-local ambiguity and top - down parsing on local decision .",0
24775,The resulting parser achieves the state - of - the - art constituent parsing results by obtaining 94.2 F 1 and dependency parsing results by obtaining 96.2 % UAS and 95.2 % LAS .,0
24776,422,0
24777,title,0
24778,Grammar as a Foreign Language,0
24779,abstract,0
24780,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,1
24781,"As a result , the most accurate parsers are domain specific , complex , and inefficient .",0
24782,"In this paper we show that the domain agnostic attention - enhanced sequence - to - sequence model achieves state - of - the - art results on the most widely used syntactic constituency parsing dataset , when trained on a large synthetic corpus that was annotated using existing parsers .",0
24783,"It also matches the performance of standard parsers when trained only on a small human - annotated dataset , which shows that this model is highly data - efficient , in contrast to sequence - to - sequence models without the attention mechanism .",0
24784,"Our parser is also fast , processing over a hundred sentences per second with an unoptimized CPU implementation .",0
24785,* Equal contribution 1 arXiv:1412.7449v3 [ cs. CL ] 9 Jun 2015,0
24786,Introduction,0
24787,Syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications .,0
24788,"This problem has been the subject of intense research for decades , and as a result , there exist highly accurate domain - specific parsers .",0
24789,"The computational requirements of traditional parsers are cubic in sentence length , and while linear - time shift - reduce constituency parsers improved in accuracy in recent years , they never matched state - of - the - art .",0
24790,"Furthermore , standard parsers have been designed with parsing in mind ; the concept of a parse tree is deeply ingrained into these systems , which makes these methods inapplicable to other problems .",0
24791,"Recently , Sutskever et al .",0
24792,"introduced a neural network model for solving the general sequenceto - sequence problem , and Bahdanau et al.",0
24793,proposed a related model with an attention mechanism that makes it capable of handling long sequences well .,0
24794,"Both models achieve state - of - the - art results on large scale machine translation tasks ( e.g. , ) .",0
24795,"Syntactic constituency parsing can be formulated as a sequence - to - sequence problem if we linearize the parse tree ( cf. ) , so we can apply these models to parsing as well .",0
24796,Our early experiments focused on the sequence - to - sequence model of Sutskever et al ..,0
24797,"We found this model to work poorly when we trained it on standard human - annotated parsing datasets ( 1M tokens ) , so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser .",1
24798,"To our surprise , the sequence - to - sequence model matched the BerkeleyParser that produced the annotation , having achieved an F 1 score of 90.5 on the test set ( section 23 of the WSJ ) .",0
24799,We suspected that the attention model of Bahdanau et al.,0
24800,might be more data efficient and we found that it is indeed the case .,0
24801,"We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .",1
24802,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .",1
24803,We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,1
24804,"This result did not require an ensemble , and as a result , the parser is also very fast .",0
24805,An ensemble further improves the score to 92.8 .,0
24806,LSTM + A Parsing Model,0
24807,Let us first recall the sequence - to - sequence LSTM model .,0
24808,The Long Short - Term Memory model of is defined as follows .,0
24809,"Let x t , ht , and mt be the input , control state , and memory state at timestep t.",0
24810,"Given a sequence of inputs ( x 1 , . . . , x T ) , the LSTM computes the h-sequence ( h 1 , . . . , h T ) and the m-sequence ( m 1 , . . . , m T ) as follows .",0
24811,"The operator denotes element - wise multiplication , the matrices W 1 , . . . , W 8 and the vector h 0 are the parameters of the model , and all the nonlinearities are computed element - wise .",0
24812,"I na deep LSTM , each subsequent layer uses the h-sequence of the previous layer for its input sequence x .",0
24813,The deep LSTM defines a distribution over output sequences given an input sequence :,0
24814,"The above equation assumes a deep LSTM whose input sequence is x = ( A 1 , . . . , A TA , B 1 , . . . , B TB ) , so ht denotes t- th element of the h-sequence of topmost LSTM .",0
24815,The matrix W o consists of the vector representations of each output symbol and the symbol ?,0
24816,"b is a Kronecker delta with a dimension for each output symbol , so softmax ( W o h TA +t ) ?",0
24817,Bt is precisely the B t 'th element of the distribution defined by the softmax .,0
24818,Every output sequence terminates with a special end - of - sequence token which is necessary in order to define a distribution over sequences of variable lengths .,0
24819,"We use two different sets of LSTM parameters , one for the input sequence and one for the output sequence , as shown in .",0
24820,Stochastic gradient descent is used to maximize the training objective which is the average over the training set of the log probability of the correct output sequence given the input sequence .,0
24821,Attention Mechanism,0
24822,An important extension of the sequence - to - sequence model is by adding an attention mechanism .,0
24823,"We adapted the attention model from which , to produce each output symbol B t , uses an attention mechanism over the encoder LSTM states .",0
24824,"Similar to our sequence - to - sequence model described in the previous section , we use two separate LSTMs ( one to encode the sequence of input words A i , and another one to produce or decode the output symbols Bi ) .",0
24825,"Recall that the encoder hidden states are denoted ( h 1 , . . . , h TA ) and we denote the hidden states of the decoder by ( d 1 , . . . , d TB ) := ( h TA +1 , . . . , h TA +T B ) .",0
24826,"To compute the attention vector at each output time t over the input words ( 1 , . . . , TA ) we define :",0
24827,"The vector v and matrices W 1 , W 2 are learnable parameters of the model .",0
24828,The vector u t has length TA and it s i - th item contains a score of how much attention should be put on the i - th hidden encoder state hi .,0
24829,These scores are normalized by softmax to create the attention mask at over encoder hidden states .,0
24830,"In all our experiments , we use the same hidden dimensionality ( 256 ) at the encoder and the decoder , so v is a vector and W 1 and W 2 are square matrices .",0
24831,"Lastly , we concatenate d t with d t , which becomes the new hidden state from which we make predictions , and which is fed to the next time step in our recurrent model .",0
24832,"In Section 4 we provide an analysis of what the attention mechanism learned , and we visualize the normalized attention vector at for all tin .",0
24833,Linearizing Parsing Trees,0
24834,"To apply the model described above to parsing , we need to design an invertible way of converting the parse tree into a sequence ( linearization ) .",0
24835,"We do this in a very simple way following a depth - first traversal order , as depicted in .",0
24836,We use the above model for parsing in the following way .,0
24837,"First , the network consumes the sentence in a left - to - right sweep , creating vectors in memory .",0
24838,"Then , it outputs the linearized parse tree using information in these vectors .",0
24839,"As described below , we use 3 LSTM layers , reverse the input sentence and normalize part - of - speech tags .",0
24840,"An example run of our LSTM + A model on the sentence "" Go. "" is depicted in ( top gray edges illustrate attention ) .",0
24841,Parameters and Initialization,0
24842,Sizes .,0
24843,"In our experiments we used a model with 3 LSTM layers and 256 units in each layer , which we call LSTM + A .",1
24844,Our input vocabulary size was 90 K and we output 128 symbols .,0
24845,Dropout .,0
24846,"Training on a small dataset we additionally used 2 dropout layers , one between LSTM 1 and LSTM 2 , and one between LSTM 2 and LSTM 3 .",1
24847,We call this model LSTM + A+D .,0
24848,POS - tag normalization .,0
24849,"Since part - of - speech ( POS ) tags are not evaluated in the syntactic parsing F1 score , we replaced all of them by "" XX "" in the training data .",0
24850,"This improved our F1 score by about 1 point , which is surprising :",0
24851,"For standard parsers , including POS tags in training data helps significantly .",0
24852,All experiments reported below are performed with normalized POS tags .,0
24853,Input reversing .,0
24854,"We also found it useful to reverse the input sentences but not their parse trees , similarly to .",0
24855,Not reversing the input had a small negative impact on the F 1 score on our development set ( about 0.2 absolute ) .,0
24856,All experiments reported below are performed with input reversing .,0
24857,Pre-training word vectors .,0
24858,The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word - vector embeddings .,1
24859,We pre-trained skip - gram embeddings of size 512 using word2vec [ 6 ] on a 10B - word corpus .,1
24860,"These embeddings were used to initialize our network but not fixed , they were later modified during training .",0
24861,We discuss the impact of pre-training in the experimental section .,0
24862,We do not apply any other special preprocessing to the data .,0
24863,"In particular , we do not binarize the parse trees or handle unaries in any specific way .",0
24864,We also treat unknown words in a naive way : we map all words beyond our 90 K vocabulary to a single UNK token .,0
24865,"This potentially underestimates our final results , but keeps our framework task - independent .",0
24866,3 Experiments,0
24867,Training Data,0
24868,We trained the model described above on 2 different datasets .,0
24869,"For one , we trained on the standard WSJ training dataset .",0
24870,"This is a very small training set by neural network standards , as it contains only 40K sentences ( compared to 60 K examples even in MNIST ) .",0
24871,"Still , even training on this set , we managed to get results that match those obtained by domain - specific parsers .",0
24872,"To exceed the previous state - of - the - art , we created another , larger training set of ? 11 M parsed sentences ( 250M tokens ) .",0
24873,"First , we collected all publicly available treebanks .",0
24874,"We used the OntoNotes corpus version 5 , the English Web Treebank and the updated and corrected Question Treebank .",0
24875,Note that the popular Wall Street Journal section of the Penn Treebank is part of the OntoNotes corpus .,0
24876,"In total , these corpora give us ? 90 K training sentences ( we held out certain sections for evaluation , as described below ) .",0
24877,"In addition to this gold standard data , we use a corpus parsed with existing parsers using the "" tri-training "" approach of .",0
24878,"In this approach , two parsers , our reimplementation of Berkeley - Parser and a reimplementation of ZPar , are used to process unlabeled sentences sampled from news appearing on the web .",0
24879,We select only sentences for which both parsers produced the same parse tree and re-sample to match the distribution of sentence lengths of the WSJ training corpus .,0
24880,Re-sampling is useful because parsers agree much more often on short sentences .,0
24881,"We call the set of ? 11 million sentences selected in this way , together with the ? 90 K golden sentences described above , the high - confidence corpus .",0
24882,"In earlier experiments , we only used one parser , our reimplementation of BerkeleyParser , to create a corpus of parsed sentences .",0
24883,In that case we just parsed ? 7 million senteces from news appearing on the web and combined these parsed sentences with the ? 90 K golden corpus described above .,0
24884,We call this the BerkeleyParser corpus .,0
24885,Evaluation,0
24886,We use the standard EVALB tool 2 for evaluation and report F 1 scores on our developments set ( section 22 of the Penn Treebank ) and the final test set ( section 23 ) in .,0
24887,"First , let us remark that our training setup differs from those reported in previous works .",0
24888,"To the best of our knowledge , no standard parsers have ever been trained on datasets numbering in the hundreds of millions of tokens , and it would be hard to do due to efficiency problems .",0
24889,"We therefore cite the semi-supervised results , which are analogous in spirit but useless data .",0
24890,shows performance of our models on the top and results from other papers at the bottom .,0
24891,"We compare to variants of the BerkeleyParser that use self - training on unlabeled data , or built an ensemble of multiple parsers , or combine both techniques .",0
24892,"We also include the best linear - time parser in the literature , the transition - based parser of .",0
24893,"It can be seen that , when training on WSJ only , a baseline LSTM does not achieve any reasonable score , even with dropout and early stopping .",0
24894,But a single attention model gets to 88.3 and an ensemble of 5 LSTM + A+D models achieves 90.5 matching a single - model BerkeleyParser on WSJ 23 .,1
24895,"When trained on the large high - confidence corpus , a single LSTM + A model achieves 92.5 and so outperforms not only the best single model , but also the best ensemble result reported previously .",1
24896,An ensemble of 5 LSTM+ A models further improves this score to 92.8 .,1
24897,Generating well - formed trees .,0
24898,"The LSTM + A model trained on WSJ dataset only produced malformed trees for 25 of the 1700 sentences in our development set ( 1.5 % of all cases ) , and the model trained on full high - confidence dataset did this for 14 sentences ( 0.8 % ) .",1
24899,"In these few cases where LSTM + A outputs a malformed tree , we simply add brackets to either the beginning or the end of the tree in order to make it balanced .",0
24900,It is worth noting that all 14 cases where LSTM + A produced unbalanced trees were sentences or sentence fragments that did not end with proper punctuation .,0
24901,"There were very few such sentences in the training data , so it is not a surprise that our model can not deal with them very well .",0
24902,Score by sentence length .,0
24903,An important concern with the sequence - to - sequence LSTM was that it may not be able to handle long sentences well .,0
24904,"We determine the extent of this problem by partitioning the development set by length , and evaluating BerkeleyParser , a baseline LSTM model without attention , and LSTM + A on sentences of each length .",0
24905,"The results , presented in , are surprising .",0
24906,"The difference between the F 1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser , 1.7 for the baseline LSTM , and 0.7 for LSTM + A .",1
24907,"So already the baseline LSTM has similar performance to the BerkeleyParser , it degrades with length only slightly .",0
24908,"Surprisingly , LSTM +A shows less degradation with length than BerkeleyParser - a full O ( n 3 ) chart parser that uses a lot more memory .",0
24909,Beam size influence .,0
24910,Our decoder uses a beam of a fixed size to calculate the output sequence of labels .,0
24911,We experimented with different settings for the beam size .,0
24912,It turns out that it is almost irrelevant .,0
24913,"We report report results that use beam size 10 , but using beam size 2 only lowers the F 1 score of LSTM +",0
24914,"A on the development set by 0.2 , and using beam size 1 lowers it by 0.5 ( to 92.8 ) .",0
24915,Beam sizes above 10 do not give any additional improvements .,0
24916,Dropout influence .,0
24917,We only used dropout when training on the small WSJ dataset and its influence was significant .,0
24918,A single LSTM +,0
24919,"A model only achieved an F 1 score of 86.5 on our development set , that is over 2 points lower than the 88.7 of a LSTM + A+D model .",0
24920,Pre-training influence .,0
24921,"As described in the previous section , we initialized the word - vector embedding with pre-trained word vectors obtained from word2 vec .",0
24922,"To test the influence of this initialization , we trained a LSTM +",0
24923,"A model on the high - confidence corpus , and a LSTM + A+ D model on the WSJ corpus , starting with randomly initialized word - vector embeddings .",0
24924,The F1 score on our development set was 0.4 lower for the LSTM + A model ( 92.9 vs 93.3 ) and 0.3 lower for the LSTM + A+D model ( 88.4 vs 88.7 ) .,0
24925,So the effect of pre-training is consistent but small .,0
24926,Performance on other datasets .,0
24927,The WSJ evaluation set has been in use for 20 years and is commonly used to compare syntactic parsers .,0
24928,But it is not representative for text encountered on the web .,0
24929,"Even though our model was trained on a news corpus , we wanted to check how well it generalizes to other forms of text .",0
24930,"To this end , we evaluated it on two additional datasets :",0
24931,QTB 1000 held - out sentences from the Question Treebank ;,0
24932,WEB the first half of each domain from the English Web Treebank ( 8310 sentences ) .,0
24933,LSTM + A trained on the high - confidence corpus ( which only includes text from news ) achieved an F 1 score of 95.7 on QTB and 84.6 on WEB .,1
24934,Our score on WEB is higher both than the best score reported in ( 83.5 ) and the best score we achieved with an in - house reimplementation of Berkeley Parser trained on human - annotated data ( 84.4 ) .,0
24935,We managed to achieve a slightly higher score ( 84.8 ) with the in - house Berkeley Parser trained on a large corpus .,0
24936,"On QTB , the 95.7 score of LSTM + A is also lower than the best score of our in - house BerkeleyParser ( 96.2 ) .",0
24937,"Still , taking into account that there were only few questions in the training data , these scores show that LSTM +",0
24938,A managed to generalize well beyond the news language it was trained on .,0
24939,Parsing speed .,0
24940,"Our LSTM + A model , running on a multi - core CPU using batches of 128 sentences on a generic unoptimized decoder , can parse over 120 sentences from WSJ per second for sentences of all lengths ( using beam - size 1 ) .",0
24941,"This is better than the speed reported for this batch size in of at 100 sentences per second , even though they run on a GPU and only on sentences of under 40 words .",0
24942,"Note that they achieve 89.7 F1 score on this subset of sentences of section 22 , while our model at beam - size 1 achieves a score of 93.7 on this subset .",0
24943,Analysis,0
24944,"As shown in this paper , the attention mechanism was a key component especially when learning from a relatively small dataset .",0
24945,"We found that the model did not overfit and learned the parsing function from scratch much faster , which resulted in a model which generalized much better than the plain LSTM without attention .",0
24946,One of the most interesting aspects of attention is that it allows us to visualize to interpret what the model has learned from the data .,0
24947,"For example , in it is shown that for translation , attention learns an alignment function , which certainly should help translating from English to French .",0
24948,shows an example of the attention model trained only on the WSJ dataset .,0
24949,"From the attention matrix , where each column is the attention vector over the inputs , it is clear that the model focuses quite sharply on one word as it produces the parse tree .",0
24950,"It is also clear that the focus moves from the first word to the last monotonically , and steps to the right deterministically when a word is consumed .",0
24951,"On the bottom of we see where the model attends ( black arrow ) , and the current output being decoded in the tree ( black circle ) .",0
24952,"This stack procedure is learned from data ( as all the parameters are randomly initialized ) , but is not quite a simple stack decoding .",0
24953,"Indeed , at the input side , if the model focuses on position i , that state has information for all words after i ( since we also reverse the inputs ) .",0
24954,"It is worth noting that , in some examples ( not shown here ) , the model does skip words .",0
24955,Related Work,0
24956,The task of syntactic constituency parsing has received a tremendous amount of attention in the last 20 years .,0
24957,Traditional approaches to constituency parsing rely on probabilistic context - free grammars ( CFGs ) .,0
24958,The focus in these approaches is on devising appropriate smoothing techniques for highly lexicalized and thus rare events or carefully crafting the model structure .,0
24959,partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model .,0
24960,"However , their model still depends on a CFG backbone and is thereby potentially restricted in its capacity .",0
24961,"Early neural network approaches to parsing , for example by also relied on strong linguistic insights .",0
24962,introduced Incremental Sigmoid Belief Networks for syntactic parsing .,0
24963,"By constructing the model structure incrementally , they are able to avoid making strong independence assumptions but inference becomes intractable .",0
24964,"To avoid complex inference methods , propose a recurrent neural network where parse trees are decomposed into a stack of independent levels .",0
24965,"Unfortunately , this decomposition breaks for long sentences and their accuracy on longer sentences falls quite significantly behind the state - of - the - art .",0
24966,used a tree - structured neural network to score candidate parse trees .,0
24967,Their model however relies again on the CFG assumption and furthermore can only be used to score candidate trees rather than for full inference .,0
24968,"Our LSTM model significantly differs from all these models , as it makes no assumptions about the task .",0
24969,"As a sequence - to - sequence prediction model it is somewhat related to the incremental parsing models , pioneered by and extended by .",0
24970,Such linear time parsers however typically need some task - specific constraints and might buildup the parse in multiple passes .,0
24971,"Relatedly , present excellent parsing results with a single left - to - right pass , but require a stack to explicitly delay making decisions and a parsing - specific transition strategy in order to achieve good parsing accuracies .",0
24972,The LSTM in contrast uses it s short term memory to model the complex underlying structure that connects the input-output pairs .,0
24973,"Recently , researchers have developed a number of neural network models that can be applied to general sequence - to - sequence problems .",0
24974,"was the first to propose a differentiable attention mechanism for the general problem of handwritten text synthesis , although his approach assumed a monotonic alignment between the input and output sequences .",0
24975,"Later , introduced a more general attention model that does not assume a monotonic alignment , and applied it to machine translation , and applied the same model to speech recognition .",0
24976,used a convolutional neural network to encode a variable - sized input sentence into a vector of a fixed dimension and used a RNN to produce the output sentence .,0
24977,Essentially the same model has been used by to successfully learn to generate image captions .,0
24978,"Finally , already in 1990 experimented with applying recurrent neural networks to the problem of syntactic parsing .",0
24979,Conclusions,0
24980,"In this work , we have shown that generic sequence - to - sequence approaches can achieve excellent results on syntactic constituency parsing with relatively little effort or tuning .",0
24981,"In addition , while we found the model of Sutskever et al. to not be particularly data efficient , the attention model of Bahdanau et al .",0
24982,"was found to be highly data efficient , as it has matched the performance of the BerkeleyParser when trained on a small human - annotated parsing dataset .",0
24983,"Finally , we showed that synthetic datasets with imperfect labels can be highly useful , as our models have substantially outperformed the models that have been used to create their training data .",0
24984,We suspect it is the case due to the different natures of the teacher model and the student model : the student model has likely viewed the teacher 's errors as noise which it has been able to ignore .,0
24985,"This approach was so successful that we obtained a new state - of - the - art result in syntactic constituency parsing with a single attention model , which also means that the model is exceedingly fast .",0
24986,This work shows that domain independent models with excellent learning algorithms can match and even outperform domain specific models .,0
24987,title,0
24988,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,0
24989,abstract,0
24990,Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results .,1
24991,"Since direct search in these generative models is difficult , they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward .",0
24992,We first present an algorithm for direct search in these generative models .,0
24993,We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects .,0
24994,"Finally , we show that explicit model combination can improve performance even further , resulting in new state - of - the - art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data .",0
24995,Introduction,0
24996,Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler .,1
24997,Let,0
24998,"Abe a parser that we want to parse with ( here one of the generative models ) , and let B be a base parser that we use to propose candidate parses which are then scored by the less - tractable parser A .",0
24999,We denote this cross - scoring setup by B ?,0
25000,A .,0
25001,The papers above repeatedly saw that the cross - scoring setup B ?,0
25002,A under which their generative models were applied outperformed the standard singleparser setup B ?,0
25003,B .,0
25004,We term this a cross - scoring gain .,0
25005,This paper asks two questions .,0
25006,"First , why do recent discriminative - to - generative cross - scoring se - * Equal contribution .",0
25007,tups B ?,0
25008,A outperform their base parsers B ?,0
25009,Perhaps generative models,0
25010,A are simply superior to the base models B and direct generative parsing ( A ?,0
25011,A ) would be better still if it were feasible .,0
25012,"If so , we would characterize the cross - scoring gain from B ?",0
25013,B to B ?,0
25014,A as a reranking gain .,0
25015,"However , it 's also possible that the hybrid system B ?",0
25016,A shows gains merely from subtle model combination effects .,0
25017,"If so , scoring candidates using some combined score A + B would be even better , which we would characterize as a model combination gain .",0
25018,It might even be the case that B is a better parser over all ( i.e. B ?,0
25019,B outperforms A ? A ) .,0
25020,"Of course , many real hybrids will exhibit both reranking and model combination gains .",0
25021,"In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .",1
25022,"In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?",1
25023,A for these generative parsers A independent of any base parsers .,0
25024,"Our findings suggest the presence of model combination effects in both generative parsers : when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser ( the RNNG discriminative parser , RD ) , performance decreases when compared to using just candidates from the base parser , i.e. , B ? A ?",0
25025,A has lower evaluation performance than B ? A ( Section 3.1 ) .,0
25026,"This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting - there are trees with higher probability under the generative model than any tree proposed by the base parser , but which would decrease evaluation performance if selected .",0
25027,"Because of this , we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems , rather than the generative model being generally superior .",0
25028,"Here we consider our second question : if crossscoring gains are at least partly due to implicit model combination , can we gain even more by combining the models explicitly ?",0
25029,"We find that this is indeed the case : simply taking a weighted average of the scores of both models when selecting a parse from the base parser 's candidate list improves over using only the score of the generative model , in many cases substantially ( Section 3.2 ) .",1
25030,"Using this technique , in combination with ensembling , we obtain new state - of - the - art results on the Penn Treebank : 94.25 F1 when training only on gold parse trees and 94.66 F1 when using external silver data .",0
25031,Decoding in generative neural models,0
25032,"All of the parsers we investigate in this work ( the discriminative parser RD , and the two generative parsers RG and LM , see Section 1 ) produce parse trees in a depth - first , left - to - right traversal , using the same basic actions : NT ( X ) , which opens a new constituent with the non-terminal symbol X ; SHIFT / GEN ( w ) , which adds a word ; and RE - DUCE , which closes the current constituent .",0
25033,"We refer to for a complete description of these actions , and the constraints on them necessary to ensure valid parse trees .",0
25034,"The primary difference between the actions in the discriminative and generative models is that , whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence , the generative models use GEN ( w ) to define a distribution over all possible words win the lexicon .",0
25035,"This stems from the generative model 's definition of a joint probability p ( x , y) over all possible sentences x and parses y .",0
25036,"To use a generative model as a parser , we are interested in finding the maximum probability parse for a given sentence .",0
25037,"This is made more complicated by not having an explicit representation for p ( y|x ) , as we do in the discriminative setting .",0
25038,"However , we can start by applying similar approximate search procedures as are used for the discriminative parser , constraining the set of actions such that it is only possible to produce the observed sentence :",0
25039,"i.e. only allow a GEN ( w ) action when w is the next terminal in the sentence , and prohibit GEN actions if all terminals have been produced .",0
25040,Action - synchronous beam search,0
25041,"Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam or even greedy search , as in the case of RD .",0
25042,"The standard beam search procedure , which we refer to as action - synchronous , maintains a beam of K partially - completed parses that all have the same number of actions taken .",0
25043,"At each stage , a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions .",0
25044,The K highest - probability successors are chosen as the next beam .,0
25045,"Unfortunately , we find that action - synchronous beam search breaks down for both generative models we explore in this work , failing to find parses thatare high scoring under the model .",0
25046,This stems from the probabilities of the actions NT ( X ) for all labels X almost always being greater than the probability of GEN ( w ) for the particular word w which must be produced next in a given sentence .,0
25047,"Qualitatively , the search procedure prefers to open constituents repeatedly up until the maximum number allowed by the model .",0
25048,"While these long chains of non-terminals will usually have lower probability than the correct sequence at the point where they finally generate the next word , they often have higher probability up until the word is generated , and so they tend to push the correct sequence off the beam before this point is reached .",0
25049,"This search failure produces very low evaluation performance : with a beam of size K = 100 , action - synchronous beam search achieves 29.1 F1 for RG and 27.4 F1 for LM on the development set .",0
25050,Word - synchronous beam search,0
25051,"To deal with this issue , we force partial parse candidates to compete with each other on a wordby - word level , rather than solely on the level of individual actions .",0
25052,The word - synchronous beam search we apply is very similar to approximate decoding procedures developed for other generative models and can be viewed as a simplified version of the procedure used in the generative top - down parsers of and .,0
25053,"In word - synchronous search , we augment the beam state space , identifying beams by tuples ( | W | , | A w | ) , where | W | is the number of words that have been produced so far in the sentence , and | A w | is the number of structural actions that have been taken since the last word was produced .",0
25054,"Intuitively , we want candidates with the same | W | = w to compete against each other .",0
25055,"For a beam of partial parses in the state ( | W | = w , | A w | = a ) , we generate a beam of successors by taking all of the next possible actions for each partial parse in the beam .",0
25056,"If the action is NT ( X ) or REDUCE , we place the resulting partial parse in the beam for state ( | W | = w , | A w | = a + 1 ) ; otherwise , if the action is GEN , we place it in a list for ( | W | = w + 1 , | A w | = 0 ) .",0
25057,"After all partial parses in the beam have been processed , we check to see if there are a sufficient number of partial parses that have produced the next word : if the beam ( | W | = w + 1 , | A w | = 0 ) contains at least K w partial parses ( the word beam size ) , we prune it to this size and continue search using this beam .",0
25058,"Otherwise , we continue building candidates for this word by pruning the beam ( | W | = w , | A w | = a + 1 ) to size K a ( the action beam size ) , and continuing search from there .",0
25059,"In practice , we found it to be most effective to use a value for K w that is a fraction of the value for K a .",0
25060,"In all the experiments we present here , we fix K a = 10 K w , with K w ranging from 10 to 100 .",0
25061,"shows F1 for decoding in both generative models on the development set , using the top - scoring parse found for a sentence when searching with the given beam size .",0
25062,"RG has comparatively larger gains in performance between the larger beam sizes , while still underperforming LM , suggesting that more search is necessary in this model .",0
25063,Experiments,0
25064,"Using the above decoding procedures , we attempt to separate reranking effects from model combination effects through a set of reranking experiments .",0
25065,"Our base experiments are performed on the Penn Treebank , using sections 2 - 21 for training , section 22 for development , and section 23 for testing .",0
25066,"For the LSTM generative model ( LM ) , we use the pre-trained model released by Choe and .",0
25067,"We train RNNG discriminative ( RD ) and generative ( RG ) models , following by using the same hyperparameter settings , and using pretrained word embeddings from for the discriminative model .",0
25068,The automaticallypredicted part - of - speech tags we use as input for RD are the same as those used by .,0
25069,"In each experiment , we obtain a set of candidate parses for each sentence by performing beam search in one or more parsers .",0
25070,We use actionsynchronous beam search ( Section 2.1 ) with beam size K = 100 for RD and word - synchronous beam ( Section 2.2 ) with K w = 100 and K a = 1000 for the generative models RG and LM .,0
25071,"In the case that we are using only the scores from a single generative model to rescore candidates taken from the discriminative parser , this setup is close to the reranking procedures originally proposed for these generative models .",0
25072,"For RG , the original work also used RD to produce candidates , but drew samples from it , whereas we use a beam search to approximate it s k- best list .",0
25073,The LM generative model was originally used to rerank a 50 - best list taken from the Charniak parser .,0
25074,"In comparison , we found higher performance for the LM model when using a candidate list from the RD parser : 93.66 F1 versus 92.79 F1 on the development data .",0
25075,"This maybe attributable to having a stronger set of candidates : with beam size 100 , RD has an oracle F1 of 98.2 , compared to 95.9 for the 50 - best list from the Charniak parser .",0
25076,Augmenting the candidate set,1
25077,"We first experiment with combining the candidate lists from multiple models , which allows us to look for potential model errors and model combination effects .",0
25078,Consider the standard reranking setup B ?,0
25079,"A , where we search in B to get a set of candidate parses for each sentence , and This does seem to be the case for both generative models , as shown in , which presents F 1 scores on the development set when varying the models used to produce the candidates and to score them .",0
25080,"Each row is a different candidate set , where the third row in each table presents results for the augmented candidate sets ; each column is a different scoring model , where the third column is the score combination setting described below .",0
25081,Going from RD ?,0
25082,RG to the augmented candidate setting RD ?,0
25083,RG ?,0
25084,RG decreases performance from 93.45 F1 to 92.78 F1 on the development set .,1
25085,This difference is statistically significant at the p < 0.05 level under a paired bootstrap test .,0
25086,"We see a smaller , but still significant , effect in the case of LM : RD ?",0
25087,"LM achieves 93.66 , compared to 93.47 for RD ?",0
25088,LM ?,0
25089,LM .,0
25090,"We can also consider the performance of RG ? RG and LM ? LM ( where we do not use candidates from RD at all , but return the highestscoring parse from searching directly in one of the generative models ) as an indicator of reranking effects : absolute performance is higher for LM ( 92.20 F1 ) than for RG ( 89.55 ) .",0
25091,"Taken together , these results suggest that model combination contributes to the success of both models , but to a larger extent for RG .",0
25092,"A reranking effect maybe a larger contributor to the success of LM , as this model achieves stronger performance on its own for the described search setting .",0
25093,Score combination,1
25094,"If the cross - scoring setup exhibits an implicit model combination effect , where strong performance results from searching in one model and scoring with the other , we might expect substantial further improvements in performance by explicitly combining the scores of both models .",0
25095,"To do so , we score each parse by taking a weighted sum of the log-probabilities assigned by both models , using an interpolation parameter which we tune to maximize F1 on the development set .",0
25096,These results are given in columns RD + RG and RD + LM in .,0
25097,"We find that combining the scores of both models improves on using the score of either model alone , regardless of the source of candidates .",1
25098,These improvements are statistically significant in all cases .,0
25099,Score combination also more than compensates for the decrease in performance we saw previously when adding in candidates from the generative model : RD ?,0
25100,RG ?,0
25101,RD + RG improves upon both RD ?,0
25102,RG and RD ?,0
25103,RG ?,0
25104,"RG , and the same effect holds for LM .",0
25105,Strengthening model combination,1
25106,"Given the success of model combination between the base model and a single generative model , we also investigate the hypothesis that the generative models are complementary .",0
25107,"The Model Combination block of shows full results on the test set for these experiments , in the PTB column .",0
25108,"The same trends we observed on the development data , on which the interpolation parameters were tuned , hold here : score combination improves results for all models ( row 3 vs. row 2 ; row 6 vs. row 5 ) , with candidate augmentation from the generative models giving a further increase ( rows 4 and 7 ) .",0
25109,"2 Combining candidates and scores from all three models ( row 9 ) , we obtain 93.94 F1 . :",1
25110,"Test F1 scores on section 23 of the PTB , by treebank training data conditions : either using only the training sections of the PTB , or using additional silver data ( + S ) .",0
25111,Semi-supervised silver data Choe and Charniak ( 2016 ) found a substantial increase in performance by training on external data in addition to trees from the Penn Treebank .,0
25112,"This silver dataset was obtained by parsing the entire New York Times section of the fifth Gigaword corpus using a product of eight Berkeley parsers and ZPar , then retaining 24 million sentences on which both parsers agreed .",0
25113,For our experiments we train RD and RG using the same silver dataset .,0
25114,3,0
25115,"The + S column in shows these results , where we observe gains over the PTB models in nearly every case .",0
25116,"As in the PTB training data setting , using all models for candidates and score combinations is best , achieving 94.66 F1 ( row 9 ) .",0
25117,"Ensembling Finally , we compare to another commonly used model combination method : ensembling multiple instances of the same model type trained from different random initializations .",1
25118,"We train ensembles of 8 copies each of RD and RG in both the PTB and silver data settings , combining scores from models within an ensemble by averaging the models ' distributions for each action ( in beam search as well as rescoring ) .",0
25119,"These results are shown in the bottom section , Ensembling , of .",0
25120,"Performance when using only the ensembled RD models ( row 10 ) is lower than rescoring a single RD model with score combinations of single models , either RD + RG ( row 3 ) or RD + LM ( row 6 ) .",1
25121,"In the PTB setting , ensembling with score combination achieves the best over all result of 94.25 ( row 13 ) .",1
25122,"In the silver training data setting , while this does improve on the analogous unensembled result ( row 8 ) , it is not better than the combination of single models when candidates from the generative models are also included ( row 9 ) .",0
25123,Discussion,0
25124,"Searching directly in the generative models yields results thatare partly surprising , as it reveals the presence of parses which the generative models prefer , but which lead to lower performance than the candidates proposed by the base model .",0
25125,"However , the results are also unsurprising in the sense that explicitly combining scores allows the reranking setup to achieve better performance than implicit combination , which uses only the scores of a single model .",0
25126,"Additionally , we see support for the hypothesis that the generative models can achieve good results on their own , with the LSTM generative model showing particularly strong and selfcontained performance .",0
25127,"While this search procedure allows us to explore these generative models , disentangling reranking and model combination effects , the increase in performance from augmenting the candidate lists with the results of the search may not be worth the required computational cost in a practical parser .",0
25128,"However , we do obtain again over state - of - theart results using simple model score combination on only the base candidates , which can be implemented with minimal cost over the basic reranking setup .",0
25129,This provides a concrete improvement for these particular generative reranking procedures for parsing .,0
25130,"More generally , it supports the idea that hybrid systems , which rely on one model to produce a set of candidates and another to determine which candidates are good , should explore combining their scores and candidates when possible .",0
25131,title,0
25132,Cloze - driven Pretraining of Self - attention Networks,1
25133,abstract,0
25134,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,1
25135,"Our model solves a cloze - style word reconstruction task , where each word is ablated and must be predicted given the rest of the text .",0
25136,"Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks , consistent with the concurrently introduced BERT model .",0
25137,"We also present a detailed analysis of a number of factors that contribute to effective pretraining , including data domain and size , model capacity , and variations on the cloze objective .",0
25138,Introduction,0
25139,Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems .,1
25140,"However , existing work has either used unidirectional ( left - to - right ) language models ( LMs ) or bi-directional ( both left - to - right and right - to - left ) LMs ( BiLMs ) where each direction is trained with an independent loss function .",0
25141,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .",1
25142,Our bi-directional transformer architecture predicts every token in the training data ( ) .,1
25143,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,1
25144,Our model separately computes both forward and backward states with * Equal contribution . :,1
25145,Illustration of the model .,0
25146,Block i is a standard transformer decoder block .,0
25147,Green blocks operate left to right by masking future time - steps and blue blocks operate right to left .,0
25148,"At the top , states are combined with a standard multi-head self - attention module whose output is fed to a classifier that predicts the center token .",0
25149,"a masked self - attention architecture , that closely resembles a language model .",0
25150,"At the top of the network , the forward and backward states are combined to jointly predict the center word .",0
25151,"This approach allows us to consider both contexts when predicting words and to incur loss for every word in the training set , if the model does not assign it high likelihood .",0
25152,"Experiments on the GLUE ) benchmark show strong gains over the state of the art for each task , including a 9.1 point gain on RTE over .",0
25153,"These improvements are consistent with , if slightly behind , those achieved by the concurrently developed BERT pretraining approach , which we will discuss in more detail in the next section .",0
25154,"We also show that it is possible to stack taskspecific architectures for NER and constituency parsing on top of our pretrained representations , and achieve new state - of - the - art performance lev-els for both tasks .",0
25155,"We also present extensive experimental analysis to better understand these results , showing that ( 1 ) cross sentence pretraining is crucial for many tasks ; ( 2 ) pre-training continues to improve performance with up to 18B tokens and would likely continue to improve with more data ; and finally ( 3 ) our novel cloze - driven training regime is more effective than predicting left and right tokens separately .",0
25156,Related work,0
25157,There has been much recent work on learning sentence - specific representations for language understanding tasks .,0
25158,learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks .,0
25159,Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data .,0
25160,Our work was inspired by ELMo and the generative pretraining ( GPT ) approach of .,0
25161,ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task .,0
25162,GPT relies on a left to right language model and an added projection layer for each downstream task without a task - specific model .,0
25163,"Our approach mostly follows GPT , though we show that our model also works well with an ELMo module on NER and constituency parsing .",0
25164,The concurrently introduced BERT model ( is a transformer encoder model that captures left and right context .,0
25165,There is significant overlap between their work and ours but there are also significant differences : our model is a bi-directional transformer language model that predicts every single token in a sequence .,0
25166,BERT is also a transformer encoder that has access to the entire input which makes it bi-directional but this choice requires a special training regime .,0
25167,"In particular , they multi-task between predicting a subset of masked input tokens , similar to a denoising autoencoder , and a next sentence prediction task .",0
25168,"In comparison , we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens .",0
25169,We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset .,0
25170,BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence - pairs with input markers that distinguish between tokens of the two sentences .,0
25171,Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task during training .,0
25172,"Finally , BERT as well as consider only a single data source to pretrain their models , either Books Corpus , or BooksCorpus and additional Wikipedia data , whereas our study ablates the effect of various amounts of training data as well as different data sources .",0
25173,Two tower model,0
25174,"Our cloze model represents a probability distribution p ( t i |t 1 , . . . , t i?1 , t i + 1 , . . . , tn ) for a sentence with n tokens t 1 , . . . , tn .",0
25175,There are two selfattentional towers each consisting of N stacked blocks : the forward tower operates left - to - right and the backward tower operates in the opposite direction .,0
25176,"To predict a token , we combine the representations of the two towers , as described in more detail below , taking care that neither representation contains information about the current target token .",0
25177,The forward tower computes the representation F l i for token i at layer l based on the forward representations of the previous layer F l?1 ? i via selfattention ; the backward tower computes representation B l i based on information from the opposite direction B l?1 ?i .,0
25178,"When examples of uneven length are batched , one of the towers may not have any context at the beginning .",0
25179,We deal with this issue by adding an extra zero state over which the selfattention mechanism can attend .,0
25180,We pretrain on individual examples as they occur in the training corpora ( 5.1 ) .,0
25181,"For News Crawl this is individual sentences while on Wikipedia , Bookcorpus , and Common Crawl examples are paragraph length .",0
25182,Sentences are prepended and appended with sample boundary markers < s >.,0
25183,Block structure,0
25184,The structure of the blocks follows most of the architectural choices described in .,0
25185,"Each block consists of two sub - blocks : the first is a multi-head self - attention module with H = 16 heads for which we mask out any subsequent time - steps , depending on if we are dealing with the forward or backward tower .",0
25186,"The second sub - block is a feed - forward module ( FFN ) of the form we apply layer normalization before the self - attention and FFN blocks instead of after , as we find it leads to more effective training .",0
25187,Sub- blocks are surrounded by a residual connection .,0
25188,Position is encoded via fixed sinusoidal position embeddings and we use a character CNN encoding of the input tokens for word - based models .,0
25189,Input embeddings are shared between the two towers .,0
25190,Combination of representations,0
25191,The forward and backward representations computed by the two towers are combined to predict the ablated word .,0
25192,To combine them we use a self - attention module which is followed by an FFN block ( 3.1 ) .,0
25193,The output of the FFN block is projected into V classes representing the types in the vocabulary .,0
25194,"When the model predicts token i , the input to the attention module are forward states FL 1 . . . FL i?1 and backward states B L i + 1 . . . B : n where n is the length of the sequence and L is the number of layers .",0
25195,We implement this by masking B L ?i and FL ?i .,0
25196,The attention query for token i is a combination of FL i?1 and B L i + 1 .,0
25197,For the base model we sum the two representations and for the larger models they are concatenated .,0
25198,Keys and values are based on the forward and backward states fed to the attention module .,0
25199,"In summary , this module has access to information about the entire input surrounding the current target token .",0
25200,"During training , we predict every token in this way .",0
25201,The output of this module is fed to an output classifier which predicts the center token .,0
25202,We use an adaptive softmax for the output classifier for the word based models and regular softmax for the BPE based models .,0
25203,"While all states that contain information about the current target word are masked in the final selfattention block during training , we found it beneficial to dis able this masking when fine tuning the Figure 2 : Illustration of fine - tuning for a singlesentence task where the output of the first and last token is fed to a task - specific classifier ( W ) .",0
25204,Masking for the final combination layer ( comb ) is removed which results in representations based on all forward and backward states ( cf. ) .,0
25205,pretrained model for downstream tasks .,0
25206,"This is especially true for tasks that label each token , such as NER , as this allows the model to access the full context including the token itself .",0
25207,Fine - tuning,0
25208,We use the following approach to fine - tune the pretrained two tower model to specific downstream tasks ) .,0
25209,Classification and regression tasks .,0
25210,"For single sentence classification tasks , we consider the language model outputs for the boundary tokens < s > which we add before the start and end of each sentence .",0
25211,The outputs are of dimension d = 1024 and we concatenate them to project to the number of classes C in the downstream task with W 1 ?,0
25212,R C2 d ; we add a bias term b ?,0
25213,RC and initialize all weights as well as the bias to zero .,0
25214,The output of the projection is softmax - normalized and the model is optimized with cross - entropy for classification tasks .,0
25215,"Regression tasks such as the Semantic Textual Similarity benchmark ( STS - B ; Cer et al. , 2017 ) use C = 1 and are trained with mean squared error .",0
25216,"For tasks involving sentence - pairs , we concatenate them and add a new separator token < sep > between them .",0
25217,We add the output of this token to the final projection W 2 ?,0
25218,R C3 d .,0
25219,Structured prediction tasks .,0
25220,For named entity recognition and parsing we use task - specific architectures which we fine - tune together with the language model but with different learning rate .,0
25221,The architectures are detailed in the respective results sections .,0
25222,The input to the architectures are the output representations of the pretrained language model .,0
25223,No Masking .,0
25224,"For fine - tuning , we found it beneficial to remove masking of the current token in the final layer that pools the output of the two towers .",0
25225,It is important to have access to information about the token to be classified for token level classification tasks such as NER but we also found this to perform better for sentence classification tasks .,0
25226,"In practice , we completely dis able masking in the combination layer so that it operates over all forward and backward states .",0
25227,"However , dis abling masking below the combination layer does not perform well .",0
25228,Optimization .,0
25229,"During fine - tuning we use larger learning rates for the new parameters , that is W 1 , W 2 , b or the task - specific architecture , compared to the pretrained model .",0
25230,"For GLUE tasks , we do so by simply scaling the output of the language model before the W 1 and W 2 projections by a factor of 16 .",0
25231,"For structured prediction tasks , we explicitly use different learning rates for the pretrained model and the task - specific parameters .",0
25232,"We fine tune with the Adam optimizer ( Kingma and Ba , 2015 ) .",0
25233,"For GLUE tasks , we dis able dropout in the language model and add 0.1 dropout between language model output and the final output projection ; for structured prediction tasks , we use 0.3 at all levels ( within the pretrained model , within the task - specific architecture , and on the weights connecting them ) .",0
25234,"In all settings , we use a batch size of 16 examples .",0
25235,"We use a cosine schedule to linearly warm up the learning rate from 1e - 07 to the target value over the first 10 % of training steps , and then anneal the learning rate to 1e - 06 , following the cosine curve for the remaining steps .",0
25236,"For GLUE tasks , we tuned the learning rate for each task and chose the best value over three settings : 1e - 04 , 5e - 05 and 3e - 05 .",0
25237,"For structured prediction tasks , we tuned on the pairs of learning rate , see the results section for details .",0
25238,"For GLUE tasks , we train three seeds for each learning rate value for three epochs and choose the model after each epoch that performs best on the validation set .",0
25239,"For structured prediction tasks , we train for up to 25 epochs and stop if the validation loss does not improve over the previous epoch .",0
25240,Experimental setup,1
25241,Datasets for pretraining,0
25242,We train the two tower model on several datasets .,0
25243,Common Crawl .,0
25244,We consider various subsets of Common Crawl which is web data .,0
25245,We follow the same pre-processing as which is based on the May 2017 Common Crawl dump .,0
25246,This setup add 20 copies of English Wikipedia resulting in about 14 % of the final dataset to be Wikipedia .,0
25247,We subsample up to 18B tokens .,0
25248,"All experiments use Common Crawl subsampled to 9B tokens , except 6.4 .",0
25249,News Crawl .,0
25250,We use up to 4.5B words of English news web data distributed as part of WMT 2018 .,0
25251,Books Corpus + Wikipedia .,0
25252,This is similar to the training data used by BERT which comprises the Books Corpus of about 800M words plus English Wikipedia data of 2.5B words .,0
25253,Pretraining hyper -parameters,0
25254,We adapt the transformer implementation available in the fairseq toolkit to our two tower architecture .,0
25255,For hyper - parameter and optimization choices we mostly follow Baevski and Auli ( 2018 ) .,0
25256,Our experiments consider three model sizes shown in : There are two CNN input models in a base and large configuration as well as a Byte - Pair - Encoding based model ( BPE ; .,0
25257,"The CNN models have unconstrained input vocabulary , and an output vocabulary limited to 1 M most common types for the large model , and 700 K most common types for the base model .",0
25258,"CNN models use an adaptive softmax in the output : the headband contains the 60K most frequent types with dimensionality 1024 , followed by a 160 K band with dimensionality 256 . with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 .",1
25259,The learning rate is linearly warmed up from 10 ? 7 to 1 for 16 K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 .,1
25260,We run experiments on DGX - 1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband .,1
25261,We also use the NCCL2 library and the torch .,1
25262,distributed package for inter - GPU communication .,0
25263,"We train models with 16 bit floating point precision , following .",0
25264,The BPE model trains much faster than the character CNN models ) .,0
25265,Results,1
25266,GLUE,1
25267,"First , we conduct experiments on the general language understanding evaluation benchmark ( GLUE ; and present a short overview of the tasks .",0
25268,More information can be found in .,0
25269,"There are two singlesentence classification tasks : First , the Corpus of Linguistic Acceptability ( CoLA ; is a binary task to judge sentence grammaticality ; evaluation is in terms of the Matthews correlation coefficient ( mcc ) .",0
25270,"Second , the Stanford Sentiment Treebank ( SST - 2 ; requires to judge if movie reviews have positive or negative sentiment ; evaluation is in terms of accuracy ( acc ) .",0
25271,There are three tasks assessing sentence similarity :,0
25272,The Microsoft Research Paragraph and the Quora Question Pairs benchmark ( QQP ) ; we evaluate in terms of F1 .,0
25273,The Semantic Textual Similarity Benchmark requires predicting a similarity score between 1 and 5 for a sentence pair ; we report the Spearman correlation coefficient ( scc ) .,0
25274,"Finally , there are four natural laguage inference tasks : the Multi - Genre Natural Language Inference ( MNLI ; , the Stanford Question Answering Dataset ( QNLI ; , the Recognizing Textual Entailment ( RTE ; .",0
25275,We exclude the Winograd NLI task from our results similar to ; and report accuracy .,0
25276,For MNLI we report both matched ( m ) and mismatched ( mm ) accuracy on test .,0
25277,We also report an average over the GLUE metrics .,0
25278,This figure is not comparable to the average on the official GLUE leaderboard since we exclude Winograd and do not report MRPC accuracy STS - B Pearson correlation as well as QQP accuracy .,0
25279,shows results for three configurations of our approach ( cf. ) .,0
25280,"The BPE model has more parameters than the CNN model but does not perform better in aggregate , however , it is faster to train .",0
25281,"All our models outperform the uni-directional transformer ( OpenAI GPT ) of , however , our model is about 50 % larger than their model .",1
25282,We also show results for the concurrently introduced STILTs and BERT .,0
25283,"Our CNN base model performs as well as STILTs in aggregate , however , on some tasks involving sentence - pairs , STILTs performs much better ( MRPC , RTE ) ; there is a similar trend for BERT .",1
25284,STILTs adds another fine - tuning step on another downstream task which is similar to the final task .,0
25285,The technique is equally applicable to our approach .,0
25286,Training examples for our model are Common Crawl paragraphs of arbitrary length .,0
25287,We expect that tailoring training examples for language model pretraining to the end tasks to significantly improve performance .,0
25288,"For example , BERT trains on exactly two sentences while as we train on entire paragraphs .",0
25289,Structured Prediction,0
25290,"We also evaluated performance on two structured predictions tasks , NER and constituency parsing .",0
25291,"For both problems , we stacked task - specific architectures from recent work on top of our pretrained two tower models .",0
25292,"We evaluate two ways of stacking : ( 1 ) ELMo-style , where the pretrained models are not fine - tuned but are linearly combined at different depths , and ( 2 ) with fine - tuning , where we set different learning rates for the task - specific layers but otherwise update all of the parameters during the task - specific training .",0
25293,Named Entity Recognition,1
25294,"We evaluated span - level F1 performance on the CoNLL 2003 Named Entity Recognition ( NER ) task , where spans of text must be segmented and labeled as Person , Organization , Location , or Miscellaneous .",0
25295,"We adopted the NER architecture in Peters et al. , a biLSTM - CRF , with two minor modifications : ( 1 ) instead of two layers of biL - STM , we only used one , and ( 2 ) a linear projection layer was added between the token embedding and biLSTM layer .",0
25296,"We did grid search on the pairs of learning rate , and found that projection - biLSTM - CRF with 1E - 03 and pretrained language model with 1E - 05 gave us the best result .",0
25297,"shows the results , with comparison to previous published ELMo BASE results the art , but fine tuning gives the biggest gain .",1
25298,Constituency Parsing,1
25299,We also report parseval F1 for Penn Treebank constituency parsing .,0
25300,We adopted the current state - of the - art architecture .,0
25301,"We again used grid search for learning rates and number of layers in parsing encoder , and used 8E - 04 for language model finetuning , 8E - 03 for the parsing model parameters , and two layers for encoder .",0
25302,shows the results .,0
25303,"Here , fine tuning is required to achieve gains over the previous state of the art , which used ELMo embeddings .",0
25304,Objective functions for pretraining,0
25305,The two - tower model is trained to predict the current token given representations of the entire left and right context ( cloze ) .,0
25306,Next we compare this choice to two alternatives : train two language models operating leftto - right and right - to - left to predict the next word for each respective direction .,0
25307,"We change the twotower model to predict the next word using the individual towers only and remove the combination module on top of the two towers ( bilm ) ; however , we continue to jointly train the two towers .",0
25308,"Second , we combine the cloze loss with the bilm loss to obtain a triplet loss which trains the model to predict the current word given both left and right context , as well as just right or left context .",0
25309,"The latter is much harder than the cloze loss since less context is available and therefore gradients for the bilm loss are much larger : the cloze model achieves perplexity of about 4 while as for the bilm it is 27 - 30 , depending on the direction .",0
25310,This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance .,1
25311,shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself .,1
25312,We conjecture that in - dividual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough .,0
25313,Domain and amount of training data,0
25314,Next we investigate how much pretraining benefits from larger training corpora and how the domain of the data influences end - task performance .,0
25315,shows that more training data can significantly increase accuracy .,0
25316,We train all models with the exact same hyper - parameter settings on Common Crawl data using the CNN base architecture for 600 K updates .,0
25317,We train on up to 18B Common Crawl tokens and the results suggest that more training data is likely to further increase performance .,0
25318,"We also experiment with Books Corpus as well as English Wikipedia , similar to .",0
25319,Examples in BooksCorpus area mix of individual sentences and paragraphs ; examples are on average 36 tokens .,0
25320,Wikipedia examples are longer paragraphs of 66 words on average .,0
25321,"To reduce the effect of training on examples of different lengths , we adopted the following strategy : we concatenate all training examples into a single string and then crop blocks of 512 consecutive tokens from this string .",0
25322,We train on a batch of these blocks ( BWiki - blck ) .,0
25323,It turns out that this strategy did notwork better compared to our existing strategy of simply using the data as is ( BWikisent ) .,0
25324,Books,0
25325,Corpus and Wikipedia performs very well on QNLI and MNLI but less well on other tasks .,0
25326,"In summary , more data for pretraining improves performance , keeping everything else equal .",0
25327,Also pretraining on corpora that retains paragraph structure performs better than individual sentences .,0
25328,Conclusion,0
25329,We presented a pretraining architecture based on a bi-directional transformer model that predicts every token in the training data .,0
25330,The model is trained with a cloze - style objective and predicts the center word given all left and right context .,0
25331,"Results on the GLUE benchmark show large gains over for each task , while experiments with model stacking set new state of the art performance levels for parsing and named entity recognition .",0
25332,"We also did extensive experimental analysis to better understand these results , showing that ( 1 ) cross sentence pretraining is crucial for many tasks ; ( 2 ) pre-training continues to improve performance up to 18B tokens and would likely continue to improve with more data ; and finally ( 3 ) our novel cloze - driven training regime is more effective than predicting left and right tokens separately .",0
25333,"In future work , we will investigate variations of our architecture .",0
25334,"In particular , we had initial success sharing the parameters of the two towers which allows training much deeper models without increasing the parameter count .",0
25335,title,0
25336,Parsing as Language Modeling,1
25337,abstract,0
25338,"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .",1
25339,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .",0
25340,Introduction,0
25341,"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .",1
25342,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .",1
25343,In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem .,0
25344,Section 2 looks more closely at three of the most relevant previous papers .,0
25345,"We then describe our exact model ( Section 3 ) , followed by the experimental setup and results ( Sections 4 and 5 ) .",0
25346,There is a one - to - one mapping between a tree and its sequential form .,0
25347,( Part - of - speech tags are not used . ),0
25348,Language Modeling,0
25349,"Formally , a language model ( LM ) is a probability distribution over strings of a language :",0
25350,where x is a sentence and t indicates a word position .,0
25351,"The efforts in language modeling go into computing P ( x t |x 1 , , x t?1 ) , which as described next is useful for parsing as well .",0
25352,Parsing as Language Modeling,0
25353,A generative parsing model parses a sentence ( x ) into it s phrasal structure ( y ) according to,0
25354,where Y ( x ) lists all possible structures of x .,0
25355,"If we think of a tree ( x , y) as a sequence ( z ) as illustrated in , we can define a probability distribution over ( x , y) as follows :",0
25356,"P ( x , y ) = P ( z ) = P ( z 1 , , z m ) = m t=1 P ( z t | z 1 , , z t?1 ) , which is equivalent to Equation ( 1 ) .",0
25357,"We have reduced parsing to language modeling and can use language modeling techniques of estimating P ( z t | z 1 , , z t?1 ) for parsing .",0
25358,Previous Work,0
25359,We look here at three neural net ( NN ) models closest to our research along various dimensions .,0
25360,"The first gives the basic language modeling architecture that we have adopted , while the other two are parsing models that have the current best results in NN parsing .",0
25361,LSTM - LM,0
25362,"The LSTM - LM of turns ( x 1 , , x t?1 ) into ht , a hidden state of an LSTM , and uses ht to guess x t :",0
25363,where,0
25364,Wis a parameter matrix and [ i ] indexes ith element of a vector .,0
25365,"The simplicity of the model makes it easily extendable and scalable , which has inspired a character - based LSTM - LM that works well for many languages and an ensemble of large LSTM - LMs for English with astonishing perplexity of 23.7 .",0
25366,"In this paper , we build a parsing model based on the LSTM - LM of .",0
25367,"observe that a phrasal structure ( y ) can be expressed as a sequence and build a machine translation parser ( MTP ) , a sequence - tosequence model , which translates x into y using a conditional probability :",0
25368,MTP,0
25369,"where the conditioning event ( x , y 1 , , y t?1 ) is modeled by an LSTM encoder and an LSTM decoder .",0
25370,"The encoder maps x into he , a set of vectors that represents x , and the decoder obtains a summary vector ( h t ) which is concatenation of the decoder 's hidden state ( h d t ) and weighted sum of word representations ( n i =1 ?",0
25371,i he i ) with an alignment vector ( ? ) .,0
25372,Finally the decoder predicts y t given ht .,0
25373,"Inspired by MTP , our model processes sequential trees .",0
25374,RNNG,0
25375,"Recurrent Neural Network Grammars ( RNNG ) , a generative parsing model , defines a joint distribution over a tree in terms of actions the model takes to generate the tree :",0
25376,"where a is a sequence of actions whose output precisely matches the sequence of symbols in z , which implies Equation is the same as Equation .",0
25377,"RNNG and our model differ in how they compute the conditioning event ( z 1 , , z t?1 ) :",0
25378,"RNNG combines hidden states of three LSTMs that keep track of actions the model has taken , an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM 's hidden state as shown in the next section .",0
25379,Model,0
25380,"Our model , the model of applied to sequential trees and we call LSTM - LM from now on , is a joint distribution over trees :",0
25381,where ht is a hidden state of an LSTM .,0
25382,"Due to lack of an algorithm that searches through an exponentially large phrase - structure space , we use an n-best parser to reduce Y ( x ) to Y ( x ) , whose size is polynomial , and use LSTM - LM to find y that satisfies",0
25383,( 4 ),0
25384,Hyper-parameters,0
25385,"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .",1
25386,We initialize starting states with previous minibatch 's last hidden states .,1
25387,"The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .",1
25388,Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .,1
25389,The learning rate is 0.25 0.85 max where is an epoch number .,1
25390,"For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .",1
25391,Experiments,0
25392,"We describe datasets we use for evaluation , detail training and development processes .",0
25393,1,0
25394,Data,0
25395,We performed better when trained on all of 24 million trees than when trained on resampled two million trees .,0
25396,"Given x , we produce Y ( x ) , 50 - best trees , with Charniak parser and find y with LSTM - LM as do with their discriminative and generative models .",0
25397,3,0
25398,Training and Development,0
25399,Supervision,0
25400,"As shown in , with 92.6 F 1 LSTM - LM ( G ) outperforms an ensemble of five MTPs and RNNG , both of which are trained on the WSJ only .",0
25401,Semi-supervision,0
25402,We compare LSTM - LM ( GS ) to two very strong semi-supervised NN parsers : an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 ( HC ) ; and an ensemble of six one - to - many sequence models trained on the HC and 4.5 millions of English - German translation sentence pairs .,0
25403,We also compare LSTM - LM ( GS ) to best performing non -NN parsers in the literature .,0
25404,Parsers ' parsing performance along with their training data is reported in .,0
25405,LSTM - LM ( GS ) outperforms all the other parsers with 93.1 F 1 .,0
25406,Results,1
25407,Improved Semi-supervision,0
25408,"Due to search errors - good trees are missing in 50 - best trees - in Charniak ( G ) , our supervised and semi-supervised models do not exhibit their full potentials when Charniak ( G ) provides Y ( x ) .",0
25409,"To mitigate the search problem , we tri-train Charniak ( GS ) on all of 24 million NYT trees in addition to the WSJ , to yield Y ( x ) .",0
25410,"As shown in , both LSTM - LM ( G ) and LSTM - LM ( GS ) are affected by the quality of Y ( x ) .",0
25411,"A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .",1
25412,"When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .",1
25413,Why an indirect method ( converting trees to dependencies ) is more accurate than a direct one ( dependency parsing ) remains unanswered .,0
25414,Conclusion,0
25415,The generative parsing model we presented in this paper is very powerful .,0
25416,"In fact , we see that a generative parsing model , LSTM - LM , is more effective than discriminative parsing models .",0
25417,We suspect building large models with character embeddings would lead to further improvement as in language modeling .,0
25418,We also wish to develop a complete parsing model using the LSTM - LM framework . :,0
25419,Evaluation of models trained on the WSJ and additional resources .,0
25420,Note that the numbers of and are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB - style trees .,0
25421,E ( LSTM - LMs ( GS ) ) is an ensemble of eight LSTM - LMs ( GS ) .,0
25422,X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM - LM .,0
25423,"For the ensemble model , we report the maximum number of trees used to train one of LSTM - LMs ( GS ) .",0
25424,at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees .,0
25425,title,0
25426,Convolutional Neural Network Architectures for Matching Natural Language Sentences,1
25427,abstract,0
25428,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",1
25429,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,0
25430,"As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .",0
25431,"The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .",0
25432,"Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .",0
25433,The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .,0
25434,Introduction,0
25435,Matching two potentially heterogenous language objects is central to many natural language applications .,1
25436,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",0
25437,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",0
25438,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .",1
25439,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,1
25440,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",1
25441,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",1
25442,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",1
25443,This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,0
25444,Our main contributions can be summarized as follows .,0
25445,"First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .",0
25446,Roadmap,0
25447,"We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .",0
25448,"Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .",0
25449,"In Section 4 , we briefly discuss the learning of the proposed architectures .",0
25450,"Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .",0
25451,Convolutional Sentence Model,0
25452,We start with proposing anew convolutional architecture for modeling sentences .,0
25453,"As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .",0
25454,"As inmost convolutional models , we use convolution units with a local "" receptive field "" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .",0
25455,Convolution,0
25456,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",0
25457,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",0
25458,and it s matrix form is z,0
25459,gives the output of feature map of type - f for location i in Layer - ;,0
25460,"w ( , f ) is the parameters for f on Layer - , with matrix form",0
25461,"? ( ) is the activation function ( e.g. , Sigmoid or Relu )",0
25462,?,0
25463,( ?1 ) i denotes the segment of Layer - ?,0
25464,"1 for the convolution at location i , whil",0
25465,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,0
25466,Max - Pooling,0
25467,"We take a max - pooling in every two - unit window for every f , after each convolution",0
25468,The effects of pooling are two - fold :,0
25469,"1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",0
25470,Length Variability,0
25471,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,0
25472,"More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .",0
25473,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",0
25474,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",0
25475,"( 2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",0
25476,"Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .",0
25477,"The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .",0
25478,"gives an example on what could happen on the first two layers with input sentence "" The cat sat on the mat "" .",0
25479,"Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in W ) to make the convolution units focus on different segments within a 3 - word window .",0
25480,"For example , some feature maps ( group 2 ) give compositions for "" the cat "" and "" cat sat "" , each being a vector .",0
25481,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .",0
25482,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",0
25483,Some Analysis on the Convolutional Architecture,0
25484,Relation to Recursive Models,0
25485,"Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .",0
25486,"First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .",0
25487,"Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .",0
25488,"With any window width k ? 3 , the type of composition would be much richer than that of RAE .",0
25489,"Second , our convolutional model can take supervised training and tune the parameters fora specific task , a property vital to our supervised learning - to - match framework .",0
25490,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",0
25491,"For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",0
25492,"Relation to "" Shallow "" Convolutional Models",0
25493,"The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .",0
25494,"This type of models , with local convolutions and a global pooling , essentially do a "" soft "" local template matching and is able to detect local features useful fora certain task .",0
25495,"Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .",0
25496,It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .,0
25497,Convolutional Matching Models,0
25498,"Based on the discussion in Section 2 , we propose two related convolutional architectures , namely ARC - I and ARC - II ) , for matching two sentences .",0
25499,Architecture - I ( ARC - I ),0
25500,"Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :",0
25501,"It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .",0
25502,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",0
25503,"Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",0
25504,"In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .",0
25505,"This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .",0
25506,Architecture - II ( ARC - II ),0
25507,"In view of the drawback of Architecture - I , we propose Architecture - II ( ARC - II ) that is built directly on the interaction space between two sentences .",0
25508,"It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .",0
25509,"Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through "" one-dimensional "" ( 1D ) convolutions .",0
25510,"For segment ion S X and segment j on S Y , we have the feature map",0
25511,where ?,0
25512,"i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",0
25513,.,0
25514,Clearly the 1D convolution preserves the location information about both segments .,0
25515,"After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )",0
25516,"( 4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :",0
25517,( 5 ),0
25518,"This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .",0
25519,The 2D - Convolution,0
25520,"After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .",0
25521,The general two - dimensional convolution is formulated as z ( ),0
25522,where ?,0
25523,concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,0
25524,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",0
25525,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",0
25526,1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .,0
25527,"contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .",0
25528,"The orders is however retained in a "" conditional "" sense .",0
25529,"Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.",0
25530,Model Generality,0
25531,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,0
25532,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",0
25533,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",0
25534,"As a result , the output for each filter f , denoted z",0
25535,"( 1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .",0
25536,"Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .",0
25537,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .",0
25538,"As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .",0
25539,"As a result , ARC - II can naturally blend two seemingly diverging processes :",0
25540,"1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .",0
25541,This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .,0
25542,Training,0
25543,We employ a discriminative training strategy with a large margin objective .,0
25544,"Suppose that we are given the following triples ( x , y + , y ? )",0
25545,"from the oracle , with x matched with y + better than with y ? .",0
25546,We have the following ranking - based loss as objective :,0
25547,"where s ( x , y) is predicted matching score for ( x , y ) , and ?",0
25548,includes the parameters for convolution layers and those for the MLP .,0
25549,The optimization is relatively straightforward for both architectures with the standard back - propagation .,0
25550,The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .,0
25551,"In other words , We use stochastic gradient descent for the optimization of models .",0
25552,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,0
25553,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",0
25554,"For small datasets ( less than 10 k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .",0
25555,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",0
25556,"Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .",0
25557,We vary the maximum length of words for different tasks to cope with its longest sentence .,0
25558,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .",0
25559,"ARC - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while ARC - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .",0
25560,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",0
25561,Experiments,0
25562,"We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .",0
25563,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .",0
25564,"Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .",0
25565,Competitor Methods,0
25566,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,0
25567,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",0
25568,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",0
25569,We use the SENNA - type sentence model for sentence representation ;,0
25570,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",0
25571,"All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .",0
25572,Experiment I : Sentence Completion,1
25573,This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .,0
25574,"Basically , we take a sentence from Reuterswith two "" balanced "" clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .",0
25575,The task is then to recover the original second clause for any given first clause .,0
25576,The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .,0
25577,"We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .",0
25578,One representative example is given as follows :,0
25579,"All models are trained on 3 million triples ( from 600K positive pairs ) , and tested on 50K positive pairs , each accompanied by four negatives , with results shown in .",0
25580,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .",0
25581,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",1
25582,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",1
25583,"It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",0
25584,"This task is slightly easier than Experiment I , with more training instances and purely random negatives .",0
25585,"It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .",0
25586,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .",0
25587,Experiment II : Matching A Response to A Tweet,0
25588,Experiment III : Paraphrase Identification,1
25589,"Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .",0
25590,is included to test our methods on matching homogenous objects .,0
25591,"Here we use the benchmark MSRP dataset , which contains 4,076 instances for training and 1,725 for test .",0
25592,We use all the training instances and report the test performance from early stopping .,0
25593,"As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .",0
25594,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .",1
25595,Discussions,0
25596,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,0
25597,"It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .",0
25598,"This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .",0
25599,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .",0
25600,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .",0
25601,It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .,0
25602,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,0
25603,"We hypothesize that the Word2 Vec embedding is trained in such away that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .",0
25604,This is in contrast with other bag - of - words models like DEEPMATCH .,0
25605,Related Work,0
25606,"Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .",0
25607,"When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .",0
25608,"Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .",0
25609,Our models are related to the long thread of work on sentence representation .,0
25610,"Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .",0
25611,There is very little work on convolutional modeling of language .,0
25612,"In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .",0
25613,"This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .",0
25614,Conclusion,0
25615,"We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .",0
25616,Empirical study shows our models can outperform competitors on a variety of matching tasks .,0
25617,title,0
25618,A Deep Cascade Model for Multi - Document Reading Comprehension,1
25619,abstract,0
25620,A fundamental trade - off between effectiveness and efficiency needs to be balanced when designing an online question answering system .,0
25621,"Effectiveness comes from sophisticated functions such as extractive machine reading comprehension ( MRC ) , while efficiency is obtained from improvements in preliminary retrieval components such as candidate document selection and paragraph ranking .",0
25622,"Given the complexity of the real - world multi-document MRC scenario , it is difficult to jointly optimize both in an end - to - end system .",0
25623,"To address this problem , we develop a novel deep cascade learning model , which progressively evolves from the documentlevel and paragraph - level ranking of candidate texts to more precise answer extraction with machine reading comprehension .",0
25624,"Specifically , irrelevant documents and paragraphs are first filtered outwith simple functions for efficiency consideration .",0
25625,"Then we jointly train three modules on the remaining texts for better tracking the answer : the document extraction , the paragraph extraction and the answer extraction .",0
25626,"Experiment results show that the proposed method outperforms the previous state - of - the - art methods on two large - scale multidocument benchmark datasets , i.e. , TriviaQA and DuReader .",0
25627,"In addition , our online system can stably serve typical scenarios with millions of daily requests in less than 50 ms .",0
25628,Introduction,0
25629,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",1
25630,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .",1
25631,"The rapid progress of MRC in recent years mostly owes to the release of the single - paragraph benchmark dataset SQuAD ) , on which various deep attention - based methods have been proposed to constantly push the state - of - the - art performance .",0
25632,"It is a significant mile - Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",0
25633,All rights reserved .,0
25634,stone that several MRC models have exceeded the performance of human annotators on the SQuAD dataset,0
25635,1 .,0
25636,"However , the SQuAD dataset makes a strong assumption that the answers are contained in the given paragraphs .",0
25637,"Besides , the parapraphs are rather short , approximately 200 words on average , while a real - world scenario usually involves multiple documents of much longer length .",0
25638,"Therefore , several latest studies begin to re-design the task into more realistic settings : the MRC models are required to read and comprehend multiple documents to reach the final answer .",0
25639,"In multi-document MRC , depending on the way of combining the two components , document selection and extractive reading comprehension , there are two categories of approaches :",0
25640,"1 ) The pipeline approach treats the document selection and extractive reading comprehension as two separate parts , where a document is firstly selected through document ranking and then passed to the MRC model for extracting the final answer ; 2 ) Several recent studies ) adopt a joint learning method to optimize both sub - tasks in a unified framework simultaneously .",0
25641,The pipeline method relies heavily on the quality of the document ranking module .,0
25642,"When it fails to give the relevant documents higher ranks or filters out the ones that contain the correct answers , the downstream MRC module has noway to recover and extract the answers of interest .",0
25643,"For the joint learning method , it is computationally expensive to jointly optimize both tasks with all the documents .",0
25644,"This computation cost limits its application to the operational online environment , such as Amazon 2 and Taobao 3 , where efficiency is a critical factor to be considered .",0
25645,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .",1
25646,The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,1
25647,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .",1
25648,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,1
25649,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",1
25650,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .",1
25651,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",1
25652,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .",1
25653,The first module takes the question and a collection of raw documents as input .,1
25654,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",1
25655,"For each of the first two modules , we define a ranking function and an extraction function .",0
25656,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .",1
25657,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .",1
25658,"The local ranking functions in different modules gradually increase in cost and complexity , to properly keep the balance between the effectiveness and efficiency .",0
25659,"The main contributions can be summarized as follow : We propose a deep cascade learning framework to address the practical multi-document machine reading comprehension task , which considers both the effectiveness and efficiency in a coarse - to - fine manner .",0
25660,"We incorporate the auxiliary document extraction and paragraph extraction tasks to the pure answer span prediction , which helps to narrow down the search space and improves the final extraction result in multi-document MRC scenario .",0
25661,We conduct extensive experiments on two largescale multi-document MRC benchmark datasets : Trivi - a QA and DuReader .,0
25662,The results show that our deep cascade model can outperform the previous state - of - the - art performance on both datasets .,0
25663,"Besides , the proposed model has also been successfully applied in our online system and stably serve various scenarios in a quick response time of less than 50 ms .",0
25664,Related Work Machine Reading Comprehension,0
25665,"Recently , we can see emerging interests in multi-document MRC research , where multiple documents are given as input .",0
25666,There are two categories of approaches : the pipeline - based approaches and the joint learning models .,0
25667,The pipeline approach firstly selects a single document via ranking and then pass it to the MRC model to extract the precise answer ) .,0
25668,"This approach gives huge burden to the document ranking model , in which the downstream MRC model has noway to extract the right answer if the relevant documents are missed .",0
25669,The joint learning approaches take all the documents into consideration and extract the answer by comparing it against other documents ( Clark and Gardner 2017 ; .,0
25670,"( Clark and Gardner 2017 ) proposes a confidence - based method with a shared normalization training objective , which enables the model to produce globally correct output .",0
25671,"proposes an extraction - then - synthesis framework , by also incorporating passage ranking to answer span prediction . ) further proposes a verification method to make use of the extracted answers in different documents to verify each other for more accurate prediction .",0
25672,"However , taking all the documents into consideration will inevitably bring more computation cost , which can be unbearable in the operational online environment .",0
25673,Our deep cascade model can serve as a proper tradeoff between the pipeline method and joint learning method .,0
25674,"It has a coarse - to - fine structure which can eliminate irrelevant documents and paragraphs in the early stages with simple features and models , and better identify more relevant answers in a well - designed multi -task deep MRC model on the remaining content .",0
25675,Cascade Learning,0
25676,"In designing online systems , trade - off between effectiveness and efficiency remains a long - standing problem .",0
25677,"Cascade learning is an alternative strategy that can better balance these two , which utilizes a sequence of functions in different stages and allows using different sets of features for different instances .",0
25678,"It is firstly introduced in the traditional classification and detection problems such as fast visual object detection , and then widely applied in ranking applications for achieving high top -k rank effectiveness in an efficient manner ( Lefakis and Fleuret 2010 ; .",0
25679,"uses an Adaboost style framework with two independent ranking functions in each stage , one for pruning the input ranked documents and the other for refining the rank order .",0
25680,"We apply the idea of cascade learning to machine reading comprehension , from a preliminary document - level and paragraph - level ranking of the candidate texts , to a more precise answer span extraction .",0
25681,"The extracted answer spans are progressively narrowed down across different levels , and the ranking and extraction functions also progressively increase in complexity for more precise answer prediction .",0
25682,The Deep Cascade Model,0
25683,"Following the overview in , our approach consists of three cascade modules : document retrieval , paragraph retrieval and answer extraction .",0
25684,"The cascade ranking functions in the first two modules aim to fast filter out the irrelevant document content based on the basic statistical and structural features , and obtain a coarse ranking for the candidate documents .",0
25685,"For the remaining document content , we design three extraction tasks at different granularities , with the goal to simultaneously extract the right document , paragraph and also the answer span .",0
25686,"A deep attention - based MRC model is designed to jointly optimize all the three extraction tasks , by sharing the common bottom layers , as is shown in .",0
25687,"The final answer is thus determined by not only the answer span prediction score , but also the corresponding document and paragraph prediction score .",0
25688,Cascade Ranking Functions,0
25689,"Given a question Q and a set of candidate documents { D i } , we first introduce the cascade ranking functions of the first two modules for pruning the documents , which gradually increases in complexity .",0
25690,Document Ranking,0
25691,This part aims at fast filtering out the irrelevant documents and obtaining a coarse ranking for the candidate documents .,0
25692,"We utilize the traditional information retrieval methods , such as BM25 and TF - IDF distance , to measure the relevance between the question and document .",0
25693,"The matching is conducted between the textual metadata of question and document , including the document title and main content .",0
25694,"Besides , the recall ratio of the question words from the document metadata is used as another feature to indicate the relevance of the document .",0
25695,"To learn the importance of different features , we use a learning - to - rank model to assign a weighted relevance score to each retrieved document .",0
25696,"By design , the first stage needs to be quick and simple , so we cast the task as a binary classification problem and adopt the pointwise logistic regression as the ranking function .",0
25697,The documents containing the answer are labeled as positive .,0
25698,"After this ranking , we only keep the top - K ranked documents for further processing .",0
25699,Paragraph Ranking,0
25700,This part aims at fast discarding the irrelevant content within each document at a paragraph level .,0
25701,"Specifically , given an output document Di = {P ij } from the previous stage , we first prune the noisy paragraphs without word or entity match .",0
25702,The simple question and paragraph textual matching features are also extracted as in that of document ranking .,0
25703,"Moreover , the document structure can contain some sort of inherent information , for example , the first paragraph within a document may tend to possess more informative content as a document abstract .",0
25704,"Therefore , we also add some structural features , such as whether the paragraph is the first or last paragraph of the document , the length of the paragraph , the length of the previous or subsequent paragraphs .",0
25705,"To understand the question , we also incorporate the question type information as several binary features if is given , e.g. for DuReader dataset .",0
25706,"To better combine different kinds of features , we adopt a scalable tree boosting method XGBoost for ranking , which is widely used to achieve state - of the - art results on many large - scale machine learning challenges .",0
25707,"Again , we use the binary logistic loss for model training and label the paragraph containing the answer as positive .",0
25708,"As a result , we select the top - N paragraphs from each document for the subsequent answer prediction .",0
25709,Multi- task Deep Attention Model,0
25710,"Given the selected P paragraphs from the top - ranked K documents , the final task is to extract an answer span to answer the question Q .",0
25711,A deep attention - based MRC model is designed to achieve this goal .,0
25712,"However , with all these documents and paragraphs , it maybe still difficult to directly conduct the pure answer prediction at a precise word level , as in that of SQuAD dataset .",0
25713,The document and paragraph information is also not fully exploited .,0
25714,"Therefore , we split the answer prediction task into three joint tasks : document extraction , paragraph extraction and answer span extraction .",0
25715,"The three tasks share the same bottom layers , which represents the semantics of the document context with respect to the question words , as is shown in .",0
25716,"By introducing the auxiliary document extraction and paragraph extraction tasks , the proposed model can progressively narrow down the search space from coarse to fine , which helps to better locate on the final answer span .",0
25717,"The final answer prediction is based on the results of all the three tasks , which is jointly optimized with a joint learning method .",0
25718,Shared Q&D,0
25719,"Modeling Given a question Q and a set of selected documents { D i } , one of the keys in MRC model lies in how to incorporate the question context into the document , so that important information can be highlighted .",0
25720,"We follow the attention & fusion mechanism used in , which is a previous state - of - the - art MRC method on SQuAD dataset .",0
25721,"Specifically , we first map each word into the vector space by concatenating its word embedding and CNN - based character embedding .",0
25722,Then we use bi-directional LSTM ( BiL - STM ) to encode the question Q and documents { D i } as :,0
25723,where e t and ct are the word embedding and character embedding of the t th word .,0
25724,"u Q t and u D tare the encoding vectors of the t th word in Q and D , respectively .",0
25725,"After the encoding , we use the co-attention method to effectively incorporate the question information into the document context , and obtain the question - aware document representation ?",0
25726,D t = j ? tj u D j .,0
25727,"We adopt the attention function used in DrQA , which computes the attention score ?",0
25728,ij by the dot products between nonlinear mappings of word representations :,0
25729,"where W l is a linear projection matrix , softmax is the normalization function , and ReLU is the nonlinear activation function .",0
25730,To combine the original representation u D t and the attention vector ?,0
25731,"D t , we adopt the fusion kernel used in for better semantic understanding :",0
25732,"( 3 ) where the fusion kernel Fuse ( , ) is actually a gating layer to combine two representations , we do not give the details here due to space limitation .",0
25733,"To model the long distance dependency issue of document context , we also introduce the self - attention layer to further align the document representation v D t against itself , as :",0
25734,where W sis a trainable bilinear projection matrix .,0
25735,Another fusion kernel is again used to combine the original and selfattentive representations .,0
25736,"For all the previous encoding and attention steps , we process each document independently given the question .",0
25737,"Finally , we obtain a question - aware representation D Di = {d",0
25738,Di t } for each word in each document .,0
25739,"For the question side , since it is generally short , we directly self - align the question to a vector r Q , which is independent from the document , as",0
25740,where w q is a trainable linear weight vector .,0
25741,The shared question and document modeling lay the foundation for the subsequent three extraction tasks .,0
25742,Based on the document and question representations D Di = {d,0
25743,"Di t } and r Q , we introduce the three joint extraction tasks .",0
25744,Document Extraction,0
25745,"In multi-document MRC , in addition to annotating the answer span , the benchmark datasets generally also annotate which documents are correct for extracting the answer , or it can also be easily obtained given the labeled answer .",0
25746,"Therefore , we also introduce an auxiliary document extraction task , to help improve the answer prediction .",0
25747,"Compared to the answer span extraction , the document extraction is relatively easier .",0
25748,The aim is to better lay the foundation for the answer prediction and help learn the shared bottom layers .,0
25749,"Firstly , we also self - align the document representation D Di = {d",0
25750,"Di t } for each selected document Di , to obtain a weighted document vector r Di as :",0
25751,"Next , the question vector r Q and document vector r Di are matched in a bilinear function fora relevance score as ,",0
25752,"where W qd is a trainable bilinear projection matrix , which helps to match the two vectors in the same space .",0
25753,"For one question , each selected document Di has a matching score s Di .",0
25754,We normalize their scores and optimize the following objective function :,0
25755,where K is the number of selected documents .,0
25756,y Di ?,0
25757,"{ 0 , 1 } denotes the label , y Di = 1 means document",0
25758,"Di contains one golden answer , otherwise y",0
25759,Di = 0 .,0
25760,Paragraph Extraction,0
25761,"In general , the golden answer usually comes from one or two paragraphs in each document .",0
25762,"We can also annotate the correct paragraphs where the answer is extracted from , by some distant supervision method ) .",0
25763,"Therefore , we introduce a midlevel paragraph extraction task , so that our model can not only distinguish among different documents , but it can also select the relevant paragraphs within each document .",0
25764,"We first organize each selected document with paragraphs , and follow the same way as in document extraction to calculate a question - paragraph matching score for each paragraph .",0
25765,"Specifically , for each paragraph in document Di with D Di = { V Pi1 , , V PiN } , we first self - align the wordlevel paragraph representation V Pij to a weighted vector representation r Pij as in Equ .",0
25766,6 . Then a bilinear matching function is used between r Q and r Pij to obtain the corresponding relevance score as :,0
25767,where W qp is the trainable bilinear projection matrix between question and paragraph .,0
25768,"For one document , each paragraph P ij in the document has a matching score s Pij .",0
25769,We normalize the scores among each document and obtains Pij as in Equ .,0
25770,"8 . In this sub - task , we optimize the average cross - entropy loss among all the selected documents and paragraphs as :",0
25771,where N is the number of remaining paragraphs for each document .,0
25772,y Pij ?,0
25773,"{ 0 , 1 } denotes the paragraph - level label for the j th paragraph in i th document .",0
25774,Answer Span Extraction,0
25775,"The ultimate goal is to predict a correct answer , where the afore - mentioned document extraction and paragraph extraction actually act as two auxiliary tasks , so that the shallow semantic representations can be better learnt .",0
25776,"In this stage , we aim to combine all the available information to accurately extract the answer from all the selected documents at a span level .",0
25777,"To make the document representation aware of information in different documents and enable a direct comparison across different documents , we concatenate all the selected documents together and introduce a muilti-document shared LSTM layer for contextual modeling as :",0
25778,where f is a manual feature vector including the popular features such as whether each document word occurs in the question words and whether the word is a sentence ending separator .,0
25779,Here we also concatenate the question vector r Q to each word representation d D t of the document for better modeling the interaction .,0
25780,"Since all the words from different documents will be passed to the shared LSTM layer , the sequence order is thus very important .",0
25781,"We follow the document ranking order obtained via the document ranking function in document retrieval module , as is shown in top of .",0
25782,"In this way , we expect that the answer prediction model can also bear the ranking relevance in document retrieval module in mind and it shows good performance in our experiment .",0
25783,"Finally , the pointer network ) is used to predict the start and end position of the answer with the probabilities ?",0
25784,1 t and ?,0
25785,"2 t , and the answer extraction model can be trained by minimizing the negative log probabilities of the true start and end indices :",0
25786,"where w a is a trainable vector , | D w | is the total number of words .",0
25787,"M is the number of question samples , y 1 i , y 2 i are the golden start and end positions across the entire documents .",0
25788,Joint Training and Prediction,0
25789,"According to the design , the three extraction tasks share the same embedding , encoding and matching layers .",0
25790,"Therefore , we propose to train them together as multi -task learning .",0
25791,The joint objective function is formulated as follows :,0
25792,where ? 1 and ?,0
25793,2 are two hyper - parameters that control the weights of those tasks .,0
25794,"To keep the training process stable , we adopt a coarseto - fine joint training strategy and progressively finetune one upper task with the joint loss .",0
25795,"Specifically , we first train the downside document extraction and paragraph extraction tasks to obtain an initial shallow representation , and then jointly train the three tasks with Equ.15 based on it .",0
25796,"Besides , when training anew upper task , we follow the method in ) and introduce a successive regularization term on the shared parameters , as :",0
25797,"where ? s , ?",0
25798,s are the shared parameters at successive training stages .,0
25799,"In this way , we can restrain the joint training process so that the shared parameters will not change so much .",0
25800,"When predicting the final answer , we take the document matching score , paragraph matching score and answer span score into consideration and choose the answer with the highest prediction score , given as :",0
25801,Experiments,0
25802,This section presents the experimental methodology .,0
25803,We first verify the effectiveness of our model on two benchmark datasets : Trivia QA and DuReader ) .,0
25804,"Then we test our model in operational online environment , which can stably and effectively serve different scenarios promptly .",0
25805,Datasets,0
25806,Off - line Benchmark Dataset,0
25807,"We choose the TriviaQA Web and DuReader benchmark datasets to test our method , since both of them are multi-document MRC datasets which is more realistic and challenging .",0
25808,Trivia,0
25809,"QA is a recently released large - scale multidocument MRC datasets , which consists of 650K contextquery - answer triples .",0
25810,"There are 95K distinct questionanswer pairs , which are authored by Trivia enthusiasts , with 6 evidence documents ( context ) per question on average , which are generated from either Wikipedia or Web search .",0
25811,"In this paper , we focus on the TriviaQA Web dataset , which contains more context data for each question .",0
25812,"DuReader is so far the largest Chinese MRC dataset , which contains 200K questions , 1 M documents and more than 420K human - summarized answers .",0
25813,"All the questions and documents are extracted from real data , by the largest Chinese search engine Baidu .",0
25814,"The average length of the documents is 396.0 words , and on average each question has 5 evidence documents , each document has about 7 paragraphs .",0
25815,On - line Environment,0
25816,"We also apply our model to the Al - iMe Chatbot system , which is an intelligent online assistant designed for creating an innovative online shopping experience in e-commerce .",0
25817,"Currently , it serves millions of customer questions per day .",0
25818,"We test our model in two practical scenarios , i.e. , e-commerce promotion and tax policy reading .",0
25819,"E-commerce promotion scenario is about consulting instructions on shopping games and sales promotion , which mostly involves with a short document with no more than 500 words .",0
25820,"Tax policy scenario is about reading tax policy articles , which can be viewed as a multi-document MRC task .",0
25821,"The length of the article is much longer , which consist of many sections and paragraphs .",0
25822,Implementation Details,0
25823,"For the cascade ranking functions , the number of selected documents K and paragraphs N are the key factors to balance the effectiveness and efficiency trade - off .",0
25824,We choose K = 4 and N = 2 for the good performance when evaluating on the dev set .,1
25825,"Since the Trivia QA documents often contain many small paragraphs , we also restructure the documents by merging consecutive paragraphs to a maximum size of 600 words for each paragraph as in ( Clark and Gardner 2017 ) .",0
25826,The detailed analysis will be given and discussed in the next section .,0
25827,"For the multi-task deep attention framework , we adopt the Adam optimizer for training , with a mini-batch size of 32 and initial learning rate of 0.0005 .",1
25828,We use the GloVe 300 dimensional word embeddings in TriviaQA and train a word2 vec word embeddings with the whole DuReader corpus for DuReader .,1
25829,The word embeddings are fixed during training .,1
25830,The hidden size of LSTM is set as 150 for TriviaQA and 128 for DuReader .,1
25831,The task - specific hyper - parameters ? 1 and ? 2 in Equ. 15 are set as ? 1 = ? 2 = 0.5 . Regularization parameter ? in Equ.,1
25832,16 is set as a small value of 0.01 .,1
25833,All models are trained on Nvidia Tesla M40 GPU with Cudnn LSTM cell in Tensorflow 1.3 .,1
25834,Off - line Evaluation,0
25835,Main Results,0
25836,"The results of our single deep cascade model 4 on TriviaQA Web and DuReader 1.0 are summarized in , respectively .",0
25837,"We can see that by adopting the deep cascade learning framework , the proposed model outperforms the previous state - of - the - art methods by an evident margin on both datasets , which validates the effectiveness of the proposed method in addressing the challenging multi-document MRC task .",1
25838,Ablation Study,0
25839,"To get better insight into our model architecture , we conduct an in - depth ablation study on the development set of DuReader and TriviaQA , which is shown in .",0
25840,"The main goal is to validate the effectiveness of the critical components in our architecture including the manual features and multi-document shared LSTM in the pure answer span extraction task , the cascade document and paragraph ranking functions for pruning irrelevant document content and the adoption of multi-task learning strategy .",0
25841,We only submit the single model without any model ensemble . 31.9 39.2 PR + BiDAF 37.55 41.81 Cross - Passage Verify 40.97 44.18 R- net 44,0
25842,"From the results , we can see that : 1 ) the shared LSTM plays an important role in answer extraction among multiple documents , the benefit lies in two parts : a ) it helps to normalize the content probability score from multiple documents so that the answers extracted from different documents can be directly compared ; b ) it can keep the ranking order from document ranking component in mind , which may serve as an additional signal when predicting the best answer .",1
25843,"By incorporating the manual features , the performance can be further improved slightly .",1
25844,"2 ) Both the preliminary cascade ranking and multi-task answer extraction strategy are vital for the final performance , which serve as a good trade - off between the pure pipeline method and fully joint learning method .",1
25845,"By removing the rich irrelevant noisy data in the cascade document and paragraph ranking stage , the downside MRC model can better extract the answer from the more relevant content data .",0
25846,"Jointly training the three extraction tasks can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",1
25847,Effectiveness v.s. Efficiency Trade - off,0
25848,"Now we further examine how the performance of our model changes with respect to the number of selected documents and paragraphs in cascade ranking stage , which is the key factor to control the effectiveness and efficiency trade - off .",0
25849,The result on DuReader development set is presented in .,0
25850,"We can see that : 1 ) By properly taking more documents or paragraphs into consideration , the performance of the model gradually increases when it reaches 4 documents and 2 paragraphs , and then the performance decreases slightly which maybe due to that much noisy data is introduced .",0
25851,"2 ) The time cost can be largely reduced by removing more irrelevant documents and paragraphs in the cascade ranking stage , while keeping the performance not change that much .",0
25852,"For example , for the best setting at 4 documents and 2 paragraphs , if we instead only keep the top - 1 paragraph for each document , the time cost will be reduced by 36.7 % , while the performance only decreases about 2.4 % .",0
25853,"As a result , we can adaptively change our model to meet the practical situation and we choose 4 documents and 2 paragraphs in our off - line experiment where effectiveness is most emphasized .",0
25854,"Advantage of Multi- task Learning Next , we also analyze the benefits brought in via the adoption of the multitask learning strategy in detail .",0
25855,The performance of jointly training the answer extraction module with different auxiliary tasks on DuReader development set is shown in Table 5 .,0
25856,"We can see that by incorporating the auxiliary document extraction or paragraph extraction task in the joint learning framework , the performance can always improve which again shows the advantage of introducing auxiliary tasks for helping to learn shared bottom representations .",0
25857,"Besides , the performance gain by adding document extraction task is larger , which maybe due to that it can better lay the foundation of the model with that information from different documents can be distinguished .",0
25858,On- line Evaluation,0
25859,Results on E-commerce and Tax data,0
25860,"We also test the effectiveness and efficiency of our model in two practical scenarios , E-commerce and tax policy reading , where realtime responses are expected and a large number of customers are being served simultaneously .",0
25861,The comparative result is shown in .,0
25862,"We can see that by introducing the cascade ranking stage and keeping the selected number properly , our method can serve the requests with a much higher speed of less than 50 ms , especially for tax scenario where the improvement is about 3 times .",0
25863,"Besides , the performance with respect to F 1 score is also largely improved with the proposed multi-document MRC model , which demonstrates the effectiveness of our method for removing the rich irrelevant noisy content in our online scenario .",0
25864,Results on Different Document Lengths,0
25865,"We further examine how the F 1 score and response time change on tax scenario when processing documents with different lengths , ranging from 50 to 2000 words .",0
25866,The result is shown in .,0
25867,"We can see that without incorporating with the cascade ranking module , the answer extraction module performs rather poorly both in effectiveness and efficiency as the document length increases .",0
25868,"In particular , when the document length exceeds 1,000 the total response time increases 3 to 6 times , while for our full cascade model only 15 ms more are needed .",0
25869,Conclusion,0
25870,"In this paper , we propose a novel deep cascade learning framework to balance the effectiveness and efficiency in the more realistic multi-document MRC .",0
25871,"We design three cascade modules , which can eliminate irrelevant document content in the earlier stages with simple features and models , and discern more relevant answers at later stages .",0
25872,The experiment results show that our method can achieve state - of the - art performance on two large - scale benchmark datasets .,0
25873,"Besides , the proposed method has also been effectively and efficiently applied in our online system .",0
25874,title,0
25875,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,1
25876,abstract,0
25877,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .",1
25878,Most approaches were based on external memory and four components proposed by Memory Network .,0
25879,The distinctive component among them was the way of finding the necessary information and it contributes to the performance .,0
25880,"Recently , a simple but powerful neural network module for reasoning called Relation Network ( RN ) has been introduced .",0
25881,"We analyzed RN from the view of Memory Network , and realized that its MLP component is able to reveal the complicate relation between question and object pair .",0
25882,"Motivated from it , we introduce Relation Memory Network ( RMN ) which uses MLP to find out relevant information on Memory Network architecture .",0
25883,It shows new state - of - the - art results in jointly trained b AbI - 10 k story - based question answering tasks and bAbI dialog - based question answering tasks .,0
25884,* Code is publicly available at : https://github.com/juung/RMN,1
25885,INTRODUCTION,0
25886,Neural network has made an enormous progress on the two major challenges in artificial intelligence : seeing and reading .,0
25887,"In both areas , embedding methods have served as the main vehicle to process and analyze text and image data for solving classification problems .",0
25888,"As for the task of logical reasoning , however , more complex and careful handling of features is called for .",0
25889,A reasoning task requires the machine to answer a simple question upon the delivery of a series of sequential information .,0
25890,"For example , imagine that the machine is given the following three sentences : "" Mary got the milk there . "" ,",0
25891,""" John moved to the bedroom . "" , and "" Mary traveled to the hallway . """,0
25892,"Once prompted with the question , "" Where is the milk ? "" , the machine then needs to sequentially focus on the two supporting sentences , "" Mary got the milk there . "" and "" Mary traveled to the hallway . "" in order to successfully determine that the milk is located in the hallway .",0
25893,"Inspired by this reasoning mechanism , J. has introduced the memory network ( MemNN ) , which consists of an external memory and four components : input feature map ( I ) , generalization ( G ) , output feature map ( O ) , and response ( R ) .",0
25894,The external memory enables the model to deal with a knowledge base without loss of information .,0
25895,Input feature map embeds the incoming sentences .,0
25896,Generalization updates old memories given the new input and output feature map finds relevant information from the memory .,0
25897,"Finally , response produces the final output .",0
25898,"Based on the memory network architecture , neural network based models like end - to - end memory network ( Mem N2N ) , gated end - to - end memory network ( GMe m N2N ) , dynamic memory network ( DMN ) , and dynamic memory network + ( DMN + ) are proposed .",0
25899,"Since strong reasoning ability depends on whether the model is able to sequentially catching the right supporting sentences that lead to the answer , the most important thing that discriminates those models is the way of constructing the output feature map .",0
25900,"As the output feature map becomes more complex , it is able to learn patterns for more complicate relations .",0
25901,"For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two embedding matrices .",0
25902,"Recently , a new architecture called Relation Network ( RN ) has been proposed as a general solution to relational reasoning .",0
25903,The design philosophy behind it is to directly capture the supporting relation between the sentences through the multi - layer perceptron ( MLP ) .,0
25904,"Despite its simplicity , RN achieves better performance than previous models without any catastrophic failure .",0
25905,The interesting thing we found is that RN can also be interpreted in terms of MemNN .,0
25906,It is composed of O and R where each corresponds to MLP which focuses on the related pair and another MLP which infers the answer .,0
25907,RN does not need to have G because it directly finds all the supporting sentences at once .,0
25908,"In this point of view , the significant component would be MLP - based output feature map .",0
25909,"As MLP is enough to recognize highly non-linear pattern , RN could find the proper relation better than previous models to answer the given question .",0
25910,"However , as RN considers a pair at a time unlike MemNN , the number of relations that RN learns is n 2 when the number of input sentence is n .",0
25911,"When n is small , the cost of learning relation is reduced by n times compared to MemNN based models , which enables more data - efficient learning .",0
25912,"However , when n increases , the performance becomes worse than the previous models .",0
25913,"In this case , the pair - wise operation increases the number of non-related sentence pairs more than the related sentence pair , thereby confuses RN 's learning .",0
25914,"has suggested attention mechanisms as a solution to filter out unimportant relations ; however , since it interrupts the reasoning operation , it may not be the most optimal solution to the problem .",0
25915,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .",1
25916,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,1
25917,"In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .",1
25918,Experiments show its state - of the - art result on the text - based question answering tasks .,0
25919,"Relation Memory Network ( RMN ) is composed of four components - embedding , attention , updating , and reasoning .",0
25920,"It takes as the inputs a set of sentences x 1 , x 2 , ... , x n and its related question u , and outputs an answer a .",0
25921,"Each of the x i , u , and a is made up of one - hot representation of words , for example ,",0
25922,RELATION MEMORY NETWORK,0
25923,".. , n i ) , V = vocabulary size , n i = number of words in sentence i ) .",0
25924,EMBEDDING COMPONENT,0
25925,"We first embed words in each x i = {x i 1 , x i 2 , x i3 , ... , x ini } and u to a continuous space multiplying an embedding matrix A ?",0
25926,R d V .,0
25927,"Then , the embedded sentence is stored and represented as a memory object mi while question is represented as q .",0
25928,"Any of the following methods are available for embedding component : simple sum ( equation 1 ) , position encoding ( J. ( equation 2 ) , concatenation ( equation 3 ) , LSTM , and GRU .",0
25929,"In case of LSTM or GRU , mi is the final hidden state of it .",0
25930,"As the following attention component takes the concatenation of mi and q , it is not necessarily the case that sentence and question have the same dimensional embedding vectors unlike previous memory - augmented neural networks .",0
25931,ATTENTION COMPONENT,0
25932,Attention component can be applied more than once depending on the problem ; illustrates 2 hop version of RMN .,0
25933,We refer to the i th embedded sentence on the t th hop as mt i .,0
25934,"To constitute the attention component , we applied simple MLP represented as gt ? .",0
25935,"It must be ended with 1 unit output layer to provide a scalar weight wt i , which leads to an attention weight ?",0
25936,ti between 0 and 1 .,0
25937,"In the beginning , a vector concatenated with m 1 i and q flows to the g 1 ? .",0
25938,"From the result of g 1 ? , attention weight ?",0
25939,1 i is calculated using additional variable ? 1 ( ?,0
25940,"1 ) to control the intensity of attention , inspired by the way Neural Turing Machine reads from the memory .",0
25941,"Then we get the related memory object r 1 , a weighted sum of ?",0
25942,1 i and memory object m 1 i for all i .,0
25943,"If there exist more hops , r 1 is directly taken to the next hop and iterates over this process with the updated memory object m 2 i .",0
25944,"All the procedures are rewritten as equation 4 , 5 , and 6 :",0
25945,UPDATING COMPONENT,0
25946,"To forget the information already used , we use intuitive updating component to renew the memory .",0
25947,It is replaced by the amount of unconsumed from the old one :,0
25948,"Contrary to other components , updating is not a mandatory component .",0
25949,"When it is considered to have 1 hop , there is no need to use this .",0
25950,REASONING COMPONENT,0
25951,"Similar to attention component , reasoning component is also made up of MLP , represented as f ? .",0
25952,It receives both q and the final result of attention component r f and then takes a softmax to produce the model answer :,0
25953,"To answer the question from a given set of facts , the model needs to memorize these facts from the past .",0
25954,"Long short term memory ( LSTM ) , one of the variants of recurrent neural network ( RNN ) , is inept at remembering past stories because of their small internal memory .",0
25955,"To cope with this problem , J. Weston & Bordes MemN2N , GMem N2N , DMN , and DMN + all follow the same structure of MemNN from abroad perspective , however , output feature map is composed in slightly different way .",0
25956,The relation between question and supporting sentences is realized from its cooperation .,0
25957,"MemN2N first calculates the relatedness of sentences in the question and memory by taking the inner product , and the sentence with the highest relatedness is selected as the first supporting sentence for the given question .",0
25958,The first supporting sentence is then added with the question and repeat the same operation with the updated memory to find the second supporting sentence .,0
25959,"GMem N2N selects the supporting sentence in the same way as MemN2N , but uses the gate to selectively add the the question to control the influence of the question information in finding the supporting sentence in the next step .",0
25960,"DMN and DMN + use output feature map based on various relatedness such as absolute difference , as well as inner product , to understand the relation between sentence and question at various points .",0
25961,"The more difficult the task , the more complex the output feature map and the generalization component to get the correct answer .",0
25962,"For a dataset experimenting the text - based reasoning ability of the model , the overall accuracy could be increased in order of MemN2N , GMe m N2N , DMN , and DMN + , where the complexity of the component increases .",0
25963,RELATION NETWORK,0
25964,Relation Network ( RN ) has emerged as a new and simpler framework for solving the general reasoning problem .,0
25965,RN takes in a pair of objects as its input and simply learns from the compositions of two MLPs represented as g ?,0
25966,and f ? .,0
25967,"The role of each MLP is not clearly defined in the original paper , but from the view of MemNN , it can be understood that g ?",0
25968,corresponds to O and f ?,0
25969,"corresponds to R. summarizes the interpretation of RN compared to MemN2N and our model , RMN .",0
25970,"To verify the role of g ? , we compare the output when pairs are made with supporting sentences and when made with unrelated sentences .",0
25971,shows the visualization result of each output .,0
25972,"When we focus on whether the value is activated or not , we can see that g ?",0
25973,distinguishes supporting sentence pair from non-supporting sentence pair as output feature map examines how relevant the sentence is to the question .,0
25974,"Therefore , we can comprehend the output of g ?",0
25975,reveals the relation between the object pair and the question and f ?,0
25976,aggregates all these outputs to infer the answer .,0
25977,bAbI story - based QA dataset bAb I story - based QA dataset is composed of 20 different types of tasks for testing natural language reasoning ability .,0
25978,Each task requires different methods to infer the answer .,0
25979,"The dataset includes a set of statements comprised of multiple sentences , a question and answer .",0
25980,A statement can be as short as two sentences and as long as 320 sentences .,0
25981,"To answer the question , it is necessary to find relevant one or more sentences to a given question and derive answer from them .",0
25982,"Answer is typically a single word but in a few tasks , answers area set of words .",0
25983,Each task is regarded as success when the accuracy is greater than 95 % .,0
25984,"There are two versions of this dataset , one that has 1 k training examples and the other with 10 k examples .",0
25985,Most of the previous models test their accuracy on 10 k dataset with trained jointly .,0
25986,bAbI dialog dataset bAb I dialog dataset ) is a set of 5 tasks within the goal - oriented context of restaurant reservation .,0
25987,"It is designed to test if model can learn various abilities such as performing dialog management , querying knowledge bases ( KBs ) , and interpreting the output of such queries .",0
25988,"The KB can be queried using API calls and 4 fields ( a type of cuisine , a location , a price range , and a party size ) .",0
25989,They should be filled to issue an API call .,0
25990,Task 1 tests the capacity of interpreting a request and asking the right questions to issue an API call .,0
25991,Task 2 checks the ability to modify an API call .,0
25992,Task 3 and 4 test the capacity of using outputs from an API call to propose options in the order of rating and to provide extra-information of what user asks for .,0
25993,Task 5 combines everything .,0
25994,"The maximum length of the dialog for each task is different : 14 for task 1 , 20 for task 2 , 78 for task 3 , 13 for task 4 , and 96 for task 5 .",0
25995,"As restaurant name , locations , and cuisine types always face new entities , there are normal and OOV test sets to assess model 's generalization ability .",0
25996,"Training sets consist fo 1 k examples , which is not a large amount of creating realistic learning conditions .",0
25997,TRAINING DETAILS,0
25998,bAbI story - based QA dataset,1
25999,We trained 2 hop RMN jointly on all tasks using 10 k dataset for model to infer the solution suited to each type of tasks .,0
26000,"We limited the input to the last 70 stories for all tasks except task 3 for which we limited input to the last 130 stories , similar to which is the hardest condition among previous models .",0
26001,"Then , we labeled each sentence with its relative position .",0
26002,"Embedding component is similar to , where story and question are embedded through different LSTMs ; 32 unit word - lookup embeddings ; 32 unit LSTM for story and question .",1
26003,"For attention component , as we use 2 hop RMN , there are g 1 ?",1
26004,"and g 2 ? ; both are three - layer MLP consisting of 256 , 128 , 1 unit with ReLU activation function . f ? is composed of 512 , 512 , and 159 units ( the number of words appearing in bAbI dataset is 159 ) of three - layer MLP with ReLU non-linearities where the final layer was a linear that produced logits fora softmax over the answer vocabulary .",0
26005,"For regularization , we use batch normalization for all MLPs .",1
26006,The softmax output was optimized with a cross - entropy loss function using the Adam optimizer with a learning rate of 2 e ?4 .,1
26007,bAbI dialog dataset,1
26008,"We trained on full dialog scripts with every model response as answer , all previous dialog history as sentences to be memorized , and the last user utterance as question .",1
26009,"Model selects the most probable response from 4,212 candidates which are ranked from a set of all bot utterances appearing in training , validation and test sets ( plain and OOV ) for all tasks combined .",1
26010,We also report results when we use match type features for dialog .,0
26011,Match type feature is an additional label on the candidates indicating if word is found on the dialog history .,0
26012,"For example , if the shows how our model solved several tasks .",0
26013,RMN 's attention component g 1 ?,0
26014,and g 2 ?,0
26015,complement each other to identify the necessary facts to answer correctly .,0
26016,Sometimes both g 1 ?,0
26017,and g 2 ?,0
26018,"concentrate on the same sentences which are all critical to answer the question , and sometimes g 1 ?",0
26019,finds a fact related to the given question and with this information g 2 ?,0
26020,chooses the key fact to answer .,0
26021,"While trained jointly , RMN learns these different solutions for each task .",0
26022,"For the task 3 , the only failed task , attention component still functions well ; it focuses sequentially on the supporting sentences .",0
26023,"However , the reasoning component , f ? , had difficulty catching the word ' before ' .",0
26024,"We could easily figure out ' before ' implies ' just before ' the certain situation , whereas RMN confused its meaning .",0
26025,"As shown in table 3 c , our model found all previous locations before the garden .",0
26026,"Still , it is remarkable that the simple MLP carried out all of these various roles .",0
26027,BABI DIALOG,1
26028,The results in the show that the RMN has the best results in any conditions .,0
26029,"Without any match type , RN and RMN outperform previous memory - augmented models on both normal and OOV tasks .",1
26030,This is mainly attributed to the impressive result on task 4 which can be interpreted as an effect of MLP based output feature map .,0
26031,"To solve task 4 , it is critical to understand the relation between ' phone number ' of user input and 'r phone ' of previous dialog as shown in .",0
26032,We assumed that inner product was not sufficient to capture their implicit similarity and performed an supporting experiment .,0
26033,"We converted RMN 's attention component to inner product based attention , and the results revealed the error rate increased to 11.3 % .",1
26034,"For the task 3 and task 5 where the maximum length is especially longer than the others , RN performs worse than MemN2N , GMe m N2N and RMN .",0
26035,The number of unnecessary object pairs created by the RN not only increases the processing time but also decreases the accuracy .,1
26036,"With the match type feature , all models other than RMN have significantly improved their performance except for task 3 compared to the plain condition .",1
26037,RMN was helped by the match type only on the OOV tasks and this implies RMN is able to find relation in the With Match condition for the normal tasks .,0
26038,"When we look at the OOV tasks more precisely , RMN failed to perform well on the OOV task 1 and 2 even though g 1 ?",0
26039,properly focused on the related object as shown in .,0
26040,We state that this originated from the fact that the number of keywords in task 1 and 2 is bigger than that in task,0
26041,"4 . In task 1 and 2 , all four keywords ( cuisine , location , number and price ) must be correctly aligned from the supporting sentence in order to make the correct API call which is harder than task",0
26042,4 . Consider the example in .,0
26043,"Supporting sentence of task 4 have one keyword out of three words , whereas supporting sentences of task 1 and 2 consist of four keywords ( cuisine , location , number and price ) out of sixteen words .",0
26044,"Different from other tasks , RMN yields the same error rate 25.1 % with MemN2N and GMe m N2N on the task 3 .",1
26045,The main goal of task 3 is to recommend restaurant from knowledge base in the order of rating .,0
26046,All failed cases are displaying restaurant where the user input is < silence > which is somewhat an ambiguous trigger to find the input relevant previous utterance .,0
26047,"As shown in , there are two different types of response to the same user input .",0
26048,"One is to check whether all the required fields are given from the previous utterances and then ask user for the missing fields or send a "" Ok let me look into some options for you . "" message .",0
26049,The other type is to recommend restaurant starting from the highest rating .,0
26050,All models show lack of ability to discriminate these two types of silences so that concluded to the same results .,0
26051,"To verify our statement , we performed an additional experiment on task 3 and checked the performance gain ( extra result is given in of Appendix B ) .",0
26052,MODEL ANALYSIS,0
26053,Effectiveness of the MLP - based output feature map,0
26054,The most important feature that distinguishes MemNN based models is the output feature map .,0
26055,summarizes the experimental results for the bAbI story - based QA dataset when replacing the RMN 's MLP - based output feature map with the idea of the previous models .,0
26056,"inner product was used in MemN2N , inner product with gate was used in GMe m N2N , and inner product and absolute difference with two embedding matrices was used in DMN and DMN + .",0
26057,"From the , the more complex the output feature map , the better the overall performance .",0
26058,"In this point of view , MLP is the effective output feature map ..",0
26059,"When memory size is small , we could observe the data - effeciency of RN .",0
26060,It shows similar performance to RMN in less time .,0
26061,"However , when the memory size increases , performance is significantly reduced compared to RMN , even though it has been learned fora longer time .",0
26062,It is even lower than itself when the memory size is 20 .,0
26063,"On the other hand , RMN maintains high performance even when the memory size increases .",0
26064,Effectiveness of the number of hops bAb I story based QA dataset differs in the number of supporting sentences by each task that need to be referenced to solve problems .,0
26065,"For example , task 1 , 2 , and 3 require single , two , and three supporting facts , respectively .",0
26066,The result of the mean error rate for each task according to the number of hops is in .,0
26067,"Overall , the number of hops is correlated with the number of supporting sentences .",0
26068,"In this respect , when the number of relations increases , RMN could reason across increasing the number of hops to 3 , 4 or more .",0
26069,CONCLUSION,0
26070,"Our work , RMN , is a simple and powerful architecture that effectively handles text - based question answering tasks when large size of memory and high reasoning ability is required .",0
26071,Multiple access to the external memory to find out necessary information through a multi-hop approach is similar to most existing approaches .,0
26072,"However , by using a MLP that can effectively deal with complex relatedness when searching for the right supporting sentences among a lot of sentences , RMN raised the state - of - the - art performance on the story - based QA and goal - oriented dialog dataset .",0
26073,"When comparing RN which also used MLP to understand relations , RMN was more effective in the case of large memory .",0
26074,"Future work will apply RMN to image based reasoning task ( e.g. , CLEVR , DAQUAR , VQA etc. ) .",0
26075,"To extract features from the image , VGG net is used in convention and outputs 196 objects of 512 dimensional vectors which also require large sized memory .",0
26076,An important direction will be to find an appropriate way to focus sequentially on related object which was rather easy in text - based reasoning .,0
26077,Number in parentheses indicates the number of supporting sentences to solve the task A MODEL DETAILS We modify the user input from < silence > to < silence >< silence > when looking for restaurant recommendations .,0
26078,This makes model to distinguish two different situations whether to ask for additional fields or to recommend restaurant .,0
26079,title,0
26080,Text Understanding with the Attention Sum Reader Network,1
26081,abstract,0
26082,Several large cloze - style context - questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .,0
26083,"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .",1
26084,"We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .",0
26085,This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document .,0
26086,Ensemble of our models sets new state of the art on all evaluated datasets .,0
26087,Introduction,0
26088,Most of the information humanity has gathered up to this point is stored in the form of plain text .,0
26089,Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence .,0
26090,One way of testing the level of text understanding is simply to ask the system questions for which the answer can be inferred from the text .,0
26091,A well - known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM 's Watson system used for the Jeopardy challenge .,0
26092,"Cloze - style questions , i.e. questions formed by removing a phrase from a sentence , are an appealing form of such questions ( for example see ) .",0
26093,"While the task is easy to evaluate , one can vary the context , the question Document :",0
26094,What was supposed to be a fantasy sports car ride at Walt Disney World Speedway turned deadly when a Lamborghini crashed into a guardrail .,0
26095,"The crash took place Sunday at the Exotic Driving Experience , which bills itself as a chance to drive your dream car on a racetrack .",0
26096,"The Lamborghini 's passenger , 36 year - old Gary Terry of Davenport , Florida , died at the scene , Florida Highway Patrol said .",0
26097,"The driver of the Lamborghini , 24 - year - old .",0
26098,Anonymization of this example that makes the task harder is shown in .,0
26099,sentence or the specific phrase missing in the question to dramatically change the task structure and difficulty .,0
26100,"One way of altering the task difficulty is to vary the word type being replaced , as in .",0
26101,The complexity of such variation comes from the fact that the level of context understanding needed in order to correctly predict different types of words varies greatly .,0
26102,"While predicting prepositions can easily be done using relatively simple models with very little context knowledge , predicting named entities requires a deeper understanding of the context .",0
26103,"Also , as opposed to selecting a random sentence from a text as in ) , the question can be formed from a specific part of the document , such as a short summary or a list of tags .",0
26104,and the statistics provided with the CBT data set .,0
26105,CNN,0
26106,Daily,0
26107,"Since such sentences often paraphrase in a condensed form what was said in the text , they are particularly suitable for testing text comprehension .",0
26108,An important property of cloze - style questions is that a large amount of such questions can be automatically generated from real world documents .,0
26109,This opens the task to data - hungry techniques such as deep learning .,0
26110,This is an advantage compared to smaller machine understanding datasets like MCTest that have only hundreds of training examples and therefore the best performing systems usually rely on handcrafted features .,0
26111,In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets .,0
26112,Then we present our own model to tackle the problem .,0
26113,Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model .,0
26114,Task and datasets,0
26115,In this section we introduce the task that we are seeking to solve and relevant large - scale datasets that have recently been introduced for this task .,0
26116,Formal Task Description,0
26117,"The task consists of answering a cloze - style question , the answer to which depends on the understanding of a context document provided with the question .",0
26118,The model is also provided with a set of possible answers from which the correct one is to be selected .,0
26119,This can be formalized as follows :,0
26120,"The training data consist of tuples ( q , d , a , A ) , where q is a question , dis a document that con-tains the answer to question q , A is a set of possible answers and a ?",0
26121,A is the ground truth answer .,0
26122,Both q and dare sequences of words from vocabulary V .,0
26123,"We also assume that all possible answers are words from the vocabulary , that is A ?",0
26124,"V , and that the ground truth answer a appears in the document , that is a ?",0
26125,d.,0
26126,Datasets,0
26127,We will now briefly summarize important features of the datasets .,0
26128,News Articles - CNN and Daily Mail,0
26129,The first two datasets 1 were constructed from a large number of news articles from the CNN and Daily Mail websites .,0
26130,"The main body of each article forms a context , while the cloze - style question is formed from one of short highlight sentences , appearing at the top of each article page .",0
26131,"Specifically , the question is created by replacing a named entity from the summary sentence ( e.g. "" Producer X will not press charges against Jeremy Clarkson , his lawyer says . "" ) .",0
26132,Furthermore the named entities in the whole dataset were replaced by anonymous tokens which were further shuffled for each example so that the model can not buildup any world knowledge about the entities and hence has to genuinely rely on the context document to search for an answer to the question .,0
26133,Qualitative analysis of reasoning patterns needed to answer questions in the CNN dataset together with human performance on this task are provided in .,0
26134,Children 's Book Test,0
26135,"The third dataset 2 , the Children 's Book Test ( CBT ) , is built from books that are freely available thanks to Project Gutenberg 3 .",0
26136,Each context document is formed by 20 consecutive sentences taken from a children 's book story .,0
26137,"Due to the lack of summary , the cloze - style question is then constructed from the subsequent ( 21 st ) sentence .",0
26138,"One can also see how the task complexity varies with the type of the omitted word ( named entity , common noun , verb , preposition ) .",0
26139,"have shown that while standard LSTM language models have human level performance on predicting verbs and prepositions , they lack behind on named entities and common nouns .",0
26140,In this article we therefore focus only on predicting the first two word types .,0
26141,"Basic statistics about the CNN , Daily Mail and CBT datasets are summarized in .",0
26142,Our Model - Attention Sum Reader,0
26143,Our model called the Attention Sum Reader ( AS Reader ) 4 is tailor - made to leverage the fact that the answer is a word from the context document .,1
26144,This is a double - edged sword .,0
26145,"While it achieves stateof - the - art results on all of the mentioned datasets ( where this assumption holds true ) , it can not produce an answer which is not contained in the document .",0
26146,"Intuitively , our model is structured as follows :",0
26147,1 .,0
26148,We compute a vector embedding of the query .,1
26149,2 .,0
26150,We compute a vector embedding of each individual word in the context of the whole document ( contextual embedding ) .,1
26151,3 .,0
26152,"Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document , we select the most likely answer .",1
26153,Formal Description,0
26154,Our model uses one word embedding function and two encoder functions .,0
26155,The word embedding function e translates words into vector representations .,0
26156,The first encoder function is a document encoder f that encodes every word from the document din the context of the whole document .,0
26157,We call this the contextual embedding .,0
26158,For convenience we will denote the contextual embedding of the i - th word ind as f i ( d ) .,0
26159,The second encoder g is used to translate the query q into a fixed length representation of the same dimensionality as each f i ( d ) .,0
26160,Both encoders use word embeddings computed bye as their input .,0
26161,Then we compute a weight for every word in the document as the dot product of its contextual embedding and the query embedding .,0
26162,This weight might be viewed as an attention over the document d.,0
26163,"To form a proper probability distribution over the words in the document , we normalize the weights using the softmax function .",0
26164,This way we model probability s i that the answer to query q appears at position i in the document d .,0
26165,Ina functional form this is :,0
26166,Finally we compute the probability that word w is a correct answer as :,0
26167,"where I ( w , d ) is a set of positions where w appears in the document d .",0
26168,We call this mechanism pointer sum attention since we use attention as a pointer over discrete tokens in the context document and then we directly sum the word 's attention across all the occurrences .,0
26169,This differs from the usual use of attention in sequence - to - sequence models where attention is used to blend representations of words into a new embedding vector .,0
26170,Our use of attention was inspired by Pointer Networks ( Ptr - Nets ) .,0
26171,A high level structure of our model is shown in .,0
26172,Model instance details,0
26173,"In our model the document encoder f is implemented as a bidirectional Gated Recurrent Unit ( GRU ) network ) whose hidden states form the contextual word embeddings , that is : Structure of the model .",0
26174,... what was supposed to be a fantasy sports car ride at @entity 3 turned deadly when a @entity 4 crashed into a guardrail .,0
26175,"the crash took place sunday at the @en-tity 8 , which bills itself as a chance to drive your dream car on a racetrack .",0
26176,"the @entity 4 's passenger , 36 year - old @entity14 of @entity 15 , @entity 16 , died at the scene , @entity 13 said .",0
26177,"the driver of the @entity 4 , 24 - year - old @entity18 of @entity 19 , @entity 16 , lost control of the vehicle , the @entity 13 said .",0
26178,...,0
26179,"officials say the driver , 24 - year - old @entity18 , lost control of a :",0
26180,Attention in an example with anonymized entities where our system selected the correct answer .,0
26181,Note that the attention is focused only on named entities . ? ?,0
26182,f i denote forward and backward contextual embeddings from the respective recurrent networks .,0
26183,The query encoder g is implemented by another bidirectional GRU network .,0
26184,"This time the last hidden state of the forward network is concatenated with the last hidden state of the backward network to form the query embedding , that is g ( q ) = ? ? g | q| ( q ) || ? ? g 1 ( q ) .",0
26185,The word embedding function e is implemented in a usual way as a look - up table V .,0
26186,"V is a matrix whose rows can be indexed by words from the vocabulary , that is e ( w ) = V w , w ? V .",0
26187,"Therefore , each row of V contains embedding of one word from the vocabulary .",0
26188,"During training we jointly optimize parameters off , g and e.",0
26189,"... @entity11 film critic @entity29 writes in his review that "" anyone nostalgic for childhood dreams of transformation will find something to enjoy in an uplifting movie that invests warm sentiment in universal themes of loss and resilience , experience and maturity . "" more : the best and worst adaptations of "" @entity "" @entity43 , @entity44 and @entity 46 star in director @entity 48's crime film about a hit man trying to save his estranged son from a revenge plot .",0
26190,@entity11 chief film critic @entity 52 writes in his review that the film ...,0
26191,stars in crime film about hit man trying to save his estranged son :,0
26192,Attention over an example where our system failed to select the correct answer ( en - tity 43 ) .,0
26193,The system was probably mislead by the co-occurring word ' film ' .,0
26194,"Namely , entity11 occurs 7 times in the whole document and 6 times it is together with the word ' film ' .",0
26195,"On the other hand , the correct answer occurs only 3 times in total and only once together with ' film ' .",0
26196,Related Work,0
26197,Several recent deep neural network architectures ) were applied to the task of text comprehension .,0
26198,The last two architectures were developed independently at the same time as our work .,0
26199,All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question .,0
26200,We will now briefly describe these architectures and compare them to our approach .,0
26201,Attentive and Impatient Readers,0
26202,Attentive and Impatient Readers were proposed in .,0
26203,The simpler Attentive Reader is very similar to our architecture .,0
26204,It also uses bidirectional document and query encoders to compute an attention in a similar way we do .,0
26205,The more complex Impatient Reader computes attention over the document after reading every word of the query .,0
26206,"However , empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets .",0
26207,"The key difference between the Attentive Reader and our model is that the Attentive Reader uses attention to compute a fixed length representation r of the document d that is equal to a weighted sum of contextual embeddings of words ind , that is r = i s if i ( d ) .",0
26208,A joint query and document embedding m is then a non-linear function of rand the query embedding g ( q ) .,0
26209,This joint embedding m is in the end compared against all candidate answers a ?,0
26210,"A using the dot product e ( a ) m , in the end the scores are normalized by softmax .",0
26211,"That is : P ( a | q , d ) ? exp ( e ( a ) m ) .",0
26212,"In contrast to the Attentive Reader , we select the answer from the context directly using the computed attention rather than using such attention for a weighted sum of the individual representations ( see Eq. 2 ) .",0
26213,The motivation for such simplification is the following .,0
26214,"Consider a context "" A UFO was observed above our city in January and again in March . "" and question "" An observer has spotted a UFO in .""",0
26215,"Since both January and March are equally good candidates , the attention mechanism might put the same attention on both these candidates in the context .",0
26216,The blending mechanism described above would compute a vector between the representations of these two words and propose the closest word as the answer - this may well happen to be February ( it is indeed the case for Word2 Vec trained on Google News ) .,0
26217,"By contrast , our model would correctly propose January or March .",0
26218,Chen et al .,0
26219,2016,0
26220,A model presented in is inspired by the Attentive Reader .,0
26221,"One difference is that the attention weights are computed with a bilinear term instead of simple dot -product , that is s i ? exp ( f i ( d ) W g ( q ) ) .",0
26222,"The document embedding r is computed using a weighted sum as in the Attentive Reader , r = i s if i ( d ) .",0
26223,"In the end P ( a | q , d ) ? exp ( e ( a ) r ) , where e is a new embedding function .",0
26224,Even though it is a simplification of the Attentive Reader this model performs significantly better than the original .,0
26225,Memory Networks,0
26226,Mem NNs ) were applied to the task of text comprehension in .,0
26227,The best performing memory networks model setup - window memory - uses windows of fixed length ( 8 ) centered around the candidate words as memory cells .,0
26228,"Due to this limited context window , the model is unable to capture dependencies out of scope of this window .",0
26229,"Furthermore , the representation within such window is computed simply as the sum of embeddings of words in that window .",0
26230,"By contrast , in our model the representation of each individual word is computed using a recurrent network , which not only allows it to capture context from the entire document but also the embedding computation is much more flexible than a simple sum .",0
26231,"To improve on the initial accuracy , a heuristic approach called self supervision is used in to help the network to select the right supporting "" memories "" using an attention mechanism showing similarities to the ours .",0
26232,Plain MemNNs without this heuristic are not competitive on these machine reading tasks .,0
26233,Our model does not need any similar heuristics .,0
26234,Dynamic Entity Representation,0
26235,The Dynamic Entity Representation model has a complex architecture also based on the weighted attention mechanism and max - pooling over contextual embeddings of vectors for each named entity .,0
26236,Pointer Networks,0
26237,Our model architecture was inspired by Ptr-Nets in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation .,0
26238,"While a Ptr - Net consists of an encoder as well as a decoder , which uses the attention to select the output at each step , our model outputs the answer in a single step .",0
26239,"Furthermore , the pointer networks assume that no input in the sequence appears more than once , which is not the casein our settings .",0
26240,Summary,0
26241,Our model combines the best features of the architectures mentioned above .,0
26242,"We use recurrent networks to "" read "" the document and the query as done in and we use attention in away similar to Ptr-Nets .",0
26243,We also use summation of attention weights in away similar to MemNNs .,0
26244,From a high level perspective we simplify all the discussed text comprehension models by removing all transformations past the attention step .,0
26245,Instead we use the attention directly to compute the answer probability .,0
26246,Evaluation,0
26247,"In this section we evaluate our model on the CNN , Daily Mail and CBT datasets .",0
26248,We show that despite the model 's simplicity its ensembles achieve state - of - the - art performance on each of these datasets .,0
26249,Training Details,0
26250,To train the model we used stochastic gradient descent with the ADAM update rule and learning rate of 0.001 or 0.0005 .,1
26251,During training we minimized the following negative log-likelihood with respect to ?:,0
26252,"where a is the correct answer for query q and document d , and ?",0
26253,represents parameters of the encoder functions f and g and of the word embedding function e.,0
26254,"The optimized probability distribution P ( a |q , d ) is defined in Eq .",0
26255,"2 . The initial weights in the word embedding matrix were drawn randomly uniformly from the interval [ ? 0.1 , 0.1 ] .",0
26256,Weights in the GRU networks were initialized by random orthogonal matrices and biases were initialized to zero .,1
26257,We also used a gradient clipping threshold of 10 and batches of size 32 .,1
26258,During training we randomly shuffled all examples in each epoch .,0
26259,"To speedup training , we always pre-fetched 10 batches worth of examples and sorted them according to document length .",0
26260,Hence each batch contained documents of roughly the same length .,0
26261,For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in .,0
26262,This guaranteed that word embeddings of named entities were used only as semantically meaningless labels not encoding any intrinsic features of the represented entities .,0
26263,This forced the model to truly deduce the answer from the single context document associated with the question .,0
26264,We also do not use pre-trained word embeddings to make our training procedure comparable to .,0
26265,We did not perform any text pre-processing since the original datasets were already tokenized .,0
26266,"We do not use any regularization since in our experience it leads to longer training times of single models , however , performance of a model ensemble is usually the same .",0
26267,This way we can train the whole ensemble faster when using multiple GPUs for parallel training .,0
26268,For Additional details about the training procedure see Appendix A.,0
26269,Evaluation Method,0
26270,We evaluated the proposed model both as a single model and using ensemble averaging .,0
26271,Although the model computes attention for every word in the document we restrict the model to select an answer from a list of candidate answers associated with each question - document pair .,0
26272,For single models we are reporting results for the best model as well as the average of accuracies for the best 20 % of models with best performance on validation data since single models display considerable variation of results due to random weight initialization 5 even for identical hyperparameter values .,0
26273,Single model performance may consequently prove difficult to reproduce .,0
26274,"What concerns ensembles , we used simple averaging of the answer probabilities predicted by ensemble members .",0
26275,"For ensembling we used 14 , 16 , 84 and 53 models for CNN , Daily Mail and CBT CN and NE respectively .",0
26276,"The ensemble models were chosen either as the top 70 % of all trained models , we call this avg ensemble .",0
26277,Alternatively we use the following algorithm :,0
26278,"We started with , results of models marked with are taken from and results marked with are taken from .",0
26279,( * ) Human results were collected on 10 % of the test set .,0
26280,the best performing model according to validation performance .,0
26281,Then in each step we tried adding the best performing model that had not been previously tried .,0
26282,We kept it in the ensemble if it did improve its validation performance and discarded it otherwise .,0
26283,This way we gradually tried each model once .,0
26284,We call the resulting model a greedy ensemble .,0
26285,Results,0
26286,Performance of our models on the CNN and Daily,0
26287,Mail datasets is summarized in shows results on the CBT dataset .,0
26288,The tables also list performance of other published models that were evaluated on these datasets .,0
26289,Ensembles of our models set new state - of - the - art results on all evaluated datasets .,0
26290,mance of our single model is a little bit worse than performance of simultaneously published models .,1
26291,Compared to our work these models were trained with Dropout regularization which might improve single model performance .,0
26292,"However , ensemble of our models outperforms these models even though they use pre-trained word embeddings .",1
26293,On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5 % .,1
26294,The average performance of the top 20 % models according to validation accuracy is 69.9 % which is even 0.5 % better than the single best - validation model .,1
26295,This shows that there were many models that performed better on test set than the best - validation model .,0
26296,Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets ..,1
26297,CBT .,0
26298,"In named entity prediction our best single model with accuracy of 68.6 % performs 2 % absolute better than the MemNN with self supervision , the averaging ensemble performs 4 % absolute better than the best previous result .",1
26299,In common noun prediction our single models is 0.4 % absolute better than Mem NN however the ensemble improves the performance to 69 % which is 6 % absolute better than MemNN .,1
26300,Analysis,0
26301,"To further analyze the properties of our model , we examined the dependence of accuracy on the length of the context document ( ) , the number of candidate answers ( ) and the frequency of the correct answer in the context .",0
26302,"On the CNN and Daily Mail datasets , the accuracy decreases with increasing document length ) .",0
26303,We hypothesize this maybe due to multiple factors .,0
26304,Firstly long documents may make the task more complex .,0
26305,Secondly such cases are quite rare in the training data ) which motivates the model to specialize on shorter contexts .,0
26306,"Finally the context length is correlated with the number of named entities , i.e. the number of possible answers which is itself negatively correlated with accuracy ( see ) .",0
26307,On the CBT dataset this negative trend seems to disappear ) .,0
26308,This supports the later two explanations since the distribution of document lengths is somewhat more uniform ) and the number of candidate answers is constant for all examples in this dataset .,0
26309,The effect of increasing number of candidate answers on the model 's accuracy can be seen in .,0
26310,"We can clearly see that as the number of candidate answers increases , the accuracy drops .",0
26311,"On the other hand , the amount of examples with large number of candidate answers is quite small ) .",0
26312,"Finally , since the summation of attention in our model inherently favours frequently occurring tokens , we also visualize how the accuracy depends on the frequency of the correct answer in the document .",0
26313,shows that the accuracy significantly drops as the correct answer gets less and less frequent in the document compared to other candidate answers .,0
26314,"On the other hand , the correct answer is likely to occur frequently ) .",0
26315,Conclusion,0
26316,In this article we presented a new neural network architecture for natural language text comprehension .,0
26317,"While our model is simpler than previously published models , it gives a new state - of - the - art accuracy on all evaluated datasets .",0
26318,An analysis by suggests that on CNN and Daily Mail datasets a significant proportion of questions is ambiguous or too difficult to answer even for humans ( partly due to entity anonymization ) so the ensemble of our models maybe very near to the maximal accuracy achievable on these datasets .,0
26319,title,0
26320,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,1
26321,abstract,0
26322,Several deep learning models have been proposed for question answering .,0
26323,"However , due to their single - pass nature , they have noway to recover from local maxima corresponding to incorrect answers .",0
26324,"To address this problem , we introduce the Dynamic Coattention Network ( DCN ) for question answering .",0
26325,The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both .,0
26326,Then a dynamic pointing decoder iterates over potential answer spans .,0
26327,This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers .,0
26328,"On the Stanford question answering dataset , a single DCN model improves the previous state of the art from 71.0 % F1 to 75. 9 % , while a DCN ensemble obtains 80.4 % F1 .",0
26329,INTRODUCTION,0
26330,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,1
26331,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",1
26332,"Hence , they did not allow for training data-intensive , expressive models such as deep neural networks .",0
26333,"To address this problem , researchers have developed large - scale datasets through semi-automated techniques .",0
26334,"Compared to their smaller , hand - annotated counterparts , these QA datasets allow the training of more expressive models .",0
26335,"However , it has been shown that they differ from more natural , human annotated datasets in the types of reasoning required to answer the questions .",0
26336,"Recently , released the Stanford Question Answering dataset ( SQuAD ) , which is orders of magnitude larger than all previous hand - annotated datasets and has a variety of qualities that culminate in a natural QA task .",0
26337,SQuAD has the desirable quality that answers are spans in a reference document .,0
26338,This constrains answers to the space of all possible spans .,0
26339,"However , show that the dataset retains a diverse set of answers and requires different forms of logical reasoning , including multi-sentence reasoning .",0
26340,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .",1
26341,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .",1
26342,Our single model obtains an F1 of 75.9 % compared to the best published result of 71.0 % .,0
26343,"In addition , our ensemble model obtains an F1 of 80.4 % compared to the second best result of 78.1 % on the official SQuAD leaderboard .",0
26344,1 illustrates an overview of the DCN .,0
26345,"We first describe the encoders for the document and the question , followed by the coattention mechanism and the dynamic decoder which produces the answer span .",0
26346,Document encoder,0
26347,Question encoder,0
26348,What plants create most electric power ?,0
26349,Coattention encoder,0
26350,The weight of boilers and condensers generally makes the power - to - weight ...,0
26351,"However , most electric power is generated using steam turbine plants , so that indirectly the world 's industry is ...",0
26352,Dynamic pointer decoder,0
26353,DOCUMENT AND QUESTION ENCODER,0
26354,"Let ( x Q 1 , x Q 2 , . . . , x Q n ) denote the sequence of word vectors corresponding to words in the question and ( x D 1 , x D 2 , . . . , x D m ) denote the same for words in the document .",0
26355,"Using an LSTM , we encode the document as : d t = LSTM enc d t?1 , x D t .",0
26356,We define the document encoding matrix as D = [ d 1 . . . d m d ? ],0
26357,?,0
26358,R ( m + 1 ) .,0
26359,"We also add a sentinel vector d ? , which we later show allows the model to not attend to any particular word in the input .",0
26360,"The question embeddings are computed with the same LSTM to share representation power : qt = LSTM enc q t?1 , x Q t .",0
26361,We define an intermediate question representation Q = [ q 1 . . . q n q ? ] ? R ( n+ 1 ) .,0
26362,"To allow for variation between the question encoding space and the document encoding space , we introduce a non-linear projection layer on top of the question encoding .",0
26363,The final representation for the question becomes :,0
26364,COATTENTION ENCODER,0
26365,"We propose a coattention mechanism that attends to the question and document simultaneously , similar to , and finally fuses both attention contexts .",0
26366,provides an illustration of the coattention encoder .,0
26367,"We first compute the affinity matrix , which contains affinity scores corresponding to all pairs of document words and question words : L = D Q ?",0
26368,R ( m + 1 ) ( n+ 1 ) .,0
26369,"The affinity matrix is normalized row - wise to produce the attention weights A Q across the document for each word in the question , and column - wise to produce the attention weights AD across the question for each word in the document :",0
26370,"Next , we compute the summaries , or attention contexts , of the document in light of each word of the question .",0
26371,We similarly compute the summaries QA D of the question in light of each word of the document .,0
26372,"Similar to , we also compute the summaries C QA D of the previous attention contexts in light of each word of the document .",0
26373,"These two operations can be done in parallel , as is shown in Eq.",0
26374,3 .,0
26375,One possible interpretation for the operation C QA Dis the mapping of question encoding into space of document encodings .,0
26376,"We define CD , a co-dependent representation of the question and document , as the coattention context .",0
26377,We use the notation [ a ; b ] for concatenating the vectors a and b horizontally .,0
26378,The last step is the fusion of temporal information to the coattention context via a bidirectional LSTM :,0
26379,"We define U = [ u 1 , . . . , um ] ?",0
26380,"R 2 m , which provides a foundation for selecting which span maybe the best possible answer , as the coattention encoding .",0
26381,DYNAMIC POINTING DECODER,0
26382,"Due to the nature of SQuAD , an intuitive method for producing the answer span is by predicting the start and end points of the span .",0
26383,"However , given a question - document pair , there may exist several intuitive answer spans within the document , each corresponding to a local maxima .",0
26384,We propose an iterative technique to select an answer span by alternating between predicting the start point and predicting the endpoint .,0
26385,This iterative procedure allows the model to recover from initial local maxima corresponding to incorrect answer spans .,0
26386,"provides an illustration of the Dynamic Decoder , which is similar to a state machine whose state is maintained by an LSTM - based sequential model .",0
26387,"During each iteration , the decoder updates its state taking into account the coattention encoding corresponding to current estimates of the start and end positions , and produces , via a multilayer neural network , new estimates of the start and end positions .",0
26388,"Let hi , s i , and e i denote the hidden state of the LSTM , the estimate of the position , and the estimate of the end position during iteration i .",0
26389,The LSTM state update is then described by Eq. 5 .,0
26390,where u si ?1 and u ei?1 are the representations corresponding to the previous estimate of the start and end positions in the coattention encoding U .,0
26391,"Given the current hidden state hi , previous start position u si ?1 , and previous end position u ei ? 1 , we estimate the current start position and end position via Eq. 6 and Eq.",0
26392,7 .,0
26393,where ?,0
26394,t and ?,0
26395,t represent the start score and end score corresponding to the tth word in the document .,0
26396,We compute ? t and ?,0
26397,t with separate neural networks .,0
26398,These networks have the same architecture but do not share parameters .,0
26399,"Based on the strong empirical performance of Maxout Networks and Highway Networks , especially with regards to deep architectures , we propose a Highway Maxout Network ( HMN ) to compute ?",0
26400,t as described by Eq .,0
26401,8 .,0
26402,The intuition behind using such model is that the QA task consists of multiple question types and document topics .,0
26403,These variations may require different models to estimate the answer span .,0
26404,Maxout provides a simple and effective way to pool across multiple model variations .,0
26405,"Here , u t is the coattention encoding corresponding to the tth word in the document .",0
26406,HMN start is illustrated in .,0
26407,"The end score , ? t , is computed similarly to the start score ? t , but using a separate HMN end .",0
26408,We now describe the HMN model : where r ?,0
26409,R is a non-linear projection of the current state with parameters W ( D ) ?,0
26410,"R 5 , m",0
26411,( 1 ) t is the output of the first maxout layer with parameters W ( 1 ) ?,0
26412,R p 3 and b ( 1 ) ?,0
26413,"R p , and m",0
26414,( 2 ) t is the output of the second maxout layer with pa -,0
26415,"( 2 ) tare fed into the final maxout layer , which has parameters W ( 3 ) ? R p12 , and b ( 3 ) ?",0
26416,R p . p is the pooling size of each maxout layer .,0
26417,The max operation computes the maximum value over the first dimension of a tensor .,0
26418,We note that there is highway connection between the output of the first maxout layer and the last maxout layer .,0
26419,"To train the network , we minimize the cumulative softmax cross entropy of the start and end points across all iterations .",0
26420,"The iterative procedure halts when both the estimate of the start position and the estimate of the end position no longer change , or when a maximum number of iterations is reached .",0
26421,Details can be found in Section 4.1,0
26422,RELATED WORK,0
26423,Statistical QA,0
26424,Traditional approaches to question answering typically involve rule - based algorithms or linear classifiers over hand - engineered feature sets .,0
26425,"proposed two baselines , one that uses simple lexical features such as a sliding window to match bags of words , and another that uses word - distances between words in the question and in the document .",0
26426,"proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base , then converts the question to a structured query with which to match the content of the knowledge base .",0
26427,described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses .,0
26428,"proposed a competitive statistical baseline using a variety of carefully crafted lexical , syntactic , and word order features .",0
26429,Neural QA,0
26430,Neural attention models have been widely applied for machine comprehension or question - answering in NLP .,0
26431,proposed an AttentiveReader model with the release of the CNN / Daily Mail cloze - style question answering dataset .,0
26432,released another dataset steming from the children 's book and proposed a window - based memory network .,0
26433,presented a pointer - style attention mechanism but performs only one attention step .,0
26434,introduced an iterative neural attention model and applied it to cloze - style machine comprehension tasks .,0
26435,"Recently , released the SQuAD dataset .",0
26436,"Different from cloze - style queries , answers include non-entities and longer phrases , and questions are more realistic .",0
26437,"For SQuAD , proposed an end - to - end neural network model that consists of a Match - LSTM encoder , originally introduced in Wang & Jiang ( 2016 a ) , and a pointer network decoder ; introduced a dynamic chunk reader , a neural reading comprehension model that extracts a set of answer candidates of variable lengths from the document and ranks them to answer the question .",0
26438,"proposed a hierarchical co-attention model for visual question answering , which achieved state of the art result on the COCO - VQA dataset .",0
26439,"In , the co-attention mechanism computes a conditional representation of the image given the question , as well as a conditional representation of the question given the image .",0
26440,"Inspired by the above works , we propose a dynamic coattention model ( DCN ) that consists of a novel coattentive encoder and dynamic decoder .",0
26441,"In our model , instead of estimating the start and end positions of the answer span in a single pass , we iteratively update the start and end positions in a similar fashion to the Iterative Conditional Modes algorithm ) .",0
26442,EXPERIMENTS,0
26443,IMPLEMENTATION DETAILS,0
26444,We train and evaluate our model on the SQuAD dataset .,0
26445,"To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .",1
26446,We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .,1
26447,We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .,1
26448,"Empirically , we found that training the embeddings consistently led to overfitting and subpar performance , and hence only report results with fixed word embeddings .",0
26449,"We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .",1
26450,All LSTMs have randomly initialized parameters and an initial state of zero .,1
26451,Sentinel vectors are randomly initialized and optimized during training .,1
26452,"For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .",1
26453,"We use dropout to regularize our network during training , and optimize the model using ADAM .",1
26454,All models are implemented and trained with Chainer .,1
26455,RESULTS,0
26456,Evaluation on the SQuAD dataset consists of two metrics .,0
26457,The exact match score ( EM ) calculates the exact string match between the predicted answer and aground truth answer .,0
26458,The F1 score calculates the overlap between words in the predicted answer and aground truth answer .,0
26459,"Because a document - question pair may have several ground truth answers , the EM and F1 for a documentquestion pair is taken to be the maximum value across all ground truth answers .",0
26460,The overall metric is then computed by averaging overall document - question pairs .,0
26461,The offical SQuAD evaluation is hosted on CodaLab 2 .,0
26462,The training and development sets are publicly available while the test set is withheld .,0
26463,"The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .",1
26464,"By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .",1
26465,Question 3 : What kind of weapons did Tesla 's treatise concern ?,0
26466,Answer : particle beam weapons Groundtruth : charged particle beam :,0
26467,Examples of the start and end conditional distributions produced by the dynamic decoder .,0
26468,Odd ( blue ) rows denote the start distributions and even ( red ) rows denote the end distributions .,0
26469,i indicates the iteration number of the dynamic decoder .,0
26470,Higher probability mass is indicated by darker regions .,0
26471,The offset corresponding to the word with the highest probability mass is shown on the right hand side .,0
26472,"The predicted span is underlined in red , and aground truth answer span is underlined in green .",0
26473,"For example , Question 1 in demonstrates an instance where the model initially guesses an incorrect start point and a correct endpoint .",0
26474,"In subsequent iterations , the model adjusts the start point , ultimately arriving at the correct start point in iteration 3 .",0
26475,"Similarly , the model gradually shifts probability mass for the endpoint to the correct word .",0
26476,Question 2 shows an example in which both the start and end estimates are initially incorrect .,0
26477,The model then settles on the correct answer in the next iteration .,0
26478,Average # Tokens in Answer :,0
26479,"Performance of the DCN for various lengths of documents , questions , and answers .",0
26480,The blue dot indicates the mean F1 at given length .,0
26481,The vertical bar represents the standard deviation of F1s at a given length .,0
26482,"While the dynamic nature of the decoder allows the model to escape initial local maxima corresponding to incorrect answers , Question 3 demonstrates a case where the model is unable to decide between multiple local maxima despite several iterations .",0
26483,"Namely , the model alternates between the answers "" charged particle beam "" and "" particle beam weapons "" indefinitely .",0
26484,"Empirically , we observe that the model , trained with a maximum iteration of 4 , takes 2.7 iterations to converge to an answer on average .",0
26485,Model Ablation,0
26486,The performance of our model and its ablations on the SQuAD development set is shown in Table 2 .,0
26487,"On the decoder side , we experiment with various pool sizes for the HMN maxout layers , using a 2 - layer MLP instead of a HMN , and forcing the HMN decoder to a single iteration .",0
26488,"Empirically , we achieve the best performance on the development set with an iterative HMN with pool size 16 , and find that the model consistently benefits from a deeper , iterative decoder network .",0
26489,"The performance improves as the number of maximum allowed iterations increases , with little improvement after 4 iterations .",0
26490,"On the encoder side , replacing the coattention mechanism with an attention mechanism similar to Wang & Jiang ( 2016 b ) by setting CD to QA D in equation 3 results in a 1.9 point F1 drop .",0
26491,"This suggests that , at an additional cost of a softmax computation and a dot product , the coattention mechanism provides a simple and effective means to better encode the document and question sequences .",0
26492,"Further studies , such as performance without attention and performance on questions requiring different types of reasoning can be found in the appendix .",0
26493,Performance across length,0
26494,One point of interest is how the performance of the DCN varies with respect to the length of document .,0
26495,"Intuitively , we expect the model performance to deteriorate with longer examples , as is the case with neural machine translation .",0
26496,"However , as in shown in , there is no notable performance degradation for longer documents and questions contrary to our expectations .",0
26497,"This suggests that the coattentive encoder is largely agnostic to long documents , and is able to focus on small sections of relevant text while ignoring the rest of the ( potentially very long ) document .",0
26498,We do note a performance degradation with longer answers .,0
26499,"However , this is intuitive given the nature of the evaluation metric .",0
26500,"Namely , it becomes increasingly challenging to compute the correct word span as the number of words increases .",0
26501,Performance across question type,0
26502,Another natural way to analyze the performance of the model is to examine its performance across question types .,0
26503,"In , we note that the mean F1 of DCN exceeds those of previous systems across all question types .",0
26504,"The DCN , like other models , is adept at "" when "" questions and struggles with the more complex "" why "" questions .",0
26505,Breakdown of F1 distribution,0
26506,"Finally , we note that the DCN performance is highly bimodal .",0
26507,"On the development set , the model perfectly predicts ( 100 % F1 ) an answer for 62.2 % of examples and predicts a completely wrong answer ( 0 % F1 ) for 16.3 % of examples .",0
26508,"That is , the model picks out partial answers only 21.5 % of the time .",0
26509,"Upon qualitative inspections of the 0 % F1 answers , some of which are shown in Appendix A.4 , we observe that when the model is wrong , it s mistakes tend to have the correct "" answer type "" ( eg. person for a "" who "" question , method for a "" how "" question ) and the answer boundaries encapsulate a well - defined phrase .",0
26510,CONCLUSION,0
26511,"We proposed the Dynamic Coattention Network , an end - to - end neural network architecture for question answering .",0
26512,"The DCN consists of a coattention encoder which learns co-dependent representations of the question and of the document , and a dynamic decoder which iteratively estimates the answer span .",0
26513,We showed that the iterative nature of the model allows it to recover from initial local maxima corresponding to incorrect predictions .,0
26514,"On the SQuAD dataset , the DCN achieves the state of the art results at 75.9 % F1 with a single model and 80.4 % F1 with an ensemble .",0
26515,The DCN significantly outperforms all other models .,0
26516,A APPENDIX,0
26517,A.1 PERFORMANCE,0
26518,"In our experiments , we also investigate a model without any attention mechanism .",0
26519,"In this model , the encoder is a simple LSTM network that first ingests the question and then ingests the document .",0
26520,The hidden states corresponding to words in the document is then passed to the decoder .,0
26521,"This model achieves 33.3 % exact match and 41.9 % F1 , significantly worse than models with attention .",0
26522,A.2 SAMPLES REQUIRING DIFFERENT TYPES OF REASONING,0
26523,"We generate predictions for examples requiring different types of reasoning , given by .",0
26524,"Because this set of examples is very limited , they do not conclusively demonstrate the effectiveness of the model on different types of reasoning tasks .",0
26525,"Nevertheless , these examples show that the DCN is a promising architecture for challenging question answering tasks including those that involve reasoning over multiple sentences .",0
26526,WHAT IS THE RANKINE CYCLE SOMETIMES CALLED ?,0
26527,"The Rankine cycle is sometimes referred to as a practical Carnot cycle because , when an efficient turbine is used , the TS diagram begins to resemble the Carnot cycle .",0
26528,Type of reasoning Lexical variation ( synonymy ),0
26529,Ground truth practical,0
26530,Carnot cycle,0
26531,ID 5730d473b7151e1900c0155 b,0
26532,"Elders are called by God , affirmed by the church , and ordained by a bishop to a ministry of Word , Sacrament , Order and Service within the church .",0
26533,"They maybe appointed to the local church , or to other valid extension ministries of the church .",0
26534,"Elders are given the authority to preach the Word of God , administer the sacraments of the church , to provide care and counseling , and to order the life of the church for ministry and mission .",0
26535,"Elders may also be assigned as District Superintendents , and they are eligible for election to the episcopacy .",0
26536,Elders serve a term of 23 years as provisional Elders prior to their ordination .,0
26537,"Ground truth bishop , the local church",0
26538,Prediction a bishop AN ALGORITHM FOR X WHICH REDUCES TO C WOULD ALLOW US TO DO WHAT ?,0
26539,ID 56e1ce08e3433e14004231a6,0
26540,This motivates the concept of a problem being hard for a complexity class .,0
26541,A problem X is hard for a class of problems C if every problem in C can be reduced to X .,0
26542,"Thus no problem in C is harder than X , since an algorithm for X allows us to solve any problem in C .",0
26543,"Of course , the notion of hard problems depends on the type of reduction being used .",0
26544,"For complexity classes larger than P , polynomial - time reductions are commonly used .",0
26545,"In particular , the set of problems that are hard for NP is the set of NP - hard problems .",0
26546,Ground truth solve any problem in C,0
26547,ID 56e0d6cf231d4119001ac424,0
26548,"After leaving Edison 's company Tesla partnered with two businessmen in 1886 , Robert Lane and Benjamin Vail , who agreed to finance an electric lighting company in Tesla 's name , Tesla Electric Light & Manufacturing .",0
26549,"The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators , the first patents issued to Tesla in the US .",0
26550,Ground truth Tesla,0
26551,Prediction Robert Lane and Benjamin Vail,0
26552,Comment,0
26553,"The model produces an incorrect prediction that corresponds to people that funded Tesla , instead of Tesla who actually designed the illumination system .",0
26554,"Empirically , we find that most mistakes made by the model have the correct type ( eg. named entity type ) despite not including types as prior knowledge to the model .",0
26555,"In this case , the incorrect response has the correct type of person .",0
26556,CYDIPPID ARE TYPICALLY WHAT SHAPE ?,0
26557,ID 57265746dd62a815002e821 a,0
26558,"Cydippid ctenophores have bodies that are more or less rounded , sometimes nearly spherical and other times more cylindrical or egg - shaped ; the common coastal "" sea gooseberry , "" Pleurobrachia , sometimes has an egg - shaped body with the mouth at the narrow end , although some individuals are more uniformly round .",0
26559,"From opposite sides of the body extends a pair of long , slender tentacles , each housed in a sheath into which it can be withdrawn .",0
26560,"Some species of cydippids have bodies that are flattened to various extents , so that they are wider in the plane of the tentacles .",0
26561,"Ground truth more or less rounded , egg - shaped",0
26562,Prediction spherical,0
26563,Comment,0
26564,"Although the mistake is subtle , the prediction is incorrect .",0
26565,"The statement "" are more or less rounded , sometimes nearly spherical "" suggests that the entity is more often "" rounded "" than "" spherical "" or "" cylindrical "" or "" egg - shaped "" ( an answer given by an annotator ) .",0
26566,"This suggests that the model has trouble discerning among multiple intuitive answers due to alack of understanding of the relative severity of "" more or less "" versus "" sometimes "" and "" other times "" .",0
26567,title,0
26568,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,1
26569,abstract,0
26570,"We introduce a new model , the Recurrent Entity Network ( EntNet ) .",0
26571,It is equipped with a dynamic long - term memory which allows it to maintain and update a representation of the state of the world as it receives new data .,0
26572,"For language understanding tasks , it can reason on - the - fly as it reads text , not just when it is required to answer a question or respond as is the case for a Memory Network ( Sukhbaatar et al. , 2015 ) .",0
26573,"Like a Neural Turing Machine or Differentiable Neural Computer ( Graves et al. , 2014;2016 ) it maintains a fixed size memory and can learn to perform location and content - based read and write operations .",0
26574,"However , unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously .",0
26575,"The EntNet sets a new state - of - the - art on the bAbI tasks , and is the first method to solve all the tasks in the 10k training examples setting .",0
26576,"We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts , which other methods are notable to solve , and can generalize past its training horizon .",0
26577,"It can also be practically used on large scale datasets such as Children 's Book Test , where it obtains competitive performance , reading the story in a single pass .",0
26578,INTRODUCTION,0
26579,The essence of intelligence is the ability to predict .,0
26580,"An intelligent agent must be able to predict unobserved facts about their environment from limited percepts , combined with their knowledge of the past .",0
26581,"In order to reason and plan , they must be able to predict how an observed event or action will affect the state of the world .",0
26582,"Arguably , the ability to maintain an estimate of the current state of the world , combined with a forward model of how the world evolves , is a key feature of intelligent agents .",0
26583,"A natural way for an agent to represent the world is to maintain a set of high - level concepts or entities together with their properties , which are updated as new information is received .",0
26584,"For example , if a percept is the textual description of an event , such as "" John walks out of the kitchen "" , the agent should learn to update its estimate of John 's location , as well as the list ( and number ) of people present in each room .",0
26585,"If John was carrying a bag , the location of the bag and the list of objects in the kitchen must also be updated .",0
26586,"When we read a story , each sentence we read or hear causes us to update our internal representation of the current state of the world within the story .",0
26587,The flow of the story is captured by the evolution of this state of the world .,0
26588,"At any given time , an agent typically receives limited information about the state of the world , and should therefore be able to infer new information through partial observation .",0
26589,"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .",1
26590,"If the second series of statements is given in the form of questions about the final state of the world together with their correct answers , the agent should be able to learn from them and its performance can be measured by the accuracy of its answers .",0
26591,"Even with this weak form of supervision , the system may learn basic dynamical constraints about the world .",0
26592,"For example , it may learn that a person or object can not be in two locations at the same time , or may learn simple update rules such as incrementing and decrementing the number of persons or objects in a room .",0
26593,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .",0
26594,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,1
26595,"The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .",1
26596,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .",1
26597,"If each cell learns to represent a concept or entity in the world , one can imagine a gating mechanism that , based on the key and content of the memory cells , will only modify the cells that concern the entities mentioned in the input .",0
26598,"In the current version of the model , there is no direct interaction between the memory cells , hence the system can be seen as multiple identical processors functioning in parallel , with distributed local memory .",0
26599,"Alternatively , the EntNet can be seen as a bank of gated RNNs ( all sharing the same parameters ) , whose hidden states correspond to latent concepts and attributes , and whose parameters describe the laws of the world according to which the attributes of objects are updated .",1
26600,"The sharing of these parameters reflects an invariance of these laws across object instances , similarly to how the weight tying scheme in a CNN reflects an invariance of image statistics across locations .",0
26601,"Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .",1
26602,"The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .",1
26603,"The EntNet is able to solve all 20 bAb I question - answering tasks , a popular benchmark of story understanding , which to our knowledge sets a new state - of - the - art .",0
26604,"Our experiments also indicate that the model indeed maintains an internal representation of the simplified world in which the stories take place , and that the model does not limit itself to storing the aspects of the world required to answer a specific question .",0
26605,"We also introduce a new reasoning task which , unlike the bAbI tasks , requires a model to use a large number of supporting facts to answer the question , and show that the EntNet outperforms both LSTMs and Memory Networks by a significant margin .",0
26606,It is also able to generalize to sequences longer than those seen during training .,0
26607,"Finally , our model also obtains competitive results on the Childrens Book Test , and performs best among models that read the text in a single pass before receiving knowledge of the question .",0
26608,MODEL,0
26609,"Our model is designed to process data in sequential form , and consists of three main parts : an input encoder , a dynamic memory and an output layer , which we now describe in detail .",0
26610,"We developed it in the context of question answering on short stories where the inputs are word sequences , but the model could be adapted to many other contexts .",0
26611,INPUT ENCODER,0
26612,The encoding layer summarizes an element of the input sequence with a vector of fixed length .,0
26613,"Typically the input element at time t is a sequence of words , e.g. a sentence or window of words .",0
26614,"One is free to choose the encoding module to be any standard sequence encoder , which is an active area of research .",0
26615,Typical choices include a bag - of - words ( BoW ) representation or the final state of a recurrent neural net ( RNN ) run over the sequence .,0
26616,"In this work , we use a simple encoder consisting of a learned multiplicative mask followed by a summation .",0
26617,"More precisely , let the input at time t be a sequence of words with embeddings {e 1 , ... , e k }.",0
26618,The vector representation of this input is then :,0
26619,"The same set of vectors {f 1 , ... , f k } are used at each time step and are learned jointly with the other parameters of the model .",0
26620,"Note that the model can choose to adopt a standard BoW representation by setting all weights in the multiplicative mask to 1 , or can choose a positional encoding model as used in .",0
26621,DYNAMIC MEMORY,0
26622,The dynamic memory is a gated recurrent network with a ( partially ) block structured weight tying scheme .,0
26623,"We divide the hidden states of the network into blocks h 1 , ... , h m ; the full hidden state is the concatenation of the h j .",0
26624,"In the experiments below , m is of the order of 5 to 20 , and each block h j is of the order of 20 to 100 units .",0
26625,"At each time step t , the content of the hidden states {h j } ( which we will call the jth memory ) are updated using a set of key vectors {w j } and the encoded input st .",0
26626,"In it s most general form , the update equations of our model are given by :",0
26627,Here ?,0
26628,"represents a sigmoid , g j is a gating function which determines how much the j th memory should be updated , andh j is the new candidate value of the memory to be combined with the existing memory h j .",0
26629,The function ?,0
26630,"can be chosen from any number of activation functions , in our experiments we use either parametric ReLU non-linearities or the identity .",0
26631,"The matrices U , V , Ware typically trainable parameters of the model , and are shared between all the blocks .",0
26632,"They can also be fixed to certain values , such as the identity or zero , to yield a simpler model which we use in some of our experiments .",0
26633,"The gating function g j contains two terms : a "" content "" term s T t h j which causes the gate to open for memory slots whose content matches the input , and a "" location "" term s T t w j which causes the gate to open for memory slots whose key matches the input .",0
26634,The final normalization step allows the model to forget previous information .,0
26635,"To see this , note that since the memories lie on the unit sphere , all information is contained in their phase .",0
26636,Adding any vector to a given memory ( other than the memory itself ) will decrease the cosine distance between the original memory and the updated one .,0
26637,"Therefore , as new information is added , old information is forgotten .",0
26638,OUTPUT MODULE,0
26639,"Whenever the model is required to produce an output , it is presented with a query vector q.",0
26640,"Specifically , the output is computed using the following equations :",0
26641,The matrices H and R are additional trainable parameters of the model .,0
26642,The output module can be viewed as a one - hop Memory Network with an additional non-linearity ?,0
26643,between the internal state and the decoder matrix .,0
26644,"If the memory slots correspond to specific words ( as we will describe in the following section ) which contain the answer , p can be viewed as a distribution over potential answers and can be used to make a prediction directly or fed into a loss function , removing the need for the last two steps .",0
26645,"The entire model ( all three components described above ) is trained via backpropagation through time , receiving gradients from anytime steps where the reader is required to produce an output , which are then propagated through the unrolled network .",0
26646,MOTIVATING EXAMPLE OF OPERATION,0
26647,We now describe a motivating example of how our model can perform reasoning on - the - fly as it is ingesting input sequences .,0
26648,"Let us suppose our model is reading a story , so the inputs are natural language sentences , and then it is required to answer questions about the story it has just read .",0
26649,Our model is free to learn the key vectors w j for each memory j.,0
26650,One choice the model could make is to associate a single memory ( via the key ) with each entity in the story .,0
26651,"The memory slot corresponding to a person could encode that person 's location , the objects they are carrying , or the people they are with , depending on what information is relevant for the task at hand .",0
26652,"As new information is received indicating that objects are acquired or discarded , or the person changes location , their memory slot will change accordingly .",0
26653,Similarly useful updates can be made for memories corresponding to object and location entities as well .,0
26654,"In fact , we could encode this choice of memories directly into our model , which we consider as a type of prior knowledge .",0
26655,"By tying the weights of the key vectors with the embeddings of specific words , we can encourage the model to record information about certain words occuring in the text which we believe to be important .",0
26656,"For example , given a list of named entities ( which could be produced by a standard tagger ) , we could make the model have a separate memory slot for each entity .",0
26657,"We consider this "" tied "" variant in our experiments .",0
26658,"Since the list of entities is independent of the training data , this variant can handle entities not seen in the training set , as long as their embeddings can be initialized in a reasonable way ( such as pre-training on a larger corpus ) .",0
26659,"Now , consider that the model reads the following two sentences , and the desired behavior of the gating function and update function at each memory as they are seen :",0
26660,Mary picked up the ball .,0
26661,Mary went to the garden .,0
26662,"As the first sentence st is ingested , and assuming memories encode entities , we would like the gates of the memories corresponding to both "" Mary "" and "" ball "" to activate .",0
26663,This is possible due to the location addressing term s T t w j which uses the key w j .,0
26664,We expect that a well trained model would learn to do this .,0
26665,"The model would hence modify both the entry corresponding to "" Mary "" to indicate that she is now carrying the ball , and also the entry corresponding to "" ball "" , to indicate that it is being carried by Mary .",0
26666,"When the second sentence is seen , we would like the model to again modify the "" Mary "" entry to indicate that she is now in the garden , and also modify the "" ball "" entry to reflect it s new location as well .",0
26667,"Assuming the information for "" Mary "" is contained in the "" ball "" memory as described before , the gate corresponding to "" ball "" can activate due to the content addressing term s T t h j , even though the word "" ball "" does not occur in the second sentence .",0
26668,"As before , the gate corresponding to the "" Mary "" entry can open due to the second term .",0
26669,"If the gating function and update function have weights such that the steps above are executed , then the memory will be in a state where questions such as "" Where is the ball ? "" or "" Where is Mary ? "" can be answered from the values of relevant memories , without the need for further complex reasoning .",0
26670,RELATED WORK,0
26671,"The EntNet is related to gated recurrent models such as the LSTM and GRU , which also use gates to fix or modify the information stored in the hidden state .",0
26672,"However , these models use scalar memory cells with full interactions between them , whereas ours has separate memory slots which could be seen as groups of hidden units with tied weights in the gating and update functions .",0
26673,"Another important difference is the content - based matching term between the input and hidden state , which is not present in these models .",0
26674,Our model also shares some similarities with the DNC / NTM framework of .,0
26675,"There , as in our model , a block of hidden states acts as a set of read - writeable memories .",0
26676,"On the other hand , the DNC has a relatively sophisticated controller network ( such as an LSTM ) which reads an input and outputs a number of interface vectors ( such as keys and weightings ) which are then combined via a softmax to read from and write to the external memory matrix .",0
26677,"In contrast , our model can be viewed as a set of separate recurrent models whose hidden states store the memory slots .",0
26678,"These hidden states are either fixed by the gates , or modified through a simple RNN - style update .",0
26679,"The bulk of the reasoning is thus performed by these parallel recurrent models , rather than through a central controller .",0
26680,"Moreover , instead of using a softmax , our model uses an independent gate for writing to each memory .",0
26681,"Our model is similar to a Memory Network and its variants in the way it produces an output using a softmax over blocks of hidden states , and our encoding layer is inspired by techniques used in those works .",0
26682,"However , Memory Networks explicitly store the entire input sequence in memory , and then sequentially update a controller 's hidden state via a softmax gating over the memories .",0
26683,"In contrast , our model keeps a fixed number of blocks of hiddens as memories and updates each block with an independent gated RNN .",0
26684,"The Dynamic Memory Network of also performs updates via a recurrent model , however it links memories to input tokens and updates them sequentially rather than in parallel .",0
26685,The weight tying scheme and the parallel gated RNNs recall the gated graph network of .,0
26686,"If we interpret our work in that context , the "" graph "" is just a set of vertices with no edges ; our gating mechanism is also somewhat different than the one they use .",0
26687,"The Comm NN model of , the Interaction Network of ( ? ) , the Neural Physics Engine of ( ? ) and the model of ( ? ) also use a set of parallel recurrent models with tied weights , but differ from our model in their use of inter-network communication and the lack of a gating mechanism .",0
26688,"Finally , there is another class of recent models that have a writeable memory arranged as ( unbounded ) stacks , linked lists or queues .",0
26689,"Our model is different from these in that we use a key - value pair array instead of a stack , and in the experiments in this work , the array is of fixed size .",0
26690,EXPERIMENTS,0
26691,In this section we evaluate our model on three different datasets .,0
26692,Training details common to all experiments can be found in Appendix A.,0
26693,SYNTHETIC WORLD MODEL TASK,1
26694,We first study our model 's properties on a toy task designed to measure the ability to keep a world model in memory .,0
26695,"In this task two agents are initially placed randomly on an 1010 grid , and at each time step a randomly chosen agent either changes direction or moves ahead .",0
26696,"After a certain number of time steps , the model is required to provide the locations of each of the agents , thus revealing its internal world model ( details can be found in Appendix B ) .",0
26697,This task is challenging because the model must combine up to T ?,0
26698,"2 supporting facts in order to answer the question correctly , and must also keep the locations of both agents in memory and update them at different times .",0
26699,"We compared the performance of a MemN2N , LSTM and EntNet .",0
26700,"For the MemN2N , we set the number of hops equal to T ? 2 and the embedding dimension to d = 20 .",1
26701,"The EntNet had embedding dimension d = 20 and 5 memory slots , and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models .",1
26702,"For each model , we repeated the experiment with 5 different initializations and reported the best performance .",0
26703,"All models were trained with ADAM with initial learning rates set by grid search over { 0.1 , 0.01 , 0.001 } and divided by 2 every 10,000 updates .",1
26704,shows the results .,0
26705,"The MemN2N has the worst performance , which degrades quickly as the length of the sequence increases .",1
26706,"The LSTM performs better , but still loses accuracy as the length of the sequence increases .",1
26707,"In contrast , the EntNet is able to solve the task in all cases .",0
26708,"The ability to generalize to sequences longer than those seen during training is a desirable property , which suggests that the network has learned the dynamics of the world it is trying to model .",0
26709,It also means the model can be trained less expensively .,0
26710,"To study this , we trained an EntNet on variable length sequences between 1 and 20 , and evaluated it on different length sequences longer than 20 .",0
26711,Results are shown in .,0
26712,We see that the model is able to achieve good performance several times past its training horizon .,0
26713,BABI TASKS,0
26714,"We next evaluate our model on the bAbI tasks , which area collection of 20 synthetic questionanswering datasets first introduced in designed to test a wide variety of reasoning abilities .",0
26715,They have since become a benchmark for memory - augmented neural networks and most of the related methods described in Section 4 have been tested on them .,0
26716,"Performance is measured using two metrics : the average error across all tasks , and the number of failed tasks ( more than 5 % error ) .",0
26717,We used version 1.2 of the dataset with 10 k samples .,0
26718,1,0
26719,Training Details,0
26720,We used a similar training setup as .,0
26721,"All models were trained with ADAM using a learning rate of ? = 0.01 , which was divided by 2 every 25 epochs until 200 epochs were reached .",0
26722,"Copying previous works , the capacity of the memory was limited to the most recent 70 sentences , except for task 3 which was limited to 130 sentences .",0
26723,"Due to the high variance in model performance for some tasks , for each task we conducted 10 runs with different initializations and picked the best model based on performance on the validation set , as it has been done in previous work .",0
26724,"In all experiments , our model had embedding dimension size d = 100 and 20 memory slots .",0
26725,"In we compare our model to various other state - of - the - art models in the literature : the larger Mem N2N reported in the appendix of , the Dynamic Memory Network of , the Dynamic Neural Turing Machine , the Neural Turing Machine and the Differentiable Neural Computer .",0
26726,"Our model is able to solve all the tasks , outperforming the other models in terms of both the number of solved tasks and the average error .",0
26727,"To analyze what kind of representations our model can learn , we conducted an additional experiment on Task 2 using a simple BoW sentence encoding and key vectors which were tied to entity embeddings .",0
26728,"This was designed to make the model more interpretable , since the weight tying forces memory slots to encode information about specific entities .",0
26729,2,0
26730,"After training , we ran the model over a story and computed the cosine distance between ?( Hh j ) and each row r i of the decoder matrix R .",0
26731,This gave us a score which measures the affinity between a given memory slot and each word in the vocabulary .,0
26732,shows the nearest neighboring words for each memory slot ( which itself corresponds to an entity ) .,0
26733,We see that the model has indeed stored locations of all of the objects and characters in its memory slots which reflect the final state of the story .,0
26734,"In particular , it has the correct answer readily stored in the memory slot of the entity being inquired about ( the milk ) .",0
26735,It also has correct location information about all other non-location entities stored in the appropriate memory slots .,0
26736,"Note that it does not store useful or correct information in the memory slots corresponding to locations , most likely because this task does not contain questions about locations ( such as "" who is in the kitchen ? "" ) .",0
26737,CHILDRE N'S BOOK TEST ( CBT ),1
26738,"We next evaluated our model on the Children 's Book Test , which is a semantic language modeling ( sentence completion ) benchmark built from children 's books that are freely available from Project Gutenberg 3 .",0
26739,Models are required to read 20 consecutive sentences from a given story and use this context to fill in a missing word from the 21st sentence .,0
26740,"More specifically , each sample consists of a tuple ( S , q , C , a ) where S is the story consisting of 20 sentences , Q is the 21st sentence with one word replaced by a special blank token , C is a set of 10 candidate answers of the same type as the missing word ( for example , common nouns or named entities ) , and a is the true answer ( which is always contained in C ) .",0
26741,"It was shown in that methods with limited memory such as LSTMs perform well on more frequent , syntax based words such as prepositions and verbs , being similar to human performance , but poorly relative to humans on more semantically meaningful words such as named entities and common nouns .",1
26742,"Therefore , most recent methods have been evaluated on the Named Entity and Common Noun subtasks , since they better test the ability of a model to make use of wider contextual information .",0
26743,Training Details,0
26744,"We adopted the same window memory approach used in , where each input corresponds to a window of text from {w ( i?b?1/2 ) ...w i ...w ( i+ ( b ?1 ) / 2 ) } centered at a candidate w i ?",0
26745,C .,0
26746,In our experiments we set b = 5 .,0
26747,All models were trained using standard stochastic gradient descent ( SGD ) with a fixed learning rate of 0.001 .,0
26748,"We used separate input encodings for the update and gating functions , and applied a dropout rate of 0.5 to the word embedding dimensions .",0
26749,"Key embeddings were tied to the embeddings of the candidate words , resulting in 10 hidden blocks , one per member of C. Due to the weight tying , we did not need a decoder matrix and used the distribution over candidates to directly produce a prediction , as described in Section 3 .",0
26750,"We found that a simpler version of the model worked best , with U = V = 0 , W = I and ?",0
26751,equal to the identity .,0
26752,"We also removed the normalization step in this simplified model , which we found to hurt performance .",0
26753,"This can be explained by the fact that the maximum frequency baseline model in has performance which is significantly higher than random , and including the normalization step hides this useful frequency - based information .",0
26754,Results,0
26755,"We draw a distinction between two setups : the single - pass setup , where the model must read the story and query in order and immediately produce an output , and the multi-pass setup , where the model can use the query to perform attention over the story .",0
26756,The first setup is more challenging 0.686 0.634 Gated - Attention Reader ( Bhuwan 0.690 0.639 EpiReader 0.697 0.674 A o A Reader 0.720 0.694 NSE,0
26757,"Adaptive Computation 0.732 0.714 because the model does not know beforehand which query it will be presented with , and must learn to retain information which is useful for a wide variety of potential queries .",0
26758,For this reason it can be viewed as a test of the model 's ability to construct a general - purpose representation of the current state of the story .,0
26759,"The second setup leverages all available information , and allows the model to use knowledge of which question will be asked when it reads the story .",0
26760,"In , we show the performance of the general EntNet , the simplified EntNet , as well as other single - pass models taken from .",0
26761,"The general EntNet performs better than the LSTMs and n-gram model on the Named Entities Task , but lags behind on the Common Nouns task .",0
26762,"The simplified EntNet outperforms all other single - pass models on both tasks , and also performs better than the Memory Network which does not use the self - supervision heuristic .",0
26763,"However , there is still a performance gap when compared to more sophisticated machine comprehension models , many of which perform multiple layers of attention over the story using query knowledge .",0
26764,The fact that the simplified EntNet is able to obtain decent performance is encouraging since it indicates that the model is able to build an internal representation of the story which it can then use to answer a relatively diverse set of queries .,0
26765,CONCLUSION,0
26766,"Two closely related challenges in artificial intelligence are designing models which can maintain an estimate of the state of a world with complex dynamics overlong timescales , and models which can predict the forward evolution of the state of the world from partial observation .",0
26767,"In this paper , we introduced the Recurrent Entity Network , a new model that makes a promising step towards the first goal .",0
26768,"Our model is able to accurately track the world state while reading text stories , which enables it to set a new state - of - the - art on the bAbI tasks , the competitive benchmark of story understanding , by being the first model to solve them all .",0
26769,"We also showed that our model is able to capture simple dynamics overlong timescales , and is able to perform competitively on a real - world dataset .",0
26770,"Although our model was able to solve all the bAbI tasks using 10 k training samples , we found that performance dropped considerably when using only 1 k samples ( see Appendix ) .",0
26771,"Most recent work on the bAbI tasks has focused on the 10k samples setting , and we would like to emphasize that solving them in the 1 k samples setting remains an open problem which will require improving the sample efficiency of reasoning models , including ours .",0
26772,"Recent works have made some progress towards the second goal of forward modeling , for instance in capturing simple physics , predicting future frames in video or responses in dialog .",0
26773,"Although we have only applied our model to tasks with textual inputs in this work , the architecture is general and future work should investigate how to combine the EntNet 's tracking abilities with such predictive models .",0
26774,A TRAINING DETAILS,0
26775,All models were implemented using Torch .,0
26776,"In all experiments , we initialized our model by drawing weights from a Gaussian distribution with mean zero and standard deviation 0.1 , except for the PReLU slopes and encoder weights which were initialized to 1 .",0
26777,"Note that the PReLU initialization is related to two of the heuristics used in , namely starting training with a purely linear model , and adding non-linearities to half of the hidden units .",0
26778,Our initialization allows the model to choose when and how much to enter the non-linear regime .,0
26779,"Initializing the encoder weights to 1 corresponds to beginning with a BoW encoding , which the model can then choose to modify .",0
26780,"The initial values of the memory slots were initialized to the key values , which we found to help performance .",0
26781,"Optimization was done with SGD or ADAM using minibatches of size 32 , and gradients with norm greater than 40 were clipped to 40 .",0
26782,A null symbol whose embedding was constrained to be zero was used to pad all sentences or windows to a fixed size .,0
26783,B DETAILS OF WORLD MODEL EXPERIMENTS,0
26784,"Two agents are initially placed at random on a 10 10 grid with 100 distinct , ... ( 9 , 10 ) , ( 10 , 10 ) } .",0
26785,At each time step an agent is chosen at random .,0
26786,"There are two types of actions : the agent can face a given direction , or can move a number of steps ahead .",0
26787,Actions are sampled until a legal action is found by either choosing to change direction or move with equal probability .,0
26788,"If they change direction , the direction is chosen between north , south , east and west with equal probability .",0
26789,"If they move , the number of steps is randomly chosen between 1 and 5 .",0
26790,A legal action is one which does not place the agent off the grid .,0
26791,"Stories are given to the network in textual form , an example of which is below .",0
26792,The first action after each agent is placed on the grid is to face a given direction .,0
26793,"Therefore , the maximum number of actions made by one agent is T ?",0
26794,"2 . The network learns word embeddings for all words in the vocabulary such as locations , agent identifiers and actions .",0
26795,"At question time , the model must predict the correct answer ( which will always be a location ) from all the tokens in the vocabulary .",0
26796,"agent 1 is at ( 2 ,8 ) agent1 faces - N age nt 2 is at ( 9 , 7 ) agent2 faces - N agent2 moves - 2 agent2 faces - E agent2 moves - 1 agent1 moves - 1 agent2 faces - S agent2 moves - 5 Q1 : where is agent 1 ?",0
26797,Q2 : where is agent 2 ?,0
26798,"A1 : ( 2 ,9 ) A2 : ( 10 , 4 )",0
26799,C ADDITIONAL RESULTS ON BABI TASKS,0
26800,"We provide some additional experiments on the bAbI tasks , in order to better understand the influence of architecture , weight tying , and amount of training data .",0
26801,shows results when a simple BoW encoding is used for the inputs .,0
26802,"Here , the EntNet still performs better than a MemN2 N which uses the same encoding scheme , indicating that the architecture has an important effect .",0
26803,"Tying the key vectors to entities did not help , and hurt performance for some tasks .",0
26804,shows results when using only 1 k training samples .,0
26805,"In this setting , the EntNet performs worse than the MemN2N. shows results for the EntNet and the DNC when models are trained on all tasks jointly .",0
26806,"We report results for the mean performance across different random seeds ( 20 for the DNC , 5 for the EntNet ) , as well as the performance for the single best seed ( measured by validation error ) .",0
26807,The DNC results for mean performance were taken from the appendix of .,0
26808,"The DNC has better performance in terms of the best seed , but also exhibits high variation across seeds , indicating that many different runs are required to achieve good performance .",0
26809,The EntNet exhibits less variation across runs and is able to solve more tasks consistently .,0
26810,"Note that reports DNC results with joint training , since results when training on each task separately were not available .",0
26811,title,0
26812,Neural Tree Indexers for Text Understanding,1
26813,abstract,0
26814,Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens .,0
26815,"In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language .",0
26816,"However , the current recursive architecture is limited by its dependence on syntactic tree .",0
26817,"In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models .",0
26818,NTI constructs a full n-ary tree by processing the input text with its node function in a bottom - up fashion .,0
26819,Attention mechanism can then be applied to both structure and node function .,0
26820,"We implemented and evaluated a binarytree model of NTI , showing the model achieved the state - of - the - art performance on three different NLP tasks : natural language inference , answer sentence selection , and sentence classification , outperforming state - of - the - art recurrent and recursive neural networks 1 .",0
26821,Introduction,0
26822,Recurrent neural networks ( RNNs ) have been successful for modeling sequence data .,0
26823,"RNNs equipped with gated hidden units and internal short - term memories , such as long shortterm memories ( LSTM ) ) have achieved a notable success in several NLP tasks including named entity recognition , constituency parsing , textual entailment recognition ) , question answering , and machine translation .",0
26824,"However , most LSTM models explored so far are sequential .",0
26825,It encodes text sequentially from left to right or vice versa and do not naturally support compositionality of language .,0
26826,Sequential LSTM models seem to learn syntactic structure from the natural language however their generalization on unseen text is relatively poor comparing with models that exploit syntactic tree structure .,0
26827,"Unlike sequential models , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .",0
26828,However its dependence on a syntactic tree architecture limits practical NLP applications .,0
26829,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",1
26830,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,1
26831,Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,1
26832,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .",0
26833,"Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .",1
26834,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :",1
26835,A binary tree form of Neural Tree Indexers ( NTI ) in the context of question answering and natural language inference .,0
26836,We insert empty tokens ( denoted by ? ) to the input text to form a full binary tree .,0
26837,( a ) NTI produces answer representation at the root node .,0
26838,This representation along with the question is used to find the answer .,0
26839,( b ) NTI learns representations for the premise and hypothesis sentences and then attentively combines them for classification .,0
26840,Dotted lines indicate attention over premise - indexed tree .,0
26841,1 shows a binary - tree model of NTI .,0
26842,"Although the model does not follow the syntactic tree structure , we empirically show that it achieved the state - of the - art performance on three different NLP applications : natural language inference , answer sentence selection , and sentence classification .",0
26843,2 Related Work,0
26844,Recurrent Neural Networks and Attention Mechanism,0
26845,RNNs model input text sequentially by taking a single token at each time step and producing a corresponding hidden state .,0
26846,The hidden state is then passed along through the next time step to provide historical sequence information .,0
26847,"Although a great success in a variety of tasks , RNNs have limitations .",0
26848,"Among them , it is not efficient at memorizing long or distant sequence .",0
26849,This is frequently called as information flow bottleneck .,0
26850,Approaches have therefore been developed to overcome the limitations .,0
26851,"For example , to mitigate the information flow bottleneck , extended RNNs with a soft attention mechanism in the context of neural machine translation , leading to improved the results in translating longer sentences .",0
26852,RNNs are linear chain - structured ; this limits its potential for natural language which can be represented by complex structures including syntactic structure .,0
26853,"In this study , we propose models to mitigate this limitation .",0
26854,Recursive Neural Networks,0
26855,"Unlike RNNs , recursive neural networks explicitly model the compositionality and the recursive structure of natural language over tree .",0
26856,The tree structure can be predefined by a syntactic parser .,0
26857,Each non - leaf tree node is associated with anode composition function which combines its children nodes and produces its own representation .,0
26858,The model is then trained by back - propagating error through structures .,0
26859,The node composition function can be varied .,0
26860,A single layer network with tanh non-linearity was adopted in recursive auto -associate memories and recursive autoencoders .,0
26861,extended this network with an additional matrix representation for each node to augment the expressive power of the model .,0
26862,Tensor networks have also been used as composition function for sentencelevel sentiment analysis task .,0
26863,"Recently , introduced S - LSTM which extends LSTM units to compose tree nodes in a recursive fashion .",0
26864,"In this paper , we introduce a novel attentive node composition function that is based on S - LSTM .",0
26865,"Our NTI model does not rely on either a parser output or a fine - grained supervision of nonleaf nodes , both required in previous work .",0
26866,"In NTI , the supervision from the target labels is provided at the root node .",0
26867,"As such , our NTI model is robust and applicable to a wide range of NLP tasks .",0
26868,We introduce attention over tree in NTI to overcome the vanishing / explode gradients challenges as shown in RNNs .,0
26869,Methods,0
26870,Our training set consists of N examples,0
26871,where the input,0
26872,"X i is a sequence of word tokens w i 1 , w i 2 , . . . , w i Ti and the output Y i can be either a single target or a sequence .",0
26873,Each input word token wt is represented by it s word embedding x t ?,0
26874,R k .,0
26875,NTI is a full n-ary tree ( and the sub-trees can be overlapped ) .,0
26876,"It has two types of transformation function : non - leaf node function f node ( h 1 , . . . , h c ) and leaf node function f leaf ( x t ) . f leaf ( x t ) computes a ( possibly nonlinear ) transformation of the input word embedding x t . f node ( h 1 , . . . , h c ) is a function of it s child nodes representation h 1 , . . . , h c , where c is the total number of child nodes of this non -leaf node .",0
26877,NTI can be implemented with different tree structures .,0
26878,"In this study we implemented and evaluated a binary tree form of NTI : a non-leaf node can take in only two direct child nodes ( i.e. , c = 2 ) .",0
26879,"Therefore , the function f node ( h l , hr ) composes it s left child node h land right child node hr .",0
26880,illustrates our NTI model that is applied to question answering ( a ) and natural language inference tasks ( b ) .,0
26881,Note that the node and leaf node functions are neural networks and are the only training parameters in NTI .,0
26882,"We explored two different approaches to compose node representations : an extended LSTM and attentive node composition functions , to be described below .",0
26883,Non - Leaf Node Composition Functions,0
26884,"We define two different methods for non-leaf node function f node ( h l , hr ) .",0
26885,LSTM - based Non- leaf Node Function ( S - LSTM ) :,0
26886,"We initiate f node ( h l , hr ) with LSTM .",0
26887,"For non - leaf node , we adopt S - LSTM , an extension of LSTM to tree structures , to learn anode representation by its children nodes .",0
26888,"Let h l t , hr t , cl t and c rt be vector representations and cell states for the left and right children .",0
26889,An S - LSTM computes a parent node representation hp t +1 and anode cell state c p t +1 as,0
26890,"where W s 1 , . . . , W s 18 ?",0
26891,R kk and biases ( for brevity we eliminated the bias terms ) are the training parameters .,0
26892,?,0
26893,and denote the elementwise sigmoid function and the element - wise vector multiplication .,0
26894,Extension of S - LSTM nonleaf node function to compose more children is straightforward .,0
26895,"However , the number of parameters increases quadratically in S - LSTM as we add more child nodes .",0
26896,Attentive Non- leaf Node Function ( ANF ) :,0
26897,"Some NLP applications ( e.g. , QA and machine translation ) would benefit from a dynamic query dependent composition function .",0
26898,We introduce ANF as anew non-leaf node function .,0
26899,"Unlike S - LSTM , ANF composes the child nodes attentively in respect to another relevant input vector q ?",0
26900,R k .,0
26901,The input vector q can be a learnable representation from a sequence representation .,0
26902,Given a matrix S AN F ?,0
26903,"R k2 resulted by concatenating the child node representations h l t , hr t and the third input vector q , ANF is defined as",0
26904,where W AN F 1 ?,0
26905,"R kk is a learnable matrix , m ?",0
26906,R 2 the attention score and ? ?,0
26907,R 2 the attention weight vector for each child .,0
26908,"f score is an attention scoring function , which can be implemented as a multi - layer perceptron ( MLP )",0
26909,or a matrix - vector product m = q S AN F .,0
26910,The matrices W score 1 and W score 2 ?,0
26911,R kk and the vector w ?,0
26912,R k are training parameters .,0
26913,e ?,0
26914,R 2 is a vector of ones and ?,0
26915,the outer product .,0
26916,We use ReLU function for non-linear transformation .,0
26917,Attention Over Tree,0
26918,"Comparing with sequential LSTM models , NTI has less recurrence , which is defined by the tree depth , log ( n ) for binary tree where n is the length of the input sequence .",0
26919,"However , NTI still needs to compress all the input information into a single representation vector of the root .",0
26920,This imposes practical difficulties when processing long sequences .,0
26921,We address this issue with attention 300 3.0 M 83.9 80.6 Dependency Tree CNN encoders 300 3.5 M 83.3 82.1 NTI - SLSTM ( Ours ) 300 3.3 M 83.9 82.4 SPINN - PI encoders 300 3.7 M 89.2 83.2 NTI - SLSTM- LSTM ( Ours ) 300 4.0 M 82.5 83.4 LSTMs attention 100 242K 85.4 82.3 LSTMs word - by - word attention 100 mechanism over tree .,0
26922,"In addition , the attention mechanism can be used for matching trees ( described in Section 4 as Tree matching NTI ) that carry different sequence information .",0
26923,We first define a global attention and then introduce a tree attention which considers the parent - child dependency for calculation of the attention weights .,0
26924,Global Attention :,0
26925,An attention neural network for the global attention takes all node representations as input and produces an attentively blended vector for the whole tree .,0
26926,This neural net is similar to ANF .,0
26927,"Particularly , given a matrix S GA ?",0
26928,R k2n?,0
26929,"1 resulted by concatenating the node representations h 1 , . . . , h 2n?1 and the relevant input representation q , the global attention is defined as",0
26930,where W GA 1 and W GA 2 ?,0
26931,R kk are training parameters and ? ?,0
26932,R 2n?1 the attention weight vector for each node .,0
26933,This attention mechanism is robust as it globally normalizes the attention score m with sof tmax to obtain the weights ?.,0
26934,"However , it does not consider the tree structure when producing the final representation h tree .",0
26935,Tree Attention :,0
26936,We modify the global attention network to the tree attention mechanism .,0
26937,The resulting tree attention network performs almost the same computation as ANF for each node .,0
26938,It compares the parent and children nodes to produce anew representation assuming that all node representations are constructed .,0
26939,Given a matrix ST A ?,0
26940,"R k3 resulted by concatenating the parent node representation hp t , the left child h l t and the right child hr t and the relevant input representation q , every non-leaf node hp t simply updates its own representation by using the following equation in a bottom - up manner .",0
26941,and this equation is similarity to the global attention .,0
26942,"However , now each non-leaf node attentively collects its own and children representations and passes towards the root which finally constructs the attentively blended tree representation .",0
26943,"Note that unlike the global attention , the tree attention locally normalizes the attention scores with sof tmax .",0
26944,Experiments,0
26945,"We describe in this section experiments on three different NLP tasks , natural language inference , question answering and sentence classification to demonstrate the flexibility and the effectiveness of NTI in the different settings .",0
26946,We trained NTI using Adam with hyperparameters selected on development set .,0
26947,The pre-trained 300 - D Glove 840B vectors were obtained for the word embeddings,0
26948,2 .,0
26949,The word embeddings are fixed during training .,0
26950,The embeddings for out - ofvocabulary words were set to zero vector .,0
26951,We pad the input sequence to form a full binary tree .,0
26952,A padding vector was inserted when padding .,0
26953,We analyzed the effects of the padding size and found out that it has no influence on the performance ( see Appendix 5.3 ) .,0
26954,The size of hidden units of the NTI modules were set to 300 .,0
26955,The models were regularized by using dropouts and an l 2 weight decay .,0
26956,3,0
26957,Natural Language Inference,1
26958,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",0
26959,"Unless otherwise noted , we follow the setting in the previous work and use an MLP for classification which takes in NTI outputs and computes the concatenation [ h p 2 n?1 ; h h 2n?1 ] , absolute difference hp 2 n ? 1 ? h h 2n?1 and elementwise product hp 2 n ?1 h h 2n?1 of the two sentence representations .",0
26960,The MLP has also an input layer with 1024 units with ReLU activation and a sof tmax output layer .,0
26961,"We explored nine different task - oriented NTI models with varying complexity , to be described below .",0
26962,"For each model , we set the batch size to 32 .",0
26963,"The initial learning , the regularization strength and the number of epoch to be trained are varied for each model .",0
26964,NTI - SLSTM : this model does not rely on f leaf transformer but uses the S - LSTM units for the non-leaf node function .,0
26965,"We set the initial learning rate to 1e - 3 and l 2 regularizer strength to 3 e - 5 , and train the model for 90 epochs .",0
26966,The neural net was regularized by 10 % input dropouts and the 20 % output dropouts .,0
26967,NTI - SLSTM - LSTM : we use LSTM for the leaf node function f leaf .,0
26968,"Concretely , the LSTM output vectors are given to NTI - SLSTM and the memory cells of the lowest level S - LSTM were initialized with the LSTM memory states .",0
26969,The hyper - parameters are the same as the previous 2 http://nlp.stanford.edu/projects/glove/,0
26970,3 More detail on hyper - parameters can be found in code .,0
26971,model .,0
26972,NTI - SLSTM node - by - node global attention :,0
26973,"This model learns inter-sentence relation with the global attention over premise - indexed tree , which is similar to word - by - word attention model of in that it attends over the premise tree nodes at every time step of hypothesis encoding .",0
26974,We tie the weight parameters of the two NTI - SLSTMs for premise and hypothesis and no f leaf transformer used .,0
26975,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 40 epochs .",0
26976,The neural net was regularized by 15 % input dropouts and the 15 % output dropouts .,0
26977,NTI - SLSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,0
26978,The hyper - parameters are the same as the previous model .,0
26979,NTI - SLSTM - LSTM node - by - node global attention : in this model we include LSTM as the leaf node function f leaf .,0
26980,Here we initialize the memory cell of S - LSTM with LSTM memory and hidden / memory state of hypothesis LSTM with premise LSTM ( the later follows the work of ) .,0
26981,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and train the model for 10 epochs .",0
26982,The neural net was regularized by 10 % input dropouts and the 15 % output dropouts .,0
26983,NTI - SLSTM - LSTM node - by - node tree attention : this is a variation of the previous model with the tree attention .,0
26984,The hyper - parameters are the same as the previous model .,0
26985,Tree matching NTI - SLSTM - LSTM global attention : this model first constructs the premise and hypothesis trees simultaneously with the NTI - SLSTM - LSTM model and then computes their matching vector by using the global attention and an additional LSTM .,0
26986,The attention vectors are produced at each hypothesis tree node and then are given to the LSTM model sequentially .,0
26987,"The LSTM model compress the attention vectors and outputs a single matching vector , which is passed to an MLP for classification .",0
26988,The MLP for this tree matching setting has an input layer with 1024 units with ReLU activation and a sof tmax output layer .,0
26989,"Unlike 's matching LSTM model which is specific to matching sequences , we use the standard LSTM units and match trees .",0
26990,"We set the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train the model for 20 epochs .",0
26991,The neural net was regularized by 20 % input dropouts and the 20 % output dropouts .,0
26992,Tree matching NTI - SLSTM - LSTM tree attention : we replace the global attention with the tree attention .,0
26993,The hyper - parameters are the same as the previous model .,0
26994,"Full tree matching NTI - SLSTM - LSTM global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node .",0
26995,Each set of the attention vectors is given to a LSTM model to achieve full tree matching .,0
26996,The last hidden states of the two LSTM models ( i.e. one for each attention vector set ) are concatenated for classification .,0
26997,The training weights are shared among the LSTM models,0
26998,The hyper - parameters are the same as the previous model .,0
26999,4 shows the results of our models .,0
27000,"For comparison , we include the results from the published state - of - the - art systems .",0
27001,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output ; which present strong baseline systems .",0
27002,The last set of methods designs inter-sentence relation with soft attention .,0
27003,Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model .,1
27004,The previous best performing model on the task performs phrase matching by using the attention mechanism .,0
27005,Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % .,1
27006,"Not surprisingly , using LSTM as leaf node function helps in learning better representations .",0
27007,Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations .,0
27008,"The node - by - node attention models improve the performance , indicating that modeling inter-sentence interaction is an important element in NLI .",1
27009,Aggregating matching vector between trees or sequences with a separate LSTM model is effective .,0
27010,The global attention seems to 85.4 45.7 CNN- MC 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 NTI - SLSTM ( Ours ) 87.8 50.5 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NTI - SLSTM- LSTM ( Ours ) 89.3 53.1 : Test accuracy for sentence classification .,0
27011,"Bin : binary , FG : fine - grained 5 classes .",0
27012,be robust on this task .,0
27013,The tree attention were not helpful as it normalizes the attention scores locally in parent - child relationship .,0
27014,Answer Sentence Selection,1
27015,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",0
27016,We experiment on WikiQA dataset constructed from Wikipedia .,0
27017,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",0
27018,We used the same setup in the language inference task except that we replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .,0
27019,where h q n and ha n are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,0
27020,"For this task , we use NTI - SLSTM - LSTM to encode answer candidate sentences and NTI - ANF - LSTM to encode the question sentences .",0
27021,Note that NTI - ANF - LSTM is relied on ANF as the non-leaf node function .,0
27022,q vector for NTI - ANF - LSTM is the answer representation produced by the answer encoding NTI - SLSTM - LSTM model .,0
27023,"We set the batch size to 4 and the initial learning rate to 1 e - 3 , and train the model for 10 epochs .",0
27024,"We used 20 % input dropouts a person park for fun Santa Claus sad , depressed , and hatred single person an outdoor concert at the parka snowmobile in a blizzard an Obama supporter is upset a woman kids playing at a park outside a Skier ski - jumping but does n't have any money a young persona mom takes a break in a park A skier preparing a trick crying because he did n't get cake a guy people play frisbee outdoors a child is playing on christmas trying his hardest to not falloff a single human takes his lunch break in the park two men play with a snowman is upset and crying on the ground :",0
27025,Nearest - neighbor phrases based on cosine similarity between learned representations .,0
27026,and no l 2 weight decay .,0
27027,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",0
27028,5 presents the results of our model and the previous models for the task .,0
27029,The classifier with handcrafted features is a SVM model trained with a set of features .,0
27030,The Bigram - CNN model is a simple convolutional neural net .,0
27031,"The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % .",1
27032,NASM improves the result further and sets a strong baseline by combining variational autoencoder with the soft attention .,1
27033,"In NASM , they adopt a deep three - layer LSTM and introduced a latent stochastic attention mechanism over the answer sentence .",0
27034,Our NTI model exceeds NASM by approximately 0.4 % on MAP for this task .,1
27035,Sentence Classification,1
27036,"Lastly , we evaluated NTI on the Stanford Sentiment Treebank ( SST ) .",0
27037,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,0
27038,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,0
27039,We use NTI - SLSTM and NTI - SLSTM - LSTM models to learn sentence representations for the task .,0
27040,The sentence representations were passed to a two - layer MLP for classification .,0
27041,"We set the batch size to 64 , the initial learning rate to 1e - 3 and l 2 regularizer strength to 3 e - 5 , and train each model for 10 epochs .",0
27042,The NTI - SLSTM model was regularized by 10 % / 20 % of input / output and 20 % / 30 % of input / output dropouts and the NTI - SLSTM - LSTM model 20 % of input and 20 % / 30 % of input / output dropouts for binary and finegrained settings .,0
27043,NTI - SLSTM-LSTM ( as shown in ) set the state - of - the - art results on both subtasks .,0
27044,Our NTI - SLSTM model performed slightly worse A dog mouth holds a retrieved ball .,1
27045,A cat nurses puppies .,0
27046,A dog sells a woman a hat .,0
27047,A brown and white dog holds a tennis ball in his mouth .,0
27048,A golden retriever nurses some other dogs puppies .,0
27049,The dog is a labrador retriever .,0
27050,The dog has a ball .,0
27051,A golden retriever nurses puppies .,0
27052,A girl is petting her dog .,0
27053,The dogs are chasing a ball .,0
27054,A mother dog checking upon her baby puppy .,0
27055,The dog is a shitzu .,0
27056,A small dog runs to catch a ball .,0
27057,A girl is petting her dog .,0
27058,A husband and wife making pizza .,0
27059,The puppy is chasing a ball .,0
27060,The hat wearing girl is petting a cat .,0
27061,The dog is a chihuahua . :,0
27062,Nearest - neighbor sentences based on cosine similarity between learned representations .,0
27063,"than its constituency tree - based counterpart , CT - LSTM model .",0
27064,The CT - LSTM model composes phrases according to the output of a sentence parser and uses anode composition function similar to S - LSTM .,0
27065,"After we transformed the input with the LSTM leaf node function , we achieved the best performance on this task .",1
27066,Qualitative Analysis,0
27067,Attention and Compositionality,0
27068,"To help analyzing the results , we output attention weights by our NTI - SLSTM node - by - node global attention model .",0
27069,shows the attention heatmaps for two sentences in the SNLI test set .,0
27070,"It shows that our model semantically aligns single or multiword expressions ( "" little child "" and "" toddler "" ; "" rock wall "" and "" stone "" ) .",0
27071,"In addition , our model is able to re-orient its attention over different parts of the hypothesis when the expression is more complex .",0
27072,"For example , for ( c ) "" rock wall in autumn "" , NTI mostly focuses on the nodes in depth 1 , 2 and 3 representing contexts related to "" a stone "" , "" leaves . "" and "" a stonewall surrounded "" .",0
27073,"Surprisingly , attention degree for the single word expression like "" stone "" , "" wall "" and "" leaves "" is lower to compare with multiword phrases .",0
27074,Sequence models lack this property as they have no explicit composition module to produce such mu - tiword phrases .,0
27075,"Finally , the most interesting pattern is that the model attends over higher level ( low depth ) tree nodes with rich semantics when considering a ( c ) longer phrase or ( d ) full sentence .",0
27076,"As shown in ( d ) , the NTI model aligns the root node representing the whole hypothesis sentence to the higher level tree nodes covering larger sub-trees in the premise .",0
27077,It certainly ignores the lower level single word expressions and only starts to attend when the words are collectively to form rich semantics .,0
27078,Learned Representations of Phrases and Sentences,0
27079,"Using cosine similarity between their representations produced by the NTI - SLSTM model , we show that NTI is able to capture paraphrases on SNLI test data .",0
27080,"As shown in , NTI seems to distinguish plural from singular forms ( similar phrases to "" a person "" ) .",0
27081,"In addition , NTI captures non-surface knowledge .",0
27082,"For example , the phrases similar to "" park for fun "" tend to align to the semantic content of fun and park , including "" people play frisbee outdoors "" .",0
27083,"The NTI model was able to relate "" Santa Claus "" to christmas and snow .",0
27084,"Interestingly , the learned representations were also able to connect implicit semantics .",0
27085,"For example , NTI found that "" sad , depressed , and hatred "" is close to the phrases like "" an Obama supporter is upset "" .",0
27086,Overall the NTI model is robust to the length of the phrases being matched .,0
27087,"Given a short phrase , NTI can retrieve longer yet semantically coherent sequences from the SNLI test set .",0
27088,"In , we show nearest - neighbor sentences from SNLI test set .",0
27089,Note that the sentences listed in the first two columns sound semantically coherent but not the ones in the last column .,0
27090,"The query sentence "" A dog sells a women a hat "" does not actually represent a common - sense knowledge and this sentence now seem to confuse the NTI model .",0
27091,"As a result , the retrieved sentence are arbitrary and not coherent .",0
27092,Effects of Padding Size,0
27093,We introduced a special padding character in order to construct full binary tree .,0
27094,Does this padding character influence the performance of the NTI models ?,0
27095,"In , we show relationship between the padding size and the accuracy on Stanford sentiment analysis data .",0
27096,Each sentence was padded to form a full binary tree .,0
27097,The x - axis represents the number of padding characters introduced .,0
27098,"When the padding size is less ( up to 10 ) , the NTI - SLSTM - LSTM model performs better .",0
27099,"However , this model tends to perform poorly or equally when the padding size is large .",0
27100,Overall we do not observe any significant performance drop for both models as the padding size increases .,0
27101,This suggests that NTI learns to ignore the special padding character while processing padded sentences .,0
27102,The same scenario was also observed while analyzing attention weights .,0
27103,The attention over the padded nodes was nearly zero .,0
27104,Discussion and Conclusion,0
27105,"We introduced Neural Tree Indexers , a class of tree structured recursive neural network .",0
27106,The NTI models achieved state - of - the - art performance on different NLP tasks .,0
27107,Most of the NTI models form deep neural networks and we think this is one reason that NTI works well even if it lacks direct linguistic motivations followed by other syntactictree - structured recursive models .,0
27108,CNN and NTI are topologically related .,0
27109,Both NTI and CNNs are hierarchical .,0
27110,"However , current implementation of NTI only operates on non-overlapping subtrees while CNNs can slide over the input to produce higher - level representations .",0
27111,NTI is flexible in selecting the node function and the attention mechanism .,0
27112,"Like CNN , the computation in the same tree - depth can be parallelized effectively ; and therefore NTI is scalable and suitable for large - scale sequence processing .",0
27113,Note that NTI can be seen as a generalization of LSTM .,0
27114,"If we construct left - branching trees in a bottom - up fashion , the model acts just like sequential LSTM .",0
27115,Different branching factors for the underlying tree structure have yet to be explored .,0
27116,"NTI can be extended so it learns to select and compose dynamic number of nodes for efficiency , essentially discovering intrinsic hierarchical structure in the input .",0
27117,title,0
27118,A BERT Baseline for the Natural Questions,1
27119,abstract,0
27120,"This technical note describes a new baseline for the Natural Questions ( Kwiatkowski et al. , 2019 ) .",0
27121,"Our model is based on BERT ( Devlin et al. , 2018 ) and reduces the gap between the model F 1 scores reported in the original dataset paper and the human upper bound by 30 % and 50 % relative for the long and short answer tasks respectively .",0
27122,"This baseline has been submitted to the official NQ leaderboard . Code , preprocessed data and pretrained model are available . https://ai.google.com/research/NaturalQuestions https://github.com/google-research/language/tree/",0
27123,"master /language/question answering / bert joint * Also affiliated with Columbia University , work done at Google .",0
27124,Introduction,0
27125,"The release of BERT has substantially advanced the state - of - the - art in a number of NLP tasks , in question answering in particular .",0
27126,"For example , as of this writing , the top 17 systems on the SQuAD 2.0 leaderboard and the top 5 systems on the CoQA leaderboard are all based on BERT .",0
27127,"The results obtained by BERT - based question answering models are also rapidly approaching the reported human performance for these datasets , with 2.5 F1 points of headroom left on SQuAD 2.0 and 6 F1 points on CoQA .",0
27128,"We hypothesize that the Natural Questions ( NQ ) might represent a substantially harder research challenge than question answering tasks like SQuAD 2.0 and CoQA , and that consequently NQ might currently be a good benchmark for the NLP community to focus on .",0
27129,"The qualities that we think make NQ more challenging than other question answering datasets are the following : ( 1 ) the questions in NQ were formulated by people out of genuine curiosity or out of need for an answer to complete another task , ( 2 ) the questions were formulated by people before they had seen the document that might contain the answer , ( 3 ) the documents in which the answer is to be found are much longer than the documents used in some of the existing question answering challenges .",0
27130,In this technical note we describe a BERT - based model for the Natural Questions .,1
27131,"BERT performs very well on this dataset , reducing the gap between the model F 1 scores reported in the original dataset paper and the human upper bound by 30 % and 50 % relative for the long and short answer tasks respectively .",0
27132,"However , there is still ample room for improvement : 22.5 F1 points for the long answer task and 23 F1 points for the short answer task .",0
27133,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .",1
27134,We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,0
27135,"In the rest of this note we give further details on how the NQ dataset was preprocessed , we explain the modeling choices we made in our BERT - based model in order to adapt it to the NQ task , and we finally present our results .",0
27136,Data Preprocessing,0
27137,"The Natural Questions ( NQ ) is a question answering dataset containing 307,373 training examples , 7,830 development examples , and 7,842 test examples .",0
27138,Each example is comprised of a google.com query and a corresponding Wikipedia page .,0
27139,Each Wikipedia page has a passage ( or long answer ) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer .,0
27140,The long and the short answer annotations can however be empty .,0
27141,"If they are both empty , then there is no answer on the page at all .",0
27142,"If the long answer annotation is non-empty , but the short answer annotation is empty , then the annotated passage answers the question but no explicit short answer could be found .",0
27143,"Finally 1 % of the documents have a passage annotated with a short answer that is "" yes "" or "" no "" , instead of a list of short spans .",0
27144,"Following we tokenize every example in NQ using a 30,522 wordpiece vocabulary , then generate multiple instances per example by concatenating a "" [ CLS ] "" token , the tokenized question , a "" [ SEP ] "" token , tokens from the content of the document , and a final "" [ SEP ] "" token , limiting the total size of each instance to 512 tokens .",0
27145,"For each document we generate all possible instances , by listing the document content starting at multiples of 128 tokens , effectively sliding a 512 token size window over the entire length of the document with astride of 128 tokens .",0
27146,On average we generate 30 instances per NQ example .,0
27147,Each instance will be processed independently by BERT .,0
27148,For each training instance we compute start and end token indices to represent the target answer span .,0
27149,"If all annotated short spans are contained in the instance , we set the start and end target indices to point to the smallest span containing all the annotated short answer spans .",0
27150,"If there are no annotated short spans but there is an annotated long answer span completely contained in the instance , we set the start and end target indices to point to the entire long answer span .",0
27151,"If no short or long span can be found in the current instance , we set the target start and end indices to point to the "" [ CLS ] "" token .",0
27152,"We dub the instances in the last category "" null instances "" .",0
27153,"Given the large size of documents in NQ and the fact that 51 % of the documents are annotated as not having an answer to the query at all , we find that about 98 % of generated instances are null , therefore for training we downsample null instances by 50 times in order to obtain a training set that has roughly as many null instances as non -null instances .",0
27154,"This leads to a training set that has approximately 500,000 instances of 512 tokens each .",0
27155,We introduce special markup tokens in the document to give the model a notion of which part of the document it is reading .,0
27156,"The special tokens we introduced are of the form "" [ Paragraph = N ] "" , "" [ Table = N ] "" , and "" [ List = N ] "" at the beginning of the N - th paragraph , list and table respectively in the document .",0
27157,This decision was based on the observation that the first few paragraphs and tables in the document are much more likely than the rest of the document to contain the annotated answer and so the model could benefit from knowing whether it is processing one of these passages .,0
27158,"Special tokens are atomic , meaning that they are not split further by the wordpiece model .",0
27159,"We finally compute for each instance a target answer type as one of five values : "" short "" for instances that contain all annotated short spans , "" yes "" and "" no "" for yes / no annotations where the instance contains the long answer span , "" long "" when the instance contains the long answer span but there is no short or yes / no answer , and "" noanswer "" otherwise .",0
27160,"Null instances correspond to the set of instances with the "" no-answer "" target answer type .",0
27161,Model,0
27162,"Formally , we define a training set instance as a four - tuple where c is a context of 512 wordpiece ids ( including question , document tokens and markup ) , s , e ? { 0 , 1 , . . . , 511 } are inclusive indices pointing to the start and end of the target answer span , and t ? { 0 , 1 , 2 , 3 , 4 } is the annotated answer type , corresponding to the labels "" short "" , "" long "" , "" yes "" , "" no "" , and "" no - answer "" .",0
27163,We define the loss of our model fora training and return the highest scoring span in the document as the predicted short answer span .,0
27164,"Note that g ( c , s , e ) is exactly the log - odds between the likelihood of an answer span ( defined by the product p start pend ) and the "" [ CLS ] "" span .",0
27165,"We select the predicted long answer span as the DOM treetop level node containing the predicted short answer span , and assign to both long and short prediction the same score equal to the maximum value of g ( c , s , e ) for the document .",0
27166,We opted to limit the complexity of this baseline model by always outputting a single short answer as prediction and we rely on the official NQ evaluation script to set thresholds to decide which of our predictions should be changed to having only along answer or no answer at all .,0
27167,We expect that improvements can be obtained by combining start / end and answer type outputs to sometimes predict yes / no answers instead of always predicting a span as the short answer .,0
27168,We also expect additional improvements to be achievable by extending the model to be able to emit short answers comprised of multiple disjoint spans .,0
27169,Experiments,0
27170,We initialized our model from a BERT model already finetuned on SQ u AD 1.1 .,1
27171,We then further finetuned the model on the training instances precomputed as described in Section 2 .,0
27172,"We trained the model by minimizing loss L from Section 3 with the Adam optimizer ( Kingma and Ba , 2014 ) with a batch size of 8 .",1
27173,"As is common practice for BERT models , we only tuned the number of epochs and the initial learning rate for finetuning and found that training for 1 epoch with an initial learning rate of 3 10 ? 5 was the best setting .",1
27174,Evaluation completed in about 5 hours on the NQ dev and test set with a single Tesla P100 GPU .,1
27175,The results obtained by our model are shown in .,0
27176,Our BERT model for NQ performs dramatically better than the models presented in the original NQ paper .,1
27177,Our model closes the gap between the F 1 score achieved by the original baseline systems and the super - annotator upper bound by 30 % for the long answer NQ task and by 50 % for the short answer NQ task .,1
27178,"However NQ appears to be still far from being solved , with more than 20 F1 points of headroom for both the long and short answer tasks .",0
27179,Conclusion,0
27180,We presented a BERT - based model as a new baseline for the newly released Natural Questions .,0
27181,We hope that this baseline can constitute a good starting point for researchers wanting to create better models for the Natural Questions and for other question answering datasets with similar characteristics .,0
27182,title,0
27183,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,1
27184,abstract,0
27185,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,1
27186,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",1
27187,"In this paper , we propose an innovated contextualized attention - based deep neural network , SDNet , to fuse context into traditional MRC models .",0
27188,Our model leverages both inter-attention and self - attention to comprehend conversation context and extract relevant information from passage .,0
27189,"Furthermore , we demonstrated a novel method to integrate the latest BERT contextual model .",0
27190,"Empirical results show the effectiveness of our model , which sets the new state of the art result in CoQA leaderboard , outperforming the previous best model by 1.6 % F 1 .",0
27191,Our ensemble model further improves the result by 2.7 % F 1 .,0
27192,INTRODUCTION,0
27193,Traditional machine reading comprehension ( MRC ) tasks share the single - turn setting of answering a single question related to a passage .,0
27194,There is usually no connection between different questions and answers to the same passage .,0
27195,"However , the most natural way humans seek answers is via conversation , which carries over context through the dialogue flow .",0
27196,"To incorporate conversation into reading comprehension , recently there are several public datasets that evaluate QA model 's efficacy in conversational setting , such as CoQA , QuAC and QBLink .",0
27197,"In these datasets , to generate correct responses , models are required to fully understand the given passage as well as the context of previous questions and answers .",0
27198,"Thus , traditional neural MRC models are not suitable to be directly applied to this scenario .",0
27199,"Existing approaches to conversational QA tasks include BiDAF + + , , DrQA + PGNet , which all try to find the optimal answer span given the passage and dialogue history .",0
27200,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .",1
27201,"Our network stems from machine reading comprehension models , but has several unique characteristics to tackle contextual understanding during conversation .",0
27202,"Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .",1
27203,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .",1
27204,"Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .",1
27205,"Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .",1
27206,Empirical results show that each of these components has substantial gains in prediction accuracy .,0
27207,"We evaluated SDNet on CoQA dataset , which improves the previous state - of - the - art model 's result by 1.6 % ( from 75.0 % to 76.6 % ) overall F 1 score .",0
27208,The ensemble model further increase the F 1 score to 79.3 % .,0
27209,"Moreover , SDNet is the first model ever to pass 80 % on CoQA 's in - domain dataset .",0
27210,APPROACH,0
27211,"In this section , we propose the neural model , SDNet , for the conversational question answering task , which is formulated as follows .",0
27212,"Given a passage C , and history question and answer utterances Q 1 , A 1 , Q 2 , A 2 , ... , Q k?1 , A k?1 , the task is to generate response A k given the latest question Q k  .",0
27213,The response is dependent on both the passage and history utterances .,0
27214,"To incorporate conversation history into response generation , we employ the idea from DrQA + PGNet to prepend the latest N rounds of utterances to the current question Q k  .",0
27215,The problem is then converted into a machine reading comprehension task .,0
27216,"In other words , the reformulate question is Q k = { Q k?N ; A k?N ; ... , Q k?1 ; A k?1 ; Q k }.",0
27217,"To differentiate between question and answering , we add symbol Q before each question and A before each answer in the experiment .",0
27218,MODEL OVERVIEW,0
27219,"Encoding layer encodes each token in passage and question into a fixed - length vector , which includes both word embeddings and contextualized embeddings .",0
27220,"For contextualized embedding , we utilize the latest result from BERT .",0
27221,"Different from previous work , we fix the parameters in BERT model and use the linear combination of embeddings from different layers in BERT .",0
27222,Integration layer uses multi - layer recurrent neural networks ( RNN ) to capture contextual information within passage and question .,0
27223,"To characterize the relationship between passage and question , we conduct word - level attention from question to passage both before and after the RNNs .",0
27224,We employ the idea of history - of - word from FusionNet to reduce the dimension of output hidden vectors .,0
27225,"Furthermore , we conduct self - attention to extract relationship between words at different positions of context and question .",0
27226,Output layer computes the final answer span .,0
27227,"It uses attention to condense the question into a fixedlength vector , which is then used in a bilinear projection to obtain the probability that the answer should start and end at each position .",0
27228,An illustration of our model SDNet is in .,0
27229,ENCODING LAYER,0
27230,We use 300 - dim Glo Ve embedding and contextualized embedding for each word in context and question .,0
27231,We employ BERT as contextualized embedding .,0
27232,"Instead of adding a scoring layer to BERT structure as proposed in , we use the transformer output from BERT as contextualized embedding in our encoding layer .",0
27233,BERT generates,0
27234,L layers of hidden states for all BPE tokens in a sentence / passage and we employ a weighted sum of these hidden states to obtain contextualized embedding .,0
27235,"Furthermore , we lock BERT 's internal weights , setting their gradients to zero .",0
27236,"In ablation studies , we will show that this weighted sum and weight - locking mechanism can significantly boost the model 's performance .",0
27237,"In detail , suppose a word w is tokenized to s BPE tokens w = {b 1 , b 2 , ... , b s } , and BERT generates",0
27238,"L hidden states for each BPE token , h l t , 1 ? l ? L , 1 ? t ? s.",0
27239,"The contextual embedding BERT w for word w is then a per-layer weighted sum of average BERT embedding , with weights ? 1 , ... , ? L .",0
27240,where D ?,0
27241,R kk is a diagonal matrix and U ?,0
27242,"R dk , k is the attention hidden size .",0
27243,"To simplify notation , we define the attention function above as Attn ( A , B , C ) , meaning we compute the attention score ?",0
27244,"ij based on two sets of vectors A and B , and use that to linearly combine vector set C .",0
27245,So the word - level attention above can be simplified as,0
27246,.,0
27247,"For each context word in C , we also include a feature vector f w including 12 - dim POS embedding , 8 - dim NER embedding , a 3 - dim exact matching vector em i indicating whether each context word appears in the question , and a normalized term frequency , following the approach in DrQA .",0
27248,"Therefore , the input vector for each context word isw",0
27249,RNN .,0
27250,"In this component , we use two separate bidirectional RNNs ( BiLSTMs ) to form the contextualized understanding for C and Q .",0
27251,where 1 ? k ?,0
27252,K and K is the number of RNN layers .,0
27253,"We use variational dropout for input vector to each layer of RNN , i.e. the dropout mask is shared over different timesteps .",0
27254,Question Understanding .,0
27255,"For each question word in Q , we employ one more layer of RNN to generate a higher level of understanding of the question .",0
27256,Self - Attention on Question .,0
27257,"As the question has integrated previous utterances , the model needs to directly relate previously mentioned concept with the current question .",0
27258,This is helpful for concept carry - over and coreference resolution .,0
27259,We thus employ self - attention on question .,0
27260,"The formula is the same as word - level attention , except that we are attending a question to itself :",0
27261,.,0
27262,Multilevel Inter - Attention .,0
27263,"After multiple layers of RNN extract different levels of understanding of each word , we conduct multilevel attention from question to context based on all layers of generated representations .",0
27264,"However , the aggregated dimensions can be very large , which is computationally inefficient .",0
27265,"We thus leverage the history - of - word idea from FusionNet ( Huang et al. , 2017 ) : we use all previous levels to compute attentions scores , but only linearly combine RNN outputs .",0
27266,"In detail , we conduct K + 1 times of multilevel attention from each RNN layer output of question to context .",0
27267,where history - of - word vectors are defined as,0
27268,].,0
27269,An additional RNN layer is applied to obtain the contextualized representation v Ci for each word in C.,0
27270,y,0
27271,Self Attention on Context .,0
27272,"Similar to questions , we conduct self attention on context to establish direct correlations between all pairs of words in C. Again , we use the history of word concept to reduce the output dimension by linearly combining v Ci .",0
27273,The self - attention is followed by an additional layer of RNN to generate the final representation of context :,0
27274,OUTPUT LAYER,0
27275,Generating Answer Span .,0
27276,"This component is to generate two scores for each context word corresponding to the probability that the answer starts and ends at this word , respectively .",0
27277,"Firstly , we condense the question representation into one vector :",0
27278,and w is a parametrized vector .,0
27279,"Secondly , we compute the probability that the answer span should start at the i - th word :",0
27280,where W S is a parametrized matrix .,0
27281,"We further fuse the start - position probability into the computation of end-position probability via a GRU , t Q = GRU ( u Q , i PS i u Ci ) .",0
27282,"Thus , the probability that the answer span should end at the i - th word is :",0
27283,"For CoQA dataset , the answer could be affirmation "" yes "" , negation "" no "" or no answer "" unknown "" .",0
27284,"We separately generate three probabilities corresponding to these three scenarios , P Y , P N , P U , respectively .",0
27285,"For instance , to generate the probability that the answer is "" yes "" , P Y , we use :",0
27286,"where WY and w Y are parametrized matrix and vector , respectively .",0
27287,Training .,0
27288,"For training , we use all questions / answers for one passage as a batch .",0
27289,"The goal is to maximize the probability of the ground - truth answer , including span start / end position , affirmation , negation and no -answer situations .",0
27290,"Equivalently , we minimize the negative log-likelihood function L:",0
27291,where i s k and i e k are the ground - truth span start and end position for the k - th question .,0
27292,"indicate whether the k - th ground - truth answer is a passage span , "" yes "" , "" no "" and "" unknown "" , respectively .",0
27293,More implementation details are in Appendix .,0
27294,Prediction .,0
27295,"During inference , we pick the largest span / yes/no/unknown probability .",0
27296,The span is constrained to have a maximum length of 15 .,0
27297,EXPERIMENTS,0
27298,"We evaluated our model on CoQA , a large - scale conversational question answering dataset .",0
27299,"In CoQA , many questions require understanding of both the passage and previous questions and answers , which poses challenge to conventional machine reading models .",0
27300,summarizes the domain distribution in CoQA .,0
27301,"As shown , CoQA contains passages from multiple domains , and the average number of question answering turns is more than 15 per passage .",0
27302,Many questions require contextual understanding to generate the correct answer . Baseline models and metrics .,0
27303,"We compare SDNet with the following baseline models : PGNet ( Seq2 Seq with copy mechanism ) , DrQA , DrQA + PGNet , BiDAF ++ and .",0
27304,"Aligned with the official leaderboard , we use F 1 as the evaluation metric , which is the harmonic mean of precision and recall at word level between the predicted answer and ground truth .",0
27305,1,0
27306,Results .,0
27307,report the performance of SDNet and baseline models .,0
27308,2,0
27309,"As shown , SDNet achieves significantly better results than baseline models .",1
27310,"In detail , the single SDNet model improves overall F 1 by 1.6 % , compared with previous state - of - art model on CoQA , Flow QA .",1
27311,"Ensemble SDNet model further improves overall F 1 score by 2.7 % , and it 's the first model to achieve over 80 % F 1 score on in - domain datasets ( 80.7 % ) .",1
27312,shows the F 1 score on development set over epochs .,0
27313,"As seen , SDNet overpasses all but one baseline models after the second epoch , and achieves state - of - the - art results only after 8 epochs .",1
27314,Ablation Studies .,0
27315,We conduct ablation studies on SDNet model and display the results in .,0
27316,The results show that removing BERT can reduce the F 1 score on development set by 7.15 % .,0
27317,"Our proposed weight sum of per-layer output from BERT is crucial , which can boost the performance by 1.75 % , compared with using only last layer 's output .",0
27318,This shows that the output from each layer in BERT is useful in downstream tasks .,0
27319,This technique can also be applied to other NLP tasks .,0
27320,"Using BERT - base instead of BERT - large pretrained model hurts the F 1 score by 2.61 % , which manifests the superiority of BERT - large model .",0
27321,"Variational dropout and self attention can each improve the performance by 0.24 % and 0.75 % , respectively .",0
27322,Contextual history .,0
27323,"In SDNet , we utilize conversation history via prepending the current question with previous N rounds of questions and ground - truth answers .",0
27324,We experimented the effect of N and show the result in .,0
27325,Excluding dialogue history ( N = 0 ) can reduce the F 1 score by as,0
27326,CONCLUSIONS,0
27327,"In this paper , we propose a novel contextual attention - based deep neural network , SDNet , to tackle conversational question answering task .",0
27328,"By leveraging inter-attention and self - attention on passage and conversation history , the model is able to comprehend dialogue flow and fuse it with the digestion of passage content .",0
27329,"Furthermore , we incorporate the latest breakthrough in NLP , BERT , and leverage it in an innovative way .",0
27330,SDNet achieves superior results over previous approaches .,0
27331,"On the public dataset CoQA , SDNet outperforms previous state - of - the - art model by 1.6 % in overall F 1 metric .",0
27332,"Our future work is to apply this model to open - domain multiturn QA problem with large corpus or knowledge base , where the target passage may not be directly available .",0
27333,This will bean even more realistic setting to human question answering .,0
27334,title,0
27335,Neural Models for Reasoning over Multiple Mentions using Coreference,1
27336,abstract,0
27337,Many problems in NLP require aggregating information from multiple mentions of the same entity which maybe far apart in the text .,0
27338,Existing Recurrent Neural Network ( RNN ) layers are biased towards short - term dependencies and hence not suited to such tasks .,0
27339,We present a recurrent layer which is instead biased towards coreferent dependencies .,0
27340,The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster .,0
27341,"Incorporating this layer into a state - of - the - art reading comprehension model improves performance on three datasets - Wikihop , LAMBADA and the b Abi AI tasks - with large gains when training data is scarce .",0
27342,Introduction,0
27343,A long - standing goal of NLP is to build systems capable of reasoning about the information present in text .,0
27344,One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,0
27345,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",1
27346,shows examples .,0
27347,"QA models which directly read text to answer questions ( commonly known as Reading Comprehension systems ) , typically consist of RNN layers .",0
27348,"RNN layers have a bias towards sequential recency , i.e. a tendency to favor short - term dependencies .",0
27349,"Attention mechanisms alleviate part of the issue , but empirical studies suggest RNNs with attention also have difficulty modeling long - term dependencies .",0
27350,"We conjecture that when training data is scarce , and inductive biases play an important role , RNN - based models would have trouble with coreference - based reasoning .",0
27351,"At the same time , systems for coreference resolution have seen a gradual increase in accuracy over the years .",0
27352,"Hence , in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency .",0
27353,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .",1
27354,This way hidden states are propagated along coreference chains and the original sequence in parallel .,1
27355,We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,1
27356,"On synthetic data specifically constructed to test coreferencebased reasoning , C - GRUs lead to a large improvement over regular GRUs .",0
27357,We show that the structural bias introduced and coreference signals are both important to reach high performance in this case .,0
27358,"On a more realistic dataset , with noisy coreference annotations , we see small but significant improvements over a state - of - the - art baseline .",0
27359,"As we reduce the training data , the gains become larger .",0
27360,"Lastly , we apply the same model to a broad - context language modeling task , where coreference resolution is an important factor , and show improved performance over state - of - the - art .",0
27361,Related Work,0
27362,Entity - based models .,0
27363,presented a generative model for jointly predicting the next word in the text and its gold - standard coreference annotation .,0
27364,"The difference in our work is that we look at the task of reading comprehension , and also work in the more practical setting of system extracted coreferences .",0
27365,"EntNets also maintain dynamic memory slots for entities , but do not use coreference signals and instead update all memories after reading each sentence , which leads to poor performance in the low - data regime ( c.f.. model references in text as explicit latent variables , but limit their work to text generation .",0
27366,used a pooling operation to aggregate entity information across multiple mentions .,0
27367,"also noted the importance of reference resolution for reading comprehension , and we compare our model to their one - hot pointer reader .",0
27368,Syntactic - recency .,0
27369,"Recent work has used syntax , in the form of dependency trees , to replace the sequential recency bias in RNNs with a syntactic recency bias .",0
27370,"However , syntax only looks at dependencies within sentence boundaries , whereas our focus here is on longer ranges .",0
27371,"Our resulting layer is structurally similar to Graph LSTMs , with an additional attention mechanism over the graph edges .",0
27372,"However , while found that using coreference did not lead to any gains for the task of relation extraction , here we show that it has a positive impact on the reading comprehension task .",0
27373,"Self - Attention models are becoming popular for modeling long - term dependencies , and may also benefit from coreference information to bias the learning of those dependencies .",0
27374,Here we focus on recurrent layers and leave such an analysis to future work .,0
27375,Part of this work was described in an unpub - lished preprint .,0
27376,The current paper extends that version and focuses exclusively on coreference relations .,0
27377,"We also report results on the WikiHop dataset , including the performance of the model in the low - data regime .",0
27378,Model,0
27379,Coref - GRU ( C - GRU ) Layer .,0
27380,"Suppose we are given an input sequence w 1 , w 2 , . . . , w T along with their word vectors x 1 , . . . , x T and annotations for the most recent coreferent antecedent for each token y 1 , . . . , y T , where y t ?",0
27381,"{ 0 , . . . , t ?",0
27382,1 } and y t = 0 denotes the null antecedent ( for tokens not belonging to any cluster ) .,0
27383,"We assume all tokens belonging to a mention in a cluster belong to that cluster , and there are C clusters in total .",0
27384,"Our recurrent layer is adapted from GRU cells , but similar extensions can be derived for other recurrent cells as well .",0
27385,The update equations in a GRU all take the same basic form given by :,0
27386,The bias for sequential recency comes from the second term U h t ?1 .,0
27387,In this work we add another term to introduce a bias towards coreferent recency instead :,0
27388,"where h yt is the hidden state of the coreferent antecedent of wt ( with h 0 = 0 ) , ?",0
27389,sand ?,0
27390,"care nonlinear functions applied to the hidden states coming from the sequential antecedent and the coreferent antecedent , respectively , and ?",0
27391,"t is a scalar weight which decides the relative importance of the two terms based on the current input ( so that , for example , pronouns may assign a higher weight for the coreference state ) .",0
27392,"When y t = 0 , ?",0
27393,"t is set to 1 , otherwise it is computed using a keybased addressing scheme , as",0
27394,", where k is a trainable key vector .",0
27395,In this work we use simple slicing func -,0
27396,"which decompose the hidden states into a sequential and a coreferent component , respectively .",0
27397,"shows an illustration of the layer , and the full update equations are given in Appendix A.",0
27398,Connection to Memory Networks .,0
27399,We can also view the model as a memory network with a memory state,0
27400,Mt at each time step which is a C d matrix .,0
27401,The rows of this memory matrix correspond to the state of each coreference cluster at time step t.,0
27402,The main difference between Coref - GRUs and atypical memory network such as EntNets lies in the fact that we use coreference annotations to read and write from the memory rather than let model learn how to access the memory .,0
27403,"With Coref - GRUs , only the content of the memories needs to be learned .",0
27404,"As we shall see in Section 4 , this turns out to be a useful bias in the low - data regime .",0
27405,Bidirectional C - GRU .,0
27406,"To extend to the bidirectional case , a second layer is fed the same sequence in the reverse direction , x T , . . . , x 1 and y t ?",0
27407,"{ 0 , t + 1 , . . . , T } now denotes the immediately descendent coreferent token from wt .",0
27408,Outputs from the two layers are then concatenated to form the bi-directional output ( see ) .,0
27409,Complexity .,0
27410,The resulting layer has the same time - complexity as that of a regular GRU layer .,0
27411,The memory complexity increases since we have to keep track of the hidden states for each coreference cluster in the input .,0
27412,"If there are C clusters and B is the batch size , the resulting complexity is by O ( BT Cd ) .",0
27413,"This scales linearly with the input size T , however we leave exploration of more efficient architectures to future work .",0
27414,Reading comprehension architecture .,0
27415,"All tasks we look at involve tuples of the form ( p , q , a , C ) , where the goal is to find the answer a from candidates C to question q with passage p as context .",0
27416,"We use the Gated - Attention ( GA ) reader as abase architecture , which computes representations of the passage by passing it through multiple bidirectional GRU layers with an attention mechanism in between layers .",0
27417,We compare the original GA architecture ( GA w/ GRU ) with one where the bidirectional GRU layers are replaced with bidirectional C - GRU layers ( GA w / C - GRU ) .,0
27418,"Performance is reported in terms of the accuracy of detecting the correct answer from C , and all models are trained using cross - entropy loss .",0
27419,When comparing two models we ensure the number of parameters are the same in each .,0
27420,Other implementation details are listed in Appendix B.,0
27421,Experiments & Results,0
27422,Method,0
27423,Avg Max # failed :,0
27424,"Accuracy on b Abi -1 K , averaged across all 20 tasks .",0
27425,"Following previous work we run each task for 10 random seeds , and report the Avg and Max ( based on dev set ) performance .",0
27426,A task is considered failed if its Max performance is < 0.95 .,0
27427,BAbi AI tasks .,1
27428,Our first set of experiments are on the 1 K training version of the synthetic b Abi AI tasks .,0
27429,"The passages and questions in this dataset are generated using templates , removing many complexities inherent in natural language , but it still provides a useful testbed for us since some tasks are specifically constructed to test the coreference - based reasoning we tackle here .",0
27430,Experiments on more natural data are described below .,0
27431,"shows a comparison of EntNets , QRNs ( the best published results on b Abi -1 K ) , and our models .",0
27432,We also include the results for a single layer version of GA Reader ( which we denote simply as Bi - GRU or Bi - C - GRU when using coreference ) to enable fair comparison with EntNets .,0
27433,In each case we see clear improvements of using C - GRU layers over GRU layers .,1
27434,"Interestingly , EntNets , which have > 99 % performance when trained with 10K examples only reach 70 % performance with 1 K training examples .",0
27435,"The Bi - C - GRU model significantly improves on this baseline , which shows that , with less data , coreference annotations can provide a useful bias for a memory network on how to read and write memories .",0
27436,"A break - down of task - wise performance is given in Appendix C. Comparing C - GRU to the GRU based method , we find that the main gains are on tasks 2 ( two supporting facts ) , 3 ( three supporting facts ) and 16 ( basic induction ) .",0
27437,All these tasks require aggregation of information across sentences to derive the answer .,0
27438,"Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .",1
27439,"On closer examination we found that this was because our simplistic coreference module which matches tokens exactly was notable to resolve "" mice "" to "" mouses "" and "" cats "" to "" cat "" .",0
27440,"On the other hand , C - GRU was significantly better than QRN on task 16 ( basic induction ) .",1
27441,We also include a baseline which uses coreference features as 1 - hot vectors appended to the input word vectors ( GA w/ GRU + 1 - hot ) .,0
27442,"This provides the model with information about the coreference clusters , but does not improve performance , suggesting that the regular GRU is unable to track the given coreference information across long distances to solve the task .",0
27443,"On the other hand , in ( left ) we show how the performance of GA w / C - GRU varies as we remove gold - standard mentions from coreference clusters , or if we replace them with random mentions ( GA w / random - GRU ) .",0
27444,"In both cases there is a sharp drop in performance , showing that specifically using coreference for connecting mentions is important .",0
27445,Wikihop dataset .,1
27446,"Next we apply our model to the Wikihop dataset , which is specifically constructed to test multi-hop reading comprehension across documents .",0
27447,"Each instance in this dataset consists of a collection of passages ( p 1 , . . . , p N ) , and a query of the form ( h , r ) where h is an entity and r is a relation .",0
27448,The task is to find the tail entity t from a set of provided candidates C .,0
27449,"As preprocessing we concatenate all documents in a random order , and extract coreference annotations from the Berkeley Entity Resolution system 62 % F 1 score on the CoNLL 2011 test set .",0
27450,We only keep the coreference clusters which contain at least one candidate from C or an entity which co-occurs with the head entity h.,0
27451,"We report results in when using the full training set , as well as when using a reduced training set of sizes 1 K and 5 K , to test the model under a low - data regime .",0
27452,In we also show the training curves of exp ( ? loss ) on the validation set .,0
27453,"We see higher performance for the C - GRU model in the low data regime , and better generalization throughout the training curve for all three settings .",1
27454,"This supports our conjecture that the GRU layer has difficulty learning the kind of coreference - based reasoning required in this dataset , and that the bias towards coreferent recency helps with that .",0
27455,"However , perhaps surprisingly , given enough data both models perform comparably .",0
27456,"This could either indicate that the baseline learns the required reasoning patterns when given enough data , or , that the bias towards corefence - based reasoning hurts performance for some other types of questions .",0
27457,"Indeed , there are 9 % questions which are answered correctly by the baseline but not by C - GRU , however , we did not find any consistent patterns among these in our analyses .",0
27458,"Lastly , we note that both models vastly outperform the best reported result of BiDAf from 1 . We believe this is because the GA models select answers from the list of candidatees , whereas BiDAF ignores those candidates .",0
27459,Method overall context,0
27460,Chu et al. 0.4900 - GA w/ GRU 0.5398 0.6677 GA w/ GRU + 1 - hot 0.5338 0.6603 GA w/ C - GRU 0.5569 0.6888 LAMBADA dataset .,0
27461,Our last set of experiments is on the broad - context language modeling task of LAMBADA dataset .,0
27462,"This dataset consists of passages 4 - 5 sentences long , where the last word needs to be predicted .",0
27463,"Interestingly , though , the passages are filtered such that human volunteers were able to predict the missing token given the full passage , but not given only the last sentence .",0
27464,"Hence , predicting these tokens involves a broader understanding of the whole passage .",0
27465,Analysis of the questions suggests that around 20 % of the questions need coreference understanding to answer correctly .,0
27466,"Hence , we apply our model which uses coreference information for this task .",0
27467,"We use the same setup as which formulated the problem as a reading comprehension one by treating the last sentence as query , and the remaining passage as context to extract the answer from .",0
27468,"In this manner only 80 % of the questions are answerable , but the performance increases substantially compared to pure language modeling based approaches .",0
27469,"For this dataset we used Stanford CoreNLP to extract coreferences , which achieved 0.63 F1 on the CoNLL test set .",0
27470,shows a comparison of the GA w/ GRU baseline and GA w/ C - GRU models .,0
27471,We see a significant gain in performance when using the layer with coreference bias .,0
27472,"Furthermore , the 1 - hot baseline which uses the same coreference information , but with sequential recency bias fails to improve over the regular GRU layer .",0
27473,"While the improvement for C - GRU is small , it is significant , and we note that questions in this dataset involve several different types of reasoning with better performance than reported here ( as of April 2018 ) .",0
27474,Since we were unable to find publications for these models we omit them here .,0
27475,out of which we only tackle one specific kind .,0
27476,The proposed GA w / C - GRU layer sets a new state - of the - art on this dataset .,0
27477,Conclusion,0
27478,"We present a recurrent layer with a bias towards coreferent recency , with the goal of tackling reading comprehension problems which require aggregating information from multiple mentions of the same entity .",0
27479,"Our experiments show that when combined with a powerful reading architecture , the layer provides a useful inductive bias for solving problems of this kind .",0
27480,"In future work , we aim to apply this model to other problems where longterm dependencies at the document level are important .",0
27481,"Noise in the coreference annotations has a detrimental effect on the performance ) , hence we also aim to explore joint models which learn to do coreference resolution and reading together .",0
27482,A C - GRU update equations,0
27483,"For simplicity , we introduce the variable mt which concatenates ( || ) the sequential and coreferent hidden states :",0
27484,Then the update equations are given by :,0
27485,The attention parameter ?,0
27486,t is given by :,0
27487,where k 1 and k 2 are trainable key vectors .,0
27488,B Implementation details,0
27489,We use K = 3 layers with the GA architecture .,0
27490,"We keep the same hyperparameter settings when using GRU or C - GRU layers , which we describe here .",0
27491,"For the bAbi dataset , we use a hidden state size of d = 64 , batch size of B = 32 , and learning rate 0.01 which is halved after every 120 updates .",0
27492,We also use dropout with rate 0.1 at the output of each layer .,0
27493,The maximum number of coreference clusters across all tasks was C = 13 .,0
27494,"Half of the tasks in this dataset are extractive , meaning the answer is present in the passage , whereas the other half are classification tasks , where the answer is in a list of candidates which may not be in the passage .",0
27495,"For the extractive tasks , we use the attention sum layer as described in the GA Reader paper .",0
27496,For the classification tasks we replace this with a softmax layer for predicting one of the classes .,0
27497,"For the Wikihop dataset , we use a hidden state size of d = 64 , batch size B = 16 , and learning rate of 0.0005 which was halved every 2500 updates .",0
27498,The maximum number of coreference clusters was set to 50 for this dataset .,0
27499,"We used dropout of 0.2 in between the intermediate layers , and initialized word embeddings with Glove .",0
27500,"We also used character embeddings , which were concatenated with the word embeddings , of size 10 .",0
27501,These were output from a CNN layer with 50 filters each of width 5 .,0
27502,"Following , we also appended a feature to the word embeddings in the passage which indicated if the token appeared in the query or not .",0
27503,"For the LAMBADA dataset , we use a hidden state size of d = 256 , batch size of B = 64 , and learning rate of 0.0005 which was halved every 2 epochs .",0
27504,"Word vectors were initialized with Glove , and dropout of 0.2 was applied after intermediate layers .",0
27505,The maximum number of coreference clusters in this dataset was 15 . :,0
27506,Breakdown of task - wise performance on bAbi dataset .,0
27507,Tasks where C - GRU is significant better / worse than either GRU or QRNs are highlighted .,0
27508,C Task - wise b Abi performance,0
27509,title,0
27510,Attention - over - Attention Neural Networks for Reading Comprehension,1
27511,abstract,0
27512,Cloze - style reading comprehension is a representative problem in mining relationship between document and query .,1
27513,"In this paper , we present a simple but novel model called attention - over - attention reader for better solving cloze - style reading comprehension task .",0
27514,"The proposed model aims to place another attention mechanism over the document - level attention and induces "" attended attention "" for final answer predictions .",0
27515,One advantage of our model is that it is simpler than related works while giving excellent performance .,0
27516,"In addition to the primary model , we also propose an N - best re-ranking strategy to double check the validity of the candidates and further improve the performance .",0
27517,"Experimental results show that the proposed methods significantly outperform various state - of the - art systems by a large margin in public datasets , such as CNN and Children 's Book Test .",0
27518,Introduction,0
27519,"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues .",1
27520,"Reading comprehension is a general problem in the real world , which aims to read and comprehend a given article or context , and answer the questions based on it .",0
27521,"Recently , the cloze - style reading comprehension problem has become a popular task in the community .",0
27522,The cloze - style query is a problem that to fill in an appropriate word in the given sentences while taking the context information into account .,0
27523,"To teach the machine to do cloze - style reading comprehensions , large - scale training data is necessary for learning relationships between the given document and query .",0
27524,"To create large - scale training data for neural networks , released the CNN / Daily Mail news dataset , where the document is formed by the news articles and the queries are extracted from the summary of the news .",0
27525,released the Children 's Book,0
27526,"Test dataset afterwards , where the training samples are generated from consecutive 20 sentences from books , and the query is formed by 21st sentence .",0
27527,"Following these datasets , avast variety of neural network approaches have been proposed , and most of them stem from the attention - based neural network , which has become a stereotype inmost of the NLP tasks and is well - known by its capability of learning the "" importance "" distribution over the inputs .",0
27528,"In this paper , we present a novel neural network architecture , called attention - over - attention model .",1
27529,"As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .",1
27530,"Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .",1
27531,"To sum up , the main contributions of our work are listed as follows .",0
27532,"the mechanism of nesting another attention over the existing attentions is proposed , i.e. attention - over - attention mechanism .",0
27533,"Unlike the previous works on introducing complex architectures or many non-trainable hyper - parameters to the model , our model is much more simple but outperforms various state - of - the - art systems by a large margin .",0
27534,We also propose an N - best re-ranking strategy to re-score the candidates in various aspects and further improve the performance .,0
27535,The following of the paper will be organized as follows .,0
27536,"In Section 2 , we will give a brief introduction to the cloze - style reading comprehension task as well as related public datasets .",0
27537,Then the proposed attention - over - attention reader will be presented in detail in Section 3 and N - best reranking strategy in Section 4 .,0
27538,The experimental results and analysis will be given in Section 5 and Section 6 .,0
27539,Related work will be discussed in Section 7 .,0
27540,"Finally , we will give a conclusion of this paper and envisions on future work .",0
27541,Cloze- style Reading Comprehension,0
27542,"In this section , we will give a brief introduction to the cloze - style reading comprehension task at the beginning .",0
27543,"And then , several existing public datasets will be described in detail .",0
27544,Task Description,0
27545,"Formally , a general Cloze - style reading comprehension problem can be illustrated as a triple :",0
27546,"D , Q , A",0
27547,"The triple consists of a document D , a query Q and the answer to the query A. Note that the answer is usually a single word in the document , which requires the human to exploit context information in both document and query .",0
27548,The type of the answer word varies from predicting a preposition given a fixed collocation to identifying a named entity from a factual illustration .,0
27549,Existing Public Datasets,0
27550,Large - scale training data is essential for training neural networks .,0
27551,Several public datasets for the cloze - style reading comprehension has been released .,0
27552,"Here , we introduce two representative and widely - used datasets .",0
27553,have firstly published two datasets :,0
27554,CNN and Daily Mail news data,0
27555,1 .,0
27556,They construct these datasets with web - crawled CNN and Daily Mail news data .,0
27557,One of the characteristics of these datasets is that the news article is often associated with a summary .,0
27558,"So they first regard the main body of the news article as the Document , and the Query is formed by the summary of the article , where one entity word is replaced by a special placeholder to indicate the missing word .",0
27559,The replaced entity word will be the Answer of the Query .,0
27560,"Apart from releasing the dataset , they also proposed a methodology that anonymizes the named entity tokens in the data , and these tokens are also re-shuffle in each sample .",0
27561,"The motivation is that the news articles are containing limited named entities , which are usually celebrities , and the world knowledge can be learned from the dataset .",0
27562,So this methodology aims to exploit general relationships between anonymized named entities within a single document rather than the common knowledge .,0
27563,The following research on these datasets showed that the entity word anonymization is not as effective as expected .,0
27564,CNN / Daily Mail,0
27565,Children 's Book Test,0
27566,"There was also a dataset called the Children 's Book Test ( CBTest ) released by , which is built on the children 's book story through Project Gutenberg 2 .",0
27567,"Different from the CNN / Daily Mail datasets , there is no summary available in the children 's book .",0
27568,So they proposed another way to extract query from the original data .,0
27569,"The document is composed of 20 consecutive sentences in the story , and the 21st sentence is regarded as the query , where one word is blanked with a special placeholder .",0
27570,"In the CBTest datasets , there are four types of sub-datasets available which are classified by the part - of - speech and named entity tag of the answer word , containing Named Entities ( NE ) , Common Nouns ( CN ) , Verbs and Prepositions .",0
27571,"In their studies , they have found that the answering of verbs and prepositions are relatively less dependent on the content of document , and the humans can even do preposi-tion blank - filling without the presence of the document .",0
27572,"The studies shown by , answering verbs and prepositions are less dependent with the presence of document .",0
27573,"Thus , most of the related works are focusing on solving NE and CN types .",0
27574,Attention - over - Attention Reader,0
27575,"In this section , we will give a detailed introduction to the proposed Attention - over - Attention Reader ( AoA Reader ) .",0
27576,"Our model is primarily motivated by , which aims to directly estimate the answer from the document - level attention instead of calculating blended representations of the document .",0
27577,"As previous studies by showed that the further investigation of query representation is necessary , and it should be paid more attention to utilizing the information of query .",0
27578,"In this paper , we propose a novel work that placing another attention over the primary attentions , to indicate the "" importance "" of each attentions .",0
27579,"Now , we will give a formal description of our proposed model .",0
27580,"When a cloze - style training triple D , Q , A is given , the proposed model will be constructed in the following steps .",0
27581,Contextual Embedding,0
27582,We first transform every word in the document D and query Q into one - hot representations and then convert them into continuous representations with a shared embedding matrix W e .,0
27583,"By sharing word embedding , both the document and query can participate in the learning of embedding and both of them will benefit from this mechanism .",0
27584,"After that , we use two bi-directional RNNs to get contextual representations of the document and query individually , where the representation of each word is formed by concatenating the forward and backward hidden states .",0
27585,"After making a trade - off between model performance and training complexity , we choose the Gated Recurrent Unit ( GRU ) as recurrent unit implementation .",0
27586,We take h doc ?,0
27587,R | D| * 2 d and h query ?,0
27588,"R | Q | * 2d to denote the contextual representations of document and query , where dis the dimension of GRU ( oneway ) .",0
27589,Pair-wise Matching Score,0
27590,"After obtaining the contextual embeddings of the document h doc and query h query , we calculate a pair - wise matching matrix , which indicates the pair - wise matching degree of one document word and one query word .",0
27591,"Formally , when given ith word of the document and jth word of query , we can compute a matching score by their dot product .",0
27592,"In this way , we can calculate every pair - wise matching score between each document and query word , forming a matrix M ? R | D | * | Q | , where the value of ith row and jth column is filled by M ( i , j ) .",0
27593,Individual Attentions,0
27594,"After getting the pair - wise matching matrix M , we apply a column - wise softmax function to get probability distributions in each column , where each column is an individual document - level attention when considering a single query word .",0
27595,We denote ?( t ) ?,0
27596,"R | D | as the document - level attention regarding query word at time t , which can be seen as a query - to - document attention .",0
27597,Attention - over - Attention,0
27598,"Different from , instead of using naive heuristics ( such as summing or averaging ) to combine these individual attentions into a final attention , we introduce another attention mechanism to automatically decide the importance of each individual attention .",0
27599,"First , we calculate a reversed attention , that is , for every document word at time t , we calculate the "" importance "" distribution on the query , to indicate which query words are more important given a single document word .",0
27600,We apply a row - wise softmax function to the pair - wise matching matrix M to get query - level attentions .,0
27601,We denote ?( t ) ?,0
27602,"R | Q | as the query - level attention regarding document word at time t , which can be seen as a document - to - query attention .",0
27603,"So far , we have obtained both query - todocument attention ?",0
27604,and document - to - query attention ?.,0
27605,Our motivation is to exploit mutual information between the document and query .,0
27606,"However , most of the previous works are only relying on query - to - document attention , that is , only calculate one document - level attention when considering the whole query .",0
27607,Then we average all the ? ( t ) to get an averaged query - level attention ?.,0
27608,"Note that , we do not apply another softmax to the ? , because averaging individual attentions do not break the normalizing condition .",0
27609,"Finally , we calculate dot product of ? and ?",0
27610,"to get the "" attended document - level attention "" s ? R | D | , i.e. the attention - over - attention mechanism .",0
27611,"Intuitively , this operation is calculating a weighted sum of each individual document - level attention ?( t ) when looking at query word at time t.",0
27612,"In this way , the contributions by each query word can be learned explicitly , and the final decision ( document - level attention ) is made through the voted result by the importance of each query word .",0
27613,Final Predictions,0
27614,"Following , we use sum attention mechanism to get aggregated results .",0
27615,"Note that the final output should be reflected in the vocabulary space V , rather than document - level attention | D| , which will make a significant difference in the performance , though did not illustrate this clearly .",0
27616,"where I ( w , D ) indicate the positions that word w appears in the document D .",0
27617,"As the training objectives , we seek to maximize the log -likelihood of the correct answer .",0
27618,"CNN News 118,497 53,063 53,185",0
27619,The proposed neural network architecture is depicted in .,0
27620,"Note that , as our model mainly adds limited steps of calculations to the AS Reader and does not employ any additional weights , the computational complexity is similar to the AS Reader .",0
27621,CBT NE CBT CN,0
27622,N - best Re-ranking Strategy,0
27623,"Intuitively , when we do cloze - style reading comprehensions , we often refill the candidate into the blank of the query to double - check its appropriateness , fluency and grammar to see if the candidate we choose is the most suitable one .",0
27624,"If we do find some problems in the candidate we choose , we will choose the second possible candidate and do some checking again .",0
27625,"To mimic the process of double - checking , we propose to use N- best re-ranking strategy after generating answers from our neural networks .",0
27626,The procedure can be illustrated as follows .,0
27627,"N - best Decoding Instead of only picking the candidate that has the highest possibility as answer , we can also extract follow - up candidates in the decoding process , which forms an N - best list .",0
27628,Refill Candidate into Query,0
27629,"As a characteristic of the cloze - style problem , each candidate can be refilled into the blank of the query to form a complete sentence .",0
27630,This allows us to check the candidate according to its context .,0
27631,Feature Scoring,0
27632,The candidate sentences can be scored in many aspects .,0
27633,"In this paper , we exploit three features to score the N - best list .",0
27634,"Global N-gram LM : This is a fundamental metric in scoring sentence , which aims to evaluate its fluency .",0
27635,This model is trained on the document part of training data .,0
27636,"Local N- gram LM : Different from global LM , the local LM aims to explore the information with the given document , so the statistics are obtained from the test - time document .",0
27637,"It should be noted that the local LM is trained sample - by - sample , it is not trained on the entire test set , which is not legal in the real test case .",0
27638,This model is useful when there are many unknown words in the test sample .,0
27639,"Word - class LM : Similar to global LM , the word - class LM is also trained on the document part of training data , but the words are converted to its word class ID .",0
27640,The word class can be obtained by using clustering methods .,0
27641,"In this paper , we simply utilized the mkcls tool for generating 1000 word classes .",0
27642,Weight Tuning,0
27643,"To tune the weights among these features , we adopt the K - best MIRA algorithm to automatically optimize the weights on the validation set , which is widely used in statistical machine translation tuning procedure .",0
27644,Re-scoring and Re-ranking,0
27645,"After getting the weights of each feature , we calculate the weighted sum of each feature in the Nbest sentences and then choose the candidate that has the lowest cost as the final answer .",0
27646,Experiments,0
27647,Experimental Setups,0
27648,The general settings of our neural network model are listed below in detail .,0
27649,Embedding Layer :,1
27650,"The embedding weights are randomly initialized with the uniformed distribution in the interval [ ? 0.05 , 0.05 ].",1
27651,CNN,0
27652,News,0
27653,CB Test NE CB Test CN Valid Test Valid Test Valid Test,0
27654,"Deep LSTM Reader 55.0 57.0 ---- Attentive Reader 61.6 63.0 ---- Human ( context+query ) --- 81.6 - 81.6 MemNN ( window + self- sup. ) 63.4 66.8 70.4 66.6 64.2 63.0 AS Reader 68.6 69.5 73.8 68.6 68.8 63.4 CAS Reader 68.2 70.0 74.2 69.2 68.2 65.7 Stanford AR 72.4 72.4 ---- GA Reader 73.0 73.8 74.9 69.0 69.0 63.9 Iterative Attention 72.6 73.3 75.2 68.6 72.1 69.2 EpiReader 73 For regularization purpose , we adopted l 2 regularization to 0.0001 and dropout rate of 0.1 .",0
27655,"Also , it should be noted that we do not exploit any pretrained embedding models .",0
27656,Hidden Layer : Internal weights of GRUs are initialized with random orthogonal matrices .,1
27657,Optimization :,0
27658,"We adopted ADAM optimizer for weight updating , with an initial learning rate of 0.001 .",1
27659,"As the GRU units still suffer from the gradient exploding issues , we set the gradient clipping threshold to 5 .",1
27660,We used batched training strategy of 32 samples .,1
27661,Dimensions of embedding and hidden layer for each task are listed in .,0
27662,"In re-ranking step , we generate 5 - best list from the baseline neural network model , as we did not observe a significant variance when changing the N - best list size .",1
27663,"All language model features are trained on the training proportion of each dataset , with 8 - gram wordbased setting and Kneser - Ney smoothing trained by SRILM toolkit .",1
27664,"The results are reported with the best model , which is selected by the performance of validation set .",0
27665,"The ensemble model is made up of four best models , which are trained using different random seed .",0
27666,"Implementation is done with Theano ( Theano Development Team , 2016 ) and Keras , and all models are trained on Tesla K40 GPU . :",1
27667,Embedding and hidden layer dimensions for each task .,0
27668,Overall Results,0
27669,Our experiments are carried out on public datasets : CNN news datasets and CBTest NE / CN datasets .,0
27670,"The statistics of these datasets are listed in , and the experimental results are given in .",0
27671,"As we can see that , our AoA Reader outperforms state - of - the - art systems by a large margin , where 2.3 % and 2.0 % absolute improvements over EpiReader in CBTest NE and CN test sets , which demonstrate the effectiveness of our model .",1
27672,"Also by adding additional features in the re-ranking step , there is another significant boost 2.0 % to 3.7 % over Ao A Reader in CBTest NE / CN test sets .",1
27673,"We have also found that our single model could stay on par with the previous best ensemble system , and even we have an absolute improvement of 0.9 % beyond the best ensemble model ( Iterative Attention ) in the CBTest NE validation set .",1
27674,"When it comes to ensemble model , our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and setup a new state - of - the - art system .",1
27675,"To investigate the effectiveness of employing attention - over - attention mechanism , we also compared our model to CAS Reader , which used predefined merging heuristics , such as sum or avg etc .",0
27676,"Instead of using pre-defined merging heuristics , and letting the model explicitly learn the weights between individual attentions results in a significant boost in the performance , where 4.1 % and 3.7 % improvements can be made in CNN validation and test set against CAS Reader .",1
27677,Effectiveness of Re-ranking Strategy,0
27678,"As we have seen that the re-ranking approach is effective in cloze - style reading comprehension task , we will give a detailed ablations in this section to show the contributions by each feature .",0
27679,"To have a thorough investigation in the re-ranking step , we listed the detailed improvements while adding each feature mentioned in Section 4 .",0
27680,"From the results in , we found that the NE and CN category both benefit a lot from the re-ranking features , but the proportions are quite different .",1
27681,"Generally speaking , in NE category , the performance is mainly boosted by the LM local feature .",1
27682,"However , on the contrary , the CN category benefits from LM global and LM wc rather than the LM local .",0
27683,"Also , we listed the weights of each feature in .",0
27684,"The LM global and LM wc are all trained by training set , which can be seen as Global Feature .",0
27685,"However , the LM local is only trained within the respective document part of test sample , which can be seen as Local Feature .",0
27686,We calculated the ratio between the global and local features and found that the NE category is much more dependent on local features than CN category .,0
27687,"Because it is much more likely to meet a new named entity than a common noun in the test phase , so adding the local LM provides much more information than that of common noun .",0
27688,"However , on the contrary , answering common noun requires less local information , which can be learned in the training data relatively .",0
27689,Quantitative Analysis,0
27690,"In this section , we will give a quantitative analysis to our AoA Reader .",0
27691,The following analyses are carried out on CBTest NE dataset .,0
27692,"First , we investigate the relations between the length of the document and corresponding accuracy .",0
27693,The result is depicted in .,0
27694,As we can see that the AoA Reader shows consistent improvements over AS Reader on the different length of the document .,0
27695,"Especially , when the length of document exceeds 700 , the improvements become larger , indicating that the AoA Reader is more capable of handling long documents .",0
27696,"Furthermore , we also investigate if the model tends to choose a high - frequency candidate than a lower one , which is shown in .",0
27697,"Not surprisingly , we found that both models do a good job when the correct answer appears more frequent in the document than the other candidates .",0
27698,This is because that the correct answer that has the highest frequency among the candidates takes up over 40 % of the test set ( 1071 out of 2500 ) .,0
27699,"But interestingly we have also found that , when the frequency rank of correct answer exceeds 7 ( less frequent among candidates ) , these models also give a relatively high performance .",0
27700,"Empirically , we think that these models tend to choose extreme cases in terms of candidate frequency ( either too high or too low ) .",0
27701,"One possible reason is that it is hard for the model to choose a candidate that has a neutral frequency as the correct answer , because of its ambiguity ( neutral choices are hard to made ) .",0
27702,Related Work,0
27703,Cloze - style reading comprehension tasks have been widely investigated in recent studies .,0
27704,We will take a brief revisit to the related works .,0
27705,"Hermann et al. have proposed a method for obtaining large quantities of D , Q , A triples through news articles and its summary .",0
27706,"Along with the release of cloze - style reading comprehension dataset , they also proposed an attention - based neural network to handle this task .",0
27707,Experimental results showed that the proposed neural network is effective than traditional baselines .,0
27708,"released another dataset , which stems from the children 's books .",0
27709,"Different from Hermann et al. 's work , the document and query are all generated from the raw story without any summary , which is much more general than previous work .",0
27710,"To handle the reading comprehension task , they proposed a window - based memory network , and self - supervision heuristics is also applied to learn hard - attention .",0
27711,"Unlike previous works , that using blended representations of document and query to estimate the answer , proposed a simple model that directly pick the answer from the document , which is motivated by the Pointer Network .",0
27712,A restriction of this model is that the answer should be a single word and appear in the document .,0
27713,Results on various public datasets showed that the proposed model is effective than previous works .,0
27714,proposed to exploit reading comprehension models to other tasks .,0
27715,They first applied the reading comprehension model into Chinese zero pronoun resolution task with automatically generated large - scale pseudo training data .,0
27716,The experimental results on OntoNotes 5.0 data showed that their method significantly outperforms various state - of - the - art systems .,0
27717,"Our work is primarily inspired by and , where the latter model is widely applied to many follow - up works .",0
27718,"Unlike the CAS Reader , we do not assume any heuristics to our model , such as using merge functions : sum , avg etc .",0
27719,"We used a mechanism called "" attention - over-attention "" to explicitly calculate the weights between different individual document - level attentions , and get the final attention by computing the weighted sum of them .",0
27720,"Also , we find that our model is typically general and simple than the recently proposed model , and brings significant improvements over these cutting edge systems .",0
27721,Conclusion,0
27722,"We present a novel neural architecture , called attention - over - attention reader , to tackle the clozestyle reading comprehension task .",0
27723,"The proposed AoA Reader aims to compute the attentions not only for the document but also the query side , which will benefit from the mutual information .",0
27724,Then a weighted sum of attention is carried out to get an attended attention over the document for the final predictions .,0
27725,"Among several public datasets , our model could give consistent and significant improvements over various state - of - theart systems by a large margin .",0
27726,The future work will be carried out in the following aspects .,0
27727,"We believe that our model is general and may apply to other tasks as well , so firstly we are going to fully investigate the usage of this architecture in other tasks .",0
27728,"Also , we are interested to see that if the machine really "" comprehend "" our language by utilizing neural networks approaches , but not only serve as a "" document - level "" language model .",0
27729,"In this context , we are planning to investigate the problems that need comprehensive reasoning over several sentences .",0
27730,title,0
27731,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,1
27732,abstract,0
27733,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .",1
27734,"In our model , a tree - based convolutional neural network ( TBCNN ) captures sentencelevel semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences .",0
27735,Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin .,0
27736,Introduction,0
27737,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,1
27738,"Provided with a premise sentence , the task is to judge whether the hypothesis can be inferred ( entailment ) , or the hypothesis can not be true .",0
27739,"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization .",1
27740,"Moreover , NLI is also related to other tasks of sentence pair modeling , including paraphrase detection , relation recognition of discourse units , etc .",0
27741,Traditional approaches to NLI mainly fall into two groups : feature - rich models and formal reasoning methods .,0
27742,"Feature - based approaches typically leverage machine learning models , but require intensive human engineering to represent lexical and syntactic information in two sentences * Equal contribution .",0
27743,Corresponding authors .,0
27744,Premise,0
27745,Two men on bicycles competing in a race .,0
27746,People are riding bikes .,0
27747,E Hypothesis,0
27748,Men are riding bicycles on the streets .,0
27749,CA few people are catching fish .,0
27750,N : Examples of relations between a premise and a hypothesis :,0
27751,"Entailment , Contradiction , and",0
27752,Neutral ( irrelevant ) .,0
27753,( .,0
27754,"Formal reasoning , on the other hand , converts a sentence into a formal logical representation and uses interpreters to search fora proof .",0
27755,"However , such approaches are limited in terms of scope and accuracy .",0
27756,"The renewed prosperity of neural networks has made significant achievements in various NLP applications , including individual sentence modeling as well as sentence matching .",0
27757,"A typical neural architecture to model sentence pairs is the "" Siamese "" structure , which involves an underlying sentence model and a matching layer to determine the relationship between two sentences .",0
27758,Prevailing sentence models include convolutional networks and recurrent / recursive networks .,0
27759,"Although they have achieved high performance , they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to the long propagation path .",0
27760,"Recently , we propose a novel tree - based convolutional neural network ( TBCNN ) to alleviate the aforementioned problems and have achieved higher performance in two sentence classification tasks .",0
27761,"However , it is less clear whether TBCNN can be harnessed to model sentence pairs for implicit logical inference , as is in the NLI task .",0
27762,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .",1
27763,"We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .",1
27764,"For example , the phrase "" riding bicycles on the streets "" in can be well recognized by TBCNN via the dependency relations dobj ( riding , bicycles ) and prep on ( riding , street ) .",0
27765,"As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .",1
27766,"A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .",1
27767,"Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .",1
27768,"To sum up , the main contributions of this paper are two - fold :",0
27769,"( 1 ) We are the first to introduce tree - based convolution to sentence pair modeling tasks like NLI ; ( 2 ) Leveraging additional heuristics further improves the accuracy while remaining low complexity , outperforming existing sentence encoding - based approaches to a large extent , including feature - rich methods and long short term memory ( LSTM ) - based recurrent networks .",0
27770,1,0
27771,Related Work,0
27772,Entailment recognition can be viewed as a task of sentence pair modeling .,0
27773,"Most neural networks in this field involve a sentence - level model , followed by one or a few matching layers .",0
27774,"They are sometimes called "" Siamese "" architectures .",0
27775,"and Yin and Schtze ( 2015 ) apply convolutional neural networks ( CNNs ) as the individual sentence model , where a set of feature detectors over successive words are designed to extract local features .",0
27776,build sentence pair models upon recurrent neural networks ( RNNs ) to iteratively integrate information along a sentence .,0
27777,dynamically construct tree structures ( analogous to parse trees ) by recursive autoencoders to detect paraphrase between two sentences .,0
27778,"As shown , inherent structural information in sentences is oftentimes important to natural language understanding .",0
27779,"The simplest approach to match two sentences , perhaps , is to concatenate their vector representations .",0
27780,Concatenation is also applied in our previous work of matching the subject and object in relation classification .,0
27781,"apply additional heuristics , namely Euclidean distance , cosine measure , and elementwise absolute difference .",0
27782,"The above methods operate on a fixed - size vector representation of a sentence , categorized as sentence encoding - based approaches .",0
27783,"Thus the matching complexity is O ( 1 ) , i.e. , independent of the sentence length .",0
27784,Word - byword similarity matrices are introduced to enhance interaction .,0
27785,"To obtain the similarity matrix , ( Arc - II ) concatenate two words ' vectors ( after convolution ) , compute Euclidean distance , and apply tensor product .",0
27786,"In this way , the complexity is of O ( n 2 ) , where n is the length of a sentence ; hence similarity matrices are difficult to scale and less efficient for large datasets .",0
27787,"Recently , introduce several context - aware methods for sentence matching .",0
27788,They report that RNNs over a single chain of two sentences are more informative than separate RNNs ; a static attention over the first sentence is also useful when modeling the second one .,0
27789,Such context - awareness interweaves the sentence modeling and matching steps .,0
27790,"In some scenarios like sentence pair re-ranking , it is not feasible to pre-calculate the vector representations of sentences , so the matching complexity is of O ( n ) .",0
27791,further develop a word - by - word attention mechanism and obtain a higher accuracy with a complexity order of O ( n 2 ) .,0
27792,Our Approach,0
27793,"We follow the "" Siamese "" architecture ( like most work in Section 2 ) and adopt a two - step strategy to classify the relation between two sentences .",0
27794,"Concretely , our model comprises two parts :",0
27795,A tree - based convolutional neural network models each individual sentence ) .,0
27796,"Notice that , the two sentences , premise and hypothesis , share a same TBCNN model ( with same parameters ) , because this part aims to capture general semantics of sentences . A matching layer combines two sentences ' information by heuristics ( ) .",0
27797,"After individual sentence models , we design a sentence matching layer to aggregate information .",0
27798,"We use simple heuristics , including concate - nation , element - wise product and difference , which are effective and efficient .",0
27799,"Finally , we add a softmax layer for output .",0
27800,"The training objective is cross - entropy loss , and we adopt mini-batch stochastic gradient descent , computed by back - propagation .",0
27801,Tree - Based Convolution,0
27802,"The tree - based convolutoinal neural network ( TBCNN ) is first proposed in our previous work 2 to classify program source code ; later , we further propose TBCNN variants to model sentences .",0
27803,This subsection details the tree - based convolution process .,0
27804,The basic idea of TBCNN is to design a set of subtree feature detectors sliding over the parse tree of a sentence ; either a constituency tree or a dependency tree applies .,0
27805,"In this paper , we prefer the dependency tree - based convolution for its efficiency and compact expressiveness .",0
27806,"Concretely , a sentence is first converted to a dependency parse tree .",0
27807,3 Each node in the dependency tree corresponds to a word in the sentence ; an edge a ?b indicates a is governed by b.,0
27808,"Edges are labeled with grammatical relations ( e.g. , nsubj ) between the parent node and its children .",0
27809,"Words are represented by pretrained vector representations , also known as word embeddings",0
27810,"Now , we consider a set of two - layer subtree feature detectors sliding over the dependency tree .",0
27811,"At a position where the parent node is p with child nodes c 1 , , c n , the output of the feature detector , y , is",0
27812,Let us assume word embeddings ( p and c i ) are of n e dimensions ; that the convolutional layer y is n c - dimensional .,0
27813,W ?,0
27814,R ncne is the weight matrix ; b ?,0
27815,R nc is the bias vector .,0
27816,r [c i ] denotes the dependency relation between p and c i .,0
27817,"f is the non-linear activation function , and we apply ReLU in our experiments .",0
27818,"After tree - based convolution , we obtain a set of feature maps , which are one - one corresponding to original words in the sentence .",0
27819,"Therefore , they may vary in size and length .",0
27820,"A dynamic pooling layer is applied to aggregate information along different parts of the tree , serving as away of semantic compositionality .",0
27821,"We use the max pooling operation , which takes the maximum value in each dimension .",0
27822,Then we add a fully - connected hidden layer to further mix the information in a sentence .,0
27823,The obtained vector representation of a sentence is denoted ash ( also called a sentence embedding ) .,0
27824,Notice that the same tree - based convolution applies to both the premise and hypothesis .,0
27825,"Tree - based convolution along with pooling enables structural features to reach the output layer with short propagation paths , as opposed to the recursive network , which is also structure - sensitive but may suffer from the problem of long propagation path .",0
27826,"By contrast , TBCNN is effective and efficient in learning such structural information .",0
27827,Matching Heuristics,0
27828,"In this part , we introduce how vector representations of individual sentences are combined to capture the relation between the premise and hypothesis .",0
27829,"As the dataset is large , we prefer O ( 1 ) matching operations because of efficiency concerns .",0
27830,"Concretely , we have three matching heuristics :",0
27831,"Concatenation of the two sentence vectors ,",0
27832,"Element - wise product , and Element - wise difference .",0
27833,"The first heuristic follows the most standard procedure of the "" Siamese "" architectures , while the latter two are certain measures of "" similarity "" or "" closeness . """,0
27834,"These matching layers are further concatenated , given by",0
27835,where h 1 ?,0
27836,R nc and h 2 ?,0
27837,"R nc are the sentence vectors of the premise and hypothesis , respectively ; "" "" denotes element - wise product ; semicolons refer to column vector concatenation .",0
27838,m ?,0
27839,R 4nc is the output of the matching layer .,0
27840,"We would like to point out that , with subsequent linear transformation , element - wise difference is a special case of concatenation .",0
27841,"If we assume the subsequent transformation takes the form of W [ h 1 h 2 ] , where W = [ W 1 W 2 ] is the weights for concatenated sentence representations , then element - wise difference can be viewed as such that",0
27842,( W 0 is the weights corresponding to element - wise difference . ),0
27843,"Thus , our third heuristic can be absorbed into the first one in terms of model capacity .",0
27844,"However , as will be shown in the experiment , explicitly specifying this heuristic significantly improves the performance , indicating that optimization differs , despite the same model capacity .",0
27845,"Moreover , word embedding studies show that linear offset of vectors can capture relationships between two words , but it has not been exploited in sentence - pair relation recognition .",0
27846,"Although element - wise distance is used to detect paraphrase in , it mainly reflects "" similarity "" information .",0
27847,"Our study verifies that vector offset is useful in capturing generic sentence relationships , akin to the word analogy task .",0
27848,Evaluation,0
27849,Dataset,0
27850,"To evaluate our TBCNN - pair model , we used the newly published Stanford Natural Language Inference ( SNLI ) dataset .",0
27851,"The dataset is constructed by crowdsourced efforts , each sentence written by humans .",0
27852,"Moreover , the SNLI dataset is magnitudes of larger than previous resources , and hence is particularly suitable for comparing neural models .",0
27853,"The target labels comprise three classes : Entailment , Contradiction , and Neutral ( two irrelevant sentences ) .",0
27854,"We applied the standard train / validation / test split , contraining 550k , 10k , and 10 k samples , respectively . additional dataset statistics , especially those relevant to dependency parse trees .",0
27855,5,0
27856,Hyperparameter Settings,0
27857,"All our neural layers , including embeddings , were set to 300 dimensions .",1
27858,"The model is mostly robust when the dimension is large , e.g. , several hundred .",0
27859,Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as apart of model parameters .,1
27860,We applied 2 penalty of 310 ? 4 ; dropout was chosen by validation with a granularity of 0.1 .,1
27861,We see that a large dropout rate ( ?,0
27862,0.3 ) hurts the performance ( and also makes training slow ) for such a large dataset as opposed to small datasets in other tasks .,0
27863,"Initial learning rate was set to 1 , and a power decay was applied .",1
27864,We used stochastic gradient descent with a batch size of 50 .,1
27865,Performance,0
27866,Table 3 compares our model with previous results .,0
27867,"As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of humanengineered features , long short term memory .",1
27868,""" cat "" refers to concatenation ; "" - "" and "" "" denote element - wise difference and product , resp .",0
27869,Model Variant,1
27870,"Valid Acc. Test Acc. ( LSTM ) - based RNNs , and traditional CNNs .",0
27871,This verifies the rationale for using tree - based convolution as the sentence - level neural model for NLI .,0
27872,compares different heuristics of matching .,0
27873,We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .,1
27874,"Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % .",1
27875,"As analyzed in Section 3.2 , the element - wise difference matching layer does not add to model complexity and can be absorbed as a special case into simple concatenation .",0
27876,"However , explicitly using such heuristic yields an accuracy boost of 1 - 2 % .",0
27877,Further applying element - wise product improves the accuracy by another 0.5 % .,1
27878,"The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , in - cluding a 1024d gated recurrent unit ( GRU ) - based RNN with "" skip - thought "" pretraining .",1
27879,"The results obtained by our model are also comparable to several attention - based LSTMs , which are more computationally intensive than ours in terms of complexity order .",0
27880,Complexity Concerns,0
27881,"For most sentence models including TBCNN , the overall complexity is at least O ( n ) .",0
27882,"However , an efficient matching approach is still important , especially to retrieval - and - reranking systems .",0
27883,"For example , in a retrieval - based question - answering or conversation system , we can largely reduce response time by performing sentence matching based on precomputed candidates ' embeddings .",0
27884,"By contrast , context - aware matching approaches as described in Section 2 involve processing each candidate given anew user - issued query , which is timeconsuming in terms of most industrial products .",0
27885,"In our experiments , the matching part ) counts 1.71 % of the total time during prediction ( single - CPU , C ++ implementation ) , showing the potential applications of our approach in efficient retrieval of semantically related sentences .",0
27886,Conclusion,0
27887,"In this paper , we proposed the TBCNN - pair model for natural language inference .",0
27888,"Our model relies on the tree - based convolutional neural network ( TBCNN ) to capture sentence - level semantics ; then two sentences ' information is combined by several heuristics including concatenation , element - wise product and difference .",0
27889,Experimental results on a large dataset show a high performance of our TBCNN - pair model while remaining a low complexity order .,0
27890,title,0
27891,Deep Fusion LSTMs for Text Semantic Matching,1
27892,abstract,0
27893,"Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .",0
27894,"In this paper , we propose a model of deep fusion LSTMs ( DF - LSTMs ) to model the strong interaction of text pair in a recursive matching way .",0
27895,"Specifically , DF - LSTMs consist of two interdependent LSTMs , each of which models a sequence under the influence of another .",0
27896,"We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .",0
27897,Experiments on two very large datasets demonstrate the efficacy of our proposed architecture .,0
27898,"Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .",0
27899,* Corresponding author and soon .,0
27900,"These models first encode two sequences into continuous dense vectors by separated neural models , and then compute the matching score based on sentence encoding .",0
27901,"In this paradigm , two sentences have no interaction until arriving final phase .",0
27902,Introduction,0
27903,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",1
27904,"Due to the semantic gap problem , text semantic matching is still a challenging problem .",0
27905,"Recently , deep learning is rising a substantial interest in text semantic matching and has achieved some great progresses .",0
27906,"According to their interaction ways , previous models can be classified into three categories :",0
27907,Weak interaction,0
27908,"Models Some early works focus on sentence level interactions , such as ARC - I , CNTN Semi-interaction Models",0
27909,"Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence , such as ABCNN , Attention LSTM .",0
27910,These models can alleviate the weak interaction problem to some extent .,0
27911,Strong Interaction Models,0
27912,"Some models build the interaction at different granularity ( word , phrase and sentence level ) , such as ARC - II , MultiGranCNN , Multi - Perspective CNN , , MatchPyramid .",0
27913,The final matching score depends on these different levels of interactions .,0
27914,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .",1
27915,"Given two texts x 1 :m and y 1 :n , we define a matching vector h i , j to represent the interaction of the subsequences x 1:i and y 1:j .",0
27916,"h i , j depends on the matching vectors h s ,t on previous interactions 1 ? s < i and 1 ? t < j.",0
27917,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .",1
27918,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",1
27919,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",1
27920,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,1
27921,The contributions of this paper can be summarized as follows .,0
27922,1 .,0
27923,"Different with previous models , DF - LSTMs model the strong interactions of two texts in a recursive matching way , which consist of two inter -and intra-dependent LSTMs .",0
27924,2 .,0
27925,"Compared to the previous works on text matching , we perform extensive empirical studies on two very large datasets .",0
27926,Experiment results demonstrate that our proposed architecture is more effective .,0
27927,3 .,0
27928,"We present an elaborate qualitative analysis of our model , giving an intuitive understanding how our model worked .",0
27929,Recursively Text Semantic Matching,0
27930,"To facilitate our model , we firstly give some definitions .",0
27931,"Given two sequences X = x 1 , x 2 , , x m and Y = y 1 , y 2 , , y n , most deep neural models try to represent their semantic relevance by a matching vector h( X , Y ) , which is followed by a score function to calculate the matching score .",0
27932,"The weak interaction methods decompose matching vector by h( X , Y ) = f ( h ( X ) , h( Y ) ) , where function f ( ) maybe one of some basic operations or the combination of them : concatenation , affine transformation , bilinear , and soon .",0
27933,"In this paper , we propose a strong interaction of two sequences to decompose matching vector h( X , Y ) in a recursive way .",0
27934,"We refer to the interaction of the subsequences x 1:i and y 1:j ash i , j ( X , Y ) , which depends on previous interactions h s , t ( X , Y ) for 1 ? s < i and 1 ? t < j. gives an example to illustrate this .",0
27935,"For sentence pair X = "" Female gymnast warm up before a competition "" , Y = "" Gymnast get ready fora competition "" , considering the interaction ) between x 1:4 = "" Female gymnast warm up "" and y 1:4 = "" Gymnast get ready for "" , which is composed by the interactions between their subsequences ( h 1 , 4 , , h 3 , 4 , h 4 ,1 , , h 4,3 ) .",0
27936,We can see that a strong interaction between two sequences can be decomposed in recursive topology structure .,0
27937,"The matching vector h i , j ( X , Y ) can be written as",0
27938,"where h i , j ( X| Y ) refers to conditional encoding of subsequence x 1:i influenced by y 1 : j .",0
27939,"Meanwhile , h i , j ( Y | X ) is conditional encoding of subsequence y 1:j influenced by subsequence x 1:i ; ? is concatenation operation .",0
27940,These two conditional encodings depend on their history encodings .,0
27941,"Based on this , we propose deep fusion LSTMs to model the matching of texts by recursive composition mechanism , which can better capture the complicated interaction of two sentences due to fully considering the interactions between subsequences .",0
27942,Long Short - Term Memory Network,0
27943,"Long short - term memory neural network ( LSTM ) is a type of recurrent neural network ( RNN ) , and specifically addresses the issue of learning long - term dependencies .",0
27944,LSTM maintains a memory cell that updates and exposes its content only when deemed necessary .,0
27945,"While there are numerous LSTM variants , here we use the LSTM architecture used by , which is similar to the architecture of ( Graves , 2013 ) but without peep - hole connections .",0
27946,"We define the LSTM units at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate o t , a memory cell ct and a hidden state ht .",0
27947,dis the number of the LSTM units .,0
27948,"The elements of the gating vectors it , ft and o tare in [ 0 , 1 ] .",0
27949,The LSTM is precisely specified as follows .,0
27950,"where x t is the input at the current time step ; T A ,b is an affine transformation which depends on parameters of the network A and b. ?",0
27951,denotes the logistic sigmoid function and denotes elementwise multiplication .,0
27952,"Intuitively , the forget gate controls the amount of which each unit of the memory cell is erased , the input gate controls how much each unit is updated , and the output gate controls the exposure of the internal memory state .",0
27953,The update of each LSTM unit can be written precisely as,0
27954,"Here , the function LSTM ( , , ) is a shorthand for Eq. ( 2 - 4 ) .",0
27955,"LSTM can map the input sequence of arbitrary length to a fixed - sized vector , and has been successfully applied to a wide range of NLP tasks , such as machine translation , language modelling , text matching and text classification .",0
27956,Deep Fusion LSTMs for Recursively Semantic Matching,0
27957,"To deal with two sentences , one straightforward method is to model them with two separate LSTMs .",0
27958,"However , this method is difficult to model local interactions of two sentences .",0
27959,"Following the recursive matching strategy , we propose a neural model of deep fusion LSTMs ( DF - LSTMs ) , which consists of two interdependent LSTMs to capture the inter -and intrainteractions between two sequences .",0
27960,gives an illustration of DF - LSTMs unit .,0
27961,"To facilitate our model , we firstly give some definitions .",0
27962,"Given two sequences X = x 1 , x 2 , , x n and Y = y 1 , y 2 , , y m , we let xi ?",0
27963,Rd denotes the embedded representation of the word x i .,0
27964,The standard LSTM has one temporal dimension .,0
27965,"When dealing with a sentence , LSTM regards the position as time step .",0
27966,"At position i of sentence x 1:n , the output hi reflects the meaning of subsequence x 1:i = x 1 , , xi .",0
27967,"To model the interaction of two sentences in a recursive way , we define h i , j to represent the interaction of the subsequences x 1:i and y 1 :j , which is computed by",0
27968,"where h ( x ) i , j denotes the encoding of subsequence x 1:i in the first LSTM influenced by the output of the second LSTM on subsequence y 1 :j ; h ( y ) i , j is the encoding of subsequence y 1 :j in the second LSTM influenced by the output of the first LSTM on subsequence x 1:i .",0
27969,"More concretely ,",0
27970,"where H i , j is information consisting of history states before position ( i , j ) .",0
27971,The simplest setting is,0
27972,"i, j?1 .",0
27973,"In this case , our model can be regarded as grid LSTMs .",0
27974,"However , there are total mn interactions in recursive matching process , LSTM could be stressed to keep these interactions in internal memory .",0
27975,"Therefore , inspired by recent neural memory network , such as neural Turing machine and memory network , we introduce two external memories to keep the history information , which can relieve the pressure on low - capacity internal memory .",0
27976,"Following ( Tran et al. , 2016 ) , we use external memory constructed by history hidden states , which is defined as At position i , j , two memory blocks M ( x ) , M ( y ) are used to store contextual information of x and y respectively .",0
27977,where h ( x ) and h ( x ) are outputs of two conditional LSTMs at different positions .,0
27978,The history information can be read from these two memory blocks .,0
27979,"We denote a read vector from external memories as r i , j ?",0
27980,"Rd , which can be computed by soft attention mechanisms .",0
27981,"where a i , j ? R K represents attention distribution over the corresponding memory M i , j ? R Kd .",0
27982,"More concretely , each scalar a i , j , k in attention distribution a i , j can be obtained : where M i , j , k ?",0
27983,"Rd represents the k - th row memory vector at position ( i , j ) , and g ( ) is an align function defined by",0
27984,where v ?,0
27985,Rd is a parameter vector and W a ?,0
27986,R d3d is a parameter matrix .,0
27987,"The history information H i , j in Eq and is computed by",0
27988,"By incorporating external memory blocks , DF - LSTMs allow network to re-read history interaction information , therefore it can more easily capture complicated and long - distance matching patterns .",0
27989,"As shown in , the forward pass of DF - LSTMs can be unfolded along two dimensional ordering .",0
27990,Related Models,0
27991,Our model is inspired by some recently proposed models based on recurrent neural network ( RNN ) .,0
27992,"One kind of models is multi-dimensional recurrent neural network ( MD - RNN ) ( Graves et al. , 2007 ; in machine learning and computer vision communities .",0
27993,"As mentioned above , if we just use the neighbor states , our model can be regarded as grid LSTMs .",0
27994,What is different is the dependency relations between the current state and history states .,0
27995,Our model uses external memory to increase its memory capacity and therefore can store large useful interactions of subsequences .,0
27996,"Thus , we can discover some matching patterns with long dependence .",0
27997,"Another kind of models is memory augmented RNN , such as long short - term memory - network and recurrent memory network , which extend memory network and equip the RNN with ability of re-reading the history information .",0
27998,"While they focus on sequence modelling , our model concentrates more on modelling the interactions of sequence pair .",0
27999,Training,0
28000,Task Specific Output,0
28001,There are two popular types of text matching tasks in NLP .,0
28002,"One is ranking task , such as community question answering .",0
28003,"Another is classification task , such as textual entailment .",0
28004,We use different ways to calculate matching score for these two types of tasks .,0
28005,"1 . For ranking task , the output is a scalar matching score , which is obtained by a linear transformation of the matching vector obtained by FD - LSTMs .",0
28006,2 .,0
28007,"For classification task , the outputs are the probabilities of the different classes , which is computed by a softmax function on the matching vector obtained by FD - LSTMs .",0
28008,Loss Function,0
28009,"Accordingly , we use two loss functions to deal with different sentence matching tasks .",0
28010,"Max - Margin Loss for Ranking Task Given a positive sentence pair ( X , Y ) and its corresponding negative pair ( X , ? ) .",0
28011,"The matching score s ( X , Y ) should be larger than s ( X , ? ) .",0
28012,"For this task , we use the contrastive max - margin criterion to train our model on matching task .",0
28013,The ranking - based loss is defined as,0
28014,Cross - entropy Loss for Classification Task,0
28015,"Given a sentence pair ( X , Y ) and its label l .",0
28016,The outputl of neural network is the probabilities of the different classes .,0
28017,The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions .,0
28018,where l is one - hot representation of the groundtruth label l ;l is predicted probabilities of labels ; C is the class number .,0
28019,Optimizer,0
28020,"To minimize the objective , we use stochastic gradient descent with the diagonal variant of Ada - Grad .",0
28021,"To prevent exploding gradients , we perform gradient clipping by scaling the gradient when the norm exceeds a threshold .",0
28022,Initialization and Hyperparameters,0
28023,Orthogonal Initialization,0
28024,"We use orthogonal initialization of our LSTMs , which allows neurons to react to the diverse patterns and is helpful to train a multi - layer network .",0
28025,Experiment,0
28026,"In this section , we investigate the empirical performances of our proposed model on two different text matching tasks : classification task ( recognizing textual entailment ) and ranking task ( matching of question and answer ) .",0
28027,Competitor Methods,0
28028,Neural bag - of - words ( NBOW ) :,1
28029,"Each sequence is represented as the sum of the embeddings of the words it contains , then they are concatenated and fed to a MLP .",1
28030,"Single LSTM : Two sequences are encoded by a single LSTM , proposed by .",1
28031,"Parallel LSTMs : Two sequences are first encoded by two LSTMs separately , then they are concatenated and fed to a MLP .",1
28032,"Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .",1
28033,"Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :",1
28034,Accuracies of our proposed model against other neural models on SNLI corpus .,0
28035,Experiment - I : Recognizing Textual Entailment,0
28036,Recognizing textual entailment ( RTE ) is a task to determine the semantic relationship between two sentences .,0
28037,We use the Stanford Natural Language Inference Corpus ( SNLI ) .,0
28038,"This corpus contains 570K sentence pairs , and all of the sentences and labels stem from human annotators .",0
28039,SNLI is two orders of magnitude larger than all other existing RTE corpora .,0
28040,"Therefore , the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper .",0
28041,"The results of DF - LSTMs outperform all the competitor models with the same number of hidden states while achieving comparable results to the state - of - the - art and using much fewer parameters , which indicate that it is effective to model the strong interactions of two texts in a recursive matching way .",0
28042,Results,0
28043,Results of MQA are shown in the .,0
28044,"we can see that the proposed model also shows its superiority on this task , which outperforms the stateof - the - arts methods on both metrics ( P@1 ( 5 ) and P@1 ( 10 ) ) with a large margin .",1
28045,"By analyzing the evaluation results of questionanswer matching in , we can see strong interaction models ( attention LSTMs , our DF - LSTMs ) consistently outperform the weak interaction models ( NBOW , parallel LSTMs ) with a large margin , which suggests the importance of modelling strong interaction of two sentences .",1
28046,Understanding Behaviors of Neurons in DF - LSTMs,0
28047,"To get an intuitive understanding of how the DF - LSTMs work on this problem , we examined the neuron activations in the last aggregation layer while evaluating the test set .",0
28048,We find that some cells are bound to certain roles .,0
28049,"We refer to h i , j , k as the activation of the kth neuron at the position of ( i , j ) , where i ?",0
28050,"{ 1 , . . . , n} and j ?",0
28051,"{ 1 , . . . , m}.",0
28052,"By visualizing the hidden state h i , j , k and analyzing the maximum activation , we can find that there exist multiple interpretable neurons .",0
28053,"For example , when some contextualized local perspectives are semantically related at point ( i , j ) of the sentence pair , the activation value of hidden neuron h i , j , k tends to be maximum , meaning that the model could capture some reasoning patterns .",0
28054,illustrates this phenomenon .,0
28055,"In , a neuron shows its ability to monitor the word pairs with the property of describing different things of the same type .",0
28056,"The activation in the patch , containing the word pair "" ( cat , dog ) "" , is much higher than others .",0
28057,"This is an informative pattern for the relation prediction of these two sentences , whose ground truth is contradiction .",0
28058,"An interesting thing is there are two "" dog "" in sentence "" Dog running with pet toy being by another dog "" .",0
28059,"Our model ignores the useless word , which indicates this neuron selectively captures pattern by contextual understanding , not just word level interaction .",0
28060,"In , another neuron shows that it can capture the local contextual interactions , such as "" ( ocean waves , beach ) "" .",0
28061,These patterns can be easily captured by final layer and provide a strong support for the final prediction .,0
28062,illustrates multiple interpretable neurons and some representative word or phrase pairs which can activate these neurons .,0
28063,These cases show that our model can capture contextual interactions beyond word level .,0
28064,Case Study for Attention Addressing,0
28065,Mechanism External memory with attention addressing mechanism enables the network explicitly to utilize the history information of two sentences simultaneously .,0
28066,"As a by-product , the obtained attention distribution over history hidden states also help us interpret the network and discover underlying dependencies present in the data .",0
28067,"To this end , we randomly sample two good cases with entailment relation from test data and visualize attention distributions over external memory constructed by last 9 hidden states .",0
28068,"As shown in , For the first sentence pair , when the word pair "" ( competition , competition ) "" are processed , the model simultaneously selects "" warm , before "" from one sentence and "" gymnast , ready , for "" from the other , which are informative patterns and indicate our model has the capacity of capturing phrase - phrase pair .",0
28069,"Another case in ( b ) also shows by attention mechanism , the network can sufficiently utilize the history information and the fusion approach allows two LSTMs to share the history information of each other .",0
28070,Error Analysis,0
28071,"Although our model DF - LSTMs are more sensitive to the discrepancy of the semantic capacity between two sentences , some cases still can not be solved by our model .",0
28072,"For example , our model gives a wrong prediction of the sentence pair "" A golden retriever nurses puppies / Puppies next to their mother "" , whose ground truth is entailment .",0
28073,"The model fails to realize "" nurses "" means "" next to "" .",0
28074,"Besides , despite the large size of the training corpus , it 's still very difficult to solve some cases , which depend on the combination of the world knowledge and context - sensitive inferences .",0
28075,"For example , given an entailment pair "" Several women are playing volleyball / The women are hitting a ball with their arms "" , all models predict "" neutral "" .",0
28076,These analysis suggests that some architectural improvements or external world knowledge are necessary to eliminate all errors instead of simply scaling up the basic model .,0
28077,Experiment - II : Matching Question and Answer,0
28078,Matching question answering ( MQA ) is atypical task for semantic matching .,0
28079,"Given a question , we need select a correct answer from some candidate answers .",0
28080,"In this paper , we use the dataset collected from Yahoo !",0
28081,Answers with the getByCategory function provided in Yahoo !,0
28082,"Answers API , which produces 963 , 072 questions and corresponding best answers .",0
28083,"We then select the pairs in which the length of questions and answers are both in the interval , thus obtaining 220 , 000 question answer pairs to form the positive pairs .",0
28084,"For negative pairs , we first use each question 's best answer as a query to retrieval top 1 , 000 re - sults from the whole answer set with Lucene , where 4 or 9 answers will be selected randomly to construct the negative pairs .",0
28085,"The whole dataset 1 is divided into training , validation and testing data with proportion 20 : 1 : 1 . Moreover , we give two test settings : selecting the best answer from 5 and 10 candidates respectively .",0
28086,Related Work,0
28087,"Our model can be regarded as a strong interaction model , which has been explored in previous methods .",0
28088,"One kind of methods is to compute similarities between all the words or phrases of the two sentences to model multiple - granularity interactions of two sentences , such as RAE ( Socher et 1 http://nlp.fudan.edu.cn/data/. al. , 2011 ) , Arc -II , ABCNN , MultiGranCNN , Multi- Perspective CNN , . firstly used this paradigm for paraphrase detection .",0
28089,The representations of words or phrases are learned based on recursive autoencoders .,0
28090,proposed to an end - to - end architecture with convolutional neural network ( Arc - II ) to model multiple - granularity interactions of two sentences .,0
28091,used LSTM to enhance the positional contextual interactions of the words or phrases between two sentences .,0
28092,The input of LSTM for one sentence does not involve another sentence .,0
28093,"Another kind of methods is to model the conditional encoding , in which the encoding of one sentence can be affected by another sentence .",0
28094,"and used LSTM to read pairs of sequences to produce a final representation , which can be regarded as interaction of two sequences .",0
28095,"By incorporating an attention mechanism , they got further improvements to the predictive abilities .",0
28096,"Different with these two kinds of methods , we model the interactions of two texts in a recursively matching way .",0
28097,"Based on this idea , we propose a model of deep fusion LSTMs to accomplish recursive conditional encodings .",0
28098,Conclusion and Future Work,0
28099,"In this paper , we propose a model of deep fusion LSTMs to capture the strong interaction for text semantic matching .",0
28100,Experiments on two large scale text matching tasks demonstrate the efficacy of our proposed model and its superiority to competitor models .,0
28101,"Besides , our visualization analysis revealed that multiple interpretable neurons in our model can capture the contextual interactions of the words or phrases .",0
28102,"In future work , we would like to investigate our model on more text matching tasks .",0
28103,title,0
28104,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,1
28105,abstract,0
28106,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .",1
28107,"In our approach , the encoding of sentence is a two - stage process .",0
28108,"Firstly , average pooling was used over word - level bidirectional LSTM ( biLSTM ) to generate a firststage sentence representation .",0
28109,"Secondly , attention mechanism was employed to replace average pooling on the same sentence for better representations .",0
28110,"Instead of using target sentence to attend words in source sentence , we utilized the sentence 's first - stage representation to attend words appeared in itself , which is called "" Inner-Attention "" in our paper .",0
28111,Experiments conducted on Stanford Natural Language Inference ( SNLI ),0
28112,"Corpus has proved the effectiveness of "" Inner-Attention "" mechanism .",0
28113,"With less number of parameters , our model outperformed the existing best sentence encoding - based approach by a large margin .",0
28114,Introduction,0
28115,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",1
28116,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .",1
28117,A few examples were given in .,0
28118,"Traditional methods to RTE has been the dominion of classifiers employing hand engineered features , which heavily relied on natural language processing pipelines and external resources .",0
28119,Formal reasoning methods were P The boy is running through a grassy area .,0
28120,The boy is in his room .,0
28121,C H A boy is running outside .,0
28122,E The boy is in a park .,0
28123,"N also explored by many researchers , but not been widely used because of its complexity and domain limitations .",0
28124,Recently published Stanford Natural Language Inference ( SNLI 1 ) corpus makes it possible to use deep learning methods to solve RTE problems .,0
28125,So far proposed deep learning approaches can be roughly categorized into two groups : sentence encoding - based models and matching encodingbased models .,0
28126,"As the name implies , the encoding of sentence is the core of former methods , while the latter methods directly model the relation between two sentences and did n't generate sentence representations at all .",0
28127,"In view of universality , we focused our efforts on sentence encoding - based model .",0
28128,"Existing methods of this kind including : LSTMs - based model , GRUsbased model , TBCNN - based model and SPINNbased model .",0
28129,Single directional LSTMs and GRUs suffer a weakness of not utilizing the contextual information from the future tokens and Convolutional Neural Networks did n't make full use of information contained in word order .,0
28130,Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions which helps to address the drawbacks mentioned above .,0
28131,A recent work by improved the performance by applying a neural attention model that did n't yield sentence embeddings .,0
28132,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",1
28133,The basic model is based on building biL - STM models on both premises and hypothesis .,1
28134,The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,1
28135,"Obtained this representation , we extended this model by utilize an Inner - Attention mechanism on both sides .",0
28136,This mechanism helps generate more accurate and focused sentence representations for classification .,0
28137,"In addition , we introduced a simple effective input strategy that get ride of same words in hypothesis and premise , which further boosts our performance .",1
28138,"Without parameter tuning , we improved the art - of - the - state performance of sentence encodingbased model by nearly 2 % .",0
28139,Our approach,0
28140,"In our work , we treated RTE task as a supervised three - way classification problem .",0
28141,The overall architecture of our model is shown in .,0
28142,"The design of this model we follow the idea of Siamese Network , that the two identical sentence encoders share the same set of weights during training , and the two sentence representations then combined together to generated a "" relation vector "" for classification .",0
28143,"As we can see from the figure , the model mainly consists of three parts .",0
28144,From top to bottom were : ( A ) .,0
28145,The sentence input module ; ( B ) .,0
28146,The sentence encoding module ; ( C ) .,0
28147,The sentence matching module .,0
28148,We will explain the last two parts in detail in the following subsection .,0
28149,And the sentence input module will be introduced in Section 3.3 .,0
28150,Sentence Encoding Module,0
28151,Sentence encoding module is the fundamental part of this model .,0
28152,"To generate better sentence representations , we employed a two - step strategy to encode sentences .",0
28153,"Firstly , average pooling layer was built on top of word - level biLSTMs to produce sentence vector .",0
28154,This simple encoder combined with the sentence matching module formed the basic architecture of our model .,0
28155,"With much less parameters , this basic model alone can outperformed art - ofstate method by a small margin .",0
28156,( refer to ) .,0
28157,"Secondly , attention mechanism was employed on the same sentence , instead of using target sentence representation to attend words in source sentence , we used the representation generated in previous stage to attend words appeared in the sentence itself , which results in a similar distribution with other attention mechanism weights .",0
28158,More attention was given to important words .,0
28159,"The idea of "" Inner-attention "" was inspired by the observation that when human read one sentence , people usually can roughly form an intuition about which part of the sentence is more important according past experience .",0
28160,And we implemented this idea using attention mechanism in our model .,0
28161,The attention mechanism is formalized as follows :,0
28162,"where Y is a matrix consisting of output vectors of biLSTM , R ave is the output of mean pooling layer , ?",0
28163,denoted the attention vector and R att is the attention - weighted sentence representation .,0
28164,Sentence Matching Module,0
28165,Once the sentence vectors are generated .,0
28166,Three matching methods were applied to extract relations between premise and hypothesis .,0
28167,Concatenation of the two representations Element - wise product Element - wise difference,0
28168,This matching architecture was first used by .,0
28169,"Finally , we used a SoftMax layer over the output of a non-linear projection of the generated matching vector for classification .",0
28170,Experiments,0
28171,DataSet,0
28172,"To evaluate the performance of our model , we conducted our experiments on Stanford Natural Language Inference ( SNLI ) corpus .",0
28173,"At 570 K pairs , SNLI is two orders of magnitude larger than all other resources of its type .",0
28174,"The dataset is constructed by crowdsourced efforts , each sentence written by humans .",0
28175,"The target labels comprise three classes : Entailment , Contradiction , and Neutral 2 Recently , proposed a Hierarchical Attention model on the task of document classification also used for but the target representation in attention their mechanism is randomly initialized .",0
28176,( two irrelevant sentences ) .,0
28177,"We applied the standard train / validation / test split , containing 550k , 10k , and 10 k samples , respectively .",0
28178,Parameter Setting,0
28179,"The training objective of our model is cross - entropy loss , and we use minibatch SGD with the Rmsprop ( Tieleman and Hinton , 2012 ) for optimization .",1
28180,The batch size is 128 .,1
28181,A dropout layer was applied in the output of the network with the dropout rate set to 0.25 .,1
28182,"In our model , we used pretrained 300D Glove 840B vectors to initialize the word embedding .",1
28183,"Out - of - vocabulary words in the training set are randomly initialized by sampling values uniformly from ( 0.05 , 0.05 ) .",1
28184,All of these embedding are not updated during training .,0
28185,We did n't tune representations of words for two reasons :,0
28186,1 . To reduced the number of parameters needed to train .,0
28187,"2 . Keep their representation stays close to unseen similar words in inference time , which improved the model 's generation ability .",0
28188,The model is implemented using open - source framework Keras .,0
28189,3,0
28190,The Input Strategy,0
28191,"In this part , we investigated four strategies to modify the input on our basic model which helps us increase performance , the four strategies are :",0
28192,"Inverting Premises Doubling Premises ( Zaremba and Sutskever , 2014 ) Doubling Hypothesis Differentiating Inputs ( Removing same words appeared in premises and hypothesis )",0
28193,Experimental results were illustrated in .,0
28194,"As we can see from it , doubling hypothesis and differentiating inputs both improved our model 's performance .",0
28195,"While the hypothesises usually much shorter than premises , doubling hypothesis may absorb this difference and emphasize the meaning twice via this strategy .",0
28196,Differentiating input strategy forces the model to focus on different part of the two sentences which may help the classification for Neutral and Contradiction examples as we observed that our model tended to assign unconfident instances to Entailment .,0
28197,"And the original input sentences appeared in While most of the words in this pair of sentences are same or close in semantic , It is hard for model to distinguish the difference between them , which resulted in labeling it with Neutral or Entailment .",0
28198,"Through differentiating inputs strategy , this kind of problems can be solved .",0
28199,Comparison Methods,0
28200,"In this part , we compared our model against the following art - of - the - state baseline approaches :",0
28201,"The cat refers to concatenation , - and denote element - wise difference and product , respectively .",0
28202,Much simpler and easy to understand .,0
28203,Results and Qualitative Analysis,0
28204,"Although the classification of RTE example is not solely relying on representations obtained from attention , it is still instructive to analysis Inner- Attention mechanism as we witnessed a large performance increase after employing it .",0
28205,We handpicked several examples from the dataset to visualize .,0
28206,"In order to make the weights more discriminated , we did n't use a uniform colour atla cross sentences .",0
28207,"That is , each sentence have it s own color atla , the lightest color and the darkest color denoted the smallest attention weight the biggest value within the sentence , respectively .",0
28208,Visualizations of Inner - Attention on these examples are depicted in .,0
28209,"We observed that more attention was given to Nones , Verbs and Adjectives .",1
28210,This conform to our experience that these words are more semantic richer than function words .,0
28211,"While mean pooling regarded each word of equal importance , the attention mechanism helps re-weight words according to their importance .",1
28212,And more focused and accurate sentence representations were generated based on produced attention vectors .,0
28213,Conclusion and Future work,0
28214,"In this paper , we proposed a bidirectional LSTMbased model with Inner - Attention to solve the RTE problem .",0
28215,We come up with an idea to utilize attention mechanism within sentence which can teach itself to attend words without the information from another one .,0
28216,The Inner- Attention mechanism helps produce more accurate sentence representa - tions through attention vectors .,0
28217,"In addition , the simple effective diversing input strategy introduced by us further boosts our results .",0
28218,And this model can be easily adapted to other sentence - matching models .,0
28219,Our future work including :,0
28220,1 .,0
28221,"Employ this architecture on other sentencematching tasks such as Question Answer , Paraphrase and Sentence Text Similarity etc .",0
28222,2 . Try more heuristics matching methods to make full use of the sentence vectors .,0
28223,title,0
28224,A Simple and Effective Approach to the Story Cloze Test,1
28225,abstract,0
28226,"In the Story Cloze Test , a system is presented with a 4 - sentence prompt to a story , and must determine which one of two potential endings is the ' right ' ending to the story .",0
28227,Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets .,0
28228,"Following this approach , we present a simpler fully - neural approach to the Story Cloze Test using skip - thought embeddings of the stories in a feed - forward network that achieves close to state - of - the - art performance on this task without any feature engineering .",0
28229,We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .,0
28230,narrative,0
28231,"1 Introduction introduced the Story Cloze Test : given a four - sentence story prompt ( or ' context ' ) , the task is to pick the ' right ' commonsense ending from two options .",0
28232,The Cloze,0
28233,"Test is intended to be a general framework for evaluating story understanding , since it ostensibly requires combining semantic understanding and commonsense knowledge of our world .",0
28234,The task is accompanied by the Rochester story ( ROCstory ) corpus .,0
28235,The training set consists of crowdsourced five - sentence stories designed to capture common events in daily life .,0
28236,The validation and testing sets consist of four - sentence prompts and labeled ' right ' and ' wrong ' story endings .,0
28237,shows such a sample story from the Rochester corpus validation set with a labeled right and wrong ending .,0
28238,"Many previous approaches to the Cloze Test have ignored the training set entirely and trained on the validation set since the former lacks ' negative ' examples ; although this greatly reduces the available training data , it circumvents the issue of obtaining negative examples during training .",0
28239,Story Context,0
28240,Bob loved to watch movies .,0
28241,He was looking forward to a three day weekend coming up .,0
28242,He made a list of his favorite movies and invited some friends over .,0
28243,He spent the weekend with his friends watching all his favorite movies .,0
28244,Right Ending : Bob had a great time .,0
28245,Wrong Ending : Bob stopped talking to those friends .,0
28246,Our contribution to this task is two - fold .,0
28247,"First , we achieve near state - of - the - art performance ( within 1.1 % ) but with a much simpler , fullyneural approach .",0
28248,"Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .",1
28249,"Second , we find that considering only the last sentence of the context outperforms models that consider the full context .",1
28250,Previous approaches focused on the accuracy achieved by either considering the whole context or ignoring the whole context of the story .,0
28251,"In sum , our approach differs from previous efforts in the joint use of three strategies : ( 1 ) using skip - thought embeddings for sentences in the story in a feed - forward neural network , ( 2 ) training the model on the provided validation set , and ( 3 ) considering the two endings with only the last sentence in the prompt .",1
28252,"This paper is structured as follows : we will discuss previous approaches to the problem and how they compare to our approach , describe our model and the experiments we ran in detail , and finally discuss reasons for our model 's superior performance and why ignoring the first three sentences of the story produces better accuracy .",0
28253,"92 presented the original Story Cloze Test , and showed that while humans could achieve 100 % accuracy on the task , a deep structured semantic model was the best performing artificial baseline , with a test - set accuracy of 58.5 % .",0
28254,"While they do consider using skip - thought embeddings for this task , they do so by choosing the ending whose embedding was closer to the average skip - thought embedding of the context .",0
28255,This only achieves a test - set accuracy of 55.2 % .,0
28256,"On the other hand , we train a feed - forward network using skip - thought embeddings .",0
28257,The Story Cloze,0
28258,"Test was the shared task at LS - DSem 2017 , and summarize the approaches by various teams on this task .",0
28259,The best - performing system by achieved a test - set accuracy of 75.2 % .,0
28260,"Like us , they train their model on the validation set , but their approach relies more heavily on feature engineering .",0
28261,"They find that they could achieve 72.4 % accuracy using just the stylistic features of the endings , suggesting that many of the ' right ' endings on this task could be identified independent of the story context .",0
28262,"Upon further investigation , find differences not only between the ' right ' and ' wrong ' endings in the validation set , but also between these and the ' right ' endings from the training set , providing some explanation for why models trained on the validation set outperform models trained on the training set - their data distributions are somewhat different .",0
28263,"Further work by established a neural baseline for models trained on the validation set , with a test - set accuracy of 74.7 % .",0
28264,They were also able to achieve a marginally better accuracy of 72.5 % ( compared to ) when using just the sentence endings and ignoring the context ; and this approach did not require any feature engineering .,0
28265,"They showed that a human can distinguish ' right ' from ' wrong ' endings without the context with 78 % accuracy , further backing the claim that the importance of context in determining the right ending is more limited than desirable on this task .",0
28266,"Their approach involves training a hierarchical bidirectional LSTM with attention to first encode sentences and then stories , with a hinge - loss objective function .",0
28267,"use skip - thought em-beddings for this task , but they encode the entire context using a GRU , with a binary classifier to determine if an ending was right or wrong .",0
28268,"They train their model on the provided training set , sampling negative examples from the training set itself .",0
28269,Their best model achieves 67.2 % accuracy on this task .,0
28270,"Currently , the comprehensive approach taken by , where they model event sequence , sentiment trajectory , and topical consistency fora hidden coherence model , achieves the state - of - the - art performance on this task , with a test - set accuracy of 77.6 % .",0
28271,Approach,0
28272,We trained several models on both the training set and the validation set of the ROCStory corpus .,0
28273,"When training a model on the training set , we obtain ' negative ' examples ( i.e. wrong endings ) by randomly choosing a sentence from another story in the corpus .",0
28274,"In this section , we describe the choice of sentence embeddings , the architecture of the models we trained , and our experimental setup .",0
28275,Embeddings,0
28276,Key to our approach is the use of skip - thought embeddings in our feedforward network ( denoted skip in ) .,0
28277,These are 4800 - dimensional embeddings of sentences trained on the task of predicting their context using the BookCorpus dataset ( a large dataset of books ) .,0
28278,"We use a pre-trained skip - thought encoder 1 to obtain the embeddings for all sentences in the training set , validation set , and test set .",0
28279,"To isolate the increase inaccuracy from using skip - thought vectors , we also experiment with learning sentence embeddings directly , for this task .",0
28280,"Unlike the skip - thought encoder that directly gives sentence embeddings , we use a bidirectional LSTM that takes in Glo Ve embeddings of each word in the sentence and returns a 4800 dimensional embedding of the sentence ( denoted GloVe in ) formed by concatenating the outputs of the forward and backward LSTMs .",0
28281,We use the Glo Ve model pretrained on Wikipedia 2014 and Gigaword 5 data 2 .,0
28282,Models,0
28283,Common to all our models is a single feed - forward neural network with a softmax - layer at the end that acts as a binary classifier .,0
28284,This neural network takes in a 4800 - dimensional input ( the same dimensionality as the skip - thought embeddings ) and returns the probability of the endings being ' right ' and ' wrong ' .,0
28285,"During inference time , we make a forward pass with each of the two possible endings , and select the ending that has a higher probability of being the ' right ' ending .",0
28286,We use two layer and three layer fully connected networks with Rectified Linear ( ReLU ) non-linearities ( refer to Appendix A for model - specific architecture ) .,0
28287,"We then experiment with different inputs to the neural network , as described below .",0
28288,No Context ( NC ),0
28289,This model attempts to identify the ' right ' ending of a story by ignoring the story context and looking only at examples of right and wrong endings .,0
28290,"As such , the input to the neural network is just the skip - thought embedding of the story ending , with 0 / 1 label indicating whether it was the ' wrong ' or ' right ' ending .",0
28291,"Last Sentence ( LS ) In this model , the input to the neural network is the sum of the skip - thought embedding of the last sentence of the prompt ( i.e. , fourth sentence in the story ) and the skip - thought embedding of the ending .",0
28292,"Essentially , we are attempting to identify the right ending based on only the ending and the preceding sentence in the story .",0
28293,"Full Context ( FC ) Here , we use a Gated Recurrent Unit ( GRU ) to encode the entire story prompt into a 4800 - dimensional vector , add it to the skipthought embedding of the story ending , and pass it as input to the neural network .",0
28294,"The input to the GRU is the skip - thought embedding of each sentence , and this model attempts to identify the right ending by considering the entire story prompt .",0
28295,Experiments,0
28296,Dataset,0
28297,"For all our experiments , we use the ROCStory corpus .",0
28298,"The corpus consists of a training set of 98,161 five - sentence stories , a validation set consisting of 1,871 foursentence stories , and a test set of 1,871 foursentence stories , with the validation and test sets providing labeled ' right ' and ' wrong ' story endings for each story .",0
28299,crowdsourced the collection of stories on Amazon Mechanical Turk ; workers were asked to compose five - sentence stories about common daily situations with a clear beginning and end .,0
28300,"To create the validation and testing sets , endings were removed from stories and an additional group of workers on Mechanical Turk were asked to provide a ' right ' ending or a ' wrong ' ending .",0
28301,"Although models trained on the validation set score higher than those trained on the training set as previously discussed , we provide the results for the same model trained on the validation set ( denoted val ) as well as the training set ( denoted trn ) in for comparison .",0
28302,Experimental Method,0
28303,"When training on the training set , we tuned hyperparameters using the validation set .",0
28304,"When training on the validation set , we holdout 10 % of the validation set , and tune hyper - parameters to find a configuration that maximizes the accuracy on the held out data .",0
28305,We use cross-entropy loss and SGD with learning rate of 0.01 .,1
28306,"During training , we save the model every 3000 iterations , and calculate the validation accuracy .",1
28307,"We train each model five times ( except the FC models , which we train once due to time considerations ) , and report the average test set accuracy of the model .",0
28308,We use the model with the highest validation accuracy in each round to calculate the test set accuracy for that round .,0
28309,We present our results in .,0
28310,Results and Discussion,0
28311,The 3 - layer feed - forward neural network trained on the validation set by summing the skip - thought embeddings of the last sentence ( LS ) of the story prompt and the ending gives the best accuracy ( 76.5 % ) .,1
28312,"This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .",0
28313,"Comparing ' val - LS- skip ' to ' val - LS - Glo Ve ' ( i.e. , using skip - thought embeddings for sentences vs. GloVe word embeddings ) , we confirm that the success of this approach lies in the sizable boost to accuracy from the use of pretrained skip - thought embeddings .",1
28314,"This is perhaps unsurprising given the success of skip - thought embeddings in story - related tasks , ) , since the model was trained on a large corpus of fiction .",0
28315,"While the BookCorpus and ROCStories draw from different distributions , it is possible that skip - thought vectors implicitly encode a general notion of typical story continuation .",0
28316,"In the absence of such a large dataset to learn such asso-ciations from , the LSTM with Glo Ve embedding inputs is unable to encode the necessary information to do well on this task .",0
28317,"We note that the model trained using only the last sentence ( LS ) of the story context has higher accuracy compared to the model that uses a GRU to encode the full context ( FC ) , and even the model which encodes the entire context .",1
28318,It is unclear from our experiments why this might be .,0
28319,"One hypothesis is that as stories near conclusion , the space of possible continuations contracts .",0
28320,"In the absence of further context , a default prior is assumed - as implicitly encoded in skip - thought vectors trained on BookCorpus - that is often correct .",0
28321,"Providing more context may conflict with the default prior , introducing uncertainty .",0
28322,"Another hypothesis is that the Mechanical Turk workers creating the validation and test data sets focused more on the fourth sentence when writing their ' right ' and ' wrong ' endings , so once again , adding context introduces error .",0
28323,"Finally , we observe that the Story Cloze",0
28324,"Test is an easier task than identifying whether a given ending is coherent or not , since the former involves a forced choice between two endings .",0
28325,"During test time , the model does not need to classify whether a given ending is ' right ' or ' wrong ' , as it learns to do during train time ; instead , it simply needs to correctly predict which ending is less wrong .",0
28326,Conclusion,0
28327,"We have shown a simple yet effective neural model that achieves high accuracy on the Cloze Test , which is within 1.1 % of the state - of - the - art approach that relies on feature engineering .",0
28328,"Additionally , we make a minor improvement on 's ' ending - only ' baseline accuracy of 72.5 % with our val - NC - skip model .",0
28329,"Finally , we also showed that , for the models tested here , using the full context actually performs worse than using just the last sentence of the context .",0
28330,Future investigation will be needed to determine whether this is a property inherent to human storytelling or a form of bias introduced during data collection .,0
28331,title,0
28332,Natural Language Comprehension with the EpiReader,1
28333,abstract,0
28334,"We present the EpiReader , a novel model for machine comprehension of text .",1
28335,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",1
28336,"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .",1
28337,"The EpiReader is an end - to - end neural model comprising two components : the first component proposes a small set of candidate answers after comparing a question to its supporting text , and the second component formulates hypotheses using the proposed candidates and the question , then reranks the hypotheses based on their estimated concordance with the supporting text .",0
28338,"We present experiments demonstrating that the EpiReader sets anew state - of - the - art on the CNN and Children 's Book Test machine comprehension benchmarks , outperforming previous neural models by a significant margin .",0
28339,Introduction,0
28340,"When humans reason about the world , we tend to formulate a variety of hypotheses and counterfactuals , then test them in turn by physical or thought experiments .",0
28341,"The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations : if several theories are consistent with the observed data , retain them all until more data is observed .",0
28342,"In this paper , we argue that the same principle can be applied to machine comprehension of natural language .",0
28343,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .",1
28344,"Comprehension of natural language by machines , at a near- human level , is a prerequisite for an extremely broad class of useful applications of artificial intelligence .",0
28345,"Indeed , most human knowledge is collected in the natural language of text .",0
28346,Machine comprehension ( MC ) has therefore garnered significant attention from the machine learning research community .,0
28347,"Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage , then scoring a system 's answers to those questions .",0
28348,We all took similar tests in school .,0
28349,"Such tests are objectively gradable and may assess a range of abilities , from basic understanding to causal reasoning to inference .",0
28350,"In the past year , two large - scale MC datasets have been released : the CNN / Daily Mail corpus , consisting of news articles from those outlets , and the Children 's Book Test ( CBT ) , consisting of short excerpts from books available through Project Gutenberg .",0
28351,The size of these datasets ( on the order of 10 5 distinct questions ) makes them amenable to data - intensive deep learning techniques .,0
28352,"Both corpora use Clozestyle questions , which are formulated by replacing a word or phrase in a given sentence with a placeholder token .",0
28353,"The task is then to find the answer that "" fills in the blank "" .",0
28354,"In tandem with these corpora , a host of neural machine comprehension models has been developed .",0
28355,We compare the EpiReader to these earlier models through training and evaluation on the CNN and CBT datasets .,0
28356,The EpiReader factors into two components .,1
28357,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,1
28358,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,1
28359,We can summarize this process as Extract ?,0
28360,Hypothesize ?,0
28361,Test 2 .,0
28362,"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment ( RTE ) , also known as natural language inference .",1
28363,This process is computationally demanding .,0
28364,"Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .",1
28365,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .",1
28366,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,1
28367,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,1
28368,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .",1
28369,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .",1
28370,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .",1
28371,This paper is organized as follows .,0
28372,In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests .,0
28373,"In Section 3 we describe the EpiReader , focusing on its two components and how they combine .",0
28374,"Section 4 discusses related work , and Section 5 details our experimental results and analysis .",0
28375,We conclude in Section 6 . The CNN and Daily Mail datasets were released together and have the same form .,0
28376,"The Daily Mail dataset is significantly larger , and our tests with this data are still in progress .",0
28377,"The Extractor performs extraction , while the Reasoner both hypothesizes and tests .",0
28378,"Problem definition , notation , datasets",0
28379,The task of the EpiReader is to answer a Cloze - style question by reading and comprehending a supporting passage of text .,0
28380,"The training and evaluation data consist of tuples ( Q , T , a * , A ) , where Q is the question ( a sequence of words {q 1 , ...q | Q | } ) , T is the text ( a sequence of words {t 1 , ... , t | T | } ) , A is a set of possible answers {a 1 , ... , a | A | } , and a * ?",0
28381,A is the correct answer .,0
28382,"All words come from a vocabulary V , and A ? T .",0
28383,"In each question , there is a placeholder token indicating the missing word to be filled in .",0
28384,Datasets,0
28385,CNN,0
28386,This corpus is built using articles scraped from the CNN website .,0
28387,"The articles themselves form the text passages , and questions are generated synthetically from short summary statements that accompany each article .",0
28388,These summary points are ( presumably ) written by human authors .,0
28389,Each question is created by replacing a named entity in a summary point with a placeholder token .,0
28390,"All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each ( Q , T ) pair .",0
28391,"This forces the model to rely only on the text , rather than learning world knowledge about the entities during training .",0
28392,The CNN corpus ( henceforth CNN ) was presented by .,0
28393,Children 's Book Test,0
28394,"This corpus is constructed similarly to CNN , but from children 's books available through Project Gutenberg .",0
28395,"Rather than articles , the text passages come from book excerpts of 20 sentences .",0
28396,"Since no summaries are provided , a question is generated by replacing a single word in the next ( i.e. 21st ) sentence .",0
28397,"The corpus distinguishes questions based on the type of word that is replaced : named entity , common noun , verb , or preposition .",0
28398,"Like , we focus only on the first two classes since showed that standard LSTM language models already achieve humanlevel performance on the latter two .",0
28399,"Unlike in the CNN corpora , named entities are not anonymized and shuffled in the Children 's Book Test ( CBT ) .",0
28400,The CBT was presented by .,0
28401,"Due to the construction of questions in each corpus , CNN and CBT assess different aspects of machine comprehension .",0
28402,"The summary points of CNN are condensed paraphrasings of information from the text , so determining the correct answer relies more on recognizing textual entailment .",0
28403,"On the other hand , a CBT question , generated from a sentence which continues the text passage ( rather than summarizes it ) , is more about making a prediction based on context .",0
28404,The EpiReader,0
28405,Overview and intuition,0
28406,The EpiReader explicitly leverages the observation that the answer to a question is often a word or phrase from the related text passage .,0
28407,This condition holds for the CNN and CBT datasets .,0
28408,"EpiReader 's first module , the Extractor , can thus select a small set of candidate answers by pointing to their locations in the supporting passage .",0
28409,"This mechanism is detailed in Section 3.2 , and was used previously by the Attention Sum Reader .",0
28410,"Pointing to candidate answers removes the need to apply a softmax over the entire vocabulary as in , which is computationally more costly and uses less - direct information about the context of a predicted answer in the supporting text .",0
28411,"EpiReader 's second module , the Reasoner , begins by formulating hypotheses using the extracted answer candidates .",0
28412,It generates each hypothesis by replacing the placeholder token in the question with an answer candidate .,0
28413,"Cloze - style questions are ideally - suited to this process , because inserting the correct answer at the placeholder location produces a well - formed , grammatical statement .",0
28414,"Thus , the correct hypothesis will "" make sense "" to a language model .",0
28415,The Reasoner then tests each hypothesis individually .,0
28416,"It compares a hypothesis to the text , split into sentences , to measure textual entailment , and then aggregates entailment overall sentences .",0
28417,This computation uses a pair of convolutional encoder networks followed by a recurrent neural network .,0
28418,The convolutional encoders generate abstract representations of the hypothesis and each text sentence ; the recurrent network estimates and aggregates entailment .,0
28419,This is described formally in Section 3.3 .,0
28420,"The end - toend EpiReader model , combining the Extractor and Reasoner modules , is depicted in .",0
28421,"Throughout our model , words will be represented with trainable embeddings .",0
28422,"We represent these embeddings using a matrix W ? R D|V | , where Dis the embedding dimension and | V | is the vocabulary size .",0
28423,The Extractor,0
28424,The Extractor is a Pointer Network .,0
28425,"It uses a pair of bidirectional recurrent neural networks , f ( ? T , T ) and g( ?",0
28426,"Q , Q ) , to encode the text passage and the question .",0
28427,?,0
28428,"T represents the parameters of the text encoder , and T ?",0
28429,"R DN is a matrix representation of the text ( comprising N words ) , whose columns are individual word embeddings ti .",0
28430,"Likewise , ?",0
28431,"Q represents the parameters of the question encoder , and Q ?",0
28432,R DN,0
28433,"Q is a matrix representation of the question ( comprising N Q words ) , whose columns are individual word embeddings q j .",0
28434,We use a recurrent neural network with gated recurrent units ( GRU ) to scan over the columns ( i.e. word embeddings ) of the input matrix .,0
28435,"We selected the GRU because it is computationally simpler than Long Short - Term Memory ( Hochreiter and Schmidhuber , 1997 ) , while still avoiding the problem of vanishing / exploding gradients often encountered when training recurrent networks .",0
28436,The GRU 's hidden state gives a representation of the ith word conditioned on preceding words .,0
28437,"To include context from proceeding words , we run a second GRU over T in the reverse direction .",0
28438,We refer to the combination as a biGRU .,0
28439,"At each step the biGRU outputs two d-dimensional encoding vectors , one for the forward direction and one for the backward direction .",0
28440,We concatenate these to yield a vector f ( t i ) ?,0
28441,R 2 d .,0
28442,"The question biGRU is similar , but we get a single - vector representation of the question by concatenating the final forward state with the initial backward state , which we denote g ( Q ) ?",0
28443,R 2 d .,0
28444,"As in Kadlec et al. , we model the probability that the ith word in text T answers question Q using",0
28445,which takes the inner product of the text and question representations followed by a softmax .,0
28446,In many cases unique words repeat in a text .,0
28447,"Therefore , we compute the total probability that word w is the correct answer using a sum : This probability is evaluated for each unique word in T .",0
28448,"Finally , the Extractor outputs the set {p 1 , ... , p K } of the K highest word probabilities from 2 , along with the corresponding set of K most probable answer words { 1 , ... , K }.",0
28449,The Reasoner,0
28450,"The indicial selection involved in gathering { 1 , ... , K } is not a smooth operation .",0
28451,"To construct an end - to - end differentiable model , we bypass this by propagating the probability estimates of the Extractor directly through the Reasoner .",0
28452,"The Reasoner begins by inserting the answer candidates , which are single words or phrases , into the question sequence Q at the placeholder location .",0
28453,"This forms K hypotheses { H 1 , ... , H K }.",0
28454,"At this point , we consider each hypothesis to have probability p ( H k ) ? pk , as estimated by the Extractor .",0
28455,The Reasoner updates and refines this estimate .,0
28456,"The hypotheses represent new information in some sense - they are statements we have constructed , albeit from words already present in the question and text passage .",0
28457,The Reasoner estimates entailment between the statements H k and the passage T .,0
28458,"We denote these estimates using e k = F ( H k , T ) , with F to be defined .",0
28459,"We start by reorganizing T into a sequence of N s sentences : T = {t 1 , . . . , t N } ? {S 1 , . . . , S Ns } , where",0
28460,Si is a sequence of words .,0
28461,"For each hypothesis and each sentence of the text , Reasoner input consists of two matrices :",0
28462,Si ?,0
28463,"R D|S i | , whose columns are the embedding vectors for each word of sentence Si , and H k ?",0
28464,"R D|H k | , whose columns are the embedding vectors for each word in the hypothesis H k .",0
28465,"The embedding vectors themselves come from matrix W , as before .",0
28466,These matrices feed into a convolutional architecture based on that of Severyn and Moschitti ( 2016 ) .,0
28467,The architecture first augments Si with matrix M ? R 2|S i | .,0
28468,"The first row of M contains the inner product of each word embedding in the sentence with the candidate answer embedding , and the second row contains the maximum inner product of each sentence word embedding with any word embedding in the question .",0
28469,"These word - matching features were inspired by similar approaches in and , where they were shown to improve entailment estimates .",0
28470,The augmented,0
28471,Si is then convolved with a bank of filters F S ?,0
28472,"R ( D+2 ) m , while H k is convolved with filters F H ?",0
28473,"R Dm , where m is the convolutional filter width .",0
28474,We add a bias term and apply a nonlinearity ( we use a ReLU ) following the convolution .,0
28475,"Maxpooling over the sequences then yields two vectors : the representation of the text sentence , r Si ?",0
28476,"RN F , and the representation of the hypothesis , r H k ?",0
28477,"RN F , where NF is the number of filters .",0
28478,We then compute a scalar similarity score between these vector representations using the bilinear form,0
28479,where R ?,0
28480,RN F N F is a matrix of trainable parameters .,0
28481,"We then concatenate the similarity score with the sentence and hypothesis representations to get a vector ,",0
28482,There are more powerful models of textual entailment that could have been used in place of this convolutional architecture .,0
28483,We adopted the approach of Severyn and Moschitti ( 2016 ) for computational efficiency .,0
28484,"The resulting sequence of N s vectors feeds into yet another GRU for synthesis , of hidden dimension d S .",0
28485,"Intuitively , it is often the case that evidence for a particular hypothesis is distributed over several sentences .",0
28486,"For instance , if we hypothesize that the football is in the park , perhaps it is because one sentence tells us that Sam picked up the football and a later one tells us that Sam ran to the park .",0
28487,3,0
28488,"The Reasoner synthesizes distributed information by running a GRU network over x ik , where i indexes sentences and represents the step dimension .",0
28489,"The final hidden state of the GRU is fed through a fully - connected layer , yielding a single scalar y k .",0
28490,This value represents the collected evidence for H k based on the text .,0
28491,"In practice , the Reasoner processes all K hypotheses in parallel and the estimated entailment of each is normalized by a softmax , e k ? exp ( y k ) .",0
28492,The reranking step performed by the Reasoner helps mitigate a significant weakness of most existing attention mechanisms .,0
28493,"Specifically , these mechanisms blend representations of all possible outcomes together using "" soft "" attention , rather than considering them discretely using "" hard "" attention .",0
28494,"This is like exploring amaze by generating an average path out of the several before you , and then attempting to follow it by walking through a wall .",0
28495,"Examining possibilities individually , as in the Reasoner module , is more natural .",0
28496,Combining components,0
28497,"Finally , we combine the evidence from the Reasoner with the probability from the Extractor .",0
28498,"We compute the output probability of each hypothesis , ? k , according to the product",0
28499,"whereby the evidence of the Reasoner can be interpreted as a correction to the Extractor probabilities , applied as an additive shift in log -space .",0
28500,"We experimented with other combinations of the Extractor and Reasoner , but we found the multiplicative approach to yield the best performance .",0
28501,After combining results from the Extractor and Reasoner to get the probabilities ?,0
28502,"k described in Eq. 4 , we optimize the parameters of the full EpiReader to minimize a cost comprising two terms , L E and L R .",0
28503,"The first term is a standard negative loglikelihood objective , which encourages the Extractor to rate the correct answer above other answers .",0
28504,This is the same loss term used in .,0
28505,This loss is given by :,0
28506,"where P ( a * | T , Q ) is as defined in Eq. 2 , and a * denotes the true answer .",0
28507,The second term is a marginbased loss on the end - to - end probabilities ? k .,0
28508,We define ? * as the probability ?,0
28509,k corresponding to the true answer word a * .,0
28510,This term is given by :,0
28511,where ?,0
28512,"is a margin hyperparameter , { 1 , ... , K } is the set of K answers proposed by the Extractor , and [ x ] + indicates truncating x to be non-negative .",0
28513,"Intuitively , this loss says that we want the end - to - end probability ?",0
28514,* for the correct answer to beat least ?,0
28515,larger than the probability ?,0
28516,i for any other answer proposed by the Extractor .,0
28517,"During training , the correct answer is occasionally missed by the Extractor , especially in early epochs .",0
28518,We counter this issue by forcing the correct answer into the top K set while training .,0
28519,When evaluating the model on validation and test examples we rely fully on the top K answers proposed by the Extractor .,0
28520,"To get the final loss term L ER , minus 2 regularization terms on the model parameters , we take a weighted combination of L E and L R :",0
28521,where ?,0
28522,is a hyperparameter for weighting the relative contribution of the Extractor and Reasoner losses .,0
28523,"In practice , we found that ?",0
28524,should be fairly large ( e.g. 10 < ? < 100 ) .,0
28525,"Empirically , we observed that the output probabilities from the Extractor often peak and saturate the first softmax ; hence , the Extractor term can come to dominate the Reasoner term without the weight ?",0
28526,( we discuss the Extractor 's propensity to overfit in Section 5 ) .,0
28527,Related Work,0
28528,The Impatient and Attentive Reader models were proposed by .,0
28529,The Attentive Reader applies bidirectional recurrent encoders to the question and supporting text .,0
28530,"It then uses the attention mechanism described in to compute a fixed - length representation of the text based on a weighted sum of the text encoder 's output , guided by comparing the question representation to each location in the text .",0
28531,"Finally , a joint representation of the question and supporting text is formed by passing their separate representations through a feedforward MLP and an answer is selected by comparing the MLP output to a representation of each possible answer .",0
28532,"The Impatient Reader operates similarly , but computes attention over the text after processing each consecutive word of the question .",0
28533,The two models achieved similar performance on the CNN and Daily Mail datasets .,0
28534,Memory Networks were first proposed by and later applied to machine comprehension by .,0
28535,"This model builds fixed - length representations of the question and of windows of text surrounding each candidate answer , then uses a weighted - sum attention mechanism to combine the window representations .",0
28536,"As in the previous Readers , the combined window representation is then compared with each possible answer to form a prediction about the best answer .",0
28537,What distinguishes Memory Networks is how they construct the question and text window representations .,0
28538,"Rather than a recurrent network , they use a specially - designed , trainable transformation of the word embeddings .",0
28539,"Most of the details for the very recent AS Reader are provided in the description of our Extractor module in Section 3.2 , so we do not summarize it further here .",0
28540,This model set the previous state - of - the - art on the CBT dataset .,0
28541,"During the write - up of this paper , another very recent model came to our attention .",0
28542,"propose using a bilinear term instead of a tanh layer to compute the attention between question and passage words , and also uses the attended word encodings for direct , pointer - style prediction as in .",0
28543,This model set the previous state - of - theart on the CNN dataset .,0
28544,"However , this model used embedding vectors pretrained on a large external corpus .",0
28545,The EpiReader borrows ideas from other models as well .,0
28546,The Reasoner 's convolutional architecture is based on Severyn and Moschitti ( 2016 ) and .,0
28547,Our use of word - level match - ing was inspired by the Parallel - Hierarchical model of and the natural language inference model of .,0
28548,"Finally , the idea of formulating and testing hypotheses for question - answering was used to great effect in IBM 's Dee pQA system for Jeopardy ! , although that was a more traditional information retrieval pipeline rather than an end - to - end neural model .",0
28549,Evaluation,0
28550,Implementation and training details,0
28551,"To train our model we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",1
28552,"The word embeddings were initialized randomly , drawing from the uniform distribution over .",1
28553,"We used batches of 32 examples , and early stopping with a patience of 2 epochs .",1
28554,Our model was implement in Theano using the Keras framework .,1
28555,The results presented below for the EpiReader were obtained by searching over a small grid of hyperparameter settings .,0
28556,"We selected the model that , on each dataset , maximized accuracy on the validation set , then evaluated it on the test set .",0
28557,We record the best settings for each dataset in .,0
28558,"As has been done previously , we train separate models on CBT 's named entity ( CBT - NE ) and common noun ( CBT - CN ) splits .",0
28559,"All our models used 2 - regularization at 0.001 , ? = 50 , and ? = 0.04 .",1
28560,We did not use dropout but plan to investigate its effect in the future .,0
28561,and also present results for ensembles of their models .,0
28562,"Time did not permit us to generate an ensemble of EpiReaders on the CNN dataset so we omit those measures ; however , EpiReader ensembles ( of seven models ) demonstrated improved performance on the CBT dataset .",0
28563,Results,0
28564,"In , we compare the performance of the EpiReader against that of several baselines , on the validation and test sets of the CBT and CNN corpora .",0
28565,We measure EpiReader performance at the output of both the Extractor and the Reasoner .,0
28566,The EpiReader achieves state - of - the - art performance across the board for both datasets .,1
28567,"On CNN , we score 2.2 % higher on test than the best previous model of .",1
28568,"Interestingly , an analysis of the CNN dataset by suggests that approximately 25 % of the test examples contain coreference errors or questions which are "" ambiguous / hard "" even for a human analyst .",0
28569,"If this estimate is accurate , then the EpiReader , achieving an absolute test accuracy of 74.0 % , is operating close to expected human performance .",0
28570,"On the other hand , ambiguity is unlikely to be distributed evenly over entities , so a good model should be able to perform at better - thanchance levels even on questions where the correct answer is uncertain .",0
28571,"If , on the 25 % of "" noisy "" questions , the model can shift it s hit rate from , e.g. , 1/10 to 1 / 3 , then there is still a fair amount of performance to gain .",0
28572,On CBT - CN our single model scores 4.0 % higher than the previous best of the AS Reader .,1
28573,The improvement on CBT - NE is more modest at 1.1 % .,1
28574,"Looking more closely at our CBT - NE results , we found that the validation and test accuracies had relatively high variance even in late epochs of training .",1
28575,"We discovered that many of the validation and test questions were asked about the same named entity , which may explain this issue .",0
28576,Analysis,0
28577,"Aside from achieving state - of - the - art results at its final output , the EpiReader framework gives a boost to its Extractor component through the joint training process .",0
28578,"In , we provide accuracy scores evaluated at the output of the Extractor .",0
28579,"These are all higher than the analogous scores reported for the AS Reader , which we verified ourselves to within negligible difference .",0
28580,"Based on our own work with that We verified that our Extractor achieved a similar rate , and of course this is vital for performance of the full system , since the Reasoner can not recover when the correct answer is not among its inputs .",0
28581,Our results demonstrate that the Reasoner often corrects erroneous answers from the Extractor .,0
28582,gives an example of this correction .,0
28583,"In the text passage , from CBT - NE , Mr. Blacksnake is pursuing Mr. Toad , presumably to eat him .",0
28584,"The dialogue in the question sentence refers to both : Mr. Toad is its subject , referred to by the pronoun "" he "" , and Mr. Blacksnake is its object .",0
28585,"In the preceding sentences , it is clear ( to a human ) that Jimmy is worried about Mr. Toad and his potential encounter with Mr. Blacksnake .",0
28586,"The Extractor , however , points most strongly to "" Toad "" , possibly because he has been referred to most recently .",0
28587,"The Reasoner corrects this error and selects "" Blacksnake "" as the answer .",0
28588,This relies on a deeper understanding of the text .,0
28589,"The named entity can , in this case , be inferred through an alternation of the entities most recently referred to .",0
28590,"This kind alternation is typical of dialogues , when two actors Mr. Blacksnake grinned and started after him , not very fast because he knew that he would n't have to run very fast to catch old Mr. Toad , and he thought the exercise would do him good . "" Still , the green meadows would n't be quite the same without old Mr. Toad .",0
28591,I should miss him if anything happened to him .,0
28592,"I suppose it would be partly my fault , too , for if I had n't pulled over that piece of bark , he probably would have stayed there the rest of the day and been safe . """,0
28593,"QUESTION : "" Maybe he wo n't meet Mr. XXXXX , "" said a little voice inside of Jimmy .",0
28594,EXTRACTOR : Toad REASONER : Blacksnake 1 .,0
28595,18 .,0
28596,21 .,0
28597,19 .,0
28598,20 . interact in turns .,0
28599,The Reasoner can capture this behavior because it examines sentences in sequence .,0
28600,Conclusion,0
28601,"In this article we presented the novel EpiReader framework for machine comprehension , and evaluated it on two large , complex datasets : CNN and CBT .",0
28602,"Our model achieves state - of - the - art results on these corpora , outperforming all previous approaches .",0
28603,"In future work , we plan to augment our framework with a more powerful model for natural language inference , and explore the effect of pretraining such a model specifically on an inference task .",0
28604,We also plan to try simplifying the model by reusing the Extractor 's biGRU encodings in the Reasoner .,0
28605,title,0
28606,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,1
28607,abstract,0
28608,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",1
28609,"In pursuit of this objective , we introduce the General Language Understanding Evaluation ( GLUE ) benchmark , a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks .",0
28610,"By including tasks with limited training data , GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks .",0
28611,GLUE also includes a hand - crafted diagnostic test suite that enables detailed linguistic analysis of models .,0
28612,We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task .,0
28613,"However , the low absolute performance of our best model indicates the need for improved general NLU systems .",1
28614,Published as a conference paper at ICLR 2019,0
28615,INTRODUCTION,0
28616,"The human ability to understand language is general , flexible , and robust .",0
28617,"In contrast , most NLU models above the word level are designed fora specific task and struggle with out - of - domain data .",0
28618,"If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs , then it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains .",0
28619,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .",1
28620,GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,1
28621,"For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .",1
28622,GLUE therefore favors models that can learn to represent linguistic knowledge in away that facilitates sample - efficient learning and effective knowledge - transfer across tasks .,0
28623,None of the datasets in GLUE were created from scratch for the benchmark ; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting .,0
28624,"Four of the datasets feature privately - held test data , which will be used to ensure that the benchmark is used fairly .",1
28625,1,0
28626,"To understand the types of knowledge learned by models and to encourage linguistic - meaningful solution strategies , GLUE also includes a set of hand - crafted analysis examples for probing trained models .",0
28627,"This dataset is designed to highlight common challenges , such as the use of world knowledge and logical operators , that we expect models must handle to robustly solve the tasks . :",0
28628,Task descriptions and statistics .,0
28629,"All tasks are single sentence or sentence pair classification , except STS - B , which is a regression task .",0
28630,MNLI has three classes ; all other classification tasks have two .,0
28631,Test sets shown in bold use labels that have never been made public in any form .,0
28632,"To better understand the challenged posed by GLUE , we conduct experiments with simple baselines and state - of - the - art sentence representation models .",0
28633,We find that unified multi-task trained models slightly outperform comparable models trained on each task separately .,0
28634,"Our best multi-task model makes use of ELMo , a recently proposed pre-training technique .",0
28635,"However , this model still achieves a fairly low absolute score .",0
28636,Analysis with our diagnostic dataset reveals that our baseline models deal well with strong lexical signals but struggle with deeper logical structure .,0
28637,"In summary , we offer : ( i ) A suite of nine sentence or sentence - pair NLU tasks , built on established annotated datasets and selected to cover a diverse range of text genres , dataset sizes , and degrees of difficulty .",0
28638,"( ii ) An online evaluation platform and leaderboard , based primarily on privately - held test data .",0
28639,"The platform is model - agnostic , and can evaluate any method capable of producing results on all nine tasks .",0
28640,( iii ) An expert - constructed diagnostic evaluation dataset .,0
28641,( iv ) Baseline results for several major existing approaches to sentence representation learning .,0
28642,RELATED WORK,0
28643,"Collobert et al. ( 2011 ) used a multi - task model with a shared sentence understanding component to jointly learn POS tagging , chunking , named entity recognition , and semantic role labeling .",0
28644,More recent work has explored using labels from core NLP tasks to supervise training of lower levels of deep neural networks ) and automatically learning cross - task sharing mechanisms for multi-task learning ) .,0
28645,"Beyond multi-task learning , much work in developing general NLU systems has focused on sentence - to - vector encoders , leveraging unlabeled data , labeled data , and combinations of these .",0
28646,"In this line of work , a standard evaluation practice has emerged , recently codified as SentEval .",0
28647,"Like GLUE , SentEval relies on a set of existing classification tasks involving either one or two sentences as inputs .",0
28648,"Unlike GLUE , SentEval only evaluates sentenceto - vector encoders , making it well - suited for evaluating models on tasks involving sentences in isolation .",0
28649,"However , cross - sentence contextualization and alignment are instrumental in achieving state - of - the - art performance on tasks such as machine translation , question answering , and natural language inference .",0
28650,GLUE is designed to facilitate the development of these methods :,0
28651,"It is model - agnostic , allowing for any kind of representation or contextualization , including models that use no explicit vector or symbolic representations for sentences whatsoever .",0
28652,GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite .,0
28653,"Many of the SentEval tasks are closely related to sentiment analysis , such as MR , SST , , and SUBJ .",0
28654,"Other tasks are so close to being solved that evaluation on them is relatively uninformative , such as MPQA and TREC question classification .",0
28655,"In GLUE , we attempt to construct a benchmark that is both diverse and difficult .",0
28656,"introduce decaNLP , which also scores NLP systems based on their performance on multiple datasets .",0
28657,"Their benchmark recasts the ten evaluation tasks as question answering , converting tasks like summarization and text - to - SQL semantic parsing into question answering using automatic transformations .",0
28658,"That benchmark lacks the leaderboard and error analysis toolkit of GLUE , but more importantly , we see it as pursuing a more ambitious but less immediately practical goal :",0
28659,"While GLUE rewards methods that yield good performance on a circumscribed set of tasks using methods like those that are currently used for those tasks , their benchmark rewards systems that make progress toward their goal of unifying all of NLU under the rubric of question answering .",0
28660,TASKS,0
28661,"GLUE is centered on nine English sentence understanding tasks , which cover abroad range of domains , data quantities , and difficulties .",0
28662,"As the goal of GLUE is to spur development of generalizable NLU systems , we design the benchmark such that good performance should require a model to share substantial knowledge ( e.g. , trained parameters ) across all tasks , while still maintaining some taskspecific components .",0
28663,"Though it is possible to train a single model for each task with no pretraining or other outside sources of knowledge and evaluate the resulting set of models on this benchmark , we expect that our inclusion of several data - scarce tasks will ultimately render this approach uncompetitive .",0
28664,We describe the tasks below and in .,0
28665,Appendix,0
28666,A includes additional details .,0
28667,"Unless otherwise mentioned , tasks are evaluated on accuracy and are balanced across classes .",0
28668,SINGLE - SENTENCE TASKS,0
28669,CoLA,0
28670,The Corpus of Linguistic Acceptability consists of English acceptability judgments drawn from books and journal articles on linguistic theory .,0
28671,Each example is a sequence of words annotated with whether it is a grammatical English sentence .,0
28672,"Following the authors , we use Matthews correlation coefficient as the evaluation metric , which evaluates performance on unbalanced binary classification and ranges from - 1 to 1 , with 0 being the performance of uninformed guessing .",0
28673,"We use the standard test set , for which we obtained private labels from the authors .",0
28674,We report a single performance number on the combination of the in - and out - of - domain sections of the test set .,0
28675,SST - 2,0
28676,The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment .,0
28677,The task is to predict the sentiment of a given sentence .,0
28678,"We use the two - way ( positive / negative ) class split , and use only sentence - level labels .",0
28679,SIMILARITY AND PARAPHRASE TASKS,0
28680,MRPC,0
28681,"The Microsoft Research Paraphrase Corpus ) is a corpus of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",0
28682,"Because the classes are imbalanced ( 68 % positive ) , we follow common practice and report both accuracy and F1 score .",0
28683,QQP,0
28684,The Quora Question Pairs 2 dataset is a collection of question pairs from the community question - answering website Quora .,0
28685,The task is to determine whether a pair of questions are semantically equivalent .,0
28686,"As in MRPC , the class distribution in QQP is unbalanced ( 63 % negative ) , so we report both accuracy and F1 score .",0
28687,"We use the standard test set , for which we obtained private labels from the authors .",0
28688,We observe that the test set has a different label distribution than the training set .,0
28689,STS - B,0
28690,The Semantic Textual Similarity Benchmark,0
28691,"( Cer et al. , 2017 ) is a collection of sentence pairs drawn from news headlines , video and image captions , and natural language inference data .",0
28692,Each pair is human - annotated with a similarity score from 1 to 5 ; the task is to predict these scores .,0
28693,"Follow common practice , we evaluate using Pearson and Spearman correlation coefficients .",0
28694,INFERENCE TASKS,0
28695,"MNLI The Multi-Genre Natural Language Inference Corpus ( Williams et al. , 2018 ) is a crowdsourced collection of sentence pairs with textual entailment annotations .",0
28696,"Given a premise sentence and a hypothesis sentence , the task is to predict whether the premise entails the hypothesis ( entailment ) , contradicts the hypothesis ( contradiction ) , or neither ( neutral ) .",0
28697,"The premise sentences are gathered from ten different sources , including transcribed speech , fiction , and government reports .",0
28698,"We use the standard test set , for which we obtained private labels from the authors , and evaluate on both the matched ( in - domain ) and mismatched ( cross - domain ) sections .",0
28699,We also use and recommend the SNLI corpus as 550 k examples of auxiliary training data .,0
28700,QNLI,0
28701,"The Stanford Question Answering Dataset ) is a question - answering dataset consisting of question - paragraph pairs , where one of the sentences in the paragraph ( drawn from Wikipedia ) contains the answer to the corresponding question ( written by an annotator ) .",0
28702,"We convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context , and filtering out pairs with low lexical overlap between the question and the context sentence .",0
28703,The task is to determine whether the context sentence contains the answer to the question .,0
28704,"This modified version of the original task removes the requirement that the model select the exact answer , but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue .",0
28705,This process of recasting existing datasets into NLI is similar to methods introduced in White et al. ) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices .,0
28706,The examples are manually constructed to foil simple statistical methods :,0
28707,Each one is contingent on contextual information provided by a single word or phrase in the sentence .,0
28708,"To convert the problem into sentence pair classification , we construct sentence pairs by replacing the ambiguous pronoun with each possible referent .",0
28709,The task is to predict if the sentence with the : Examples from the diagnostic set .,0
28710,Fwd ( resp. Bwd ) denotes the label when sentence 1 ( resp. sentence 2 ) is the premise .,0
28711,"Labels are entailment ( E ) , neutral ( N ) , or contradiction ( C ) .",0
28712,"Examples are tagged with the phenomena they demonstrate , and each phenomenon belongs to one of four broad categories ( in parentheses ) .",0
28713,pronoun substituted is entailed by the original sentence .,0
28714,We use a small evaluation set consisting of new examples derived from fiction books 5 that was shared privately by the authors of the original corpus .,0
28715,"While the included training set is balanced between two classes , the test set is imbalanced between them ( 65 % not entailment ) .",0
28716,"Also , due to a data quirk , the development set is adversarial : hypotheses are sometimes shared between training and development examples , so if a model memorizes the training examples , they will predict the wrong label on corresponding development set example .",0
28717,"As with QNLI , each example is evaluated separately , so there is not a systematic correspondence between a model 's score on this task and it s score on the unconverted original task .",0
28718,We call converted dataset WNLI ( Winograd NLI ) .,0
28719,EVALUATION,0
28720,The GLUE benchmark follows the same evaluation model as SemEval and Kaggle .,0
28721,"To evaluate a system on the benchmark , one must run the system on the provided test data for the tasks , then upload the results to the website gluebenchmark.com for scoring .",0
28722,The benchmark site shows per-task scores and a macro-average of those scores to determine a system 's position on the leaderboard .,0
28723,"For tasks with multiple metrics ( e.g. , accuracy and F1 ) , we use an unweighted average of the metrics as the score for the task when computing the overall macro-average .",0
28724,The website also provides fine - and coarse - grained results on the diagnostic dataset .,0
28725,See Appendix,0
28726,D for details .,0
28727,DIAGNOSTIC DATASET,0
28728,"Drawing inspiration from the FraCaS suite and the recent Build - It - Break - It competition ( Ettinger et al. , 2017 ) , we include a small , manually - curated test set for the analysis of system performance .",0
28729,"While the main benchmark mostly reflects an application - driven distribution of examples , our diagnostic dataset highlights a pre-defined set of phenomena that we believe are interesting and important for models to capture .",0
28730,We show the full set of phenomena in .,0
28731,Each diagnostic example is an NLI sentence pair with tags for the phenomena demonstrated .,0
28732,"The NLI task is well - suited to this kind of analysis , as it can easily evaluate the full set of skills involved in ( ungrounded ) sentence understanding , from resolution of syntactic ambiguity to pragmatic reasoning with world knowledge .",0
28733,"We ensure the data is reasonably diverse by producing examples fora variety of linguistic phenomena and basing our examples on naturally - occurring sentences from several domains ( news , Reddit , Wikipedia , academic papers ) .",0
28734,"This approaches differs from that of FraCaS , which was designed to test linguistic theories with a minimal and uniform set of examples .",0
28735,A sample from our dataset is shown in .,0
28736,Annotation Process,0
28737,"We begin with a target set of phenomena , based roughly on those used in the FraCaS suite .",0
28738,"We construct each example by locating a sentence that can be easily made to demonstrate a target phenomenon , and editing it in two ways to produce an appropriate sentence pair .",0
28739,We make minimal modifications so as to maintain high lexical and structural overlap within each sentence pair and limit superficial cues .,0
28740,"We then label the inference relationships between the sentences , considering each sentence alternatively as the premise , producing two labeled examples for each pair ( 1100 total ) .",0
28741,"Where possible , we produce several pairs with different labels fora single source sentence , to have minimal sets of sentence pairs that are lexically and structurally very similar but correspond to different entailment relationships .",0
28742,"The resulting labels are 42 % entailment , 35 % neutral , and 23 % contradiction .",0
28743,Evaluation,0
28744,"Since the class distribution in the diagnostic set is not balanced , we use R 3 ( Gorodkin , 2004 ) , a three - class generalization of the Matthews correlation coefficient , for evaluation .",0
28745,"In light of recent work showing that crowdsourced data often contains artifacts which can be exploited to perform well without solving the intended task , we audit the data for such artifacts .",0
28746,"We reproduce the methodology of Gururangan et al. , training two fastText classifiers to predict entailment labels on SNLI and MNLI using only the hypothesis as input .",0
28747,"The models respectively get near - chance accuracies of 32.7 % and 36.4 % on our diagnostic data , showing that the data does not suffer from such artifacts .",0
28748,"To establish human baseline performance on the diagnostic set , we have six NLP researchers annotate 50 sentence pairs ( 100 entailment examples ) randomly sampled from the diagnostic set .",0
28749,"Interannotator agreement is high , with a Fleiss 's ? of 0.73 .",0
28750,"The average R 3 score among the annotators is 0.80 , much higher than any of the baseline systems described in Section 5 .",0
28751,Intended Use,0
28752,"The diagnostic examples are hand - picked to address certain phenomena , and NLI is a task with no natural input distribution , so we do not expect performance on the diagnostic set to reflect overall performance or generalization in downstream applications .",0
28753,Performance on the analysis set should be compared between models but not between categories .,0
28754,"The set is provided not as a benchmark , but as an analysis tool for error analysis , qualitative model comparison , and development of adversarial examples .",0
28755,BASELINES,0
28756,"For baselines , we evaluate a multi- task learning model trained on the GLUE tasks , as well as several variants based on recent pre-training methods .",0
28757,We briefly describe them here .,0
28758,See Appendix,0
28759,B for details .,0
28760,We implement our models in the AllenNLP library .,0
28761,Original code for the baselines is available at https://github.com/nyu-mll/GLUE-baselines and a newer version is available at https://github.com/jsalt18-sentence-repl/jiant.,1
28762,Architecture,0
28763,"Our simplest baseline architecture is based on sentence - to - vector encoders , and sets aside GLUE 's ability to evaluate models with more complex structures .",1
28764,"Taking inspiration from Conneau et al. , the model uses a two - layer , 1500D ( per direction ) BiLSTM with max pooling and 300D Glo Ve word embeddings ( 840B Common Crawl version ; .",0
28765,"For single - sentence tasks , we encode the sentence and pass the resulting vector to a classifier .",0
28766,"For sentence - pair tasks , we encode sentences independently to produce vectors u , v , and pass [ u ; v ; | u ? v| ; u * v ] to a classifier .",0
28767,The classifier is an MLP with a 512D hidden layer .,0
28768,"We also consider a variant of our model which for sentence pair tasks uses an attention mechanism inspired by between all pairs of words , followed by a second BiLSTM with max pooling .",0
28769,"By explicitly modeling the interaction between sentences , these models fall outside the sentence - to - vector paradigm .",0
28770,Pre-Training,0
28771,We augment our base model with two recent methods for pre-training : ELMo and CoVe .,0
28772,We use existing trained models for both .,0
28773,ELMo uses a pair of two - layer neural language models trained on the Billion Word Benchmark .,0
28774,"Each word is represented by a contextual embedding , produced by taking a Training We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across tasks , and classifiers trained separately for each task .",0
28775,"For each training update , we sample a task to train with a probability proportional to the number of training examples for each task .",0
28776,"We train our models with Adam ( Kingma & Ba , 2015 ) with initial learning rate 10 ? 4 and batch size 128 .",0
28777,We use the macro-average score as the validation metric and stop training when the learning rate drops below 10 ? 5 or performance does not improve after 5 validation checks .,0
28778,"We also train a set of single - task models , which are configured and trained identically , but share no parameters .",0
28779,"To allow for fair comparisons with the multi-task analogs , we do not tune parameter or training settings for each task , so these single - task models do not generally represent the state of the art for each task .",0
28780,Sentence Representation Models,0
28781,"Finally , we evaluate the following trained sentence - to - vector encoder models using our benchmark : average bag - of - words using GloVe embeddings ( CBoW ) , , , , and GenSen .",0
28782,"For these models , we only train task - specific classifiers on the representations they produce .",0
28783,BENCHMARK RESULTS,0
28784,We train three runs of each model and evaluate the run with the best macro-average development set performance ( see in Appendix C ) .,0
28785,"For single - task and sentence representation models , we evaluate the best run for each individual task .",0
28786,We present performance on the main benchmark tasks in .,0
28787,We find that multi-task training yields better overall scores over single - task training amongst models using attention or ELMo .,1
28788,"Attention generally has negligible or negative aggregate effect in single task training , but helps in multi-task training .",0
28789,"We see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings , particularly for single - sentence tasks .",1
28790,Using CoVe has mixed effects over using only Glo Ve .,0
28791,"Among the pre-trained sentence representation models , we observe fairly consistent gains moving from CBoW to Skip - Thought to Infersent and GenSen .",1
28792,"Relative to the models trained directly on the GLUE tasks , InferSent is competitive and GenSen outperforms all but the two best .",1
28793,"Looking at results per task , we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task .",1
28794,"On the other hand , for STS - B , models trained directly on the task lag significantly behind the performance of the best sentence representation model .",1
28795,"Finally , there are tasks for which no model does particularly well .",0
28796,"On WNLI , no model exceeds most - frequent - class guessing ( 65.1 % ) and we substitute the model predictions for the most -frequent baseline .",1
28797,"On RTE and in aggregate , even our best baselines leave room for improvement .",1
28798,These early results indicate that solving GLUE is beyond the capabilities of current models and methods .,0
28799,ANALYSIS,0
28800,We analyze the baselines by evaluating each model 's MNLI classifier on the diagnostic set to get a better sense of their linguistic capabilities .,0
28801,Results are presented in .,0
28802,Coarse Categories,0
28803,Overall performance is low for all models :,0
28804,The highest total score of 28 still denotes poor absolute performance .,0
28805,"Performance tends to be higher on Predicate - Argument Structure and lower on Logic , though numbers are not closely comparable across categories .",0
28806,"Unlike on the main benchmark , the multi-task models are almost always outperformed by their single - task counterparts .",0
28807,"This is perhaps unsurprising , since with our simple multi-task training regime , there is likely some destructive interference between MNLI and the other tasks .",0
28808,"The models trained on the GLUE tasks largely outperform the pretrained sentence representation models , with the exception of GenSen .",0
28809,"Using attention has a greater influence on diagnostic scores than using ELMo or CoVe , which we take to indicate that attention is especially important for generalization in NLI .",0
28810,Fine - Grained Subcategories,0
28811,Most models handle universal quantification relatively well .,0
28812,"Looking at relevant examples , it seems that relying on lexical cues such as "" all "" often suffices for good performance .",0
28813,"Similarly , lexical cues often provide good signal in morphological negation examples .",0
28814,We observe varying weaknesses between models .,0
28815,Double negation is especially difficult for the GLUE - trained models that only use GloVe embeddings .,0
28816,"This is ameliorated by ELMo , and to some degree CoVe .",0
28817,"Also , attention has mixed effects on overall results , and models with attention tend to struggle with downward monotonicity .",0
28818,"Examining their predictions , we found that the models are sensitive to hypernym / hyponym substitution and word deletion as a signal of entailment , but predict it in the wrong direction ( as if the substituted / deleted word were in an upward monotone context ) .",0
28819,This is consistent with recent findings by McCoy & Linzen ( 2019 ) that these systems use the subsequence relation between premise and hypothesis as a heuristic shortcut .,0
28820,"Restrictivity examples , which often depend on nuances of quantifier scope , are especially difficult for almost all models .",0
28821,"Overall , there is evidence that going beyond sentence - to - vector representations , e.g. with an attention mechanism , might aid performance on out - of - domain data , and that transfer methods like ELMo and CoVe encode linguistic information specific to their supervision signal .",0
28822,"However , increased representational capacity may lead to overfitting , such as the failure of attention models in downward monotone contexts .",0
28823,"We expect that our platform and diagnostic dataset will be useful for similar analyses in the future , so that model designers can better understand their models ' generalization behavior and implicit knowledge .",0
28824,CONCLUSION,0
28825,"We introduce GLUE , a platform and collection of resources for evaluating and analyzing natural language understanding systems .",0
28826,"We find that , in aggregate , models trained jointly on our tasks see better performance than the combined performance of models trained for each task separately .",0
28827,"We confirm the utility of attention mechanisms and transfer learning methods such as ELMo in NLU systems , which combine to outperform the best sentence representation models on the GLUE benchmark , but still leave room for improvement .",0
28828,"When evaluating these models on our diagnostic dataset , we find that they fail ( often spectacularly ) on many linguistic phenomena , suggesting possible avenues for future work .",0
28829,"In sum , the question of how to design general - purpose NLU models remains unanswered , and we believe that GLUE can provide fertile soil for addressing this challenge .",0
28830,A ADDITIONAL BENCHMARK DETAILS,0
28831,QNLI,0
28832,"To construct a balanced dataset , we select all pairs in which the most similar sentence to the question was not the answer sentence , as well as an equal amount of cases in which the correct sentence was the most similar to the question , but another distracting sentence was a close second .",0
28833,Our similarity metric is based on CBoW representations with pre-trained GloVe embeddings .,0
28834,"This approach to converting pre-existing datasets into NLI format is closely related to recent work by , as well as to the original motivation for textual entailment presented by .",0
28835,Both argue that many NLP tasks can be productively reduced to textual entailment .,0
28836,B ADDITIONAL BASELINE DETAILS B.1 ATTENTION MECHANISM,0
28837,"We implement our attention mechanism as follows : given two sequences of hidden states u 1 , u 2 , . . . , u M and v 1 , v 2 , . . . , v N , we first compute matrix H where H ij = u i v j .",0
28838,"For each u i , we get attention weights ?",0
28839,"i by taking a softmax over the i throw of H , and get the corresponding context vector ?",0
28840,i = j ?,0
28841,ij v j by taking the attention - weighted sum of the v j .,0
28842,We pass a second BiLSTM with max pooling over the sequence [ u 1 ;?,0
28843,"1 ] , . . . [ u M ;? M ] to produce u .",0
28844,We process the v j vectors analogously to obtain v .,0
28845,"Finally , we feed [ u ; v ; | u ? v | ; u * v ] into a classifier .",0
28846,B.2 TRAINING,0
28847,"We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across tasks , and classifiers trained separately for each task .",0
28848,"For each training update , we sample a task to train with a probability proportional to the number of training examples for each task .",0
28849,"We scale each task 's loss inversely proportional to the number of examples for that task , which we found to improve overall performance .",0
28850,"We train our models with Adam ( Kingma & Ba , 2015 ) with initial learning rate 10 ? 3 , batch size 128 , and gradient clipping .",0
28851,"We use macro-average score overall tasks as our validation metric , and perform a validation check every 10 k updates .",0
28852,We divide the learning rate by 5 whenever validation performance does not improve .,0
28853,We stop training when the learning rate drops below 10 ?5 or performance does not improve after 5 validation checks .,0
28854,B.3 SENTENCE REPRESENTATION MODELS,0
28855,We evaluate the following sentence representation models :,0
28856,1 .,0
28857,"CBoW , the average of the Glo Ve embeddings of the tokens in the sentence . , a BiLSTM with max - pooling trained to predict the discourse marker ( because , so , etc. ) relating two sentences on data derived from TBC .",0
28858,We use the variant trained for eight - way classification .,0
28859,"5 . GenSen , a sequence - to - sequence model trained on a variety of supervised and unsupervised objectives .",0
28860,"We use the variant of the model trained on both MNLI and SNLI , the Skip - Thought objective on TBC , and a constituency parsing objective on the Billion Word Benchmark .",0
28861,"We train task - specific classifiers on top of frozen sentence encoders , using the default parameters from SentEval .",0
28862,See,0
28863,https://github.com/nyu-mll/SentEval for details and code .,0
28864,C DEVELOPMENT SET RESULTS,0
28865,The GLUE website limits users to two submissions per day in order to avoid overfitting to the private test data .,0
28866,"To provide a reference for future work on GLUE , we present the best development set results achieved by our baselines in .",0
28867,D BENCHMARK WEBSITE DETAILS,0
28868,"GLUE 's online platform is built using React , Redux and TypeScript .",0
28869,We use Google Firebase for data storage and Google Cloud Functions to host and run our grading script when a submission is made .,0
28870,shows the visual presentation of our baselines on the leaderboard .,0
28871,E ADDITIONAL DIAGNOSTIC DATA DETAILS,0
28872,"The dataset is designed to allow for analyzing many levels of natural language understanding , from word meaning and sentence structure to high - level reasoning and application of world knowledge .",0
28873,"To make this kind of analysis feasible , we first identify four broad categories of phenomena : Lexical Semantics , Predicate - Argument Structure , Logic , and Knowledge .",0
28874,"However , since these categories are vague , we divide each into a larger set of fine - grained subcategories .",0
28875,Descriptions of all of the fine - grained categories are given in the remainder of this section .,0
28876,"These categories are just one lens that can be used to understand linguistic phenomena and entailment , and there is certainly room to argue about how examples should be categorized , what the categories should be , etc .",0
28877,"These categories are not based on any particular linguistic theory , but broadly based on issues that linguists have often identified and modeled in the study of syntax and semantics .",0
28878,"The dataset is provided not as a benchmark , but as an analysis tool to paint in broad strokes the kinds of phenomena a model mayor may not capture , and to provide a set of examples that can serve for error analysis , qualitative model comparison , and development of adversarial examples that expose a model 's weaknesses .",0
28879,"Because the distribution of language is somewhat arbitrary , it will not be helpful to compare performance of the same model on different categories .",0
28880,"Rather , we recommend comparing performance that different models score on the same category , or using the reported scores as a guide for error analysis .",0
28881,We show coarse - grain category counts and label distributions of the diagnostic set in .,0
28882,E.1 LEXICAL SEMANTICS,0
28883,These phenomena center on aspects of word meaning .,0
28884,Lexical Entailment,0
28885,"Entailment can be applied not only on the sentence level , but the word level .",0
28886,"For example , we say "" dog "" lexically entails "" animal "" because anything that is a dog is also an animal , and "" dog "" lexically contradicts "" cat "" because it is impossible to be both at once .",0
28887,"This relationship applies to many types of words ( nouns , adjectives , verbs , many prepositions , etc. ) and the relationship between lexical and sentential entailment has been deeply explored , e.g. , in systems of natural logic .",0
28888,"This connection often hinges on monotonicity in language , so many Lexical Entailment examples will also be tagged with one of the Monotone categories , though we do not do this in every case ( see .",0
28889,Morphological Negation,0
28890,"This is a special case of lexical contradiction where one word is derived from the other : from "" affordable "" to "" unaffordable "" , "" agree "" to "" disagree "" , etc .",0
28891,"We also include examples like "" ever "" and "" never "" .",0
28892,"We also label these examples with Negation or Double Negation , since they can be viewed as involving a word - level logical negation .",0
28893,"Factivity Propositions appearing in a sentence maybe in any entailment relation with the sentence as a whole , depending on the context in which they appear .",0
28894,"In many cases , this is determined by lexical triggers ( usually verbs or adverbs ) in the sentence .",0
28895,"For example ,",0
28896,""" I recognize that X "" entails "" X "" .",0
28897,""" I did not recognize that X "" entails "" X "" .",0
28898,""" I believe that X "" does not entail "" X "" .",0
28899,""" I am refusing to do X "" contradicts "" I am doing X "" .",0
28900,""" I am not refusing to do X "" does not contradict "" I am doing X "" .",0
28901,""" I almost finished X "" contradicts "" I finished X "" .",0
28902,""" I barely finished X "" entails "" I finished X "" .",0
28903,"Constructions like "" I recognize that X "" are often called factive , since the entailment ( of X above , regarded as a presupposition ) persists even under negation .",0
28904,"Constructions like "" I am refusing to do X "" above are often called implicative , and are sensitive to negation .",0
28905,"There are also cases where a sentence ( non - ) entails the existence of an entity mentioned in it , for example "" I have found a unicorn "" entails "" A unicorn exists "" while "" I am looking fora unicorn "" does n't necessarily entail "" A unicorn exists "" .",0
28906,"Readings where the entity does not necessarily exist are often called intensional readings , since they seem to deal with the properties denoted by a description ( its intension ) rather than being reducible to the set of entities that match the description ( it s extension , which in cases of non-existence will be empty ) .",0
28907,We place all examples involving these phenomena under the label of Factivity .,0
28908,"While it often depends on context to determine whether a nested proposition or existence of an entity is entailed by the overall statement , very often it relies heavily on lexical triggers , so we place the category under Lexical Semantics .",0
28909,Symmetry / Collectivity,0
28910,"Some propositions denote symmetric relations , while others do not .",0
28911,"For example , "" John married Gary "" entails "" Gary married John "" but "" John likes Gary "" does not entail "" Gary likes John "" .",0
28912,"Symmetric relations can often be rephrased by collecting both arguments into the subject : "" John met Gary "" entails "" John and Gary met "" .",0
28913,"Whether a relation is symmetric , or admits collecting its arguments into the subject , is often determined by its headword ( e.g. , "" like "" , "" marry "" or "" meet "" ) , so we classify it under Lexical Semantics .",0
28914,Redundancy,0
28915,"If a word can be removed from a sentence without changing its meaning , that means the word 's meaning was more - or - less adequately expressed by the sentence ; so , identifying these cases reflects an understanding of both lexical and sentential semantics .",0
28916,Named Entities,0
28917,Words often name entities that exist in the world .,0
28918,"There are many different kinds of understanding we might wish to understand about these names , including their compositional structure ( for example , the "" Baltimore Police "" is the same as the "" Police of the City of Baltimore "" ) or their real - world referents and acronym expansions ( for example , "" SNL "" is "" Saturday Night Live "" ) .",0
28919,"This category is closely related to World Knowledge , but focuses on the semantics of names as lexical items rather than background knowledge about their denoted entities .",0
28920,"Quantifiers Logical quantification in natural language is often expressed through lexical triggers such as "" every "" , "" most "" , "" some "" , and "" no "" .",0
28921,"While we reserve the categories in Quantification and Monotonicity for entailments involving operations on these quantifiers and their arguments , we choose to regard the interchangeability of quantifiers ( e.g. , in many cases "" most "" entails "" many "" ) as a question of lexical semantics .",0
28922,E.2 PREDICATE - ARGUMENT STRUCTURE,0
28923,An important component of understanding the meaning of a sentence is understanding how its parts are composed together into a whole .,0
28924,"In this category , we address issues across that spectrum , from syntactic ambiguity to semantic roles and coreference .",0
28925,"Syntactic Ambiguity : Relative Clauses , Coordination Scope",0
28926,These two categories deal purely with resolving syntactic ambiguity .,0
28927,Relative clauses and coordination scope are both sources of a great amount of ambiguity in English .,0
28928,Prepositional phrases,0
28929,Prepositional phrase attachment is a particularly difficult problem that syntactic parsers in NLP systems continue to struggle with .,0
28930,"We view it as a problem both of syntax and semantics , since prepositional phrases can express a wide variety of semantic roles and often semantically apply beyond their direct syntactic attachment .",0
28931,Core Arguments,0
28932,"Verbs select for particular arguments , particularly subjects and objects , which might be interchangeable depending on the context or the surface form .",0
28933,"One example is the ergative alternation : "" Jake broke the vase "" entails "" the vase broke "" but "" Jake broke the vase "" does not entail "" Jake broke "" .",0
28934,"Other rearrangements of core arguments , such as those seen in Symmetry / Collectivity , also fall under the Core Arguments label .",0
28935,"Alternations : Active / Passive , Genitives / Partitives , Nominalization , Datives",0
28936,All four of these categories correspond to syntactic alternations that are known to follow specific patterns in English :,0
28937,"Active / Passive : "" I saw him "" is equivalent to "" He was seen by me "" and entails "" He was seen "" .",0
28938,"Genitives / Partitives : "" the elephant 's foot "" is the same thing as "" the foot of the elephant "" .",0
28939,"Nominalization : "" I caused him to submit his resignation "" entails "" I caused the submission of his resignation "" .",0
28940,"Datives : "" I baked him a cake "" entails "" I baked a cake for him "" and "" I baked a cake "" but not "" I baked him "" .",0
28941,Ellipsis / Implicits,0
28942,"Often , the argument of a verb or other predicate is omitted ( elided ) in the text , with the reader filling in the gap .",0
28943,We can construct entailment examples by explicitly filling in the gap with the corrector incorrect referents .,0
28944,"For example , the premise "" Putin is so entrenched within Russias ruling system that many of its members can imagine no other leader "" entails "" Putin is so entrenched within Russias ruling system that many of it s members can imagine no other leader than Putin "" and contradicts "" Putin is so entrenched within Russias ruling system that many of its members can imagine no other leader than themselves . """,0
28945,"This is often regarded as a special case of anaphora , but we decided to split out these cases from explicit anaphora , which is often also regarded as a case of coreference ( and attempted to some degree in modern coreference resolution systems ) .",0
28946,Anaphora / Coreference,0
28947,Coreference refers to when multiple expressions refer to the same entity or event .,0
28948,"It is closely related to Anaphora , where the meaning of an expression depends on another ( antecedent ) expression in context .",0
28949,"These two phenomena have significant overlap ; for example , pronouns ( "" she "" , "" we "" , "" it "" ) are anaphors that are co-referent with their antecedents .",0
28950,"However , they also may occur independently , such as coreference between two definite noun phrases ( e.g. , "" Theresa May "" and the "" British Prime Minister "" ) that refer to the same entity , or anaphora from a word like "" other "" which requires an antecedent to distinguish something from .",0
28951,In this category we only include cases where there is an explicit phrase ( anaphoric or not ) that is co-referent with an antecedent or other phrase .,0
28952,We construct examples for these in much the same way as for Ellipsis / Implicits .,0
28953,Intersectivity,0
28954,"Many modifiers , especially adjectives , allow non-intersective uses , which affect their entailment behavior .",0
28955,For example :,0
28956,"Intersective : "" He is a violinist and an old surgeon "" entails "" He is an old violinist "" and "" He is a surgeon "" .",0
28957,"Non-intersective : "" He is a violinist and a skilled surgeon "" does not entail "" He is a skilled violinist "" .",0
28958,"Non-intersective : "" He is a fake surgeon "" does not entail "" He is a surgeon "" .",0
28959,"Generally , an intersective use of a modifier , like "" old "" in "" old men "" , is one which maybe interpreted as referring to the set of entities with both properties ( they are old and they are men ) .",0
28960,"Linguists often formalize this using set intersection , hence the name .",0
28961,Intersectivity is related to Factivity .,0
28962,"For example , "" fake "" maybe regarded as a counter - implicative modifier , and these examples will be labeled as such .",0
28963,"However , we choose to categorize intersectivity under predicate - argument structure rather than lexical semantics , because generally the same word will admit both intersective and non-intersective uses , so it maybe regarded as an ambiguity of argument structure .",0
28964,Restrictivity Restrictivity is most often used to refer to a property of uses of noun modifiers .,0
28965,"In particular , a restrictive use of a modifier is one that serves to identify the entity or entities being described , whereas a non-restrictive use adds extra details to the identified entity .",0
28966,The distinction can often be highlighted by entailments :,0
28967,"Restrictive : "" I finished all of my homework due today "" does not entail "" I finished all of my homework "" .",0
28968,"Non-restrictive : "" I got rid of all those pesky bedbugs "" entails "" I got rid of all those bedbugs "" .",0
28969,"Modifiers that are commonly used non-restrictively are appositives , relative clauses starting with "" which "" or "" who "" , and expletives ( e.g. "" pesky "" ) .",0
28970,Non-restrictive uses can appear in many forms .,0
28971,E.3 LOGIC,0
28972,"With an understanding of the structure of a sentence , there is often a baseline set of shallow conclusions that can be drawn using logical operators and often modeled using the mathematical tools of logic .",0
28973,"Indeed , the development of mathematical logic was initially guided by questions about natural language meaning , from Aristotelian syllogisms to Fregean symbols .",0
28974,The notion of entailment is also borrowed from mathematical logic .,0
28975,"Propositional Structure : Negation , Double Negation , Conjunction , Disjunction , Conditionals",0
28976,"All of the basic operations of propositional logic appear in natural language , and we tag them where they are relevant to our examples :",0
28977,"Negation : "" The cat sat on the mat "" contradicts "" The cat did not sit on the mat "" .",0
28978,Double negation :,0
28979,""" The market is not impossible to navigate "" entails "" The market is possible to navigate "" .",0
28980,"Conjunction : "" Temperature and snow consistency must be just right "" entails "" Temperature must be just right "" .",0
28981,"Disjunction : "" Life is either a daring adventure or nothing at all "" does not entail , but is entailed by , "" Life is a daring adventure "" .",0
28982,"Conditionals : "" If both apply , they are essentially impossible "" does not entail "" They are essentially impossible "" .",0
28983,Conditionals are more complicated because their use in language does not always mirror their meaning in logic .,0
28984,"For example , they maybe used at a higher level than the at - issue assertion :",0
28985,""" If you think about it , it 's the perfect reverse psychology tactic "" entails "" It 's the perfect reverse psychology tactic "" .",0
28986,"Quantification : Universal , Existential",0
28987,"Quantifiers are often triggered by words such as "" all "" , "" some "" , "" many "" , and "" no "" .",0
28988,There is a rich body of work modeling their meaning in mathematical logic with generalized quantifiers .,0
28989,"In these two categories , we focus on straightforward inferences from the natural language analogs of universal and existential quantification :",0
28990,"Universal : "" All parakeets have two wings "" entails , but is not entailed by , "" My parakeet has two wings "" .",0
28991,"Existential : "" Some parakeets have two wings "" does not entail , but is entailed by , "" My parakeet has two wings "" .",0
28992,"Monotonicity : Upward Monotone , Downward Monotone , Non-Monotone",0
28993,Monotonicity is a property of argument positions in certain logical systems .,0
28994,"In general , it gives away of deriving entailment relations between expressions that differ on only one subexpression .",0
28995,"In language , it can explain how some entailments propagate through logical operators and quantifiers .",0
28996,"For example , "" pet "" entails "" pet squirrel "" , which further entails "" happy pet squirrel "" .",0
28997,"We can demonstrate how the quantifiers "" a "" , "" no "" and "" exactly one "" differ with respect to monotonicity :",0
28998,""" I have a pet squirrel "" entails "" I have a pet "" , but not "" I have a happy pet squirrel "" .",0
28999,""" I have no pet squirrels "" does not entail "" I have no pets "" , but does entail "" I have no happy pet squirrels "" . "" I have exactly one pet squirrel "" entails neither "" I have exactly one pet "" nor "" I have exactly one happy pet squirrel "" .",0
29000,"In all of these examples , "" pet squirrel "" appears in what we call the restrictor position of the quantifier .",0
29001,We say :,0
29002,""" a "" is upward monotone in its restrictor : an entailment in the restrictor yields an entailment of the whole statement . "" no "" is downward monotone in its restrictor : an entailment in the restrictor yields an entailment of the whole statement in the opposite direction . "" exactly one "" is non-monotone in its restrictor : entailments in the restrictor do not yield entailments of the whole statement .",0
29003,"In this way , entailments between sentences that are built off of entailments of sub- phrases almost always rely on monotonicity judgments ; see , for example , Lexical Entailment .",0
29004,"However , because this is such a general class of sentence pairs , to keep the Logic category meaningful we do not always tag these examples with monotonicity .",0
29005,"Richer Logical Structure : Intervals / Numbers , Temporal",0
29006,"Some higher - level facets of reasoning have been traditionally modeled using logic , such as actual mathematical reasoning ( entailments based off of numbers ) and temporal reasoning ( which is often modeled as reasoning about a mathematical timeline ) .",0
29007,"Intervals / Numbers : "" I have had more than 2 drinks tonight "" entails "" I have had more than 1 drink tonight "" .",0
29008,"Temporal : "" Mary left before John entered "" entails "" John entered after Mary left "" .",0
29009,title,0
29010,Teaching Machines to Read and Comprehend,0
29011,abstract,0
29012,Teaching machines to read natural language documents remains an elusive challenge .,0
29013,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",1
29014,In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .,0
29015,This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure .,0
29016,Introduction,0
29017,Progress on the path from shallow bag - of - words information retrieval algorithms to machines capable of reading and understanding documents has been slow .,0
29018,"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database .",1
29019,"Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets , and the difficulty in structuring statistical models flexible enough to learn to exploit document structure .",0
29020,"While obtaining supervised natural language reading comprehension data has proved difficult , some researchers have explored generating synthetic narratives and queries .",0
29021,Such approaches allow the generation of almost unlimited amounts of supervised data and enable researchers to isolate the performance of their algorithms on individual simulated phenomena .,0
29022,"Work on such data has shown that neural network based models hold promise for modelling reading comprehension , something that we will build upon here .",0
29023,"Historically , however , many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments , as such closed worlds inevitably fail to capture the complexity , richness , and noise of natural language .",0
29024,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,1
29025,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",1
29026,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,1
29027,We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,1
29028,These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures .,0
29029,"This allows a model to focus on the aspects of a document that it believes will help it answer a question , and also allows us to visualises its inference process .",0
29030,We compare these neural models to a range of baselines and heuristic benchmarks based upon a traditional frame semantic analysis provided by a state - of - the - art natural language processing :,0
29031,Percentage of time that the correct answer is contained in the top N most frequent entities in a given document .,0
29032,( NLP ) pipeline .,0
29033,"Our results indicate that the neural models achieve a higher accuracy , and do so without any specific encoding of the document or query structure .",0
29034,Supervised training data for reading comprehension,0
29035,The reading comprehension task naturally lends itself to a formulation as a supervised learning problem .,0
29036,"Specifically we seek to estimate the conditional probability p ( a | c , q ) , where c is a context document , q a query relating to that document , and a the answer to that query .",0
29037,"For a focused evaluation we wish to be able to exclude additional information , such as world knowledge gained from co-occurrence statistics , in order to test a model 's core capability to detect and understand the linguistic relationships between entities in the context document .",0
29038,Such an approach requires a large training corpus of document - query - answer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing .,0
29039,This limitation has meant that most work in this area has taken the form of unsupervised approaches which use templates or syntactic / semantic analysers to extract relation tuples from the document to form a knowledge graph that can be queried .,0
29040,"Here we propose a methodology for creating real - world , large scale supervised training data for learning reading comprehension models .",0
29041,"Inspired by work in summarisation , we create two machine reading corpora by exploiting online newspaper articles and their matching summaries .",0
29042,We have collected 93k articles from the CNN 1 and 220 k articles from the Daily Mail 2 websites .,0
29043,"Both news providers supplement their articles with a number of bullet points , summarising aspects of the information contained in the article .",0
29044,Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents .,0
29045,We construct a corpus of document - queryanswer triples by turning these bullet points into Cloze style questions by replacing one entity at a time with a placeholder .,0
29046,This results in a combined corpus of roughly 1 M data points .,0
29047,Code to replicate our datasets - and to apply this method to other sources - is available online 3 .,0
29048,Entity replacement and permutation,0
29049,"Note that the focus of this paper is to provide a corpus for evaluating a model 's ability to read and comprehend a single document , not world knowledge or co-occurrence .",0
29050,To understand that distinction consider for instance the following Cloze form queries ( created from headlines in the Daily Mail validation set ) : a),0
29051,The hi-tech bra that helps you beat breast X ; b ) Could Saccharin help beat X ? ; c),0
29052,Can fish oils help fight prostate X ?,0
29053,"An ngram language model trained on the Daily Mail would easily correctly predict that ( X = cancer ) , regardless of the contents of the context document , simply because this is a very frequently cured entity in the Daily Mail corpus .",0
29054,Original Version,0
29055,Anonymised Version,0
29056,Context,0
29057,"The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the "" Top Gear "" host , his lawyer said Friday .",0
29058,"Clarkson , who hosted one of the most - watched television shows in the world , was dropped by the BBC Wednesday after an internal investigation by the British broadcaster found he had subjected producer Oisin Tymon "" to an unprovoked physical and verbal attack . """,0
29059,". . . the ent381 producer allegedly struck by ent 212 will not press charges against the "" ent 153 "" host , his lawyer said friday . ent 212 , who hosted one of the most - watched television shows in the world , was dropped by the ent381 wednesday after an internal investigation by the ent180 broadcaster found he had subjected producer ent193 "" to an unprovoked physical and verbal attack . "" . . .",0
29060,Query,0
29061,Producer,0
29062,"X will not press charges against Jeremy Clarkson , his lawyer says .",0
29063,producer,0
29064,"X will not press charges against ent212 , his lawyer says .",0
29065,Answer,0
29066,Oisin Tymon ent193,0
29067,"To prevent such degenerate solutions and create a focused task we anonymise and randomise our corpora with the following procedure , a ) use a coreference system to establish coreferents in each data point ; b ) replace all entities with abstract entity markers according to coreference ; c ) randomly permute these entity markers whenever a data point is loaded .",0
29068,Compare the original and anonymised version of the example in .,0
29069,Clearly a human reader can answer both queries correctly .,0
29070,"However in the anonymised setup the context document is required for answering the query , whereas the original version could also be answered by someone with the requisite background knowledge .",0
29071,"Therefore , following this procedure , the only remaining strategy for answering questions is to do so by exploiting the context presented with each question .",0
29072,Thus performance on our two corpora truly measures reading comprehension capability .,0
29073,"Naturally a production system would benefit from using all available information sources , such as clues through language and co-occurrence statistics .",0
29074,"gives an indication of the difficulty of the task , showing how frequent the correct answer is contained in the top N entity markers in a given document .",0
29075,Note that our models do n't distinguish between entity markers and regular words .,0
29076,This makes the task harder and the models more general .,0
29077,Models,0
29078,So far we have motivated the need for better datasets and tasks to evaluate the capabilities of machine reading models .,0
29079,"We proceed by describing a number of baselines , benchmarks and new models to evaluate against this paradigm .",0
29080,"We define two simple baselines , the majority baseline ( maximum frequency ) picks the entity most frequently observed in the context document , whereas the exclusive majority ( exclusive frequency ) chooses the entity most frequently observed in the context but not observed in the query .",0
29081,The idea behind this exclusion is that the placeholder is unlikely to be mentioned twice in a single Cloze form query .,0
29082,Symbolic Matching Models,0
29083,"Traditionally , a pipeline of NLP models has been used for attempting question answering , that is models that make heavy use of linguistic annotation , structured world knowledge and semantic parsing and similar NLP pipeline outputs .",0
29084,"Building on these approaches , we define a number of NLP - centric models for our machine reading task .",0
29085,"Frame - Semantic Parsing Frame-semantic parsing attempts to identify predicates and their arguments , allowing models access to information about "" who did what to whom "" .",0
29086,Naturally this kind of annotation lends itself to being exploited for question answering .,0
29087,We develop a benchmark that makes use of frame- semantic annotations which we obtained by parsing our model with a state - of the - art frame - semantic parser .,0
29088,As the parser makes extensive use of linguistic information we run these benchmarks on the unanonymised version of our corpora .,0
29089,There is no significant advantage in this as the frame - semantic approach used here does not possess the capability to generalise through a language model beyond exploiting one during the parsing phase .,0
29090,"Thus , the key objective of evaluating machine comprehension abilities is maintained .",0
29091,"Extracting entity - predicate triplesdenoted as ( e 1 , V , e 2 ) - from both the query q and context document d , we attempt to resolve queries using a number of rules with an increasing recall / precision trade - off as follows . :",0
29092,Resolution strategies using PropBank triples .,0
29093,"x denotes the entity proposed as answer , V is a fully qualified PropBank frame ( e.g. give .01 . V ) .",0
29094,Strategies are ordered by precedence and answers determined accordingly .,0
29095,This heuristic algorithm was iteratively tuned on the validation data set .,0
29096,"For reasons of clarity , we pretend that all PropBank triples are of the form ( e 1 , V , e 2 ) .",0
29097,"In practice , we take the argument numberings of the parser into account and only compare like with like , except in cases such as the permuted frame rule , where ordering is relaxed .",0
29098,"In the case of multiple possible answers from a single rule , we randomly choose one .",0
29099,Word Distance Benchmark,0
29100,We consider another baseline that relies on word distance measurements .,0
29101,"Here , we align the placeholder of the Cloze form question with each possible entity in the context document and calculate a distance measure between the question and the context around the aligned entity .",0
29102,"This score is calculated by summing the distances of every word in q to their nearest aligned word ind , where alignment is defined by matching words either directly or as aligned by the coreference system .",0
29103,We tune the maximum penalty per word ( m = 8 ) on the validation data .,0
29104,Neural Network Models,0
29105,Neural networks have successfully been applied to a range of tasks in NLP .,0
29106,"This includes classification tasks such as sentiment analysis or POS tagging , as well as generative problems such as language modelling or machine translation .",0
29107,We propose three neural models for estimating the probability of word type a from document d answering query q:,0
29108,"where V is the vocabulary 4 , and W ( a ) indexes row a of weight matrix W and through a slight abuse of notation word types double as indexes .",0
29109,"Note that we do not privilege entities or variables , the model must learn to differentiate these in the input sequence .",0
29110,"The function g ( d , q ) returns a vector embedding of a document and query pair .",0
29111,The Deep LSTM,0
29112,"Reader Long short - term memory ( LSTM , ) networks have recently seen considerable success in tasks such as machine translation and language modelling .",0
29113,"When used for translation , Deep LSTMs have shown a remarkable ability to embed long sequences into a vector representation which contains enough information to generate a full translation in another language .",0
29114,Our first neural model for reading comprehension tests the ability of Deep LSTM encoders to handle significantly longer sequences .,0
29115,"We feed our documents one word at a time into a Deep LSTM encoder , after a delimiter we then also feed the query into the encoder .",0
29116,Alternatively we also experiment with processing the query then the document .,0
29117,The result is that this model processes each document query pair as a single long sequence .,0
29118,Given the embedded document and query the network predicts which token in the document answers the query .,0
29119,"We employ a Deep LSTM cell with skip connections from each input x ( t ) to every hidden layer , and from every hidden layer to the output y(t ) :",0
29120,x,0
29121,"y ( t , k ) = W ky h(t , k ) + b ky where || indicates vector concatenation h(t , k ) is the hidden state for layer k at time t , and i , f , o are the input , forget , and output gates respectively .",0
29122,"Thus our Deep LSTM Reader is defined by g LSTM ( d , q ) = y (| d| + | q | ) with input x ( t ) the concatenation of d and q separated by the delimiter |||.",0
29123,The Attentive Reader The Deep LSTM,0
29124,Reader must propagate dependencies overlong distances in order to connect queries to their answers .,0
29125,The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition .,0
29126,This attention model first encodes the document and the query using separate bidirectional single layer LSTMs .,0
29127,We denote the outputs of the forward and backward LSTMs as ? ? y ( t ) and ? ? y ( t ) respectively .,0
29128,"The encoding u of a query of length | q | is formed by the concatenation of the final forward and backward outputs , u = ? ? y q ( | q | ) || ? ? y q ( 1 ) .",0
29129,"For the document the composite output for each token at position t is , yd ( t ) = ? ? yd ( t ) || ? ? yd ( t ) .",0
29130,The representation r of the document dis formed by a weighted sum of these output vectors .,0
29131,"These weights are interpreted as the degree to which the network attends to a particular token in the document when answering the query : m ( t ) = tanh ( W ym yd ( t ) + W um u ) , s ( t ) ? exp ( w ms m ( t ) ) , r = yd s , where we are interpreting yd as a matrix with each column being the composite representation yd ( t ) of document token t.",0
29132,The variable s ( t ) is the normalised attention at token t.,0
29133,Given this attention score the embedding of the document r is computed as the weighted sum of the token embeddings .,0
29134,The model is completed with the definition of the joint document and query embedding via a nonlinear combination :,0
29135,"g AR ( d , q ) = tanh ( W rg r + W ug u ) .",0
29136,The Attentive Reader can be viewed as a generalisation of the application of Memory Networks to question answering .,0
29137,That model employs an attention mechanism at the sentence level where each sentence is represented by a bag of embeddings .,0
29138,The Attentive Reader employs a finer grained token level attention mechanism where the tokens are embedded given their entire future and past context in the input document .,0
29139,The Impatient Reader,0
29140,The Attentive Reader is able to focus on the passages of a context document that are most likely to inform the answer to the query .,0
29141,We can go further by equipping the model with the ability to reread from the document as each query token is read .,0
29142,At each token i of the query q the model computes a document representation vector r ( i ) using the bidirectional embedding y q,0
29143,"The result is an attention mechanism that allows the model to recurrently accumulate information from the document as it sees each query token , ultimately outputting a final joint document query representation for the answer prediction , g IR ( d , q ) = tanh ( W rg r ( | q | ) + W qg u ) .",0
29144,Empirical Evaluation,0
29145,"Having described a number of models in the previous section , we next evaluate these models on our reading comprehension corpora .",0
29146,Our hypothesis is that neural models should in principle be well suited for this task .,0
29147,"However , we argued that simple recurrent models such as the LSTM probably have insufficient expressive power for solving tasks that require complex inference .",0
29148,We expect that the attention - based models would therefore outperform the pure LSTM - based approaches .,1
29149,"Considering the second dimension of our investigation , the comparison of traditional versus neural approaches to NLP , we do not have a strong prior favouring one approach over the other .",0
29150,"While numerous publications in the past few years have demonstrated neural models outperforming classical methods , it remains unclear how much of that is a side - effect of the language modelling capabilities intrinsic to any neural model for NLP .",0
29151,"The entity anonymisation and permutation aspect of the task presented here may end up levelling the playing field in that regard , favouring models capable of dealing with syntax rather than just semantics .",0
29152,"With these considerations in mind , the experimental part of this paper is designed with a threefold aim .",0
29153,"First , we want to establish the difficulty of our machine reading task by applying a wide range of models to it .",0
29154,"Second , we compare the performance of parse - based methods versus that of neural models .",0
29155,"Third , within the group of neural models examined , we want to determine what each component contributes to the end performance ; that is , we want to analyse the extent to which an LSTM can solve this task , and to what extent various attention mechanisms impact performance .",0
29156,All model hyperparameters were tuned on the respective validation sets of the two corpora .,0
29157,Our experimental results are in Frame - semantic benchmark,0
29158,"While the one frame - semantic model proposed in this paper is clearly a simplification of what could be achieved with annotations from an NLP pipeline , it does highlight the difficulty of the task when approached from a symbolic NLP perspective .",0
29159,Two issues standout when analysing the results in detail .,0
29160,"First , the frame - semantic pipeline has a poor degree of coverage with many relations not being picked up by our PropBank parser as they do not adhere to the default predicate - argument structure .",0
29161,This effect is exacerbated by the type of language used in the highlights that form the basis of our datasets .,0
29162,"The second issue is that the frame - semantic approach does not trivially scale to situations where several sentences , and thus frames , are required to answer a query .",0
29163,This was true for the majority of queries in the dataset .,0
29164,Word distance benchmark,1
29165,"More surprising perhaps is the relatively strong performance of the word distance benchmark , particularly relative to the frame - semantic benchmark , which we had expected to perform better .",1
29166,"Here , again , the nature of the datasets used can explain aspects of this result .",0
29167,"Where the frame - semantic model suffered due to the language used in the highlights , the word distance model benefited .",0
29168,"Particularly in the case of the Daily Mail dataset , highlights frequently have significant lexical overlap with passages in the accompanying article , which makes it easy for the word distance benchmark .",0
29169,"For instance the query "" Tom Hanks is friends with X 's manager , Scooter Brown "" has the phrase "" ... turns out he is good friends with Scooter Brown , manager for Carly Rae Jepson "" in the context .",0
29170,The word distance benchmark correctly aligns these two while the frame - semantic approach fails to pickup the friendship or management relations when parsing the query .,0
29171,We expect that on other types of machine reading data where questions rather than Cloze queries are used this particular model would perform significantly worse .,0
29172,Neural models,1
29173,"Within the group of neural models explored here , the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models .",1
29174,This is consistent with our hypothesis that attention is a key ingredient for machine reading and question answering due to the need to propagate information overlong distances .,0
29175,The Deep LSTM,1
29176,"Reader performs surprisingly well , once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences , even when they are up to two thousand tokens in length .",1
29177,"However this model does fail to match the performance of the attention based models , even though these only use single layer LSTMs .",0
29178,The poor results of the Uniform Reader support our hypothesis of the significance of the attention mechanism in the Attentive model 's performance as the only difference between these models is that the attention variables are ignored in the Uniform Reader .,0
29179,The precision@recall statistics in again highlight the strength of the attentive approach .,0
29180,We can visualise the attention mechanism as a heatmap over a context document to gain further insight into the models ' performance .,0
29181,The highlighted words show which tokens in the document were attended to by the model .,0
29182,In addition we must also take into account that the vectors at each . . .,0
29183,. . . token integrate long range contextual information via the bidirectional LSTM encoders .,0
29184,depicts heat maps for two queries that were correctly answered by the Attentive Reader .,0
29185,"In both cases confidently arriving at the correct answer requires the model to perform both significant lexical generalsiation , e.g. ' killed ' ? ' deceased ' , and co-reference or anaphora resolution , e.g. ' ent119 was killed ' ?",0
29186,' he was identified . ',0
29187,However it is also clear that the model is able to integrate these signals with rough heuristic indicators such as the proximity of query words to the candidate answer .,0
29188,Conclusion,0
29189,The supervised paradigm for training machine reading and comprehension models provides a promising avenue for making progress on the path to building full natural language understanding systems .,0
29190,We have demonstrated a methodology for obtaining a large number of document - queryanswer triples and shown that recurrent and attention based neural networks provide an effective modelling framework for this task .,0
29191,Our analysis indicates that the Attentive and Impatient Readers are able to propagate and integrate semantic information overlong distances .,0
29192,In particular we believe that the incorporation of an attention mechanism is the key contributor to these results .,0
29193,The attention mechanism that we have employed is just one instantiation of a very general idea which can be further exploited .,0
29194,"However , the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size .",0
29195,There are still many queries requiring complex inference and long range reference resolution that our models are not yet able to answer .,0
29196,As such our data provides a scalable challenge that should support NLP research into the future .,0
29197,"Further , significantly bigger training data sets can be acquired using the techniques we have described , undoubtedly allowing us to train more expressive and accurate models .",0
29198,"Note that these examples were chosen as they were short , the average CNN validation document contained 763 tokens and 27 entities , thus most instances were significantly harder to answer than these examples .",0
29199,A Model hyperparameters,0
29200,The precise hyperparameters used for the various attentive models are as in .,0
29201,All models were trained using asynchronous RmsProp with a momentum of 0.9 and a decay of 0.95 .,0
29202,Model,0
29203,Hidden,0
29204,B Performance across document length,0
29205,"To understand how the model performance depends on the size of the context , we plot performance versus document lengths in Figures 4 and 5 .",0
29206,"The first figure ) plots a sliding window of performance across document length , showing that performance of the attentive models degrades slightly as documents increase in length .",0
29207,"The second figure shows the cumulative performance with documents up to length N , showing that while the length does impact the models ' performance , that effect becomes negligible after reaching a length of ~ 500 tokens .",0
29208,C Additional Heatmap Analysis,0
29209,We expand on the analysis of the attention mechanism presented in the paper by including visualisations for additional queries from the CNN validation dataset below .,0
29210,We consider examples from the Attentive Reader as well as the Impatient Reader in this appendix .,0
29211,C.1 Attentive Reader,0
29212,Positive Instances shows two positive examples from the CNN validation set that require reasonable levels of lexical generalisation and co-reference in order to be answered .,0
29213,"The first query in contains strong lexical cues through the quote , but requires identifying the entity quoted , which is non-trivial in the context document .",0
29214,The final positive example ( also in ) demonstrates the fearlessness of our model . :,0
29215,Attention heat maps from the Attentive Reader for two more correctly answered validation set queries .,0
29216,"Both examples require significant lexical generalisation and co-reference resolution to find the correct answers ent201 and ent214 , respectively .",0
29217,show examples of queries where the Attentive Reader fails to select the correct answer .,0
29218,"The two examples in highlight a fairly common phenomenon in the data , namely ambiguous queries , where - at least following the anonymisation processmultiple entities are plausible answers even when evaluated manually .",0
29219,"Note that in both cases the query searches for an entity marker that describes a geographic location , preceded by the word "" in "" .",0
29220,Negative Instances,0
29221,"Here it is unclear whether the placeholder refers to apart of town , town , region or country .",0
29222,contains two additional negative cases .,0
29223,The first failure is caused by the co-reference entity selection process .,0
29224,"The correct entity , ent15 , and the predicted one , ent81 , both refer to the same person , but not being clustered together .",0
29225,"Arguably this is a difficult clustering as one entity refers to "" Kate Middleton "" and the other to "" The Duchess of Cambridge "" .",0
29226,"The right example shows a situation in which the model fails as it perhaps gets too little information from the short query and then selects the wrong cue with the term "" claims "" near the wrongly identified entity ent1 ( correct : ent74 ) .",0
29227,C.2 Impatient Reader,0
29228,"To give a better intuition for the behaviour of the Impatient Reader , we use a similar visualisation technique as before .",0
29229,"However , this time around we highlight the attention at every time step as the model updates it s focus while moving through a given query .",0
29230,shows how the attention of the Impatient Reader changes and becomes increasingly more accurate as the model :,0
29231,Two more correctly answered validation set queries .,0
29232,"The left example ( entity ent315 ) requires correctly attributing the quote , which does not appear trivial with a number of other candidate entities in the vicinity .",0
29233,The right hand side shows our model is not afraid of Chuck Norris ( en t 164 ) . :,0
29234,Attention heat maps from the Attentive Reader for two wrongly answered validation set queries .,0
29235,"In the left case the model returns ent85 ( correct : ent67 ) , in the right example it gives ent24 ( correct : ent64 ) .",0
29236,In both cases the query is unanswerable due to its ambiguous nature and the model selects a plausible answer .,0
29237,considers larger parts of the query .,0
29238,"Note how the attention is distributed fairly arbitraty at first , slowly focussing on the correct entity ent 5 only once the question has sufficiently been parsed .",0
29239,title,0
29240,U - Net : Machine Reading Comprehension with Unanswerable Questions,1
29241,abstract,0
29242,Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing .,0
29243,A key subtask is to reliably predict whether the question is unanswerable .,0
29244,"In this paper , we propose a unified model , called U - Net , with three important components : answer pointer , no - answer pointer , and answer verifier .",0
29245,We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens .,0
29246,"The universal node encodes the fused information from both the question and passage , and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U - Net.",0
29247,"Different from the stateof - art pipeline models , U - Net can be learned in an end - to - end fashion .",0
29248,The experimental results on the SQuAD 2.0 dataset show that U - Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0 .,0
29249,Introduction,0
29250,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",1
29251,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .",1
29252,"The best systems have even surpassed human performance on the Stanford Question Answering Dataset ( SQuAD ) , one of the most widely used MRC benchmarks .",0
29253,"However , one of the limitations of the SQuAD task is that each question has a correct answer in the context passage , therefore most models just need to select the most relevant text span as the answer , without necessarily checking whether it is indeed the answer to the question .",0
29254,"To remedy the deficiency of SQuAD , developed SQuAD 2.0 that combines SQuAD with new unanswerable questions .",0
29255,shows two examples of unanswerable questions .,0
29256,The new dataset requires the MRC systems to know what they do n't know .,0
29257,"To do well on MRC with unanswerable questions , the model needs to comprehend the question , reason among the * Corresponding Author .",0
29258,"Article : Endangered Species Act Paragraph : "" ...",0
29259,"Other legislation followed , including the Migratory Bird Conservation Act of 1929 , a 1937 treaty prohibiting the hunting of right and gray whales , and the Bald Eagle Protection Act of 1940 .",0
29260,These later laws had a low cost to society the species were relatively rareand little opposition was raised .,0
29261,Question,0
29262,1 : Which laws faced significant opposition ?,0
29263,Plausible Answer : later laws Question,0
29264,2 : What was the name of the 1937 treaty ?,0
29265,Plausible Answer : Bald Eagle Protection,0
29266,"Act passage , judge the unanswerability and then identify the answer span .",0
29267,"Since extensive work has been done on how to correctly predict the answer span when the question is answerable ( e.g. , SQuAD 1.1 ) , the main challenge of this task lies in how to reliably determine whether a question is not answerable from the passage .",0
29268,There are two kinds of approaches to model the answerability of a question .,0
29269,One approach is to directly extend previous MRC models by introducing a no-answer score to the score vector of the answer span .,0
29270,But this kind of approaches is relatively simple and can not effectively model the answerability of a question .,0
29271,Another approach introduces an answer verifier to determine whether the question is unanswerable ) .,0
29272,"However , this kind of approaches usually has a pipeline structure .",0
29273,"The answer pointer and answer verifier have their respective models , which are trained separately .",0
29274,"Intuitively , it is unnecessary since the underlying comprehension and reasoning of language for these components is the same .",0
29275,"In this paper , we decompose the problem of MRC with unanswerable questions into three sub - tasks : answer pointer , no - answer pointer , and answer verifier .",1
29276,"Since these three sub - tasks are highly related , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some meta-knowledge .",0
29277,We propose the U - Net to incorporate these three sub - tasks into a unified model :,1
29278,"1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",1
29279,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .",1
29280,The universal node acts on both question and passage to learn whether the question is answerable .,1
29281,"Different from the previous pipeline models , U - Net can be learned in an end - to - end fashion .",0
29282,Our experimental results on the SQuAD 2.0 dataset show that U - Net effectively predicts the unanswerability of questions and achieves an F1 score of 72.6 .,0
29283,The contributions of this paper can be summarized as follows .,0
29284,"We decompose the problem of MRC with unanswerable questions into three sub - tasks and combine them into a unified model , which uses the shared encoding and interaction layers .",0
29285,"Thus , the three - tasks can be trained simultaneously in an end - to - end fashion .",0
29286,We introduce a universal node to encode the common information of the question and passage .,0
29287,"Thus , we can use a unified representation to model the question and passage , which makes our model more condensed . U - Net is very easy to implement yet effective .",0
29288,Proposed Model,0
29289,"Formally , we can represent the MRC problem as : given a set of tuples ( Q , P , A ) , where Q = ( q 1 , q 2 , , q m ) is the question with m words , P = ( p 1 , p 2 , , p n ) is the context passage with n words , and A = p rs : re is the answer with r sand re indicating the start and end points , the task is to estimate the conditional probability P ( A|Q , P ) .",0
29290,The architecture of our proposed U - Net is illustrated in .,0
29291,"U - Net consists of four major blocks : Unified Encoding , Multi - Level Attention , Final Fusion , and Prediction .",0
29292,"As shown in , we first combine the embedded representation of the question and passage with a universal node u and pass them through a BiLSTM to encode the whole text .",0
29293,We then use the encoded representation to do the information interaction .,0
29294,Then we use the encoded and interacted representation to fuse the full representation and feed them into the final prediction layers to do the multi -task training .,0
29295,We will describe our model in details in the following .,0
29296,( A ) Unified Encoding,0
29297,"Embedding Following the successful models on SQuAD 1.1 , we first embed both the question and the passage with the following features .",0
29298,"Glove embedding ( Pennington , Socher , and Manning 2014 ) and Elmo embedding ) are used as basic embeddings .",0
29299,"Besides , we use POS embedding , NER embedding , and a feature embedding that includes the exact match , lower - case match , lemma match , and a TF - IDF feature ) .",0
29300,represented as a d-dim embedding by combining the features / embedding described above .,0
29301,Universal Node,0
29302,"We create a universal node u , which is a key factor in our model and has several roles in predicting the unanswerability of question Q .",0
29303,We expect this node to learn universal information from both passage and question .,0
29304,"This universal node is added and connects the passage and question at the phase of embedding , and then goes along with the whole representation , so it is a key factor in information representation .",0
29305,"Since the universal node is in between and later shared between passage and question , it has an abstract semantic meaning rather than just a word embedding .",0
29306,"Also , the universal node is later shared in the attention interaction mechanism and used in both the answer boundary detection and classification tasks , so this node carries massive information and has several important roles in our whole model construction .",0
29307,The universal node u is first represented by a d -dim randomly - initialized vector .,0
29308,"We concatenated question representation , universal node representation , passage representation together as :",0
29309,is a joint representation of question and passage .,0
29310,Word - level Fusion,0
29311,"Then we first use two - layer bidirectional LSTM ( BiLSTM ) to fuse the joint representation of question , universal node , and passage .",0
29312,"where H l is the hidden states of the first BiLSTM , representing the low - level semantic information , and H h is the hidden states of the second BiLSTM , representing the highlevel semantic information .",0
29313,"Finally , we concatenate H land H h together and pass them through the third BiLSTM and obtain a full representation Hf .",0
29314,"Thus , H = [ H l ; H h ; Hf ] represents the deep fusion information of the question and passage on word - level .",0
29315,"When a BiLSTM is applied to encode representations , it learns the semantic information bi-directionally .",0
29316,"Since the universal node u is between the question and passage , its hidden states h m + 1 can learn both question and passage information .",0
29317,"When the passage - question pair was encoded as a unified representation and information flows via the BiLSTM , the universal node has an important role in information representation .",0
29318,( B ) Multi - Level Attention,0
29319,"To fully fuse the semantic representation of the question and passage , we use the attention mechanism ( Bahdanau , Cho , and Bengio 2014 ) to capture their interactions on different levels .",0
29320,"We expected that we could simply use self - attention on the encoded representation H for interaction between question and passage , which contains both bi-attention ) and self - attention ) of the question and passage .",0
29321,But we found that it performed slightly worse than the traditional bi-directional attention with the universal node included .,0
29322,"Therefore , we use a bi-directional attention between the question and passage .",0
29323,"We first divide H into two representations : attached passage H q and attached question H p , and let the universal node representation h m + 1 attached to both the passage and question , i.e. ,",0
29324,Note h m + 1 is shared by H q and H p .,0
29325,"Here the universal node works as a special information carrier , and both passage and question can focus attention information on this node so that the connection between them is closer than a traditional biattention interaction .",0
29326,Since both,0
29327,"H q = [ H l q ; H h q ; Hf q ] and H p = [ H l p ; H hp ; Hf p ] are concatenated by three - level representations , we followed previous work FusionNet to construct their iterations on three levels .",0
29328,Take the first level as an example .,0
29329,We first compute the affine matrix of H l q and H l p by,0
29330,where S ?,0
29331,R ( m+ 1 ) ( n + 1 ) ; W 1 and W 2 are learnable parameters .,0
29332,"Next , a bi-directional attention is used to compute the interacted representation H l q and H l p .",0
29333,where softmax ( ) is column - wise normalized function .,0
29334,"We use the same attention layer to model the interactions for all the three levels , and get the final fused representation H l , H h , Hf for the question and passage respectively .",0
29335,"Note that while dealing with the attention output of the universal node , we added two outputs from passage - toquestion attention and question - to - passage attention .",0
29336,"So after the interaction , the fused representation H l , H h , Hf still have the same length as the encoded representation H l , H hand Hf .",0
29337,( C ) Final Fusion,0
29338,"After the three - level attentive interaction , we generate the final fused information for the question and passage .",0
29339,"We concatenate all the history information : we first concatenate the encoded representation H and the representation after attention H ( again , we use H l , H h , Hf , and H l , H h , Hf to represent 3 different levels of representation for the two previous steps respectively ) .",0
29340,"Following the success of DenseNet ( Huang , Liu , and Weinberger 2016 ) , we concatenate the input and output of each layer as the input of the next layer .",0
29341,"First , we pass the concatenated representation H through a BiLSTM to get H A .",0
29342,where the representation H A is a fusion of information from different levels .,0
29343,"Then we concatenate the original embedded representation V and H A for better representation of the fused information of passage , universal node , and question .",0
29344,"Finally , we use a self - attention layer to get the attention information within the fused information .",0
29345,The self - attention layer is constructed the same way as :,0
29346,where A is the representation after self - attention of the fused information A .,0
29347,Next we concatenated representation H A and A and pass them through another BiLSTM layer :,0
29348,Now,0
29349,O is the final fused representation of all the information .,0
29350,"At this point , we divide O into two parts : O P , O Q , representing the fused information of the question and passage respectively .",0
29351,"Note for the final representation , we attach the universal node only in the passage representation O P .",0
29352,This is because we need the universal node as a focus for the pointer when the question is unanswerable .,0
29353,These will be fed into the next decoder prediction layer .,0
29354,( D ) Prediction,0
29355,"The prediction layer receives fused information of passage O P and question O Q , and tackles three prediction tasks :",0
29356,"( 1 ) answer pointer , ( 2 ) no-answer pointer and answer verifier .",0
29357,"First , we use a function shown below to summarize the question information O Q into a fixed - dim representation c q .",0
29358,where W q is a learnable weight matrix and o Q i represents the i th word in the question representation .,0
29359,"Then we feed c q into the answer pointer to find boundaries of answers , and the classification layer to distinguish whether the question is answerable .",0
29360,( i ) Answer Pointer,0
29361,"We use this answer pointer to detect the answer boundaries from the passage when the question is answerable ( i.e. , the answer is a span in the passage ) .",0
29362,This layer is a classic pointer net structure .,0
29363,"We use two trainable matrices W sand W e to estimate the probability of the answer start and end boundaries of the i th word in the passage , ? i and ?",0
29364,i .,0
29365,"Note here when the question is answerable , we do not consider the universal node in answer boundary detection , so we have i > 0 ( i = 0 is the universal node in the passage representation ) .",0
29366,The loss function for the answerable question pairs is :,0
29367,where a and bare the ground - truth of the start and end boundary of the answer .,0
29368,( ii ) No- Answer Pointer,0
29369,Then we use the same pointer for questions that are not answerable .,0
29370,Here the loss L N,0
29371,A is :,0
29372,?,0
29373,0 and ?,0
29374,"0 correspond to the position of the universal node , which is at the front of the passage representation O p .",0
29375,"For this scenario , the loss is calculated for the universal node .",0
29376,"Additionally , since there exits a plausible answer for each unanswerable question in SQuAD 2.0 , we introduce an auxiliary plausible answer pointer to predict the boundaries of the plausible answers .",0
29377,"The plausible answer pointer has the same structure as the answer pointer , but with different parameters .",0
29378,"Thus , the total loss function is :",0
29379,where ? and ?,0
29380,are the output of the plausible answer pointer ; a * and b * are the start and end boundary of the unanswerable answer .,0
29381,The no-answer pointer and plausible answer pointer are removed attest phase .,0
29382,( iii ) Answer Verifier,0
29383,We use the answer verifier to distinguish whether the question is answerable .,0
29384,Answer verifier applies a weighted summary layer to summarize the passage information into a fixed - dim representation c q ( as shown in Eq. ) .,0
29385,And we use the weight matrix obtained from the answer pointer to get two representations of the passage .,0
29386,Then we use the universal node o m + 1 and concatenate it with the summary of question and passage to make a fixed vector,0
29387,"This fixed F includes the representation c q representing the question information , and c sand c e representing the passage information .",0
29388,"Since these representations are highly summarized specially for classification , we believe that this passage - question pair contains information to distinguish whether this question is answerable .",0
29389,"In addition , we include the universal node as a supplement .",0
29390,"Since the universal node is pointed at when the question is unanswerable and this node itself already contains information collected from both the passage and question during encoding and information interaction , we believe that this node is important in distinguishing whether the question is answerable .",0
29391,"Finally , we pass this fixed vector F through a linear layer to obtain the prediction whether the question is answerable .",0
29392,where ?,0
29393,"is a sigmoid function , W f is a learnable weight matrix .",0
29394,Here we use the cross - entropy loss in training .,0
29395,where ? ?,0
29396,"{ 0 , 1 } indicates whether the question has an answer in the passage .",0
29397,"Compared with other relatively complex structures developped for this MRC task , our U - Net model passes the original question and passage pair through embedding and encoding , which then interacts with each other , yielding fused information merged from all the levels .",0
29398,The entire architecture is very easy to construct .,0
29399,"After we have the fused representation of the question and passage , we pass them through the pointer layer and a fused information classification layer in a multi -task setup .",0
29400,Training,0
29401,We jointly train the three tasks by combining the three loss functions .,0
29402,The final loss function is :,0
29403,where ? ?,0
29404,"{ 0 , 1 } indicates whether the question has an answer in the passage , LA , L N A and L AV are the three loss functions of the answer pointer , no - answer pointer , and answer verifier .",0
29405,"Although the three tasks could have different weights in the final loss function and be further fine - tuned after joint training , here we just consider them in the same weight and do not fine - tune them individually .",0
29406,"At the test phase , we first use the answer pointer to find a potential answer to the question , while the verifier layer judges whether the question is answerable .",0
29407,"If the classifier predicts the question is unanswerable , we consider the answer extracted by the answer pointer as plausible .",0
29408,"In this way , we get the system result .",0
29409,Experiment Datasets,0
29410,"Recently , machine reading comprehension and question answering have progressed rapidly , owing to the computation ability and publicly available high - quality datasets such as SQuAD .",0
29411,"Now new research efforts have been devoted to the newly released answer extraction test with unanswerable questions , SQ u AD 2.0 .",0
29412,It is constructed by combining question - answer pairs selected from SQuAD 1.0 and newly crafted unanswerable questions .,0
29413,These unanswerable questions are created by workers that were asked to pose questions that can not be answered based on the paragraph alone but are similar to the answerable questions .,0
29414,It is very difficult to distinguish these questions from the answerable ones .,0
29415,We evaluate our model using this data set .,0
29416,"It contains over 100,000 + questions on 500 + wikipedia articles .",0
29417,Implementation Details,0
29418,"We use Spacy to process each question and passage to obtain tokens , POS tags , NER tags and lemmas tags of each text .",1
29419,"We use 12 dimensions to embed POS tags , 8 for NER tags .",1
29420,"We use 3 binary features : exact match , lower - case match and lemma match between the question and passage .",1
29421,We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .,1
29422,All the LSTM blocks are bi-directional with one single layer .,1
29423,"We set the hidden layer dimension as 125 , attention layer dimension as 250 .",1
29424,"We added a dropout layer overall the modeling layers , including the embedding layer , at a dropout rate of 0.3 .",1
29425,We use Adam optimizer with a learning rate of 0.002 ( Kingma and Ba 2014 ) .,1
29426,"During training , we omit passage with over 400 words and question with more than 50 words .",0
29427,"For testing , when the passage has over 600 words and the question is over 100 words , we simply label these questions as unanswerable .",0
29428,Main Results,0
29429,"Our model achieves an F 1 score of 74.0 and an EM score of 70.3 on the development set , and an F 1 score of 72.6 and an EM score of 69.2 on Test set 1 , as shown in .",1
29430,Our model outperforms most of the previous approaches .,1
29431,"Comparing to the best - performing systems , our model has a simple architecture and is an end - to - end model .",1
29432,"In fact , among all the end - to - end models , we achieve the best F1 scores .",1
29433,We believe that the performance of the U - Net can be boosted with an additional post -processing step to verify answers using approaches such as .,0
29434,Ablation Study,0
29435,We also do an ablation study on the SQuAD 2.0 development set to further test the effectiveness of different components in our model .,0
29436,"In , we show four different configurations .",0
29437,"First , we remove the universal node U .",0
29438,We let the negative examples focus on the plausible answer spans instead of focusing on the universal node U .,0
29439,"This results in a loss of 2.6 % F1 score on the development set , showing that the universal node U indeed learns information about whether the question is answerable .",0
29440,We also tried to make the universal node U only attached to the passage representation when passing the attention layer .,0
29441,"Our results showed that when node U is shared , as it is called ' universal ' , it learns information interaction between the question and passage , and when it is not shared , the performance slightly degraded .",1
29442,"As for the approaches to encode the representations , we pass both the question and passage through a shared BiL - STM .",0
29443,"To test the effectiveness of this , we ran the experiment using separate BiLSTMs on embedded question and passage representations .",0
29444,"Results show that the performance dropped slightly , suggesting sharing BiLSTM is an effective method to improve the quality of the encoder .",1
29445,"After removing the plausible answer pointer , the performance also dropped , indicating the plausible answers are useful to improve the model even though they are incorrect .",1
29446,"After removing the answer verifier , the performance dropped greatly , indicating it is vital for our model .",1
29447,"Lastly , we run a test using a more concise configuration .",0
29448,"In the second block ( multi - level attention ) of the U - Net , we do not split the output of the encoded presentation and let it pass through a self - attention layer .",1
29449,The bidirectional attention is removed .,0
29450,"In this way , our model uses only one unified - 0.7 - 1.1 no classification 63.5 68.5 - 6.8 - 5.5",0
29451,Self - Attn Only 69.7 73.5 - 0.5,0
29452,- 0.5 : Comparison of different configurations for our U - Net model .,0
29453,representation of the question and passage at all time .,0
29454,We simply pass this representation layer by layer to get the final result .,0
29455,"Compared to the bi-attention model , the F1 - score decreases 0.5 % .",0
29456,Multi- task Study,0
29457,We also run an experiment to test the performance of our multi -task model .,0
29458,We select different losses that participate in the training procedure to observe the performance affected by answer boundary detector classification .,0
29459,shows the performance .,0
29460,"Here we use EM * and F 1 * to represent the EM and F 1 score when the classification is not part of the task , which makes it very much like the task in SQuAD 1.1 .",0
29461,Loss EM * F1 * Classification,0
29462,"Acc . To test our classifier performance , we do not use backward propagation over the loss of answer boundary detection and simply run a classification task .",0
29463,Results ( the first two rows in ) show that there is a large gain when using the multi - task model .,0
29464,"The answer boundary detection task helps the encoder learn information between the passage and question and also feed information into the universal node , therefore we can use a summarized representation of the passage and question as well as the universal node to distinguish whether the question is answerable , i.e. , help improve classification .",0
29465,"For the answer boundary detection task , we find that the multi -task setup ( i.e. , the classification layer participates in the training process ) does not help its performance .",0
29466,"Since the classifier and pointer layer shared the encoding process , we originally expected that classification information can help detect answer boundaries .",0
29467,But this is not the case .,0
29468,"We think this is also reasonable since distinguishing whether the question is answerable is mainly focusing on the interactions between the passage - question pair , so once the question is predicted as answerable or not , it has nothing to do with the answer boundaries .",0
29469,This is consistent with how human - beings do this classification task .,0
29470,We also run the test over SQuAD 1.1 development test to evaluate the performance .,0
29471,"Due to a condensed structure , our model achieves an F 1 * score of less than 86 % , which is not a very competitive score on SQ uAD 1.1 test .",0
29472,"But as shown above , our model achieves a good score in SQuAD 2.0 test , which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks .",0
29473,"Overall , we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously .",0
29474,Study on the Different Thresholds of Unanswerability Classification,0
29475,The output b of the answer verifier is the probability of a question being unanswerable .,0
29476,"The smaller the output , the lower the probability of unanswerability is .",0
29477,"In SQuAD 2.0 , the proportions of unanswerable questions are different in the training and test sets .",0
29478,"The default threshold 0.5 is optimized on the training set , but not suitable for the test set .",0
29479,"Therefore , it is reasonable to set a proper threshold to manually adapt to the test set .",0
29480,"As mentioned in SQuAD 2.0 paper , different thresholds for answerability prediction result in fluctuated scores between answerable and unanswerable questions .",0
29481,"Here we show the variation of the F 1 score with different thresholds in The threshold between [ 0 , 1 ] is used to decide whether a question can be answered .",0
29482,"When the threshold is set to 0 , all questions are considered as answerable .",0
29483,"As we can see , when the threshold is set to 0.5 , F1 score of answerable questions is similar to that of unanswerable questions .",0
29484,"When we increase the threshold ( i.e. , more likely to predict the question as unanswerable ) , performance for answerable questions degrades , and improves for unanswerable questions .",0
29485,This is as expected .,0
29486,"We can see that the overall F 1 score is slightly better , which is consistent with the idea from SQ uAD 2.0 .",0
29487,"In addition , we find that for larger thresholds , the variance between EM and F 1 is narrowed since EM and F 1 scores for unanswerable questions are the same .",0
29488,"Finally , we set the threshold to be 0.7 for the submission system to SQuAD evaluation .",0
29489,Related Work,0
29490,End - to - end Models for MRC,0
29491,"Currently , end - to - end neural network models have achieved great successes for machine reading comprehension .",0
29492,"Most of these models consist of three components : encoder , interaction , and pointer .",0
29493,The BiLSTM is widely used for encoding the embedded representation .,0
29494,"For the interaction , bidirectional attention mechanism is very effective to fuse information of the question and passage .",0
29495,"Finally , a pointer network is used to predict the span boundaries of the answer .",0
29496,"Specifically , in SQuAD test , there are approaches to combine match - LSTM and pointer networks to produce boundaries of the answer and employ variant bidirectional attention mechanism to match the question and passage mutually .",0
29497,"In our model , we learn from previous work and develop a condensed end - to - end model for the SQuAD 2.0 task .",0
29498,"Different from the previous models , we use a unified representation to encode the question and passage simultaneously , and introduce a universal node to encode the fused information of the question and passage , which also plays an important role to predict the unanswerability of a question .",0
29499,MRC with Unanswerable Questions,0
29500,MRC with unanswerable questions is a more challenging task .,0
29501,Previous work,0
29502,Levy et al. ; has attempted to normalize a no-answer score depending on the probability of all answer spans and still detect boundaries at the same time .,0
29503,But the scores of the answer span predictions are not very discriminative in distinguishing whether the question is answerable .,0
29504,"Therefore , this kind of approaches , though relatively simple , can not effectively deal with the answerability of a question .",0
29505,introduced an answer verifier idea to construct a classification layer .,0
29506,"However , this kind of approaches usually has a pipeline structure .",0
29507,The answer pointer and answer verifier have their respective models that are trained separately .,0
29508,Multi - task models,0
29509,"Different from existing work , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some metaknowledge .",0
29510,"Intuitively , answer prediction and answer verification are related tasks since the underlying comprehension and reasoning of language for these components is the same .",0
29511,"Therefore , we construct a multi - task model to solve three sub - tasks : answer pointer , no - answer pointer , and answer verifier .",0
29512,Conclusion and Future Work,0
29513,"In this paper , we regard the MRC with unanswerable questions as multi-task learning problems and propose the U - Net , a simple end - to - end model for MRC challenges .",0
29514,U - Net has good performance on SQ u AD 2.0 .,0
29515,"We first add a universal node to learn a fused representation from both the question and passage , then use a concatenated representation to pass through encoding layers .",0
29516,We only treat question and passage differently during attention interactions .,0
29517,"In the rest blocks of U - Net , we still use the unified representation containing both the question and passage representation .",0
29518,"Finally , we train the U - Net as a multi-task framework to determine the final answer boundaries as well as whether the question is answerable .",0
29519,Our model has very simple structure yet achieves good results on SQuAD 2.0 test .,0
29520,"Our future work is to reconstruct the structure of U - Net by replacing the current multi-level attention block with a simpler self - attention mechanism , which we believe can capture the question and passage information , and intuitively is also coherent with the rest of our U - Net model .",0
29521,"In addition , we will improve the answer boundary detection performance based on some of the previous successful models .",0
29522,"Since our model actually does not achieve very competitive performance in the boundary detection task yet still has a good overall performance on SQuAD 2.0 test , we are optimistic that our U - Net model is potentially capable of achieving better performance .",0
29523,"Furthermore , our model has a simple structure and is easy to implement , therefore we believe that our model can be easily modified for various datasets .",0
29524,title,0
29525,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,1
29526,abstract,0
29527,Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks .,0
29528,These models appear promising for applications such as language modeling and machine translation .,0
29529,"However , they scale poorly in both space and time as the amount of memory grows - limiting their applicability to real - world domains .",0
29530,"Here , we present an end - to - end differentiable memory access scheme , which we call Sparse Access Memory ( SAM ) , that retains the representational power of the original approaches whilst training efficiently with very large memories .",0
29531,"We show that SAM achieves asymptotic lower bounds in space and time complexity , and find that an implementation runs 1,000 faster and with 3,000 less physical memory than non-sparse models .",0
29532,"SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one - shot Omniglot character recognition , and can scale to tasks requiring 100,000s of time steps and memories .",0
29533,"As well , we show how our approach can be adapted for models that maintain temporal associations between memories , as with the recently introduced Differentiable Neural Computer .",0
29534,Introduction,0
29535,"Recurrent neural networks , such as the Long Short - Term Memory ( LSTM ) , have proven to be powerful sequence learning models .",0
29536,"However , one limitation of the LSTM architecture is that the number of parameters grows proportionally to the square of the size of the memory , making them unsuitable for problems requiring large amounts of long - term memory .",0
29537,"Recent approaches , such as Neural Turing Machines ( NTMs ) and Memory Networks , have addressed this issue by decoupling the memory capacity from the number of model parameters .",0
29538,We refer to this class of models as memory augmented neural networks ( MANNs ) .,1
29539,"External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .",0
29540,"Nonetheless , MANNs have had limited success in real world application .",0
29541,"A significant difficulty in training these models results from their smooth read and write operations , which incur linear computational overhead on the number of memories stored per time step of training .",0
29542,"Even worse , they require duplication of the entire memory at each time step to perform backpropagation through time ( BPTT ) .",0
29543,"To deal with sufficiently complex problems , such as processing a book , or Wikipedia , this overhead becomes prohibitive .",0
29544,"For example , to store 64 memories , a straightforward implementation of the NTM trained over a sequence of length 100 consumes ?",0
29545,"30 MiB physical memory ; to store 64,000 memories the overhead exceeds 29 GiB ( see ) .",0
29546,"In this paper , we present a MANN named SAM ( sparse access memory ) .",1
29547,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .",1
29548,"To test whether the model is able to learn with this sparse approximation , we examined its performance on a selection of synthetic and natural tasks : algorithmic tasks from the NTM work , Babi reasoning tasks used with Memory Networks and Omniglot one - shot classification .",0
29549,We also tested several of these tasks scaled to longer sequences via curriculum learning .,0
29550,"For large external memories we observed improvements in empirical run-time and memory overhead by up to three orders magnitude over vanilla NTMs , while maintaining near- identical data efficiency and performance .",0
29551,"Further , in Supplementary",0
29552,D we demonstrate the generality of our approach by describing how to construct a sparse version of the recently published Differentiable Neural Computer .,0
29553,"This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .",0
29554,Background,0
29555,Attention and content - based addressing,0
29556,An external memory M ?,0
29557,RN,0
29558,"M is a collection of N real - valued vectors , or words , of fixed size M .",0
29559,"A soft read operation is defined to be a weighted average over memory words ,",0
29560,where w ?,0
29561,RN is a vector of weights with non-negative entries that sum to one .,0
29562,Attending to memory is formalized as the problem of computing w.,0
29563,"A content addressable memory , proposed in , is an external memory with an addressing scheme which selects w based upon the similarity of memory words to a given query q .",0
29564,"Specifically , for the ith read weight w ( i ) we define ,",0
29565,"where dis a similarity measure , typically Euclidean distance or cosine similarity , and f is a differentiable monotonic transformation , typically a softmax .",0
29566,We can think of this as an instance of kernel smoothing where the network learns to query relevant points q.,0
29567,"Because the read operation ( 1 ) and content - based addressing scheme ( 2 ) are smooth , we can place them within a neural network , and train the full model using backpropagation .",0
29568,Memory Networks,0
29569,"One recent architecture , Memory Networks , make use of a content addressable memory that is accessed via a series of read operations and has been successfully applied to a number of question answering tasks .",0
29570,"In these tasks , the memory is pre-loaded using a learned embedding of the provided context , such as a paragraph of text , and then the controller , given an embedding of the question , repeatedly queries the memory by content - based reads to determine an answer .",0
29571,Neural Turing Machine,0
29572,"The Neural Turing Machine is a recurrent neural network equipped with a content - addressable memory , similar to Memory Networks , but with the additional capability to write to memory overtime .",0
29573,"The memory is accessed by a controller network , typically an LSTM , and the full model is differentiable - allowing it to be trained via BPTT .",0
29574,"A write to memory ,",0
29575,"consists of a copy of the memory from the previous time step M t?1 decayed by the erase matrix R t indicating obsolete or inaccurate content , and an addition of new or updated information At .",0
29576,The erase matrix R t = w W t e T t is constructed as the outer product between a set of write weights w W t ?,0
29577,"[ 0 , 1 ] N and erase vector e t ?",0
29578,"[ 0 , 1 ] M . The add matrix A T = w W ta T t is the outer product between the write weights and anew write word at ?",0
29579,"R M , which the controller outputs .",0
29580,Architecture,0
29581,"This paper introduces Sparse Access Memory ( SAM ) , anew neural memory architecture with two innovations .",0
29582,"Most importantly , all writes to and reads from external memory are constrained to a sparse subset of the memory words , providing similar functionality as the NTM , while allowing computational and memory efficient operation .",0
29583,"Secondly , we introduce a sparse memory management scheme that tracks memory usage and finds unused blocks of memory for recording new information .",0
29584,"For a memory containing N words , SAM executes a forward , backward step in ?( log N ) time , initializes in ? ( N ) space , and consumes ? ( 1 ) space per time step .",0
29585,"Under some reasonable assumptions , SAM is asymptotically optimal in time and space complexity ( Supplementary A ) .",0
29586,Read,0
29587,The sparse read operation is defined to be a weighted average over a selection of words in memory :,0
29588,wherew R t ?,0
29589,"RN contains K number of non-zero entries with indices s 1 , s 2 , . . . , s K ; K is a small constant , independent of N , typically K = 4 or K =",0
29590,"8 . We will refer to sparse analogues of weight vectors w asw , and when discussing operations that are used in both the sparse and dense versions of our model use w.",0
29591,We wish to constructw R t such thatr t ? rt .,0
29592,For content - based reads where w,0
29593,"R t is defined by , an effective approach is to keep the K largest non -zero entries and set the remaining entries to zero .",0
29594,We can computew R t naively in O ( N ) time by calculating w R t and keeping the K largest values .,0
29595,"However , linear - time operation can be avoided .",0
29596,"Since the K largest values in w R t correspond to the K closest points to our query qt , we can use an approximate nearest neighbor data - structure , described in Section 3.5 , to calculatew R tin O ( log N ) time .",0
29597,"Sparse read can be considered a special case of the matrix - vector product defined in ( 1 ) , with two key distinctions .",0
29598,"The first is that we pass gradients for only a constant K number of rows of memory per time step , versus N , which results in a negligible fraction of non-zero error gradient per timestep when the memory is large .",0
29599,"The second distinction is in implementation : by using an efficient sparse matrix format such as Compressed Sparse Rows ( CSR ) , we can compute ( 4 ) and its gradients inconstant time and space ( see Supplementary A ) .",0
29600,Write,0
29601,The write operation is SAM is an instance of ( 3 ) where the write weightsw W tare constrained to contain a constant number of non-zero entries .,0
29602,"This is done by a simple scheme where the controller writes either to previously read locations , in order to update contextually relevant memories , or the least recently accessed location , in order to overwrite stale or unused memory slots with fresh content .",0
29603,The introduction of sparsity could be achieved via other write schemes .,0
29604,"For example , we could use a sparse content - based write scheme , where the controller chooses a query vector q W t and applies writes to similar words in memory .",0
29605,"This would allow for direct memory updates , but would create problems when the memory is empty ( and shift further complexity to the controller ) .",0
29606,We decided upon the previously read / least recently accessed addressing scheme for simplicity and flexibility .,0
29607,The write weights are defined as,0
29608,where the controller outputs the interpolation gate parameter ?,0
29609,t and the write gate parameter ? t .,0
29610,"The write to the previously read locations w R t?1 is purely additive , while the least recently accessed word I Ut is set to zero before being written to .",0
29611,"When the read operation is sparse ( w R t?1 has K non -zero entries ) , it follows the write operation is also sparse .",0
29612,"We define I Ut to bean indicator over words in memory , with a value of 1 when the word minimizes a usage measure Ut",0
29613,If there are several words that minimize,0
29614,Ut then we choose arbitrarily between them .,0
29615,We tried two definitions of Ut .,0
29616,The first definition is a time - discounted sum of write weights U ( 1 ),0
29617,where ?,0
29618,is the discount factor .,0
29619,"This usage definition is incorporated within Dense Access Memory ( DAM ) , a dense - approximation to SAM that is used for experimental comparison in Section 4 .",0
29620,"The second usage definition , used by SAM , is simply the number of time - steps since a non-negligible memory access :",0
29621,U,0
29622,"Here , ?",0
29623,is a tuning parameter that we typically choose to be 0.005 .,0
29624,We maintain this usage statistic inconstant time using a custom data - structure ( described in ) .,0
29625,Finally we also use the least recently accessed word to calculate the erase matrix .,0
29626,R t = I Ut 1 T is defined to be the expansion of this usage indicator where 1 is a vector of ones .,0
29627,"The total cost of the write is constant in time and space for both the forwards and backwards pass , which improves on the linear space and time dense write ( see Supplementary A ) .",0
29628,Controller,0
29629,We use a one layer LSTM for the controller throughout .,0
29630,"At each time step , the LSTM receives a concatenation of the external input , x t , the word , r t?1 read in the previous time step .",0
29631,"The LSTM then produces a vector , pt = ( q t , at , ? t , ? t ) , of read and write parameters for memory access via a linear layer .",0
29632,"The word read from memory for the current time step , rt , is then concatenated with the output of the LSTM , and this vector is fed through a linear layer to form the final output , y t .",0
29633,The full control flow is illustrated in .,0
29634,Efficient backpropagation through time,0
29635,We have already demonstrated how the forward operations in SAM can be efficiently computed in O ( T log N ) time .,0
29636,"However , when considering space complexity of MANNs , there remains a dependence on Mt for the computation of the derivatives at the corresponding time step .",0
29637,"A naive implementation requires the state of the memory to be cached at each time step , incurring a space overhead of O ( N T ) , which severely limits memory size and sequence length .",0
29638,"Fortunately , this can be remedied .",0
29639,"Since there are only O ( 1 ) words that are written at each time step , we instead track the sparse modifications made to the memory at each timestep , apply them in - place to compute Mt in O ( 1 ) time and O ( T ) space .",0
29640,"During the backward pass , we can restore the state of Mt from M t+1 in O ( 1 ) time by reverting the sparse modifications applied at time step t.",0
29641,As such the memory is actually rolled back to previous states during backpropagation ( Supplementary .,0
29642,"At the end of the backward pass , the memory ends rolled back to the start state .",0
29643,"If required , such as when using truncating BPTT , the final memory state can be restored by making a copy of MT prior to calling backwards in O ( N ) time , or by re-applying the T sparse updates in O ( T ) time .",0
29644,Approximate nearest neighbors,0
29645,"When querying the memory , we can use an approximate nearest neighbor index ( ANN ) to search over the external memory for the K nearest words .",0
29646,"Where a linear KNN search inspects every element in memory ( taking O ( N ) time ) , an ANN index maintains a structure over the dataset to allow for fast inspection of nearby points in O ( log N ) time .",0
29647,"In our case , the memory is still a dense tensor that the network directly operates on ; however the ANN is a structured view of its contents .",0
29648,Both the memory and the ANN index are passed through the network and kept in sync during writes .,0
29649,However there are no gradients with respect to the ANN as its function is fixed .,0
29650,"We considered two types of ANN indexes : FLANN 's randomized k-d tree implementation that arranges the datapoints in an ensemble of structured ( randomized k- d ) trees to search for nearby points via comparison - based search , and one that uses locality sensitive hash ( LSH ) functions that map points into buckets with distance - preserving guarantees .",0
29651,We used randomized k-d trees for small word sizes and LSHs for large word sizes .,0
29652,"For both ANN implementations , there is an O ( log N ) cost for insertion , deletion and query .",0
29653,We also rebuild the ANN from scratch every N insertions to ensure it does not become imbalanced .,0
29654,We measured the forward and backward times of the SAM architecture versus the dense DAM variant and the original NTM ( details of setup in Supplementary E ) .,0
29655,"SAM is over 100 times faster than the NTM when the memory contains one million words and an exact linear - index is used , and 1600 times faster with the k -d tree ) .",0
29656,With an ANN the model runs in sublinear time with respect to the memory size .,0
29657,"SAM 's memory usage per time step is independent of the number of memory words , which empirically verifies the O ( 1 ) space claim from Supplementary A. For 64 K memory words SAM uses 53 MiB of physical memory to initialize the network and 7.8 MiB to run a 100 step forward and backward pass , compared with the NTM which consumes 29 GiB.",0
29658,Results,0
29659,Speed and memory benchmarks,0
29660,Learning with sparse memory access,1
29661,"We have established that SAM reaps a huge computational and memory advantage of previous models , but can we really learn with SAM 's sparse approximations ?",0
29662,"We investigated the learning cost of inducing sparsity , and the effect of placing an approximate nearest neighbor index within the network , by comparing SAM with its dense variant DAM and some established models , the NTM and the LSTM .",0
29663,We trained each model on three of the original NTM tasks .,0
29664,"1 . Copy : copy a random input sequence of length 1 - 20 ,",0
29665,"2 . Associative Recall : given 3 - 6 random ( key , value ) pairs , and subsequently a cue key , return the associated value .",0
29666,"3 . Priority Sort : Given 20 random keys and priority values , return the top 16 keys in descending order of priority .",0
29667,We chose these tasks because the NTM is known to perform well on them .,0
29668,"shows that sparse models are able to learn with comparable efficiency to the dense models and , surprisingly , learn more effectively for some tasks - notably priority sort and associative recall .",1
29669,This shows that sparse reads and writes can actually benefit early - stage learning in some cases .,0
29670,Full hyperparameter details are in Supplementary C.,0
29671,Scaling with a curriculum,1
29672,The computational efficiency of SAM opens up the possibility of training on tasks that require storing a large amount of information overlong sequences .,0
29673,"Here we show this is possible in practice , by scaling tasks to a large scale via an exponentially increasing curriculum .",0
29674,"We parametrized three of the tasks described in Section 4.2 : associative recall , copy , and priority sort , with a progressively increasing difficulty level which characterises the length of the sequence and number of entries to store in memory .",0
29675,"For example , level specifies the input sequence length for the copy task .",0
29676,We exponentially increased the maximum level h when the network begins to learn the fundamental algorithm .,0
29677,"Since the time taken fora forward and backward pass scales O (T ) with the sequence length T , following a standard linearly increasing curriculum could potentially take O ( T 2 ) , if the same amount of training was required at each step of the curriculum .",0
29678,"Specifically , h was doubled whenever the average training loss dropped below a threshold fora number of episodes .",0
29679,"The level was sampled for each minibatch from the uniform distribution over integers U ( 0 , h) .",0
29680,"We compared the dense models , NTM and DAM , with both SAM with an exact nearest neighbor index ( SAM linear ) and with locality sensitive hashing ( SAM ANN ) .",0
29681,"The dense models contained 64 memory words , while the sparse models had 2 10 6 words .",0
29682,These sizes were chosen to ensure all models use approximately the same amount of physical memory when trained over 100 steps .,0
29683,"For all tasks , SAM was able to advance further than the other models , and in the associative recall task , SAM was able to advance through the curriculum to sequences greater than 4000 ( ) .",1
29684,"Note that we did not use truncated backpropagation , so this involved BPTT for over 4000 steps with a memory size in the millions of words .",0
29685,"To investigate whether SAM was able to learn algorithmic solutions to tasks , we investigated its ability to generalize to sequences that far exceeded those observed during training .",0
29686,"Namely we trained SAM on the associative recall task up to sequences of length 10 , 000 , and found it was then able to generalize to sequences of length 200,000 ( ) .",0
29687,Question answering on the Babi tasks,1
29688,[ 20 ] introduced toy tasks they considered a prerequisite to agents which can reason and understand natural language .,0
29689,"They are synthetically generated language tasks with a vocab of about 150 words that test various aspects of simple reasoning such as deduction , induction and coreferencing .",0
29690,We tested the models ( including the Sparse Differentiable Neural Computer described in Supplementary D ) on this task .,0
29691,The full results and training details are described in Supplementary G.,0
29692,"The MANNs , except the NTM , are able to learn solutions comparable to the previous best results , failing at only 2 of the tasks .",1
29693,"The SDNC manages to solve all but 1 of the tasks , the best reported result on Babi that we are aware of .",1
29694,Notably the best prior results have been obtained by using supervising the memory retrieval ( during training the model is provided annotations which indicate which memories should be used to answer a query ) .,0
29695,"More directly comparable previous work with end - to - end memory networks , which did not use supervision , fails at 6 of the tasks .",0
29696,"Both the sparse and dense perform comparably at this task , again indicating the sparse approximations do not impair learning .",0
29697,We believe the NTM may perform poorly since it lacks a mechanism which allows it to allocate memory effectively .,1
29698,Learning on real world data,1
29699,"Finally , we demonstrate that the model is capable of learning in a non-synthetic dataset .",0
29700,"Omniglot is a dataset of 1623 characters taken from 50 different alphabets , with 20 examples of each character .",0
29701,"This dataset is used to test rapid , or one - shot learning , since there are few examples of each character but many different character classes .",0
29702,"Following , we generate episodes where a subset of characters are randomly selected from the dataset , rotated and stretched , and assigned a randomly chosen label .",0
29703,"At each time step an example of one of the characters is presented , along with the correct label of the proceeding character .",0
29704,Each character is presented 10 times in an episode ( but each presentation maybe anyone of the 20 examples of the character ) .,0
29705,"In order to succeed at the task the model must learn to rapidly associate a novel character with the correct label , such that it can correctly classify subsequent examples of the same character class .",0
29706,"Again , we used an exponential curriculum , doubling the number of additional characters provided to the model whenever the cost was reduced under a threshold .",0
29707,"After training all MANNs for the same length of time , a validation task with 500 characters was used to select the best run , and this was then tested on a test set , containing all novel characters for different sequence lengths ( ) .",0
29708,"All of the MANNs were able to perform much better than chance , even on sequences ?",0
29709,4 longer than seen during training .,0
29710,"SAM outperformed other models , presumably due to its much larger memory capacity .",1
29711,"Previous results on the Omniglot curriculum task are not identical , since we used 1 - hot labels throughout and the training curriculum scaled to longer sequences , but our results with the dense models are comparable (?",0
29712,"0.4 errors with 100 characters ) , while the SAM is significantly better ( 0.2 < errors with 100 characters ) . :",0
29713,Test errors for the Omniglot task ( described in the text ) for the best runs ( as chosen by the validation set ) .,0
29714,The characters used in the test set were not used in validation or training .,0
29715,All of the MANNs were able to perform much better than chance with ?,1
29716,"500 characters ( sequence lengths of ? 5000 ) , even though they were trained , at most , on sequences of ? 130 ( chance is 0.002 for 500 characters ) .",0
29717,This indicates they are learning generalizable solutions to the task .,0
29718,"SAM is able to outperform other approaches , presumably because it can utilize a much larger memory .",0
29719,Discussion,0
29720,Scaling memory systems is a pressing research direction due to potential for compelling applications with large amounts of memory .,0
29721,"We have demonstrated that you can train neural networks with large memories via a sparse read and write scheme that makes use of efficient data structures within the network , and obtain significant speedups during training .",0
29722,"Although we have focused on a specific MANN ( SAM ) , which is closely related to the NTM , the approach taken here is general and can be applied to many differentiable memory architectures , such as Memory Networks .",0
29723,It should be noted that there are multiple possible routes toward scalable memory architectures .,0
29724,"For example , prior work aimed at scaling Neural Turing Machines used reinforcement learning to train a discrete addressing policy .",0
29725,"This approach also touches only a sparse set of memories at each time step , but relies on higher variance estimates of the gradient during optimization .",0
29726,"Though we can only guess at what class of memory models will become staple in machine learning systems of the future , we argue in Supplementary A that they will be no more efficient than SAM in space and time complexity if they address memories based on content .",0
29727,"We have experimented with randomized k-d trees and LSH within the network to reduce the forward pass of training to sublinear time , but there maybe room for improvement here .",0
29728,"K -d trees were not designed specifically for fully online scenarios , and can become imbalanced during training .",0
29729,"Recent work in tree ensemble models , such as Mondrian forests , show promising results in maintaining balanced hierarchical set coverage in the online setting .",0
29730,"An alternative approach which maybe well - suited is LSH forests , which adaptively modifies the number of hashes used .",0
29731,It would bean interesting empirical investigation to more fully assess different ANN approaches in the challenging context of training a neural network .,0
29732,"Humans are able to retain a large , task - dependent set of memories obtained in one pass with a surprising amount of fidelity .",0
29733,Here we have demonstrated architectures that may one day compete with humans at these kinds of tasks .,0
29734,Supplementary Information,0
29735,A Time and space complexity,0
29736,"Under a reasonable class of content addressable memory architectures A , SAM is optimal in time and space complexity .",0
29737,Existing lower bounds assert that for any data structure a ?,0
29738,"A , a requires ?( log N ) time and ? ( N ) space to perform a read operation .",0
29739,The SAM memory architecture proposed in this paper is contained within A as it computes the approximate nearest neighbors problem in fixed dimensions .,0
29740,"As we will show , SAM requires O ( log N ) time to query and maintain the ANN , O ( 1 ) to perform all subsequent sparse read , write , and error gradient calculations .",0
29741,It requires O ( N ) space to initialize the memory and O ( 1 ) to store intermediate sparse tensors .,0
29742,We thus conclude it is optimal in asymptotic time and space complexity .,0
29743,A.1 Initialization,0
29744,"Upon initialization , SAM consumes O ( N ) space and time to instantiate the memory and the memory Jacobian .",0
29745,"Furthermore , it requires O( N ) time and space to initialize auxiliary data structures which index the memory , such as the approximate nearest neighbor which provides a content - structured view of the memory , and the least accessed ring , which maintains the temporal ordering in which memory words are accessed .",0
29746,"These initializations represent an unavoidable one - off cost that does not recur per step of training , and ultimately has little effect on training speed .",0
29747,For the remainder of the analysis we will concentrate on the space and time cost per training step .,0
29748,A.2 Read,0
29749,"Recall the sparse read operation , r",0
29750,"As K is chosen to be a fixed constant , it is clear we can compute in O ( 1 ) time .",0
29751,"During the backward pass , we seethe gradients are sparse with only K non -zero terms ,",0
29752,where 0 is a vector of M zeros .,0
29753,Thus they can both be computed inconstant time by skipping the computation of zeros .,0
29754,"Furthermore by using an efficient sparse matrix format to store these matrices and vectors , such as the CSR , we can represent them using at most 3 K values .",0
29755,"Since the read wordrt and its respective error gradient is the size of a single word in memory ( M elements ) , the overall space complexity is O ( 1 ) per time step for the read .",0
29756,A.3 Write,0
29757,"Recall the write operation ,",0
29758,"where At = w W ta T t is the add matrix , Et = Mt?1",0
29759,"Rt is the erase matrix , and Rt = I Ut 1 T is defined to be the erase weight matrix .",0
29760,"We chose the write weights to bean interpolation between the least recently accessed location and the previously read locations ,",0
29761,"For sparse reads where w R t =w R t is a sparse vector with K non-zeros , the write weights w W t is also sparse with K + 1 non-zeros : 1 for the least recently accessed location and K for the previously read locations .",0
29762,Thus the sparse - dense outer product At = w W ta T t can be performed in O ( 1 ) time as K is a fixed constant .,0
29763,"Since Rt = I Ut 1 T can be represented as a sparse matrix with one single non -zero , the erase matrix Et can also .",0
29764,As At and Et are sparse matrices we can then add them component - wise to the dense Mt?1 in O ( 1 ) time .,0
29765,By analogous arguments the backward pass can be computed in O ( 1 ) time and each sparse matrix can be represented in O ( 1 ) space .,0
29766,"We avoid caching the modified memory , and thus duplicating it , by applying the write directly to the memory .",0
29767,"To restore its prior state during the backward pass , which is crucial to gradient calculations at earlier time steps , we roll the memory it back by reverting the sparse modifications with an additional O ( 1 ) time overhead .",0
29768,"The location of the least recently accessed memory can be maintained in O ( 1 ) time by constructing a circular linked list that tracks the indices of words in memory , and preserves a strict ordering of relative temporal access .",0
29769,"The first element in the ring is the least recently accessed word in memory , and the last element in the ring is the most recently modified .",0
29770,"We keep a "" head "" pointer to the first element in the ring .",0
29771,"When a memory word is randomly accessed , we can push its respective index to the back of the ring in O ( 1 ) time by redirecting a small number of pointers .",0
29772,When we wish to pop the least recently accessed memory ( and write to it ) we move the head to the next element in the ring in O ( 1 ) time .,0
29773,Each circle represents an instance of the SAM core at a given time step .,0
29774,The grey box marks the dense memory .,0
29775,"Each core holds a reference to the single instance of the memory , and this is represented by the solid connecting line above each core .",0
29776,"We see during the forward pass , the memory 's contents are modified sparsely , represented by the solid horizontal lines .",0
29777,"Instead of caching the changing memory state , we store only the sparse modifications - represented by the dashed white boxes .",0
29778,"During the backward pass , we "" revert "" the cached modifications to restore the memory to its prior state , which is crucial for correct gradient calculations .",0
29779,A.4 Content - based addressing,0
29780,"As discussed in Section 3.5 we can calculate the content - based attention , or read weights w R t , in O ( log N ) time using an approximate nearest neighbor index that views the memory .",0
29781,"We keep the ANN index synchronized with the memory by passing it through the network as a non-differentiable member of the network 's state ( so we do not pass gradients for it ) , and we update the index upon each write or erase to memory in O ( log N ) time .",0
29782,"Maintaining and querying the ANN index represents the most expensive part of the network , which is reasonable as content - based addressing is inherently expensive .",0
29783,"For the backward pass computation , specifically calculating ?",0
29784,"L ? q t and ?L ?M t with respect tow R t , we can once again compute these using sparse matrix operations in O ( 1 ) time .",0
29785,This is because the K non -zero locations have been determined during the forward pass . :,0
29786,Schematic showing how the controller interfaces with the external memory in our experiments .,0
29787,"The controller ( LSTM ) output ht is used ( through a linear projection , pt ) to read and write to the memory .",0
29788,"The result of the read operation rt is combined with ht to produce output y t , as well as being feed into the controller at the next timestep ( r t?1 ) .",0
29789,C Training details,0
29790,Here we provide additional details on the training regime used for our experiments used in .,0
29791,"To avoid bias in our results , we chose the learning rate that worked best for DAM ( and not SAM ) .",0
29792,"We tried learning rates { 10 ?6 , 5 10 ?5 , 10 ?5 , 5 10 ?4 , 10 ?4 } and found that DAM trained best with 10 ?5 .",0
29793,"We also tried values of K { 4 , 8 , 16 } and found no significant difference in performance across the values .",0
29794,"We used 100 hidden units for the LSTM ( including the controller LSTMs ) , a minibatch of 8 , 8 asynchronous workers to speedup training , and RMSProp to optimize the controller .",0
29795,We used 4 memory access heads and configured SAM to read from only K = 4 locations per head .,0
29796,D Sparse Differentiable Neural Computer,0
29797,Recently proposed a novel MANN the Differentiable Neural Computer ( DNC ) .,0
29798,The two innovations proposed by this model area new approach to tracking memory freeness ( dynamic memory allocation ) and a mechanism for associating memories together ( temporal memory linkage ) .,0
29799,"We demonstrate here that the approaches enumerated in the paper can be adapted to new models by outlining a sparse version of this model , the Sparse Differentiable Neural Computer ( SDNC ) , which learns with similar data efficiency while retaining the computational advantages of sparsity .",0
29800,D.1 Architecture,0
29801,"For brevity , we will only explain the sparse implementations of these two items , for the full model details refer to the original paper .",0
29802,The mechanism for sparse memory reads and writes was implemented identically to SAM .,0
29803,It is possible to implement a scalable version of the dynamic memory allocation system of the DNC avoiding any O(N ) operations by using a heap .,0
29804,"However , because it is practical to run the SDNC with many more memory words , reusing memory is less crucial so we did not implement this and used the same usage tracking as in SAM .",0
29805,"The temporal memory linkage in the DNC is a system for associating and recalling memory locations which were written in a temporal order , for exampling storing and retrieving a list .",0
29806,"In the DNC this is done by maintaining a temporal linkage matrix Lt ? [ 0 , 1 ] N N . Lt[i , j ] represents the degree to which location i was written to after location j.",0
29807,"This matrix is updated by tracking the precedence weighting pt , where pt ( i ) represents the degree to which location i was written to .",0
29808,The memory linkage is updated according to the following recurrence,0
29809,The temporal linkage,0
29810,Lt can be used to compute read weights following the temporal links either forward,0
29811,The read head then uses a 3 - way softmax to select between a content - based read or following the forward or backward weighting .,0
29812,"Naively , the link matrix requires O (N 2 ) memory and computation although proposes a method to reduce the computational cost to O( N log N ) and O ( N ) memory cost .",0
29813,"In order to maintain the scaling properties of the SAM , we wish to avoid any computational dependence on N .",0
29814,We do this by maintaining two sparse matrices,0
29815,"Nt , Pt ?",0
29816,"[ 0 , 1 ] N {K L } that approximate Lt and L T t respectively .",0
29817,We store these matrices in Compressed Sparse Row format .,0
29818,They are defined by the following updates :,0
29819,"Additionally , pt is , as with the other weight vectors maintained as a sparse vector with at most KL non -zero entries .",0
29820,This means that the outer product of wtp T t?1 has at most K 2 L non -zero entries .,0
29821,"In addition to the updates specified above , we also constrain each row of the matrices Nt and Pt to have at most KL non -zero entriesthis constraint can be applied in O ( K 2 L ) because at most KL rows change in the matrix .",0
29822,Once these matrices are applied the read weights following the temporal links can be computed similar to before :,0
29823,"Note , the number of locations we read from , K , does not have to equal the number of outward and inward links we preserve , KL .",0
29824,"We typically choose KL = 8 as this is still very fast to compute ( 100 s in total to calculate Nt , Pt , pt , fr t , b rt on a single CPU thread ) and we see no learning benefit with larger KL .",0
29825,"In order to compute the gradients , Nt and Pt need to be stored .",0
29826,"This could be done by maintaining a sparse record of the updates applied and reversing them , similar to that performed with the memory as described in Section 3.4 .",0
29827,"However , for implementation simplicity we did not pass gradients through the temporal linkage matrices .",0
29828,D.2 Results,0
29829,We benchmarked the speed and memory performance of the SDNC versus a naive DNC implementation ( details of setup in Supplementary E ) .,0
29830,The results are displayed in .,0
29831,"Here , the computational benefits of sparsity are more pronounced due to the expensive ( quadratic time and space ) temporal transition table operations in the DNC .",0
29832,"We were only able to run comparative benchmarks up to N = 2048 , as the DNC quickly exceeded the machine 's physical memory for larger values ; however even at this modest memory size we see a speed increase of ?",0
29833,440 and physical memory reduction of ? 240 .,0
29834,"Note , unlike the SAM memory benchmark in Section 4 we plot the total memory consumption , i.e. the memory overhead of the initial start state plus the memory overhead of unrolling the core over a sequence .",0
29835,This is because the SDNC and DNC do not have identical start states .,0
29836,"The sparse temporal transition matrices N0 , P0 ?",0
29837,"[ 0 , 1 ] N N{K } consume much less memory than the corresponding L0 ?",0
29838,"[ 0 , 1 ] N N in the DNC .",0
29839,In order to compare the models on an interesting task we ran the DNC and SDNC on the Babi task ( this task is described more fully in the main text ) .,0
29840,The results are described in Supplementary G and demonstrate the SDNC is capable of learning competitively .,0
29841,"In particular , it achieves the best report result on the Babi task .",0
29842,E Benchmarking details,0
29843,"Each model contained an LSTM controller with 100 hidden units , an external memory containing N slots of memory , with word size 32 and 4 access heads .",0
29844,"For speed benchmarks , a minibatch size of 8 was used to ensure fair comparison - as many dense operations ( e.g. matrix multiplication ) can be batched efficiently .",0
29845,"For memory benchmarks , the minibatch size was set to 1 .",0
29846,"We used Torch7 to implement SAM , DAM , NTM , DNC and SDNC .",0
29847,"Eigen v 3 was used for the fast sparse tensor operations , using the provided CSC and CSR formats .",0
29848,All benchmarks were run on a Linux desktop running Ubuntu 14.04.1 with 32 GiB of RAM and an Intel Xeon E5-1650 3.20 GHz processor with power scaling disabled .,0
29849,F Generalization on associative recall L : We tested the generalization of SAM on the associative recall task .,0
29850,"We train each model up to a difficulty level , which corresponds to the task 's sequence length , of 10 , 000 , and evaluate on longer sequences .",0
29851,"The SAM models ( with and without ANN ) are able to perform much better than chance ( 48 bits ) on sequences of length 200 , 000 .",0
29852,G Babi results,0
29853,See the main text fora description of the Babi task and its relevance .,0
29854,Here we report the best and mean results for all of the models on this task .,0
29855,The task was encoded using straightforward 1 - hot word encodings for both the input and output .,0
29856,"We trained a single model on all of the tasks , and used the 10,000 examples per task version of the training set ( a small subset of which we used as a validation set for selecting the best run and hyperparameters ) .",0
29857,"Previous work has reported best results , which with only 15 runs is a noisy comparison , so we additionally report the mean and variance for all runs with the best selected hyperparameters :",0
29858,Test results for the best run ( chosen by validation set ) on the Babi task .,0
29859,The model was trained and tested jointly on all tasks .,0
29860,All tasks received approximately equal training resources .,0
29861,"Both SAM and DAM pass all but 2 of the tasks , without any supervision of their memory accesses .",0
29862,"SDNC achieves the best reported result on this task with unsupervised memory access , solving all but 1 task .",0
29863,"We 've included comparison with memory networks , both with supervision of memories ( Mem Net S ) and , more directly comparable with our approach , learning end - to - end ( MemNets U ) .",0
29864,LSTM DNC SDNC DAM SAM NTM 1 : 1 supporting fact 30.9 1.5,0
29865,title,0
29866,Stochastic Answer Networks for Natural Language Inference,1
29867,abstract,0
29868,We utilize a stochastic answer network ( SAN ) to explore multi-step inference strategies in Natural Language Inference .,0
29869,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",0
29870,This can potentially model more complex inferences than the existing single - step inference methods .,0
29871,"Our experiments show that SAN achieves state - of - the - art results on four benchmarks : Stanford Natural Language Inference ( SNLI ) , MultiGenre Natural Language Inference ( MultiNLI ) , SciTail , and Quora Question Pairs datasets .",0
29872,Motivation,0
29873,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",1
29874,"This task is challenging , since it requires a model to fully understand the sentence meaning , ( i.e. , lexical and compositional semantics ) .",0
29875,"For instance , the following example from MultiNLI dataset illustrates the need fora form of multi-step synthesis of information between premise :",0
29876,""" If you need this book , it is probably too late unless you are about to take an SAT or GRE . "" , and hypothesis :",0
29877,""" It 's never too late , unless you 're about to take a test . """,0
29878,"To predict the correct relation between these two sentences , the model needs to first infer that "" SAT or GRE "" is a "" test "" , and then pick the correct relation , e.g. , contradiction .",0
29879,This kind of iterative process can be viewed as a form of multi-step inference .,0
29880,"To best of our knowledge , all of works on NLI use a single step inference .",0
29881,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .",1
29882,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",1
29883,"We show that our model outperforms single - step inference and further achieves the state - of - the - art on SNLI , MultiNLI , SciTail , and Quora Question Pairs datasets .",0
29884,Multi- step inference with SAN,0
29885,"The natural language inference task as defined here involves a premise P = {p 0 , p 1 , ... , p m?1 } of m words and a hypothesis H = {h 0 , h 1 , ... , h n?1 } of n words , and aims to find a logic relationship R between P and H , which is one of labels in a close set : entailment , neutral and contradiction .",0
29886,"The goal is to learn a model f ( P , H ) ?",0
29887,R.,0
29888,"Ina single - step inference architecture , the model directly predicts R given P and H as input .",0
29889,"In our multi-step inference architecture , we additionally incorporate a recurrent state st ; the model processes multiple passes through P and H , iteratively refining the state st , before finally generating the output at step t = T , where T is an a priori chosen limit on the number of inference steps .",0
29890,describes in detail the architecture of the stochastic answer network ( SAN ) used in this study ; this model is adapted from the MRC multistep inference literature .,0
29891,"Compared to the original SAN for MRC , in the SAN for NLI we simplify the bottom layers and Selfattention layers since the length of the premise and hypothesis is short ) .",0
29892,We also modify the answer module from prediction a text span to an NLI classification label .,0
29893,"Overall , it contains four different layers :",0
29894,1 ) the lexicon encoding layer computes word representations ; 2 ) the contextual encoding layer modifies these representations in context ; 3 ) the memory generation layer gathers all information from the premise and hypothesis and forms a,0
29895,Information Gathering Layer s t - 1 s t s t +1 s t - 1 s t s t+1,0
29896,Memory : Architecture of the Stochastic Answer Network ( SAN ) for Natural Language Inference .,0
29897,""" working memory "" for the final answer module ; 4 ) the final answer module , a type of multi-step network , predicts the relation between the premise and hypothesis .",0
29898,Lexicon Encoding Layer .,0
29899,First we concatenate word embeddings and character embeddings to handle the out - of - vocabulary words,0
29900,1 .,0
29901,"Following , we use two separate twolayer position - wise feedforward network to obtain the final lexicon embedings , E p ?",0
29902,R dm and E h ?,0
29903,"R dn , for the tokens in P and H , respectively .",0
29904,"Here , dis the hidden size .",0
29905,Contextual Encoding Layer .,0
29906,"Two stacked BiL - STM layers are used on the lexicon encoding layer to encode the context information for each word in both P and H . Due to the bidirectional layer , it doubles the hidden size .",0
29907,"We use a maxout layer ( Goodfellow et al. , 2013 ) on the BiLSTM to shrink its output into its original hidden size .",0
29908,"By a concatenation of the outputs of two BiLSTM layers , we obtain C p ?",0
29909,R 2 dm and Ch ?,0
29910,"R 2 dn as representation of P and H , respectively .",0
29911,We omit POS Tagging and Name Entity Features for simplicity Memory Layer .,0
29912,We construct our working memory via an attention mechanism .,0
29913,"First , a dotproduct attention is adopted like in to measure the similarity between the tokens in P and H. Instead of using a scalar to normalize the scores as in , we use a layer projection to transform the contextual information of both C p and Ch :",0
29914,"where A is an attention matrix , and dropout is applied for smoothing .",0
29915,Note that ?,0
29916,p and ?,0
29917,"h is transformed from C p and Ch by one layer neural network ReLU ( W 3 x ) , respectively .",0
29918,"Next , we gather all the information on premise and hypothesis by : U p = C p ; Ch A ? R 4 dm and U h = Ch ; C p A ? R 4 dn .",0
29919,"The semicolon ; indicates vector / matrix concatenation ; A is the transpose of A. Last , the working memory of the premise and hypothesis is generated by using a BiLSTM based on all the information gathered :",0
29920,"Formally , our answer module will compute over T memory steps and output the relation label .",0
29921,"At the beginning , the initial state s 0 is the summary of the M h :",0
29922,"Here , x t is computed from the previous state s t?1 and memory M p :",0
29923,.,0
29924,"Following , one layer classifier is used to determine the relation at each step t ? { 0 , 1 , . . . , T ? 1}.",0
29925,"At last , we utilize all of the T outputs by averaging the scores :",0
29926,Each,0
29927,"Pr t is a probability distribution overall the relations , { 1 , . . . , | R |}.",0
29928,"During training , we apply stochastic prediction dropout before the above averaging operation .",0
29929,"During decoding , we average all outputs to improve robustness .",0
29930,"This stochastic prediction dropout is similar in motivation to the dropout introduced by ( Srivastava et al. , 2014 ) .",0
29931,"The difference is that theirs is dropout at the intermediate node - level , whereas ours is dropout at the final layer - level .",0
29932,Dropout at the node - level prevents correlation between features .,0
29933,"Dropout at the final layer level , where randomness is introduced to the averaging of predictions , prevents our model from relying exclusively on a particular step to generate correct output .",0
29934,Experiments,0
29935,Dataset,0
29936,"Here , we evaluate our model in terms of accuracy on four benchmark datasets .",0
29937,"SNLI ( Bowman et al. , 2015 ) contains 570 k human annotated sentence pairs , in which the premises are drawn from the captions of the Flickr30 corpus , and hypothesis are manually annotated .",0
29938,"MultiNLI contains 433 k sentence pairs , which are collected similarly as SNLI .",0
29939,"However , the premises are collected from abroad range of genre of American English .",0
29940,The test and development sets are further divided into in - domain ( matched ) and cross - domain ( mismatched ) sets .,0
29941,The Quora Question Pairs dataset is proposed for paraphrase identification .,0
29942,"It contains Note that it only contains two types of labels , so is a binary task .",0
29943,Implementation details,0
29944,The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .,1
29945,We fix word embedding with 300 - dimensional GloVe word vectors .,1
29946,"For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .",1
29947,3,0
29948,So lexicon embeddings are d =600 - dimensions .,1
29949,The embedding for the out - of - vocabulary is zeroed .,1
29950,"The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .",1
29951,The projection size in the attention layer is set to 256 .,1
29952,"To speedup training , we use weight normalization .",1
29953,"The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .",1
29954,The mini - batch size is set to 32 .,1
29955,Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .,1
29956,Results,0
29957,One main question which we would like to address is whether the multi-step inference help on NLI .,0
29958,We fixed the lower layer and only compare different architectures for the output layer :,0
29959,1 .,0
29960,Single-step : Predict the relation using Eq 2 based on s 0 and x 0 .,0
29961,"Here ,",0
29962,SAN :,0
29963,The multi-step inference model .,0
29964,We use 5 - steps with the prediction dropout rate 0.2 on the all experiments .,0
29965,shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .,1
29966,"For example , on SciTail dataset , SAN outperforms the single - step model by .",1
29967,We compare our results with the state - of - the - art in and BERT use a large amount of external knowledge or a large scale pretrained contextual embeddings .,0
29968,"However , SAN is still competitive these models .",0
29969,"On SciTail dataset , SAN even outperforms GPT .",1
29970,"Due to the space limitation , we only list two top models .",0
29971,We further utilize BERT as a feature extractor 6 and use the SAN answer module on top of it .,0
29972,"Comparing with Single - step baseline , the proposed model obtains + 2.8 improvement on the Sc - iTail test set ( 94.0 vs 91.2 ) and + 2.1 improvement on the SciTail dev set ( 96.1 vs 93.9 ) .",1
29973,This shows the generalization of the proposed model which can be easily adapted on other models,0
29974,7 .,0
29975,Analysis :,0
29976,How many steps it needs ?,0
29977,We search the number of steps t from 1 to 10 .,0
29978,"We observe that when t increases , our model obtains a better improvement ( e.g. , 86.7 ( t = 2 ) ) ; however when t = 5 or t = 6 , it achieves best results ( 89.4 ) on SciTail dev set and then begins to downgrade",0
29979,"4 For direct comparison , this has the same three lower layers as We run BERT ( the base model ) to extract embeddings of both premise and hypothesis and then feed it to answer models fora fair comparison .",0
29980,"7 Due to highly time consumption and space limitation , we omit the results using BERT on SNLI / MNLI / Quora Question dataset .",0
29981,Model,0
29982,MultiNLI Test Matched Mismatched DIIN 78.8 77.8 BERT 85.9 SAN 79.3 78.7 SNLI Dataset ( Accuracy % ) ESIM+ELM o 88.7 GPT 89.9 SAN 88.7,0
29983,Quora Question Dataset ( Accuracy% ) 88.4 89.1 SAN 89.4 SciTail Dataset ( Accuracy% ) 77.3 GPT 88.3 SAN 88.4 the performance .,0
29984,"Thus , we sett = 5 in all our experiments .",0
29985,We also looked internals of our answer module by dumping predictions of each step ( the max step is set to 5 ) .,0
29986,Here is an example 8 from MutiNLI dev set .,0
29987,Our model produces total 5 labels at each step and makes the final decision by voting neutral .,0
29988,"Surprising , we found that human annotators also gave different 5 labels : contradiction , neutral , neutral , neutral , neutral .",0
29989,It shows robustness of our model which uses collective wise .,0
29990,"Finally , we analyze our model on the annotated subset 9 of development set of MultiNLI .",0
29991,"It contains 1,000 examples , each tagged by categories shown in .",0
29992,"Our model outperforms the best system in RepEval 2017 inmost cases , except on "" Conditional "" and "" Tense Difference "" categories .",1
29993,"We also find that SAN works extremely well on "" Active / Passive "" and "" Paraphrase "" categories .",1
29994,"Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the "" Antonym "" category .",1
29995,"In particular , on the most challenging "" Long Sentence "" and "" Quantity / Time "" categories , SAN 's result is substantially better than previous systems .",1
29996,This demonstrates the robustness of multi-step inference .,0
29997,Conclusion,0
29998,We explored the use of multi-step inference in natural language inference by proposing a stochastic answer network ( SAN ) .,0
29999,"Rather than directly predicting the results ( e.g. relation R such as entailment or not ) given the input premise P and hypothesis H , SAN maintains a state st , which it iteratively refines over multiple passes on P and H in order to make a prediction .",0
30000,"Our state - of - theart results on four benchmarks ( SNLI , MultiNLI , SciTail , Quora Question Pairs ) show the effectiveness of this multi-step inference architecture .",0
30001,"In future , we would like to incorporate the pertrained contextual embedding , e.g. , and GPT into our model and multi-task learning .",0
30002,title,0
30003,Neural Natural Language Inference Models Enhanced with External Knowledge,1
30004,abstract,0
30005,Modeling natural language inference is a very challenging task .,1
30006,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",1
30007,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?",1
30008,"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?",1
30009,"In this paper , we enrich the state - of - the - art neural natural language inference models with external knowledge .",0
30010,We demonstrate that the proposed models improve neural NLI models to achieve the state - of - the - art performance on the SNLI and MultiNLI datasets .,0
30011,Introduction,0
30012,Reasoning and inference are central to both human and artificial intelligence .,0
30013,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",1
30014,"In general , modeling informal inference in language is a very challenging and basic problem towards achieving true natural language understanding .",0
30015,"In the last several years , larger annotated datasets were made available , e.g. , the and MultiNLI datasets , which made it feasible to train rather complicated neuralnetwork - based models that fit a large set of parameters to better model NLI .",0
30016,Such models have shown to achieve the state - of - the - art performance .,0
30017,"While neural networks have been shown to be very effective in modeling NLI with large training data , they have often focused on end - to - end training by assuming that all inference knowledge is learnable from the provided training data .",0
30018,"In this paper , we relax this assumption and explore whether external knowledge can further help NLI .",0
30019,Consider an example :,0
30020,p : A lady standing in a wheat field .,0
30021,h : A person standing in acorn field .,0
30022,"In this simplified example , when computers are asked to predict the relation between these two sentences and if training data do not provide the knowledge of relationship between "" wheat "" and "" corn "" ( e.g. , if one of the two words does not appear in the training data or they are not paired in any premise - hypothesis pairs ) , it will be hard for computers to correctly recognize that the premise contradicts the hypothesis .",0
30023,"In general , although in many tasks learning tabula rasa achieved state - of - the - art performance , we believe complicated NLP problems such as NLI could benefit from leveraging knowledge accumulated by humans , particularly in a foreseeable future when machines are unable to learn it by themselves .",0
30024,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .",1
30025,We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,1
30026,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",1
30027,"In addition to attaining the state - of - theart performance , we are also interested in understanding how external knowledge contributes to the major components of typical neural - networkbased NLI models .",0
30028,Related Work,0
30029,"Early research on natural language inference and recognizing textual entailment has been performed on relatively small datasets ( refer to MacCartney ( 2009 ) fora good literature survey ) , which includes a large bulk of contributions made under the name of RTE , such as , among many others .",0
30030,"More recently the availability of much larger annotated data , e.g. , and MultiNLI , has made it possible to train more complex models .",0
30031,These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter-sentence attention .,0
30032,Sentence - encoding - based models use Siamese architecture .,0
30033,The parametertied neural networks are applied to encode both the premise and the hypothesis .,0
30034,Then a neural network classifier is applied to decide relationship between the two sentences .,0
30035,"Different neural networks have been utilized for sentence encoding , such as LSTM , GRU , CNN , BiL - STM and its variants , self - attention network , and more complicated neural networks .",0
30036,"Sentence - encoding - based models transform sentences into fixed - length vector representations , which may help a wide range of tasks .",0
30037,The second set of models use inter-sentence attention .,0
30038,"Among them , were among the first to propose neural attention - based models for NLI .",0
30039,"proposed an enhanced sequential inference model ( ESIM ) , which is one of the best models so far and is used as one of our baselines in this paper .",0
30040,In this paper we enrich neural - network - based NLI models with external knowledge .,0
30041,"Unlike early work on NLI ) that explores external knowledge in conventional NLI models on relatively small NLI datasets , we aim to merge the advantage of powerful modeling ability of neural networks with extra external inference knowledge .",0
30042,We show that the proposed model improves the state - of - the - art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets .,0
30043,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may have more benefit .",0
30044,"In addition to attaining the state - of - the - art performance , we are also interested in understanding how external knowledge affect major components of neural - network - based NLI models .",0
30045,"In general , external knowledge has shown to be effective in neural networks for other NLP tasks , including word embedding , machine translation , language modeling , and dialogue systems .",0
30046,Neural - Network - Based NLI,0
30047,Models with External Knowledge,0
30048,"In this section we propose neural - network - based NLI models to incorporate external inference knowledge , which , as we will show later in Section 5 , achieve the state - of - the - art performance .",0
30049,In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of neural - network - based NLI modeling .,0
30050,shows a high - level general view of the proposed framework .,0
30051,"While specific NLI systems vary in their implementation , typical state - of - theart NLI models contain the main components ( or equivalents ) of representing premise and hypothesis sentences , collecting local ( e.g. , lexical ) inference information , and aggregating and composing local information to make the global decision at the sentence level .",0
30052,"We incorporate and investigate external knowledge accordingly in these major NLI components : computing co-attention , collecting local inference information , and composing inference to make final decision .",0
30053,External Knowledge,0
30054,"As discussed above , although there exist relatively large annotated data for NLI , can machines learn all inference knowledge needed to perform NLI from the data ?",0
30055,"If not , how can neural networkbased NLI models benefit from external knowledge and how to build NLI models to leverage it ?",0
30056,"We study the incorporation of external , inference - related knowledge in major components of neural networks for natural language inference .",0
30057,"For example , intuitively knowledge about synonymy , antonymy , hypernymy and hyponymy between given words may help model soft - alignment between premises and hypotheses ; knowledge about hypernymy and hyponymy may help capture entailment ; knowledge about antonymy and co-hyponyms ( words sharing the same hypernym ) may benefit the modeling of contradiction .",0
30058,"In this section , we discuss the incorporation of basic , lexical - level semantic knowledge into neural NLI components .",0
30059,"Specifically , we consider external lexical - level inference knowledge between word w i and w j , which is represented as a vector r ij and is incorporated into three specific components shown in .",0
30060,We will discuss the details of how r ij is constructed later in the experiment setup section ( Section 4 ) but instead focus on the proposed model in this section .,0
30061,"Note that while we study lexical - level inference knowledge in the paper , if inference knowledge about larger pieces of text pairs ( e.g. , inference relations between phrases ) are available , the proposed model can be easily extended to handle that .",0
30062,"In this paper , we instead let the NLI models to compose lexicallevel knowledge to obtain inference relations between larger pieces of texts .",0
30063,Encoding Premise and Hypothesis,0
30064,"Same as much previous work , we encode the premise and the hypothesis with bidirectional LSTMs ( BiLSTMs ) .",0
30065,"The premise is represented as a = ( a 1 , . . . , am ) and the hypothesis is b = ( b 1 , . . . , b n ) , where m and n are the lengths of the sentences .",0
30066,"Then a and bare embedded into d e - dimensional vectors [ E ( a 1 ) , . . . , E( a m ) ] and [ E ( b 1 ) , . . . , E ( b n ) ] using the embedding matrix E ?",0
30067,"R de |V | , where | V | is the vocabulary size and E can be initialized with the pre-trained word embedding .",0
30068,"To represent words in its context , the premise and the hypothesis are fed into BiLSTM encoders to obtain context - dependent hidden states a sand b s :",0
30069,"where i and j indicate the i - th word in the premise and the j - th word in the hypothesis , respectively .",0
30070,Knowledge - Enriched Co-Attention,0
30071,"As discussed above , soft - alignment of word pairs between the premise and the hypothesis may benefit from knowledge - enriched co-attention mechanism .",0
30072,Given the relation features r ij ?,0
30073,"R dr between the premise 's i - th word and the hypothesis's j- th word derived from the external knowledge , the co-attention is calculated as :",0
30074,The function F can be any non-linear or linear functions .,0
30075,"In this paper , we use F ( r ij ) = ? 1 ( r ij ) , where ?",0
30076,is a hyper - parameter tuned on the development set and 1 is the indication function as follows :,0
30077,"Intuitively , word pairs with semantic relationship , e.g. , synonymy , antonymy , hypernymy , hyponymy and co-hyponyms , are probably aligned together .",0
30078,We will discuss how we construct external knowledge later in Section 4 .,0
30079,We have also tried a twolayer MLP as a universal function approximator in function F to learn the underlying combination function but did not observe further improvement over the best performance we obtained on the development datasets .,0
30080,Soft - alignment is determined by the coattention matrix e ?,0
30081,"R mn computed in Equation , which is used to obtain the local relevance between the premise and the hypothesis .",0
30082,"For the hidden state of the i - th word in the premise , i.e. , a s i ( already encoding the word itself and its context ) , the relevant semantics in the hypothesis is identified into a context vector ac i using e ij , more specifically with Equation .",0
30083,",",0
30084,where ? ?,0
30085,R mn and ? ?,0
30086,R mn are the normalized attention weight matrices with respect to the 2 - axis and 1 - axis .,0
30087,"The same calculation is performed for each word in the hypothesis , i.e. , b s j , with Equation to obtain the context vector b c j .",0
30088,Local Inference Collection with External Knowledge,0
30089,"By way of comparing the inference - related semantic relation between a s i ( individual word representation in premise ) and ac i ( context representation from hypothesis which is align to word a s i ) , we can model local inference ( i.e. , word - level inference ) between aligned word pairs .",0
30090,"Intuitively , for example , knowledge about hypernymy or hyponymy may help model entailment and knowledge about antonymy and co-hyponyms may help model contradiction .",0
30091,"Through comparing a s i and ac i , in addition to their relation from external knowledge , we can obtain word - level inference information for each word .",0
30092,The same calculation is performed for b s j and b c j .,0
30093,"Thus , we collect knowledge - enriched local inference information :",0
30094,where a heuristic matching trick with difference and element - wise product is used .,0
30095,The last terms in Equation ( 7 ) ( 8 ) are used to obtain word - level inference information from external knowledge .,0
30096,"Take Equation as example , r ij is the relation feature between the i - th word in the premise and the j - th word in the hypothesis , but we care more about semantic relation between aligned word pairs between the premise and the hypothesis .",0
30097,"Thus , we use a soft - aligned version through the soft - alignment weight ?",0
30098,ij .,0
30099,"For the i - th word in the premise , the last term in Equation is a word - level inference information based on external knowledge between the i - th word and the aligned word .",0
30100,The same calculation for hypothesis is performed in Equation .,0
30101,G is a nonlinear mapping function to reduce dimensionality .,0
30102,"Specifically , we use a 1 - layer feed - forward neural network with the ReLU activation function with a shortcut connection , i.e. , concatenate the hidden states after ReLU with the input n j=1 ?",0
30103,ij r ij ( or m i= 1 ?,0
30104,ij r ji ) as the output am i ( or b m j ) .,0
30105,Knowledge - Enhanced Inference Composition,0
30106,"In this component , we introduce knowledgeenriched inference composition .",0
30107,"To determine the overall inference relationship between the premise and the hypothesis , we need to explore a composition layer to compose the local inference vectors ( a m and b m ) collected above :",0
30108,"Here , we also use BiLSTMs as building blocks for the composition layer , but the responsibility of BiLSTMs in the inference composition layer is completely different from that in the input encoding layer .",0
30109,The BiLSTMs here read local inference vectors ( a m and b m ) and learn to judge the types of local inference relationship and distinguish crucial local inference vectors for overall sentence - level inference relationship .,0
30110,"Intuitively , the final prediction is likely to depend on word pairs appearing in external knowledge that have some semantic relation .",0
30111,Our inference model converts the output hidden vectors of BiLSTMs to the fixed - length vector with pooling operations and puts it into the final classifier to determine the overall inference class .,0
30112,"Particularly , in addition to using mean pooling and max pooling similarly to ESIM , we propose to use weighted pooling based on external knowledge to obtain a fixed - length vector as in Equation .",0
30113,"In our experiments , we regard the function H as a 1 - layer feed - forward neural network with ReLU activation function .",0
30114,"We concatenate all pooling vectors , i.e. , mean , max , and weighted pooling , into the fixed - length vector and then put the vector into the final multilayer perceptron ( MLP ) classifier .",0
30115,The MLP has one hidden layer with tanh activation and softmax output layer in our experiments .,0
30116,"The entire model is trained end - to - end , through minimizing the cross - entropy loss .",0
30117,4 Experiment Set - Up,0
30118,Representation of External Knowledge,0
30119,Lexical Semantic Relations,0
30120,"As described in Section 3.1 , to incorporate external knowledge ( as a knowledge vector r ij ) to the state - of - theart neural network - based NLI models , we first explore semantic relations in WordNet , motivated by MacCartney ( 2009 ) .",0
30121,"Specifically , the relations of lexical pairs are derived as described in ( 1 ) - ( 4 ) below .",0
30122,"Instead of using Jiang - Conrath WordNet distance metric ( Jiang and Conrath , 1997 ) , which does not improve the performance of our models on the development sets , we add anew feature , i.e. , co-hyponyms , which consistently benefit our models .",0
30123,( 1 ) Synonymy :,0
30124,"It takes the value 1 if the words in the pair are synonyms in WordNet ( i.e. , belong to the same synset ) , and 0 otherwise .",0
30125,"For example , [ felicitous , good ] = 1 , [ dog , wolf ] = 0 .",0
30126,( 2 ) Antonymy :,0
30127,"It takes the value 1 if the words in the pair are antonyms in WordNet , and 0 otherwise .",0
30128,"For example , [ wet , dry ] = 1 .",0
30129,( 3 ) Hypernymy :,0
30130,"It takes the value 1 ? n / 8 if one word is a ( direct or indirect ) hypernym of the other word in WordNet , where n is the number of edges between the two words in hierarchies , and 0 otherwise .",0
30131,Note that we ignore pairs in the hierarchy which have more than 8 edges in between .,0
30132,( 5 ) Co-hyponyms :,0
30133,"It takes the value 1 if the two words have the same hypernym but they do not belong to the same synset , and 0 otherwise .",0
30134,"For example , [ dog , wolf ] = 1 .",0
30135,"As discussed above , we expect features like synonymy , antonymy , hypernymy , hyponymy and cohyponyms would help model co-attention alignment between the premise and the hypothesis .",0
30136,Knowledge of hypernymy and hyponymy may help capture entailment ; knowledge of antonymy and co-hyponyms may help model contradiction .,0
30137,Their final contributions will be learned in end - to - end model training .,0
30138,We regard the vector r ?,0
30139,"R dr as the relation feature derived from external knowledge , where d r is 5 here .",0
30140,"In addition , reports some key statistics of these features .",0
30141,"In addition to the above relations , we also use more relation features in WordNet , including instance , instance of , same instance , entailment , member meronym , member holonym , substance meronym , substance holonym , part meronym , part holonym , summing up to 15 features , but these additional features do not bring further improvement on the development dataset , as also discussed in Section 5 .",0
30142,Relation Embeddings,0
30143,In the most recent years graph embedding has been widely employed to learn representation for vertexes and their relations in a graph .,0
30144,"In our work here , we also capture the relation between any two words in WordNet through relation embedding .",0
30145,"Specifically , we employed TransE , a widely used graph embedding methods , to capture relation embedding between any two words .",0
30146,We used two typical approaches to obtaining the relation embedding .,0
30147,The first directly uses 18 relation embeddings pretrained on the WN18 dataset .,0
30148,"Specifically , if a word pair has a certain type relation , we take the corresponding relation embedding .",0
30149,"Sometimes , if a word pair has multiple relations among the 18 types ; we take an average of the relation embedding .",0
30150,"The second approach uses TransE 's word embedding ( trained on WordNet ) to obtain relation embedding , through the objective function used in TransE , i.e. , l ? t ? h , where l indicates relation embedding , t indicates tail entity embedding , and h indicates head entity embedding .",0
30151,"Note that in addition to relation embedding trained on WordNet , other relational embedding resources exist ; e.g. , that trained on Freebase ( WikiData ) , but such knowledge resources are mainly about facts ( e.g. , relationship between Bill Gates and Microsoft ) and are less for commonsense knowledge used in general natural language inference ( e.g. , the color yellow potentially contradicts red ) .",0
30152,NLI Datasets,0
30153,"In our experiments , we use Stanford Natural Language Inference ( SNLI ) dataset and Multi - Genre Natural Language Inference ( MultiNLI ) dataset , which focus on three basic relations between a premise and a potential hypothesis : the premise entails the hypothesis ( entailment ) , they contradict each other ( contradiction ) , or they are not related ( neutral ) .",0
30154,We use the same data split as in previous work and classification accuracy as the evaluation metric .,0
30155,"In addition , we test our models ( trained on the SNLI training set ) on anew test set , which assesses the lexical inference abilities of NLI systems and consists of is used to extract semantic relation features between words .",0
30156,The words are lemmatized using Stanford CoreNLP 3.7.0 .,0
30157,The premise and the hypothesis sentences fed into the input encoding layer are tokenized .,0
30158,Training Details,0
30159,"For duplicability , we release our code 1 .",0
30160,All our models were strictly selected on the development set of the SNLI data and the in - domain development set of MultiNLI and were then tested on the corresponding test set .,0
30161,The main training details are as follows : the dimension of the hidden states of LSTMs and word embeddings are 300 .,1
30162,"The word embeddings are initialized by 300D GloVe 840B , and out - of - vocabulary words among them are initialized randomly .",1
30163,All word embeddings are updated during training .,0
30164,"Adam ( Kingma and Ba , 2014 ) is used for optimization with an initial learning rate of 0.0004 .",1
30165,The mini - batch size is set to 32 .,1
30166,Note that the above hyperparameter settings are same as those used in the baseline ESIM model .,0
30167,ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli,0
30168,( the ESIM core code has also been adapted to summarization and questionanswering tasks ) .,0
30169,The trade - off ? for calculating co-attention in Equation shows the results of state - of - the - art models on the SNLI dataset .,0
30170,"Among them , ESIM is one of the previous state - of - the - art systems with an 88.0 % test - set accuracy .",0
30171,"The proposed model , namely Knowledge - based Inference Model ( KIM ) , which enriches ESIM with external knowledge , obtains an accuracy of 88.6 % , the best single - model performance reported on the SNLI dataset .",1
30172,The difference between ESIM and KIM is statistically significant under the one - tailed paired t- test at the 99 % significance level .,1
30173,Note that the KIM model reported here uses five semantic relations described in Section 4 .,0
30174,"In addition to that , we also use 15 semantic relation features , which does not bring additional gains in performance .",0
30175,These results highlight the effectiveness of the five semantic relations described in Section 4 .,0
30176,"To further investigate external knowledge , we add TransE relation embedding , and again no further improvement is observed on both the development and test sets when TransE relation embedding is used ( concatenated ) with the semantic relation vectors .",0
30177,"We consider this is due to the fact that TransE embedding is not specifically sensitive to inference information ; e.g. , it does not model co-hyponyms features , and its potential benefit has already been covered by the semantic relation features used .",0
30178,shows the performance of models on the MultiNLI dataset .,1
30179,"The baseline ESIM achieves 76.8 % and 75.8 % on in - domain and cross - domain test set , respectively .",1
30180,"If we extend the ESIM with external knowledge , we achieve significant gains to 77.2 % and 76.4 % respectively .",1
30181,"Again , the gains are consistent on SNLI and MultiNLI , and we expect they would be orthogonal to other factors when external knowledge is added into other stateof - the - art models .",0
30182,Ablation Results,0
30183,Figure 2 displays the ablation analysis of different components when using the external knowledge .,0
30184,"To compare the effects of external knowledge under different training data scales , we ran-Model Test LSTM Att. 83.5 DF - LSTMs 84.6 TC - LSTMs 85.1 Match- LSTM 86.1 LSTMN 86.3 Decomposable Att. 86.8 NTI 87.3 Re-read LSTM 87.5 BiMPM 87.5 DIIN 88.0 BCN + CoVe 88.1 CAFE 88.5",0
30185,ESIM 88.0 KIM,0
30186,( This paper ) 88.6 : Accuracies of models on SNLI .,0
30187,Model In Cross,0
30188,CBOW 64.8 64.5 BiLSTM 66.9 66.9 DiSAN 71.0 71.4 Gated BiLSTM 73.6 SS BiLSTM 73.6 DIIN * 77.8 78.8 CAFE 78.7 77.9,0
30189,"ESIM domly sample different ratios of the entire training set , i.e. , 0.8 % , 4 % , 20 % and 100 % .",0
30190,""" A "" indicates adding external knowledge in calculating the coattention matrix as in Equation , "" I "" indicates adding external knowledge in collecting local inference information as in Equation , and "" C "" indicates adding external knowledge in composing inference as in Equation .",0
30191,"When we only have restricted training data , i.e. , 0.8 % training set ( about 4,000 samples ) , the baseline ESIM has a poor accuracy of 62.4 % .",0
30192,"When we only add external knowledge in calculating co-attention ( "" A "" ) , the accuracy increases to 66.6 % ( + absolute 4.2 % ) .",0
30193,"When we only utilize external knowledge in collecting local inference information ( "" I "" ) , the accuracy has a significant gain , to 70.3 % ( + absolute 7.9 % ) .",0
30194,"When we only add external knowledge in inference composition ( "" C "" ) , the accuracy gets a smaller gain to 63.4 % ( + absolute 1.0 % ) .",0
30195,"The comparison indicates that "" I "" plays the most important role among the three components in using external knowledge .",0
30196,"Moreover , when we com-pose the three components ( "" A , I , C "" ) , we obtain the best result of 72.6 % ( + absolute 10.2 % ) .",0
30197,"When we use more training data , i.e. , 4 % , 20 % , 100 % of the training set , only "" I "" achieves a significant gain , but "" A "" or "" C "" does not bring any significant improvement .",0
30198,"The results indicate that external semantic knowledge only helps co-attention and composition when limited training data is limited , but always helps in collecting local inference information .",0
30199,"Meanwhile , for less training data , ?",0
30200,is usually set to a larger value .,0
30201,"For example , the optimal ?",0
30202,"on the development set is 20 for 0.8 % training set , 2 for the 4 % training set , 1 for the 20 % training set and 0.2 for the 100 % training set .",0
30203,displays the results of using different ratios of external knowledge ( randomly keep different percentages of whole lexical semantic relations ) under different sizes of training data .,0
30204,Note that here we only use external knowledge in collecting local inference information as it always works well for different scale of the training set .,0
30205,Better accuracies are achieved when using more external knowledge .,0
30206,"Especially under the condition of restricted training data ( 0.8 % ) , the model obtains a large gain when using more than half of external knowledge . :",0
30207,"Accuracies of models of incorporating external knowledge into different NLI components , under different sizes of training data ( 0.8 % , 4 % , 20 % , and the entire training data ) .",0
30208,Test Set,0
30209,Analysis on the,0
30210,"In addition , shows the results on a newly published test set .",0
30211,Compared with the performance on the SNLI test .,0
30212,"set , the performance of the three baseline models dropped substantially on the test set , with the differences ranging from 22.3 % to 32.8 % inaccuracy .",0
30213,"Instead , the proposed KIM achieves 83.5 % on this test set ( with only a 5.1 % drop in performance ) , which demonstrates its better ability of utilizing lexical level inference and hence better generalizability .",0
30214,displays the accuracy of ESIM and KIM in each replacement - word category of the test set .,0
30215,"KIM outperforms ESIM in 13 out of 14 categories , and only performs worse on synonyms .",0
30216,Analysis by Inference Categories,0
30217,"We perform more analysis ) using the supplementary annotations provided by the MultiNLI dataset , which have 495 samples ( about 1 / 20 of the entire development set ) for both in - domain and out - domain set .",0
30218,We compare against the model outputs of the ESIM model across 13 categories of inference .,0
30219,reports the results .,0
30220,We can see that KIM outperforms ESIM on overall accuracies on both in - domain and cross - domain subset of development set .,0
30221,"KIM outperforms or equals ESIM in 10 out of 13 categories on the cross - domain setting , while only 7 out of 13 categories on in - domain setting .",0
30222,It indicates that external knowledge helps more in crossdomain setting .,0
30223,"Especially , for antonym category in cross - domain set , KIM outperform ESIM significantly (+ absolute 5.0 % ) as expected , because antonym feature captured by external knowledge would help unseen cross - domain samples .",0
30224,"ple , the premise is "" An African person standing in a wheat field "" and the hypothesis "" A person standing in acorn field "" .",0
30225,"As the KIM model knows that "" wheat "" and "" corn "" are both a kind of cereal , i.e , the co-hyponyms relationship in our relation features , KIM therefore predicts the premise contradicts the hypothesis .",0
30226,"However , the baseline ESIM can not learn the relationship between "" wheat "" and "" corn "" effectively due to lack of enough samples in the training sets .",0
30227,"With the help of external knowledge , i.e. , "" wheat "" and "" corn "" having the same hypernym "" cereal "" , KIM predicts contradiction correctly .",0
30228,Case Study,0
30229,Conclusions,0
30230,"Our neural - network - based model for natural language inference with external knowledge , namely KIM , achieves the state - of - the - art accuracies .",0
30231,"The model is equipped with external knowledge in its main components , specifically , in calculating coattention , collecting local inference , and composing inference .",0
30232,We provide detailed analyses on our model and results .,0
30233,The proposed model of infusing neural networks with external knowledge may also help shed some light on tasks other than NLI .,0
30234,title,0
30235,End - To - End Memory Networks,1
30236,abstract,0
30237,We introduce a neural network with a recurrent attention model over a possibly large external memory .,0
30238,"The architecture is a form of Memory Network [ 23 ] but unlike the model in that work , it is trained end - to - end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .",0
30239,It can also be seen as an extension of RNNsearch [ 2 ] to the case where multiple computational steps ( hops ) are performed per output symbol .,0
30240,The flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering [ 22 ] and to language modeling .,0
30241,"For the former our approach is competitive with Memory Networks , but with less supervision .",0
30242,"For the latter , on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs .",0
30243,In both cases we show that the key concept of multiple computational hops yields improved results .,0
30244,"Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the service of answering a question or completing a task , and models that can describe long term dependencies in sequential data .",0
30245,"Recently there has been a resurgence in models of computation using explicit storage and a notion of attention [ 23 , 8 , 2 ] ; manipulating such a storage offers an approach to both of these challenges .",0
30246,"In [ 23 , 8 , 2 ] , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the actions of neural networks .",0
30247,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .",1
30248,Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,1
30249,"The model in that work was not easy to train via backpropagation , and required supervision at each layer of the network .",0
30250,"The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .",1
30251,"Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .",1
30252,"We will show experimentally that the multiple hops over the long - term memory are crucial to good performance of our model on these tasks , and that training the memory representation can be integrated in a scalable manner into our end - to - end neural network model .",0
30253,"Our model takes a discrete set of inputs x 1 , ... , x n that are to be stored in the memory , a query q , and outputs an answer a .",0
30254,"Each of the x i , q , and a contains symbols coming from a dictionary with V words .",0
30255,"The model writes all x to the memory up to a fixed buffer size , and then finds a continuous representation for the x and q.",0
30256,The continuous representation is then processed via multiple hops to output a .,0
30257,This allows backpropagation of the error signal through multiple memory accesses back to the input during training .,0
30258,1,0
30259,Single Layer,0
30260,"We start by describing our model in the single layer case , which implements a single memory hop operation .",0
30261,We then show it can be stacked to give multiple hops in memory .,0
30262,Input memory representation :,0
30263,"Suppose we are given an input set x 1 , .. , xi to be stored in memory .",0
30264,"The entire set of {x i } are converted into memory vectors {m i } of dimension d computed by embedding each x i in a continuous space , in the simplest case , using an embedding matrix A ( of size dV ) .",0
30265,"The query q is also embedded ( again , in the simplest case via another embedding matrix B with the same dimensions as A ) to obtain an internal state u .",0
30266,"In the embedding space , we compute the match between u and each memory mi by taking the inner product followed by a softmax :",0
30267,where Softmax ( z i ) = e zi / j e zj .,0
30268,Defined in this way p is a probability vector over the inputs .,0
30269,Output memory representation :,0
30270,Each x i has a corresponding output vector c i ( given in the simplest case by another embedding matrix C ) .,0
30271,"The response vector from the memory o is then a sum over the transformed inputs c i , weighted by the probability vector from the input :",0
30272,"Because the function from input to output is smooth , we can easily compute gradients and backpropagate through it .",0
30273,"Other recently proposed forms of memory or attention take this approach , notably Bahdanau et al. and Graves et al. , see also .",0
30274,Generating the final prediction :,0
30275,"In the single layer case , the sum of the output vector o and the input embedding u is then passed through a final weight matrix W ( of size V d ) and a softmax to produce the predicted label : = Softmax ( W ( o + u ) )",0
30276,The overall model is shown in .,0
30277,"During training , all three embedding matrices A , B and C , as well as Ware jointly learned by minimizing a standard cross - entropy loss between and the true label a .",0
30278,Training is performed using stochastic gradient descent ( see Section 4.2 for more details ) .,0
30279,Multiple Layers,0
30280,We now extend our model to handle K hop operations .,0
30281,The memory layers are stacked in the following way :,0
30282,The input to layers above the first is the sum of the output o k and the input u k from layer k ( different ways to combine o k and u k are proposed later ) :,0
30283,"Each layer has it s own embedding matrices A k , C k , used to embed the inputs {x i }.",0
30284,"However , as discussed below , they are constrained to ease training and reduce the number of parameters .",0
30285,"At the top of the network , the input to W also combines the input and the output of the top memory layer : = Softmax ( W u K+1 ) = Softmax ( W ( o K + u K ) ) .",0
30286,We explore two types of weight tying within the model :,0
30287,1 .,0
30288,"Adjacent : the output embedding for one layer is the input embedding for the one above , i.e. A k+1 = C k .",0
30289,"We also constrain ( a ) the answer prediction matrix to be the same as the final output embedding , i.e W T = C K , and ( b ) the question embedding to match the input embedding of the first layer , i.e. B = A 1 .",0
30290,"2 . Layer - wise ( RNN - like ) : the input and output embeddings are the same across different layers , i.e. A 1 = A 2 = ... = AK and C 1 = C 2 = ... = C K .",0
30291,"We have found it useful to add a linear mapping H to the update of u between hops ; that is , u k+1 = Hu k + o k .",0
30292,This mapping is learnt along with the rest of the parameters and used throughout our experiments for layer - wise weight tying .,0
30293,A three - layer version of our memory model is shown in .,0
30294,"Overall , it is similar to the Memory Network model in , except that the hard max operations within each layer have been replaced with a continuous weighting from the softmax .",0
30295,"Note that if we use the layer - wise weight tying scheme , our model can be cast as a traditional RNN where we divide the outputs of the RNN into internal and external outputs .",0
30296,"Emitting an internal output corresponds to considering a memory , and emitting an external output corresponds to predicting a label .",0
30297,"From the RNN point of view , u in and Eqn. 4 is a hidden state , and the model generates an internal output p ( attention weights in ) using A .",0
30298,"The model then ingests p using C , updates the hidden state , and soon 1 .",0
30299,"Here , unlike a standard RNN , we explicitly condition on the outputs stored in memory during the K hops , and we keep these outputs soft , rather than sampling them .",0
30300,"Thus our model makes several computational steps before producing an output meant to be seen by the "" outside world "" .",0
30301,Related Work,0
30302,A number of recent efforts have explored ways to capture long - term structure within sequences using RNNs or LSTM - based models .,0
30303,"The memory in these models is the state of the network , which is latent and inherently unstable overlong timescales .",0
30304,The LSTM - based models address this through local memory cells which lock in the network state from the past .,0
30305,"In practice , the performance gains over carefully trained RNNs are modest ( see ) .",0
30306,"Our model differs from these in that it uses a global memory , with shared read and write functions .",0
30307,"However , with layer - wise weight tying our model can be viewed as a form of RNN which only produces an output after a fixed number of time steps ( corresponding to the number of hops ) , with the intermediary steps involving memory input / output operations that update the internal state .",0
30308,Some of the very early work on neural networks by Steinbuch and Piske and Taylor considered a memory that performed nearest - neighbor operations on stored input vectors and then fit parametric models to the retrieved sets .,0
30309,This has similarities to a single layer version of our model .,0
30310,Subsequent work in the 1990 's explored other types of memory .,0
30311,"For example , Das et al . and Mozer et al.",0
30312,introduced an explicit stack with push and pop operations which has been revisited recently by in the context of an RNN model .,0
30313,"Closely related to our model is the Neural Turing Machine of Graves et al. , which also uses a continuous memory representation .",0
30314,"The NTM memory uses both content and address - based access , unlike ours which only explicitly allows the former , although the temporal features that we will introduce in Section 4.1 allow a kind of address - based access .",0
30315,"However , in part because we always write each memory sequentially , our model is somewhat simpler , not requiring operations like sharpening .",0
30316,"Furthermore , we apply our memory model to textual reasoning tasks , which qualitatively differ from the more abstract operations of sorting and recall tackled by the NTM .",0
30317,Our model is also related to Bahdanau et al ..,0
30318,"In that work , a bidirectional RNN based encoder and gated RNN based decoder were used for machine translation .",0
30319,The decoder uses an attention model that finds which hidden states from the encoding are most useful for outputting the next translated word ; the attention model uses a small neural network that takes as input a concatenation of the current hidden state of the decoder and each of the encoders hidden states .,0
30320,A similar attention model is also used in Xu et al. for generating image captions .,0
30321,"Our "" memory "" is analogous to their attention mechanism , although is only over a single sentence rather than many , as in our case .",0
30322,"Furthermore , our model makes several hops on the memory before making an output ; we will see below that this is important for good performance .",0
30323,"There are also differences in the architecture of the small network used to score the memories compared to our scoring approach ; we use a simple linear layer , whereas they use a more sophisticated gated architecture .",0
30324,"We will apply our model to language modeling , an extensively studied task .",0
30325,Goodman showed simple but effective approaches which combine n-grams with a cache .,0
30326,Bengio et al.,0
30327,"ignited interest in using neural network based models for the task , with RNNs and LSTMs showing clear performance gains over traditional methods .",0
30328,"Indeed , the current state - of - the - art is held by variants of these models , for example very large LSTMs with Dropout or RNNs with diagonal constraints on the weight matrix .",0
30329,"With appropriate weight tying , our model can be regarded as a modified form of RNN , where the recurrence is indexed by memory lookups to the word sequence rather than indexed by the sequence itself .",0
30330,Synthetic Question and Answering Experiments,0
30331,We perform experiments on the synthetic QA tasks defined in ( using version 1.1 of the dataset ) .,0
30332,"A given QA task consists of a set of statements , followed by a question whose answer is typically a single word ( in a few tasks , answers area set of words Note that for each question , only some subset of the statements contain information needed for the answer , and the others are essentially irrelevant distractors ( e.g. the first sentence in the first example ) .",0
30333,"In the Memory Networks of Weston et al. , this supporting subset was explicitly indicated to the model during training and the key difference between that work and this one is that this information is no longer provided .",0
30334,"Hence , the model must deduce for itself at training and test time which sentences are relevant and which are not .",0
30335,"Formally , for one of the 20 QA tasks , we are given example problems , each having a set of I sentences {x i } where I ? 320 ; a question sentence q and answer a .",0
30336,"Let the jth word of sentence i be x ij , represented by a one - hot vector of length V ( where the vocabulary is of size V = 177 , reflecting the simplistic nature of the QA language ) .",0
30337,The same representation is used for the question q and answer a .,0
30338,"Two versions of the data are used , one that has 1000 training problems per task and a second larger one with 10,000 per task .",0
30339,Model Details,0
30340,"Unless otherwise stated , all experiments used a K = 3 hops model with the adjacent weight sharing scheme .",0
30341,"For all tasks that output lists ( i.e. the answers are multiple words ) , we take each possible combination of possible outputs and record them as a separate answer vocabulary word .",0
30342,Sentence Representation :,0
30343,In our experiments we explore two different representations for the sentences .,0
30344,The first is the bag - of - words ( BoW ) representation that takes the sentence,0
30345,".. , x in } , embeds each word and sums the resulting vectors : e.g mi = j",0
30346,Ax ij and c i = j Cx ij .,0
30347,The input vector u representing the question is also embedded as a bag of words : u = j Bq j .,0
30348,"This has the drawback that it can not capture the order of the words in the sentence , which is important for some tasks .",0
30349,We therefore propose a second representation that encodes the position of words within the sentence .,0
30350,"This takes the form : mi = j l j Ax ij , where is an element - wise multiplication .",0
30351,"l j is a column vector with the structure l kj = ( 1 ? j / J ) ? ( k /d ) ( 1 ? 2 j / J ) ( assuming 1 - based indexing ) , with J being the number of words in the sentence , and dis the dimension of the embedding .",0
30352,"This sentence representation , which we call position encoding ( PE ) , means that the order of the words now affects mi .",0
30353,"The same representation is used for questions , memory inputs and memory outputs .",0
30354,Temporal Encoding :,0
30355,"Many of the QA tasks require some notion of temporal context , i.e. in the first example of Section 2 , the model needs to understand that Sam is in the bedroom after he is in the kitchen .",0
30356,"To enable our model to address them , we modify the memory vector so",0
30357,is the ith row of a special matrix TA that encodes temporal information .,0
30358,The output embedding is augmented in the same way with a matrix Tc ( e.g. c i = j Cx ij + T C ( i ) ) .,0
30359,Both TA and T C are learned during training .,0
30360,"They are also subject to the same sharing constraints as A and C. Note that sentences are indexed in reverse order , reflecting their relative distance from the question so that x 1 is the last sentence of the story .",0
30361,"Learning time invariance by injecting random noise : we have found it helpful to add "" dummy "" memories to regularize TA .",0
30362,"That is , at training time we can randomly add 10 % of empty memories to the stories .",0
30363,We refer to this approach as random noise ( RN ) .,0
30364,Training Details,0
30365,"The training procedure we use is the same as the QA tasks , except for the following .",0
30366,"For each mini-batch update , the 2 norm of the whole gradient of all parameters is measured 5 and if larger than L = 50 , then it is scaled down to have norm L.",1
30367,This was crucial for good performance .,0
30368,"We use the learning rate annealing schedule from , namely , if the validation cost has not decreased after one epoch , then the learning rate is scaled down by a factor 1.5 .",1
30369,"Training terminates when the learning rate drops below 10 ? 5 , i.e. after 50 epochs or so .",0
30370,"Weights are initialized using N ( 0 , 0.05 ) and batch size is set to 128 .",1
30371,"On the Penn tree dataset , we repeat each training 10 times with different random initializations and pick the one with smallest validation cost .",1
30372,"However , we have done only a single training run on Text8 dataset due to limited time constraints .",0
30373,"compares our model to RNN , LSTM and Structurally Constrained Recurrent Nets ( SCRN ) baselines on the two benchmark datasets .",0
30374,Note that the baseline architectures were tuned in to give optimal perplexity 6 .,0
30375,Our MemN2N approach achieves lower perplexity on both datasets ( 111 vs 115 for RNN / SCRN on Penn and 147 vs 154 for LSTM on Text8 ) .,0
30376,"Note that MemN2N has ? 1.5x more parameters than RNNs with the same number of hidden units , while LSTM has ? 4x more parameters .",0
30377,"We also vary the number of hops and memory size of our MemN2N , showing the contribution of both to performance ; note in particular that increasing the number of hops helps .",0
30378,"In , we show how Mem N2N operates on memory with multiple hops .",0
30379,It shows the average weight of the activation of each memory position over the test set .,0
30380,"We can see that some hops concentrate only on recent words , while other hops have more broad attention overall memory locations , which is consistent with the idea that succesful language models consist of a smoothed n-gram model and a cache .",0
30381,"Interestingly , it seems that those two types of hops tend to alternate .",0
30382,"Also note that unlike a traditional RNN , the cache does not decay exponentially : it has roughly the same average activation across the entire memory .",0
30383,This maybe the source of the observed improvement in language modeling .,0
30384,Baselines,0
30385,We compare our approach 2 ( abbreviated to MemN2N ) to a range of alternate models :,0
30386,"MemNN : The strongly supervised AM + NG + NL Memory Networks approach , proposed in .",1
30387,This is the best reported approach in that paper .,0
30388,It uses a max operation ( rather than softmax ) at each layer which is trained directly with supporting facts ( strong supervision ) .,0
30389,"It employs n-gram modeling , nonlinear layers and an adaptive number of hops per query .",0
30390,MemNN- WSH :,1
30391,A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training .,1
30392,"Since we are unable to backpropagate through the max operations in each layer , we enforce that the first memory hop should share at least one word with the question , and that the second memory hop should share at least one word with the first hop and at least one word with the answer .",0
30393,"All those memories that conform are called valid memories , and the goal during training is to rank them higher than invalid memories using the same ranking criteria as during strongly supervised training .",0
30394,"LSTM : A standard LSTM model , trained using question / answer pairs only ( i.e. also weakly supervised ) .",1
30395,"For more detail , see .",0
30396,Results,0
30397,Language Modeling Experiments,0
30398,The goal in language modeling is to predict the next word in a text sequence given the previous words x .,0
30399,We now explain how our model can easily be applied to this task .,0
30400,"118 111 -----  We now operate on word level , as opposed to the sentence level .",0
30401,Thus the previous N words in the sequence ( including the current ) are embedded into memory separately .,0
30402,"Each memory cell holds only a single word , so there is no need for the BoW or linear mapping representations used in the QA tasks .",0
30403,We employ the temporal embedding approach of Section 4.1 .,0
30404,"Since there is no longer any question , q in is fixed to a constant vector 0.1 ( without embedding ) .",0
30405,The output softmax predicts which word in the vocabulary ( of size V ) is next in the sequence .,0
30406,"A cross-entropy loss is used to train model by backpropagating the error through multiple memory layers , in the same manner as the QA tasks .",0
30407,"To aid training , we apply ReLU operations to half of the units in each layer .",0
30408,"We use layer - wise ( RNN - like ) weight sharing , i.e. the query weights of each layer are the same ; the output weights of each layer are the same .",0
30409,"As noted in Section 2.2 , this makes our architecture closely related to an RNN which is traditionally used for language modeling tasks ; however here the "" sequence "" over which the network is recurrent is not in the text , but in the memory hops .",0
30410,"Furthermore , the weight tying restricts the number of parameters in the model , helping generalization for the deeper models which we find to be effective for this task .",0
30411,We use two different datasets :,0
30412,Penn Tree Bank :,0
30413,"This consists of 929k / 73 k / 82 k train / validation / test words , distributed over a vocabulary of 10 k words .",0
30414,The same preprocessing as was used .,0
30415,"Text8 : This is a a pre-processed version of the first 100M million characters , dumped from Wikipedia .",0
30416,This is split into 93.3M / 5.7M / 1M character train / validation / test sets .,0
30417,"All word occurring less than 5 times are replaced with the < UNK > token , resulting in a vocabulary size of ?44 k .",0
30418,Conclusions and Future Work,0
30419,In this work we showed that a neural network with an explicit memory and a recurrent attention mechanism for reading the memory can be successfully trained via backpropagation on diverse tasks from question answering to language modeling .,0
30420,Compared to the Memory Network implementation of there is no supervision of supporting facts and so our model can be used in a wider range of settings .,0
30421,"Our model approaches the same performance of that model , and is significantly better than other baselines with the same level of supervision .",0
30422,"On language modeling tasks , it slightly outperforms tuned RNNs and LSTMs of comparable complexity .",0
30423,On both tasks we can see that increasing the number of memory hops improves performance .,0
30424,"However , there is still much to do .",0
30425,"Our model is still unable to exactly match the performance of the memory networks trained with strong supervision , and both fail on several of the 1 k QA tasks .",0
30426,"Furthermore , smooth lookups may not scale well to the case where a larger memory is required .",0
30427,"For these settings , we plan to explore multiscale notions of attention or hashing , as proposed in . :",0
30428,Examples of attention weights during different memory hops for the bAbi tasks .,0
30429,The model is PE + LS + RN with 3 memory hops that is trained separately on each task with 10 k training data .,0
30430,The support column shows which sentences are necessary for answering questions .,0
30431,"Although this information is not used , the model succesfully learns to focus on the correct support sentences on most of the tasks .",0
30432,The hop columns show where the model put more weight ( indicated by values and blue color ) during its three hops .,0
30433,The mistakes made by the model are highlighted by red color .,0
30434,Appendix A Results on 10 k QA dataset,0
30435,title,0
30436,PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION,1
30437,abstract,0
30438,Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question - aware passage attention model and selfmatching attention model .,0
30439,Our research proposes phase conductor ( PhaseCond ) for attention models in two meaningful ways .,0
30440,"First , PhaseCond , an architecture of multi-layered attention models , consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow .",0
30441,"Second , we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives .",0
30442,"We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset , showing that our model significantly outperforms both stateof - the - art single - layered and multiple - layered attention models .",0
30443,We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models .,0
30444,* Authors ' contributions are equally important to this work .,0
30445,INTRODUCTION,0
30446,"Attention - based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning , and speech recognition .",0
30447,"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors .",1
30448,uses attention mechanism in Pointer Network to detect an answer boundary by predicting the start and the end indices in the passage .,0
30449,introduces a bi-directional attention flow network that attention models are decoupled from the recurrent neural networks .,0
30450,employs a coattention mechanism that attends to the question and document together .,0
30451,uses a gated attention network that includes both question and passage match and self - matching attentions .,0
30452,Both and employs the structure of multi-hops or iterative aligner to repeatedly fuse the passage representation with the question representation as well as the passage representation itself .,0
30453,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .",1
30454,There are two motivations .,0
30455,"First , previous research on the self - attention model is to purely capture long - distance dependencies , and therefore a multi-hops architecture is used to alternatively captures question - aware passage representations and refines the results by using a self - attention model .",1
30456,"In contrast to the multi-hops and interactive architecture , our motivation of using the self - attention model for machine comprehension is to propagate answer evidence which is derived from the preceding question - passage representation layers .",0
30457,"This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .",0
30458,"Second , unlike the domains such as machine translation which jointly align and translate words , question - passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs .",1
30459,"Despite the attention models ' success on the machine comprehension task , there has not been any other work exploring learning to encode multiple representations of question or passage from different perspectives for different parts of attention functions .",0
30460,"More specifically , most approaches use two same question representations U for the question - passage attention model ?( H , U ) U , where H is the passage representation .",0
30461,Our hypothesis is that attention models can be more effective by learning different encoders fora question representation U and a question representation V from different aspects .,0
30462,The key differences between our proposed model and competing approaches are summarized at .,0
30463,Our contributions are threefold :,0
30464,"1 ) we proposed a phase conductor for attention models containing multiple phases , each with a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow , 2 ) we present an improved attention function for question - passage attention based on two kinds of encoders : an independent question encoder and a weight - sharing encoder jointly considering the question and the passage , as opposed to most previous works which only using the same encoder for one attention model , and 3 ) we provide both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models .",0
30465,Experimental results show that our proposed PhaseCond lead to significant performance improvements over the state - of - the - art single - layered and multilayered attention models .,0
30466,"Moreover , we observe several meaningful trends : a ) during the questionpassage attention phase , repeatedly attending the passage with the same question representation "" forces "" each passage word to become increasingly closer to the original question representation , and therefore increasing the number of layers has a risk of degrading the network performance , b ) during the self - attention phase , the self - attention 's alignment weights of the second layer become noticeably "" sharper "" than the first layer , suggesting the importance of fully propagating evidence through the passage itself .",0
30467,MODEL ARCHITECTURE,0
30468,"We proposed phased conductor model ( or PhaseCond ) , which consisting of multiple phases and each phase has two parts , a stack of attention layers",0
30469,Land a stack of fusion layers F controlling information flow .,0
30470,"In our model , a fusion layer F can bean inner fusion layer F inner inside of a stack of attention layers , or an outer fusion layer F outer immediately following a stack of attention layers .",0
30471,"Without loss of generality , PhaseCond 's configurable computational path for two - phase , a question - passage attention phase containing N question - passage attention layers L Q , and a selfattention phase containing K self - attention layers L S , can be defined as question - attended passage representation is directly matching against itself , for the purpose of propagating information through the whole passage detailed in Section 2.3 .",0
30472,"For each self - attention layer , we configure an inner fusion layer to obtain a gated representation that is learned to decide how much of the current output is fused by the input from the previous layer detailed in Section 2.3.1 .",0
30473,"Finally , the fused vectors are sent to the output layer to predict the boundary of the answer span described in Section 2.4 .",0
30474,ENCODER LAYERS,0
30475,The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations .,0
30476,Here we choose a bi-directional Long Short - Term Memory ( LSTM ) to obtain more abstract representations for words in passages and questions .,0
30477,"Different from the commonly used approaches that every single model has exactly one question and passage encoder , our encoder layers simultaneously calculate multiple question and passage representations , for the purpose of serving different parts of attention functions of different phases .",0
30478,"We use two types of encoders , independent encoder and shared encoder .",0
30479,"In terms of independent encoder , a bi-directional LSTM is used to",0
30480,where v Q j ?,0
30481,R 2d are concatenated hidden states of two independent BiLSTM for the j - th question word and dis the hidden size .,0
30482,"In terms of shared encoder , we jointly produce new representation h P 1 , . . . , h P n and u Q 1 , . . . , u Q m for the passage and question via a shared bi-directional LSTM ,",0
30483,where h P i ?,0
30484,R 2 d and u Q j ?,0
30485,"R 2d are concatenated hidden states of BiLSTM for the i - th passage word and j- th question word , sharing the same trainable BiLSTM parameters .",0
30486,QUESTION - PASSAGE ATTENTION LAYERS,0
30487,The process of representing a passage with a question essentially includes two sub - tasks :,0
30488,"1 ) calculating the similarity between the question and different parts of the passage , and 2 ) representing the passage part with the given question depending on how similar they are .",0
30489,A single question - passage attention layer is illustrated in .,0
30490,"In this model , at the t - th layer an alignment matrix At ?",0
30491,"R m , whose shape equals the number of words n in a passage multiplied by the number of words min a question , is derived by aligning the passage representation at the t ?",0
30492,"1 layer with the shared weight question representation ,",0
30493,where h t?1,0
30494,i is the the i - th passage word representation at the t ?,0
30495,"1 layer , h 0 i equals to h P i calculated from Eq 2 , u Q j calculated from Eq 3 is the same for all the layers , the alignment matrix element At ( i , j ) is a scalar , denoting the similarity between the i - th passage word and the j - th question word by using dot product of the passage word vector and the question word vector .",0
30496,"Given the alignment matrix element as weights , we compute the new passage representation ht i for the t- th layer by using weighted average overall the independent question representation v Q calculated from Eq 1 , as shown in the following .",0
30497,OUTER FUSION LAYERS,0
30498,"For each question - passage attention layer , it s output of ht i , where t ?",0
30499,"N , is concatenated to form the final output vector to represent the i - th passage word",0
30500,Increasing the number of layers,0
30501,N allows an increasingly more complex representation fora passage word .,0
30502,"In order to regulate the flow of N question - passage attention layers and to prevent the over - fitting problem , we use fusion layers , which is highway networks using of GRUlike gating units and taking C 0 i as it s input :",0
30503,where t ?,0
30504,"K ,",0
30505,"K is the number of fusion layers , W t C , W t z are the weights , b t C , b t z are the bias of t-th fusion layer , and the transform gate z t is a non-linear activation function .",0
30506,The final result of fusion layers C Ni ?,0
30507,R 2N dis sent to self - attention models as input for processing .,0
30508,SELF - ATTENTION LAYERS,0
30509,"Following the question - passage attention layers , self - attention layers propagate evidence through the passage context .",0
30510,"This process is similar in spirit to the steps of exploring similarity or redundancy between answer candidates ( e.g. , "" J.F.K "" and "" Kennedy "" can , in fact , be equivalent despite their different surface forms ) that have been shown to be very effective during answer merging stage .",0
30511,"More generally , propagating evidence among the passage words allows correct answers to have better evidence for the question than the rest part of the passage .",0
30512,"For a single self - attention layer , we first compute a self alignment matrix St ?",0
30513,"R nn by comparing the passage representation itself , where h t ?1 i is the i - th passage word as input for the t- th self - attention layer , initial value h 0 i is defined as the final fused result C Ni from question - passage attention model in section 2.2.1 .",0
30514,"Given the alignment matrix element as weights , evidences are propagate from the previous layer to the next to produce the new passage representation ht i by using the weighted average overall the t ?",0
30515,1 layer passage representation :,0
30516,where h t?,0
30517,1 k is the passage representation for the k - th word at the t ?,0
30518,"1 self - attention layer , B ti ?",0
30519,"R 2N dis the output the self - attention layer and it will be sent to a fusion layer , described in section 2.3.1 , to obtain the t- th layer passage representation ht i .",0
30520,INNER FUSION LAYERS,0
30521,"To efficiently propagate evidence through the passage , we refine the self - attended representations by using multiple layers .",0
30522,"At the end of each self - attention layer , a GRU - like gating mechanism is used to decide what information to store and send to the next self - attention layer , by merging the newly produced representation of the current layer and the input representation from the previous layer , B",0
30523,"where W t B , W t fare the weights , b t B , b t fare the bias of t-th fusion layer , and ft is a non-linear activation function .",0
30524,"The output ht i , whose dimensions are the same as its input vector B ti , is then sent to the next layer of self - attention model as input to calculate Eq 9 and Eq 10 .",0
30525,OUTPUT LAYERS,0
30526,We directly follow and use a memory - based answer pointer networks to predict boundary of the answer .,0
30527,The memory - based answer pointer network contains multiple hops .,0
30528,"For the t- th hop , the pointer network produces the probability distribution of the start index pt sand the end index pt e using a pointer network respectively .",0
30529,"If the t- th hop is not the last hop , then the hidden states for the start and end indices are transformed and fed into the next - hop prediction .",0
30530,The training loss is defined as the sum of the negative log probabilities of the last hop start and end indices averaged overall examples .,0
30531,EXPERIMENTS AND ANALYSIS,0
30532,This paper focuses on the Stanford Question Answering Dataset ( SQuAD ) to train and evaluate our model .,0
30533,"SQuAD , which has gained a significant attention recently , is a largescale dataset consisting of more than 100,000 questions manually created through crowdsourcing on 536 Wikipedia articles .",0
30534,"The dataset is randomly partitioned into a training set ( 80 % ) , a development set ( 10 % ) , and a blinded test set ( 10 % ) .",0
30535,"Two metrics are used to perform evaluation : Exact Match ( EM ) score which calculates the ratio of questions that are answered correctly by exact string match , and F 1 score which calculates the harmonic mean of the precision and recall between predicted answers and ground true answers at the character level .",0
30536,TRAINING DETAILS,0
30537,Our input for the encoding layer in Section 2.1 includes a list of commonly used features .,0
30538,"We use pre-trained GloVe 100 - dimensional word vectors , parts - of - speech tag features , named - entity tag feature , and binary features of exact matching which indicate if a passage word can be exactly matched to any question word and vice versa .",0
30539,"Following , we also use question type ( what , how , who , when , which , where , why , be , and other ) features where each type is represented by a trainable embedding .",0
30540,We use CNN with 100 one - dimensional filters with width 5 to encode character level embedding .,0
30541,The hidden size is set as 128 for all the LSTM layers .,0
30542,Dropout are used for all the learnable parameters with a ratio as 0.2 .,0
30543,"We use the Adam optimizer ( Kingma & Ba , 2014 ) with an initial learning rate of 0.0006 , which is halved when a bad checkpoint is met .",0
30544,MAIN RESULTS OF MODEL COMPARISON,0
30545,"We compare our proposed model PhaseCond with a multi-layered attention model , the Iterative Aligner , as well as various other recently published systems , which include a single - layered model , BIDAF , and a single - layered model containing both the question - passage attention and self - attention , RNET .",0
30546,"We first compare our proposed model PhaseCond with Iterative Aligner , which is employed by two top ranked systems and MReader on the SQuAD leaderboard 1 .",0
30547,"Since our goal is to show the effectiveness of our proposed model PhaseCond , we use a baseline system implementing MReader for the direct comparison .",0
30548,"All the experiment settings are the same for PhaseCond and Iterative Aligner including the number of attention layers , input features , optimizer and learning rate , number of training steps and etc .",0
30549,"As shown in which summarizes the performance of single models , we achieve steady improvements when 1 ) additional question encoders are used to extend the passage - question attention function , denoted as QPAtt + , as detailed in Section 2.1 and Section 2.2 , and 2 ) on top of that , using PhaseCond making our model better than using Iterative Aligner .",0
30550,"Specifically , PhaseCond 's computational path for two question - aware passage attention layers L Q and two self - attention layers L S goes from L Q 1 ? L Q 2 ?",0
30551,F outer ? L S 1 ?,0
30552,F inner ? L S 2 ?,0
30553,F inner .,0
30554,"On the other hand , Iterative Aligner builds path in turn through different kinds of attention layers :",0
30555,"To perform a fair comparison as much as possible , we collect the results of BiDAF and RNET from their recently published papers instead of using the up - to - date performance scores posted on the SQuAD Leaderboard .",0
30556,"Our directly available baseline is one implementation of MReader , re-named as Iterative Aligner which has very similar results as those of MReader posted on the SQuAD Leaderboard on Jul 14 , 2017 .",0
30557,Single Model Ensemble Models Dev Set,0
30558,"Test Set Dev Set Test Set Attention - based Systems EM / F1 EM / F1 EM / F1 EM / F1 BiDAF 67.7 / 77.3 68.0 / 77.3 73.3 / 81.1 73.3 / 81.1 RNET 71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader N / A 71.0 / 80.1 N/A 74.3 / 82.4 Iterative Aligner As shown in , in the single model setting , our model PhaseCond is clearly more effective than all the single - layered models ( BiDAF and RNET ) and multi -layered models ( MReader and Iterative Aligner ) .",0
30559,"We draw the same conclusion for the ensemble model setting , despite that the RNET works better on the Dev EM measure .",0
30560,"The EM result of our baseline Iterative Aligner is lower than RNET , confirming that the problem is not caused by our proposed model .",1
30561,"Our explanations is that 1 ) RNET uses a different feature set ( e.g. , Glo Ve 300 dimensional word vectors are employed ) and different encoding steps ( e.g. , three GRU layers are used for encoding question and passage representations ) , and 2 ) RNET uses a different ensemble method from our implementation .",1
30562,shows the performance with different number of layers for both question - passage attention phase and self - attention phase .,1
30563,We change the layer number separately to compare the performance .,0
30564,"For the question - passage attention phase , using single layer does n't degrade the performance significantly from the default setting of two layers , resulting in a different conclusion from ; .",1
30565,"Intuitively , this is largely expected because representing the passage repeatedly with the same question does n't constantly add more information .",0
30566,"In contrast , multiple stacking layers are needed to allow the evidence fully propagated through the passage .",1
30567,This is exactly what we observed in two stacking layered self - attention phase .,0
30568,ANALYSIS ON ATTENTION LAYERS,0
30569,"In , we visualize the attention matrices for each layer to show dynamic attention changes .",0
30570,The model is based on the main setting which has two question - passage layers and two self - attention layers .,0
30571,We observed several critical trends .,0
30572,"First , the first layer of the question - passage attention phase can successfully align question keywords with the corresponding passage keywords , as shown in .",0
30573,"For example , the question keyword "" represented "" have been successfully aligned with related passage keywords "" champion "" , "" defeated "" , and "" earned "" .",0
30574,"Second , patterns of striped color in indicate similar weights among all the passage words , meaning that it becomes indistinguishable among passage words , and therefore adding another layer of question - passage attention model degrades the alignment quality dramatically .",0
30575,"This observation is meaningful which ( c ) The first layer of self - attention . Generally , the darker the color is the higher the weight is ( the only exception is which contains negative values ) .",0
30576,"Given the question "" Which NFL team represented the AFC at Super Bowl 50 ? "" , the system correctly detects the answer "" Denver Broncos "" from the passage part "" The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 2410 to earn their third Super Bowl title . """,0
30577,shows that repeatedly representing a passage word regarding the same question representation can make the passage embedding become closer to the original question representation .,0
30578,"Third , when comparing and , we observed that the color is diluted for most of the weights in the second layer of self - attention phase , meanwhile a small portion of weights is strengthened , suggesting that information propagation is converging .",0
30579,"For example , in as the last attention layer , the phrase "" Denver Broncos "" becomes more concentrated on the phrase "" Carolina Panthers "" than that of .",0
30580,"In contrast , "" Denver Broncos "" becomes less focused on the other keywords ( e.g. , "" champion "" and "" title "" ) of the same passage .",0
30581,CONCLUSION,0
30582,"In this paper , we introduce a general framework PhaseCond , on multi-layered attention models with two phases including a question - aware passage representation phase and an evidence propagation phase .",0
30583,"The question - aware passage representation phase has a stack of question - aware passage attention models , followed by outer fusion layers that regularize concatenated passage representations .",0
30584,"The evidence propagation phase has a stack of self - attention layers , each of which is followed by inner fusion layers that control the information to propagate and output .",0
30585,"Also , an improved attention mechanism for PhaseCond is proposed based on a popular dot -product attention function by simultaneously encoding both the independent question embedding layers , the weight - sharing question embedding layer and weight - sharing passage embedding layer .",0
30586,The experimental results show that our model significantly outperforms single - layered or multiple - layered attention networks on blinded test data of SQuAD .,0
30587,"Moreover , our in - depth quantitative analysis and visualizations provide meaningful findings for both question - aware passage attention mechanism and self - matching attention mechanism .",0
30588,title,0
30589,Parameter Re-Initialization through Cyclical Batch Size Schedules,1
30590,abstract,0
30591,Optimal parameter initialization remains a crucial problem for neural network training .,1
30592,A poor weight initialization may take longer to train and / or converge to sub-optimal solutions .,0
30593,"Here , we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process .",0
30594,We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training .,0
30595,"We evaluate our methods through extensive experiments on tasks in language modeling , natural language inference , and image classification .",0
30596,"We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to 61 % , in addition to its flexibility in enabling snapshot ensembling and use with adversarial training .",0
30597,Introduction,0
30598,"Despite many promising empirical results at using stochastic optimization methods to train highly non-convex modern deep neural networks , we still lack theoretically robust practical methods which are able to escape saddle points and / or sub-optimal local minima and converge to parameters that retain high testing performance .",0
30599,This lack of understanding leads to practical training challenges .,0
30600,Stochastic Gradient Descent ( SGD ) is currently the de-facto optimization method for training deep neural networks ( DNNs ) .,0
30601,"Through extensive hyper - parameter tuning , SGD can avoid poor local optima and achieve good generalization ability .",0
30602,One important hyper - parameter that can significantly affect SGD performance is the weight initialization .,0
30603,"For instance , initializing the weights to all zeros or all ones leads to extremely poor performance .",0
30604,"Different approaches have been proposed for weight initialization such as Xavier , MSRA , Ortho , LSUV .",0
30605,These are mostly agnostic to the model architecture and the specific learning task .,0
30606,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,1
30607,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .",1
30608,This problem has been explored extensively in Bayesian optimization .,0
30609,"For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .",1
30610,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .",1
30611,"As argued in , both learning rate and batch size can be used to control the noise of SGD but the latter has an advantage in that it allows more parallelization opportunity .",0
30612,The idea of using batch size to control the noise in a simple cyclical schedule was recently proposed in .,0
30613,"Here , we build upon this work by studying different cyclical annealing strategies for a wide range of problems .",1
30614,"Additionally , we discuss how this can be combined with anew adversarial regularization scheme recently proposed in , as well as prior work in order to obtain ensembles of models at no additional cost .",0
30615,"In summary , our contributions are as follows :",0
30616,"We explore different cyclical batch size ( CBS ) schedules for training neural networks inspired by Bayesian statistics , particularly adaptive MCMC methods .",0
30617,The CBS schedule leads to multiple perplexity improvement ( up to 7.91 ) in language modeling and minor improvements in natural language inference and image classification .,0
30618,"Furthermore , we show that CBS schedule can alleviate problems with overfitting and sub-optimal parameter initialization .",0
30619,"Additionally , CBS schedules require up to 3 fewer SGD iterations due to larger batch sizes , which allows for more parallelization opportunity .",0
30620,This reflects the benefit of cycling the batch size instead of the learning rate as in prior work We showcase the flexibility of CBS schedules for use with additional techniques .,0
30621,We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost .,0
30622,"In addition , we show that CBS schedule can be combined with other approaches such as the recently proposed adversarial regularization to yield further classification accuracy improvement of 0.26 % .",0
30623,Related Work,0
30624,"[ 5 ] introduced Xavier initialization , which keeps the variance of input and output of all layers within a similar range in order to prevent vanishing or exploding values in both the forward and backward passes .",0
30625,"Building off this idea , explored anew strategy known as MSRA to keep the variance constant for all convolutional layers .",0
30626,"proposed an orthogonal initialization ( Ortho ) to achieve faster convergence , and more recently , combined ideas from previous work and showed that a unit variance orthogonal initialization is beneficial for deep models .",0
30627,show that the noise of SGD is controlled by the ratio of learning rate to batch size .,0
30628,The authors argued that the SGD algorithm can be derived through Euler - Maruyama discretization of a Stochastic Differential Equation ( SDE ) .,0
30629,"The SDE dynamics are governed by a "" noise scale "" g ?",0
30630,"N / B for the learning rate , N the training dataset size , and B the batch size .",0
30631,They conclude that a higher noise scale prevents SGD from settling into sharper minima .,0
30632,"This result supports a prior empirical observation that under certain mild assumptions such as NB , the effect of dividing the learning rate by a constant factor is equivalent to that of multiplying the batch size by the same constant factor .",0
30633,"In related work , applied this understanding and used batch size as a knob to control the noise , and empirically showed that the baseline performance could be matched .",0
30634,further explored how to use second - order information and adversarial training to control the noise for training large batch size .,0
30635,"showed using a statistical mechanics argument that many other hyper - parameters in neural network training , e.g. data quality , can also act as temperature knobs .",0
30636,Methods,0
30637,"The goal of neural network optimization is to solve an empirical risk minimization , with a loss function of the form :",0
30638,where ?,0
30639,"is the model parameters , X is the training dataset and l ( x , ? ) is the loss function .",0
30640,Here N = | X | is the cardinality of the training set .,0
30641,"In SGD , a mini-batch , B ? { 1 , 2 , ... , N } is used to compute an ( unbiased ) gradient , i.e. , gt = 1",0
30642,"| B | x?B ? ? l ( x , ? t ) , and this is typically used to optimize ( 1 ) in the form :",0
30643,where ?,0
30644,"t is the learning rate ( step size ) at iteration t , and commonly annealed during training .",0
30645,By Bayes ',0
30646,"Theorem , given the input data , X , a prior distribution on the model parameters , P ( ? ) , and a likelihood function , P ( X |? ) , the posterior distribution , P (? | X ) , is :",0
30647,"From this Bayesian perspective , the goal of the neural network training is to find the Maximum - A- Posteriori ( MAP ) point for a given prior distribution .",0
30648,"Note that in this context weight initialization and prior distribution are similar , that is a better prior distribution would lead to more informative posterior .",0
30649,"In general , it maybe difficult to design a better prior given only data and a model architecture .",0
30650,"Additionally , the high dimensionality of the NN 's parameter space renders various approaches such as adaptive priors intractable ( e.g. adaptive MCMC algorithms ) .",0
30651,"Hence , we look into an adaptive weight "" re-initialization "" strategy .",0
30652,We start with an input prior ( weight initialization ) and compute an approximate MAP point by annealing the noise in SGD .,0
30653,"Once we compute the MAP point , we use it as anew initialization of the neural network weights , and restart the noise annealing schedule .",0
30654,We then iteratively repeat this process through the training process .,0
30655,"One approach to controlling the level of noise in SGD is via the learning rate , which is the approach used in .",0
30656,"However , as discussed in , the batch size can also be used to control SGD noise .",0
30657,The motivation for this is that larger batch sizes allow for parallel execution which can accelerate training .,0
30658,We implement weight re-initialization through cyclical batch size schedules .,0
30659,"The SGD training process is divided into one or more cycles , and in single cycle we gradually increase the batch size to decrease noise .",0
30660,"As the noise level of SGD is annealed , ? will approaches a local minima i.e. , an approximate MAP point of P ( ? | X ) .",0
30661,"Then at the beginning of the subsequent cycle we drop the batch size back down to the initial value , which increases the noise in SGD and "" re-initializes "" the neural network parameters using the previous estimate .",0
30662,Several CBS schedules are shown in .,0
30663,Results,0
30664,We perform a variety of experiments across different tasks and neural network architectures in natural language processing as well as image classification .,0
30665,"We report our experimental findings on language tasks in section 3.1 , and image classification in section 3.2 .",0
30666,We illustrate that CBS schedules can alleviate sub-optimal initialization in section 3.3 .,0
30667,We follow the baseline training method for each task ( for details please see Appendix A ) .,0
30668,"Alongside testing / validation performance , we also report the number of training iterations ( lower values are preferred ) .",0
30669,Language Results,1
30670,Language modeling is a challenging problem due to the complex and long - range interactions between distant words .,1
30671,"One hope is that large / deep models might be able to capture these complex interactions , but large models easily overfit on these tasks and exhibit large gaps between training set and testing set performance .",0
30672,"CBS schedules effectively help us avoid overfitting , and in addition snapshot ensembling enables even greater performance .",1
30673,We evaluate a large variety of CBS schedules to positive results as shown in .,0
30674,"Results are measured in perplexity , a standard figure of merit for evaluating the quality of language models by measuring its prediction of the empirical distribution of words ( lower perplexity value is better ) .",0
30675,"As we can see , the best performing CBS schedules result in significant improvements in perplexity ( up to 7.91 ) over the baseline schedules and also offer reductions in the number of SGD training iterations ( up to 33 % ) .",1
30676,"For example , CBS schedules achieve improvement of 7.91 perplexity improvement on WikiText 2 via CBS - 1 - T and reduce the SGD iterations from 164 k to 111 k via the CBS - 1 - A schedule .",0
30677,Notice that almost all CBS schedules outperform the baseline schedule .,1
30678,shows the training and testing perplexity of the L2 model on PTB and WikiTest 2 as trained via the baseline schedule along with our best CBS schedule ( from ) .,0
30679,Notice the cyclical spikes in training and testing perplexity .,0
30680,"The peaks occur during decreases in batch size , i.e. , increases in noise scale , which could help to escape sub-optimal local minima , and the troughs occur during increases in batch size , i.e. , decreases with noise scale .",0
30681,"In order to support our claim that CBS schedules are especially useful for counteracting overfitting , we conducted additional language modeling experiments on models L1' , L2 ' with PTB and WT2 which use significantly lower dropout ( 0.2 and 0.3 ) than the original L1 , L2 models ( 0.5 and 0.65 ) .",0
30682,"Because these models heavily overfit the training data , we report both the final testing perplexity as well as the best testing perplexity achieve during training .",0
30683,"As seen in To further explore the properties of cyclical batch size schedules , we also evaluate these schedules on natural language inference tasks , as shown in .",0
30684,"In our experiments , CBS schedules do not yield large performance improvements on models like E1 which exhibit smaller disparities between training and testing performance .",1
30685,This is inline with our limitation in that CBS is more effective for models which tend to overfit .,0
30686,"On the other hand , we see a large reduction in training iterations by up to 62 % which is due to higher effective batch size used in CBS than baseline .",0
30687,We also test our CBS schedules on Cifar - 10 and ImageNet. 3 reports the testing accuracy and the number of training iterations for different models on Cifar - 10 .,0
30688,"We see that the CBS schedules match baseline performance , but the number of training iterations used in CBS schedules is up to 2 fewer .",0
30689,Image Classification Results,1
30690,"As seen in , the training curves of CBS schedules also exhibit the aforementioned cyclical spikes both in training loss and testing accuracy .",1
30691,"Similarly in the previously discussed language experiments , these spikes correspond to cycles in the CBS schedules and can bethought of as re-initializations of the neural network weights .",0
30692,We observe that CBS achieves similar performance to the baseline .,1
30693,"We offer further support for the hypothesis that CBS schedules are more effective for overfitting neural networks with experiments on model C4 , which achieves 94.35 % training accuracy and 55 . 55 % testing accuracy on Cifar - 10 .",0
30694,"With CBS - 15 , we see 90.71 % training accuracy and 56. 44 % testing accuracy , which is a larger improvement than that offered by CBS on convolutional models on Cifar - 10 .",1
30695,We also explore combining CBS with the recent adversarial regularization proposed by .,0
30696,Combining CBS - 15 on C2 with this strategy improves accuracy to 94.82 % .,1
30697,This outperforms other schedules shown in .,0
30698,Applying snapshot ensembling on C3 trained with CBS - 15 - 2 leads to improved accuracy of 93. 56 % as compared to 92.58 % .,1
30699,"After ensembling ResNet50 on Imagenet with snapshots from the last two cycles , the performance increases to 76.401 % from 75.336 % .",1
30700,Sub-optimal Initialization,0
30701,"Various effective initialization methods have been proposed previously ; however , when presented with new architectures and new tasks , initialization still needs to be explored empirically and often the final performance varies greatly with different initializations .",0
30702,"In this section , we test if CBS schedules can alleviate the problem of sub-optimal initialization .",0
30703,We test a Gaussian initialization with mean 0 and standard deviation 0.1 on an AlexNet - like model ( C1 ) .,0
30704,The baseline ( BL ) training follows the same setting as described in Appendix A and achieves final accuracy 84.27 % .,0
30705,"For CBS , we use cycle width of 10 with 3 steps .",0
30706,"In particular , CBS 1 denotes a constant learning rate , and achieves final accuracy 85.41 % .",0
30707,CBS 2 decays the learning rate by a factor of 5 at epoch 75 and achieves final accuracy 84.95 % .,0
30708,We keep learning rate high during training because a high noise level helps ?,0
30709,escape sub-optimal local minima .,0
30710,Notice that all CBS methods achieve better generalization performance than the baseline .,0
30711,Conclusions,0
30712,In this work we explored different cyclical batch size ( CBS ) schedules for training neural networks .,0
30713,"We framed the motivation behind CBS schedules through the lens of Bayesian statistical methods , in particular adaptive MCMC algorithms , which seek out better estimates of the posterior starting with a ( poor ) prior distribution .",0
30714,"In the context of neural network training , this translates to re-initialization of the weights via cycling between large and small batch sizes which control the noise in SGD .",0
30715,"We show empirical results which find this cyclical batch size schedule can significantly outperform fixed batch size baselines , especially in networks prone to overfitting or initialized poorly , on the tasks of language modeling , natural language inference , and image classification with LSTMs , CNNs , and ResNets .",0
30716,"In our language modeling experiments , we see that a wide variety of CBS schedules outperform the baseline by up to 7.91 perplexity and up to 33 % fewer training iterations .",0
30717,"For natural language inference and image classification tasks , we observe a reduction in the number of training iterations of up to 61 % , which translates directly into reduced runtime .",0
30718,"Finally , we demonstrate the flexibility of CBS as a building block for ensembling and adversarial training methods .",0
30719,"Ensembling on language modeling yields improvements of up to 11.22 perplexity over the baseline and on image classification , an improvement of up to 1.07 % accuracy .",0
30720,Adversarial training in conjunction with CBS gives a bump in image classification accuracy of 0.26 % .,0
30721,Limitations,0
30722,"We believe that it is very important for every work to state its limitations ( in general , but in particular in this area ) .",0
30723,We performed an extensive variety of experiments on different tasks in order to comprehensively test the algorithm .,0
30724,The primary limitation of our work is that cyclical batch size schedules introduce another hyper - parameter that requires manual tuning .,0
30725,"We note that this is also true for cyclical learning rate schedules , and hope to address this using second order methods as part of future work .",0
30726,"Furthermore , for well initialized models which are not prone to overfitting , single snapshot CBS achieves similar performance to the baseline , although the cyclical ensembling provides a modicum of improvement .",0
30727,A Training Details,0
30728,"Here we catalogue details regarding all tasks , datasets , models , batch schedules , and other hyperparameters used in our experiments .",0
30729,"In all experiments , we try to copy as many hyper - parameters from the original papers as possible .",0
30730,Tasks : We train networks to perform the following supervised learning tasks :,0
30731,Image classification .,0
30732,The network is trained to classify the content of images within a fixed set of object classes .,0
30733,Language modeling .,0
30734,The network is trained to predict the last token in a sequence of English words .,0
30735,Natural Language Inference .,0
30736,"The network is trained to classify the relationship between pairs of English sentences such as that of entailment , contradiction , or neutral .",0
30737,Datasets :,0
30738,We train networks on the following datasets .,0
30739,Cifar ( image classification ) .,0
30740,"The two Cifar ( i.e. , Cifar - 10 / Cifar - 100 ) datasets contain 50 k training images and 10 k testing images , and 10/100 label classes . ImageNet ( image classification ) .",0
30741,"The ILSVRC 2012 classification dataset consists of 1000 label classes , with a total of 1.2 million training images and 50,000 validation images .",0
30742,"During training , we crop the image to 224 224 . PTB ( language modeling ) .",0
30743,The Penn Tree Bank dataset consists of preprocessed and tokenized sentences from the Wall Street Journal .,0
30744,"The training set is 929 k words , the validation set 73 k words , and test set 82 k words .",0
30745,"The total vocabulary size is 10 k , and all words outside the vocabulary are replaced by a placeholder token . WikiText 2 ( language modeling ) .",0
30746,The Wikitext 2 dataset is modeled after the Penn Tree Bank dataset and consists of preprocessed and tokenized sentences from Wikipedia .,0
30747,"The training set is 2089 k words , the validation set 218 k words , and the test set 246 k words .",0
30748,"The total vocabulary size is 33 k , and all words outside the vocabulary are replaced by a placeholder token . SNLI ( natural language inference ) .",0
30749,"The SNLI dataset consists of pairs of sentences annotated with one of three labels regarding textual entailment information : contradiction , neutral , or entailment .",0
30750,"The training set contains 550 k pairs , and the validation set contains 10 k pairs .",0
30751,MultiNLI ( natural language inference .,0
30752,The MultiNLI dataset is modeled after the SNLI dataset and contains a training set of 393 k pairs and a validation set of 20 k pairs .,0
30753,Model Architecture .,0
30754,We implement the following neural network architectures .,0
30755,"C1 . AlexNet - like on Cifar - 10 dataset as in [ C1 ] , trained on the task of image classification .",0
30756,"We train for 200 epochs with an initial learning rate 0.02 which we decay by a factor of 5 at epoch 30 , 60 .",0
30757,"In particular , we use initial learning rate 0.05 for cyclic scheduling . C2 . WRes Net 16 - 4 on Cifar - 10 dataset , trained on the task of image classification .",0
30758,"We train for 200 epochs with an initial learning rate 0.1 which we decay by a factor of 5 at epoch 60 , 120 , and 180 . C3 . ResNet20 on Cifar - 10 dataset .",0
30759,"We train it for 160 epochs with initial learning rate 0.1 , and decay a factor of 5 at epoch 80 , 120 .",0
30760,"In particular , we use initial learning rate 0.05 for cyclic scheduling .",0
30761,C4 . MLP3 network from .,0
30762,The network consists of 3 fully connected layers with 512 units each and ReLU activations .,0
30763,"As a baseline , we train this network with vanilla SGD for 240 epochs with a batch size of 100 and an initial learning rate of 0.1 , which is decayed by a factor of 10 at 150 and 225 epochs .",0
30764,"I1 . ResNet50 on Image Net dataset , trained on the task of image classification for 90 epochs with initial learning rate 0.1 which we decay by a factor of 10 at epoch 30 , 60 and 80 .",0
30765,"L1 . Medium Regularized LSTM , trained on the task of language modeling .",0
30766,"We use 50 % dropout on non-recurrent connections and train for 39 epochs with initial learning rate of 20 , decaying by a factor of 1.2 every epoch after epoch 6 .",0
30767,We set a backpropagation - through - time limit of 35 steps and clip the max gradient norm at 0.25 .,0
30768,"L2 . Large Regularized LSTM , trained on the task of language modeling .",0
30769,"We use 65 % dropout on non-recurrent connections and train for 55 epochs with initial learning rate of 20 , decaying by a factor of 1.15 every epoch after epoch",0
30770,We set a backpropagation - through - time limit of 35 steps and clip the max gradient norm at 0.5 .,0
30771,"L1 ' , L2 ' Identical to L1 , L2 except for lower dropout : 0.2 , 0.3 respectively .",0
30772,"Leads to significant overfitting , evidenced by test perplexity curve in . E1 . ESIM .",0
30773,"We train the base ESIM model without the tree - LSTM , as in , on the task of natural language inference with ADAM for 10 epochs on MultiNLI and also SNLI .",0
30774,Training Schedules :,0
30775,We use the following batch size schedules,0
30776,BL .,0
30777,Use a fixed small batch size as specified in the original paper introducing the model or as is standard . CBS - k (-n ) .,0
30778,"Use a Cyclical Batch Size schedule , where k is the width of each step measured in epochs and n is the integer number of steps per cycle .",0
30779,When n is not specified it refers to the default value of 4 .,0
30780,"At the beginning of each cycle the batch size is initialized to the base batch size , and after each step it is then doubled . CBS -k (-n ) - A . Use an aggressive Cyclical Batch Size schedule , which is equivalent to the original CBS schedule except after every step the batch size is quadrupled . CBS -k (-n ) - T . Use a triangular Cyclical Batch Size schedule , which is modeled after the triangular schedule .",0
30781,"Each cycle consists of n steps doubling the batch size after each step , then n ?",0
30782,2 symmetrical steps halving the batch size after each step .,0
30783,"In all language modeling CBS experiments , we use an initial batch size of 10 , that is , half the baseline batch size as reported in the respective papers of each baseline model tested .",0
30784,The intuition behind starting with a smaller batch size is to introduce additional noise to help models escape sub-optimal local minima .,0
30785,"For adversarial training used in image classification , we use FGSM method to generate adversarial examples .",0
30786,Adversarial training is implemented for the first half training epochs .,0
30787,B Additional Results,0
30788,This section shows additional experiment results .,0
30789,title,0
30790,Sentence Similarity Learning by Lexical Decomposition and Composition,1
30791,abstract,0
30792,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",1
30793,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",0
30794,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",0
30795,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",0
30796,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",0
30797,"Finally , a similarity score is estimated over the composed feature vectors .",0
30798,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",0
30799,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,0
30800,It plays an important role for a variety of tasks in both NLP and IR communities .,0
30801,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",0
30802,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",0
30803,"However , sentence similarity learning has following challenges :",0
30804,1 .,0
30805,There is a lexical gap between semantically equivalent sentences .,0
30806,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",0
30807,"2 . Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",0
30808,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",0
30809,3 .,0
30810,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",0
30811,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",0
30812,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",0
30813,How we can extract and utilize those information becomes another challenge .,0
30814,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms for a longtime .",0
30815,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",0
30816,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",0
30817,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",0
30818,The third challenge did not get much,0
30819,narrative,0
30820,E1,0
30821,The research is to sockeye .,0
30822,E2,0
30823,The study is [ not related ] to salmon .,0
30824,E3,0
30825,The research is relevant to salmon .,0
30826,E4,0
30827,"The study is relevant to sockeye , instead of coho .",0
30828,E5,0
30829,"The study is relevant to sockeye , rather than flounder .:",0
30830,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",0
30831,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",0
30832,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",0
30833,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",1
30834,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",1
30835,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",1
30836,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",1
30837,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",1
30838,"Finally , the composed feature vector is utilized to predict the sentence similarity .",1
30839,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",0
30840,"In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",0
30841,Then we evaluate our model on answer sentence selection and paraphrase identifications tasks ( Section 4 ) .,0
30842,Model Overview,0
30843,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",0
30844,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",0
30845,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",0
30846,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",0
30847,shows an overview of our sentence similarity model .,0
30848,"Given a pair of sentences Sand T , our task is to calculate a similarity score sim ( S , T ) in following steps :",0
30849,Word Representation .,0
30850,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .",0
30851,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",0
30852,Semantic Matching .,0
30853,"In order to judge the similarity between two sentences , we need to check whether each semantic unit in one sentence is covered by the other sentence , or vice versa .",0
30854,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",0
30855,"In our model , we treat each word as a primitive semantic unit , and calculate a semantic matching vector ?",0
30856,i for each word s i by composing part or full word vectors in the other sentence T .,0
30857,"In this way , we can match a word s i to a word or phrase in T . Similarly , for the reverse direction , we also calculate all semantic matching vectorst j in T .",0
30858,We explore different f match functions later in Section 3 .,0
30859,Decomposition .,0
30860,"After the semantic matching phase , we have the semantic matching vectors of ?",0
30861,i and t j .,0
30862,We interpret ?,0
30863,i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,0
30864,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ?",0
30865,i ( ort j ) .,0
30866,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",0
30867,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ?",0
30868,"i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",0
30869,"Formally , we define the decomposition function as :",0
30870,", our goal in this step is how to utilize those information .",0
30871,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",0
30872,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",0
30873,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",0
30874,"Therefore , our model composes the similar component matrix and dissimilar component matrix into a feature vector S ( or T ) with the composition function :",0
30875,Similarity assessing .,0
30876,"In the final stage , we concatenate the two feature vectors ( Sand T ) and predict the final similarity score :",0
30877,3 An End - to - End Implementation Section 2 gives us a glance of our model .,0
30878,"In this section , we describe details of each phase .",0
30879,Semantic Matching Functions,0
30880,This subsection describes our specifications for the semantic matching function f match in Eq.,0
30881,( 1 ) .,0
30882,The goal off match is to generate a semantic matching vector ?,0
30883,i for s i by composing the vectors from T .,0
30884,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ?",0
30885,A mn computes the cosine similarity between words s i and t j as,0
30886,"Then , we define three different semantic matching functions over A mn :",0
30887,"where k = argmax j a i , j .",0
30888,The idea of the global function is to consider all word vectors t j in T .,0
30889,A semantic matching vector ?,0
30890,"i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",0
30891,The max function moves to the other extreme .,0
30892,It generates the semantic matching vector by selecting the most similar word vector t k from T .,0
30893,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",0
30894,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,0
30895,Decomposition Functions,0
30896,This subsection describes the implementations for the decomposition function f decomp in Eq.,0
30897,( 2 ) .,0
30898,The intention off decomp is to decompose a word vector s j based on its semantic matching vector ?,0
30899,j into a similar component s + i and a dissimilar component s ?,0
30900,"i , where s +",0
30901,i indicates the semantics of s i covered by ?,0
30902,i and s ?,0
30903,i indicates the uncovered part .,0
30904,"We implement three types of decomposition function : rigid , linear and orthogonal .",0
30905,The rigid decomposition only adapts to the max version off match .,0
30906,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ?",0
30907,i .,0
30908,"If yes , the vector s i is dispatched to the similar component s + i , and the dissimilar component is assigned with a zero vector",0
30909,0 .,0
30910,"Otherwise , the vector s i is assigned to the dissimilar component s ?",0
30911,i .,0
30912,Eq. gives the formal definition :,0
30913,The motivation for the linear decomposition is that the more similar between s i and ?,0
30914,"i , the higher proportion of s i should be assigned to the similar component .",0
30915,"First , we calculate the cosine similarity ?",0
30916,between s i and ?,0
30917,i .,0
30918,"Then , we decompose s i linearly based on ?.",0
30919,Eq. gives the corresponding definition :,0
30920,The orthogonal decomposition is to decompose a vector in the geometric space .,0
30921,Based on the semantic matching vector ?,0
30922,"i , our model decomposes s i into a parallel component and a perpendicular component .",0
30923,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ?",0
30924,i .,0
30925,Eq. gives the concrete definitions .,0
30926,Composition Functions,0
30927,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,0
30928,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,0
30929,"Inspired from Kim , we utilize a two - channel convolutional neural networks ( CNN ) and design filters based on various order of n-grams , e.g. , unigram , bigram and trigram .",0
30930,The CNN model involves two sequential operations : convolution and max - pooling .,0
30931,"For the convolution operation , we define a list of filters {w o }.",0
30932,"The shape of each filter is d h , where dis the dimension of word vectors and h is the window size .",0
30933,"Each filter is applied to two patches ( a window size h of vectors ) from both similar and dissimilar channels , and generates a feature .",0
30934,Eq. ( 10 ) expresses this process .,0
30935,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",0
30936,"Therefore , after these two operations , each filter generates only one feature .",0
30937,We define several filters by varying the window size and the initial values .,0
30938,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",0
30939,Similarity Assessment Function,0
30940,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,0
30941,"We employ a linear function to sum up all the features and apply a sigmoid function to constrain the similarity within the range [ 0 , 1 ] .",0
30942,Training,0
30943,We train our sentence similariy model by maximizing the likelihood on a training set .,0
30944,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti area pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",0
30945,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",0
30946,"Otherwise , we assign Li = 0 .",0
30947,We implement the mathematical expressions with Theano and use Adam for optimization .,0
30948,Experiment,0
30949,Experimental Setting,0
30950,We evaluate our model on two tasks : answer sentence selection and paraphrase identification .,0
30951,"The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence , and the performance is measured by mean average precision ( MAP ) and mean reciprocal rank ( MRR ) .",0
30952,We experiment on two datasets : QASent and Wiki QA .,0
30953,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",0
30954,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,0
30955,The metrics include the accuracy and the positive class F 1 score .,0
30956,"We experiment on the Microsoft Research Paraphrase corpus ( MSRP ) , which includes 2753 true and 1323 false instances in the training set , and 1147 true and 578 false instances in the test set .",0
30957,We build a development set by randomly selecting 100 true and 100 false instances from the training set .,0
30958,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",0
30959,Model Properties,0
30960,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",0
30961,The choice of these options may affect the final performance .,0
30962,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",0
30963,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,0
30964,"First , we evaluated the effectiveness of various semantic matching functions .",0
30965,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",0
30966,presents the results .,0
30967,We found that the max function worked better than the global function on both MAP and MRR .,0
30968,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",0
30969,"But after we enlarged the window size to 4 , the performance dropped .",0
30970,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",0
30971,"Therefore , we use the local - 3 function in the following experiments .",0
30972,"Second , we studied the effect of various decomposition operations .",0
30973,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",0
30974,shows the performance .,0
30975,We found that the rigid operation got the worst result .,0
30976,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",0
30977,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",0
30978,"Therefore , we choose the orthogonal operation in the following experiments .",0
30979,"Third , we tested the influence of various filter types .",0
30980,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",0
30981,We generate 500 filters for each filter type ( with different initial values ) .,0
30982,Experimental results are shown in .,0
30983,"At the beginning , adding higher - order ngram filters was helpful for the performance .",0
30984,"The performance reached to the peak , when we used the win - 3 filters .",0
30985,"After that , adding more complex filters decreased the performance .",0
30986,"Therefore , the trigram is the best granularity for our model .",0
30987,"In the following experiments , we utilize filter types in win - 3 .",0
30988,Comparing with State - of - the - art Models,0
30989,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .",0
30990,QASent dataset .,1
30991,"presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",0
30992,1 .,0
30993,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",0
30994,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",0
30995,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",1
30996,"Therefore , the lower - level granularity is an indispensable factor for a good performance .",0
30997,"conducted word alignment for a sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",0
30998,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",0
30999,"dos introduced the attention mechanism into the CNN model , and learnt sentence representation by considering the influence of the other sentence .",0
31000,They got better performance than all the other previous work .,0
31001,Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,0
31002,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",1
31003,Wiki QA dataset .,1
31004,presents the results of our model and several state - of - the - art models .,0
31005,constructed the dataset and reimplemented several baseline models .,0
31006,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,1
31007,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,0
31008,The corresponding performance is given at the fourth row of .,0
31009,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",0
31010,The semantic matching phase in our model is similar to the attention mechanism .,0
31011,"But different from the previous models , our model utilizes both the similarity and dissimilarity simultaneously .",0
31012,The last row of shows that our model is more effective than the other models .,0
31013,MSRP dataset .,1
31014,granularity and modeled interaction features at each level for a pair of sentences .,0
31015,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,0
31016,"However , their model heavily depends on the pretraining strategy .",0
31017,"Without pretraining , they got a much worse performance ( the second row of ) .",0
31018,proposed a similar model to .,0
31019,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",0
31020,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Database .",0
31021,Their model outperformed without the need of pretraining ( the sixth row of ) .,0
31022,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",0
31023,applied their attention - based CNN model on this dataset .,0
31024,"By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .",0
31025,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",1
31026,"However , the best performance so far on this dataset is obtained by .",0
31027,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",0
31028,"Therefore , the deep learning methods still have along way to go for this task .",0
31029,Related Work,0
31030,The semantic matching functions in subsection 3.1 are inspired from the attention - based neural machine translation .,0
31031,"However , most of the previous work using the attention mechanism in only LSTM models .",0
31032,Whereas our model introduces the attention mechanism into the CNN model .,0
31033,A similar work is the attention - based CNN model proposed by .,0
31034,"They first build an attention matrix for a sentence pair , and then directly take the attention matrix as anew channel of the CNN model .",0
31035,"Differently , our model uses the attention matrix ( or similarity matrix ) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix , and then feeds these two matrixes into a two - channel CNN model .",0
31036,The model can then focus much on the interactions between similar and dissimilar parts of a sentence pair .,0
31037,Conclusion,0
31038,"In this work , we proposed a model to assess sentence similarity by decomposing and composing lexical semantics .",0
31039,"To bridge the lexical gap problem , our model represents each word with its context vector .",0
31040,"To extract features from both the similarity and dissimilarity of a sentence pair , we designed several methods to decompose the word vector into a similar component and a dissimilar component .",0
31041,"To extract features at multiple levels of granularity , we employed a two - channel CNN model and equipped it with multiple types of ngram filters .",0
31042,Experimental results show that our model is quite effective on both the answer sentence selection task and the paraphrase identification task .,0
31043,title,0
31044,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,1
31045,abstract,0
31046,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,1
31047,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,1
31048,"Given a question , the aim of the task is to find the most similar question from a QA knowledge base .",0
31049,"In this paper , we propose a Multi - task Sentence Encoding Model ( MSEM ) for the PI problem , wherein a connected graph is employed to depict the relation between sentences , and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem .",0
31050,"In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .",0
31051,The experiments show the superiority of our proposed method as compared with the existing sentence matching models .,0
31052,I. INTRODUCTION,0
31053,Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .,0
31054,"There are full - blown applications like Amazon 's Alexa , Apple 's Siri , Baidu 's DuerOS , Google 's Assistant and Microsoft 's Cortana .",0
31055,"Generally , there are two types of question answering systems : ( 1 ) information retrievalbased ( IR - based ) , and ( 2 ) generation - based .",0
31056,"In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .",0
31057,"The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .",0
31058,"In recent years , neural network models have achieved great success in sentence matching .",0
31059,"Depends on whether to use crosssentence features or not , sentence - matching models can be classified roughly into two types : ( 1 ) encoding - based , and ( 2 ) interaction - based .",0
31060,It is generally accepted that the interactionbased models could get better performance than the encodingbased models on certain datasets because they have abundant interaction features .,0
31061,"However , the leaderboards of published large datasets such as SNLI and MultiNLI encourage conducting research on the encoding - based models around the semantic representation , because the encoding - based models can learn vector representations of individual sentences , which can be further applied to other natural language processing tasks .",0
31062,Models in practical QA systems have two main disadvantages .,0
31063,"Firstly , they consider the semantic sentence matching as a binary classification problem , assuming that samples are independent of one another as default .",0
31064,"However , the paraphrase relation between sentences could be transmitted .",0
31065,"For example , if question1 and question2 are paraphrases , and question2 and question3 are paraphrases , we can infer that question1 and question3 are also paraphrases .",0
31066,"Secondly , because of the hard time delay constraint in the online prediction procedure of a traditional IR - based QA system , as shown in , existing models often play the role of a re-rank module that depends on the results from the question analysis and recall module .",0
31067,"Thus they could only re-rank a few candidates from term - based index recall modules like Lucene , instead of retrieving the most similar question from all candidates .",0
31068,"In this paper , we aim to address these two challenges .",0
31069,The main contributions of this work are summarized as follows :,0
31070,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",1
31071,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .",1
31072,We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method .,0
31073,"First , it proves that we can achieve better performance with multi -task learning .",0
31074,"Besides , our method can achieve state - of - the - art performance compared with existing encoding - based models and interaction - based models .",0
31075,II .,0
31076,RELATED WORKS,0
31077,A. Natural Language Sentence Matching,0
31078,Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .,0
31079,It plays a central role in a large number of natural language processing tasks .,0
31080,"For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .",0
31081,The developments of deep neural networks and the emergence of large - scale annotated datasets have led great progress on NLSM tasks .,0
31082,"Depending on whether the crosssentence features or attention from one sentence to another were used , two types of deep neural network models have been proposed for NLSM .",0
31083,"The first type of models is encoding - based , where sentences are encoded into sentence vectors without any cross interaction , then the matching decision is made solely based on the two sentence vectors .",0
31084,"Typical representatives of such methods include Stack - augmented Parser - Interpreter Neural Network ( SPINN ) , Shortcut - Stacked Sentence Encoders ( SSE ) , or Gumbel Tree - LSTM .",0
31085,"The other type of methods , called interaction - based model , make use of cross interaction of small units ( such as words ) to express word - level or phrase - level alignments for performance improvements .",0
31086,"The main representatives are Enhanced Sequential Inference Model ( ESIM ) , Bilateral Multi - Perspective Matching Model ( BiMPM ) , and Densely Interactive Inference Network ( DIIN ) model .",0
31087,"Generally , the second type of methods captures more interactive features between the two input sentences , so it can achieve better performance .",0
31088,"On the other hand , the encoding - based model is much smaller and easier to train , and the vector representations can be further used for sentence clustering , semantic search , visualization and many other tasks .",0
31089,"The advantages of encoding - based models are much more significant to QA systems in the industry , so we focus on the research of encoding - based models .",0
31090,B. Approximate Nearest Neighbor,0
31091,"Approximate nearest neighbor ( ANN ) search has been a hot topic over decades and plays an important role in machine learning , computer vision and information retrieval etc .",0
31092,"For dense real - valued vectors , such as vector representations of images or natural languages , many data structures and algorithms have been proposed to improve the retrieval efficiency of ANN search .",0
31093,"There are four types of mainstream methods , including tree structure based , hashing based , quantization based , and graph based .",0
31094,"In the task of image retrieval , ANN index technologies have been used to build efficient image retrieval systems .",0
31095,"In this paper , inspired by research on image retrieval systems , we propose a semantic retrieval framework for QA systems , which combines ANN search technology and sentence encoding technology .",0
31096,III .,0
31097,PROPOSED METHOD,0
31098,A.,0
31099,The Overall Architecture,0
31100,"Motivated by the fact that the questions in a large QA knowledge base are not independent , we try to utilize the paraphrase relationship among questions to facilitate modeling of question representation .",0
31101,Each sentence is regarded as a vertex and an edge is added between a pair of vertices if they are paraphrases .,0
31102,"In this way we can build an undirected graph to represent the paraphrase relationship among sentences , wherein a connected sub - graph can be seen as a sentence cluster with similar intent , as shown in .",0
31103,"On this basis , we could train a multi-class classification model for sentence intent classification .",0
31104,Two sentences can be considered as paraphrases if they are classified into the same class by the model .,0
31105,"As shows , we employ a multi-task learning method to simultaneously train a sentence matching model and a sentence intent classification model by sharing the sentence encoder between two tasks .",0
31106,Take Quora Question,0
31107,"Pairs dataset as an instance : input data question1 and question 2 will be encoded as sentence representation , by the sentence encoder .",0
31108,"On the one hand , we use a softmax layer and cross entropy loss function to train the multi-class sentence intent classification model as follows :",0
31109,"is the class index of sentence intent , and M represents the number of classes .",0
31110,"On the other hand , for the convenience of integrating with approximate nearest neighbor ( ANN ) libraries , which only support cosine distance , Euclidean distance , Manhattan distance , Hamming distance , or Dot ( Inner )",0
31111,"Product distance , we use a simple cosine matching layer instead of a more complicated multi -layer perceptron as in .",0
31112,"A = sigmoid. * ( ( , ) ? ) ;",0
31113,( 3 ),0
31114,"In , ? and are hyperparameters .",0
31115,Equation is loss function of the matching layer .,0
31116,The overall loss function is as follows :,0
31117,wherein ( 5 ) is a hyperparameter for balancing the loss of each task in the multi - task learning and 0 ?,0
31118,? 1 .,0
31119,B. Sentence Encoder,0
31120,The overall architecture of our sentence encoder is illustrated in .,0
31121,The encoder transforms the input sentence into a fixed - length embedding .,0
31122,The details of each component in the sentence encoder will be described in the subsections that follow .,0
31123,C. Word Representation Layer,0
31124,Word Representation,0
31125,Layer is consists of word embedding and character representation .,0
31126,"For word embedding , we use pretrained Glo Ve word embeddings to represent each word as a d-dimensional vector .",0
31127,"For character representation , we infuse randomly initialized values to max - pooling convolution layer to compute the character representation of each word .",0
31128,We concatenate the word embedding and character representation to get the final word representation .,0
31129,D. Bidirectional Gated Recurrent Unit,0
31130,We use Bidirectional Gated Recurrent Unit ( BiGRU ) to maintain the sequential information about the sentence being modeled .,0
31131,BiGRU is consists of a forward directional GRU and a backward directional GRU .,0
31132,"Forward directional GRU process inputs sequence from left to right , backward directional GRU on the contrary .",0
31133,Let us describe how the tth hidden unit is computed : ?,0
31134,in denotes the element - wise product .,0
31135,"In GRU cell , context information of tth hidden unit is carried over by the last hidden unit state ?",0
31136,LR > .,0
31137,E. Attention Recurrent Unit,0
31138,We propose an Attention Recurrent Unit ( ARU ) based on the attention mechanism .,0
31139,"As shown in , the term "" Gate "" stands for the element - wise forget gate .",0
31140,"We apply position - wise multi-head attention to compute the context representation ( self - attention computed here ) , and use it to compute the linear transformation ?",0
31141,d with the input and the forget gate :,0
31142,= sigmoid.^ > +^ X ;,0
31143,Position - wise multi-head attention is a variant of multi-head attention as shown below :,0
31144,Mpos ? ?,0
31145,"$ is a weight matrix where n is the number of words on sentence , which will be updated during training .",0
31146,We use a dynamic average ( DA ) pooling to improve the sequence encoding capability further :,0
31147,"After that , we use a highway connection to connect input and output :",0
31148,"Compared with GRU , the computation of the tth hidden unit is no more depending on the last hidden unit state ? LR > , so that ( 10 ) , ( 11 ) , ( 12 ) could be computed in parallel on GPU effectively and ( 15 ) just needs a simple computation .",0
31149,"In this section , we also refer to the works of Quasi - RNN and SRU .",0
31150,F. Feed - Forward Network,0
31151,We use a feed - forward network same as the Transformer .,0
31152,"It uses a multi - layer perceptron with two layers and uses activation function ReLU , as follows :",0
31153,FFN function is applied to each output state of ARU .,0
31154,"The term "" Add "" in represents the residual connection and the term "" Norm "" represents the layer normalization .",0
31155,The output of FFN would be incorporated with the Add & Norm layer to simplify the network 's optimization .,0
31156,G. Attentive Pooling,0
31157,"We perform an attentive pooling operation over the output of the FFN , which would convert them into a fixed - length vector .",0
31158,It can be formulated as follows :,0
31159,= softmax . x X tanh ( x>,0
31160,7 ) ;,0
31161,"In , ? ?",0
31162,"$ is the output of FFN , where is the number of words in the sentence and is the number of hidden units of ARU .",0
31163,x> ? ? $ s and x X ? ? s [ are two weight matrices where K and are hyperparameters that could beset manually .,0
31164,"After the attentive pooling layer , the output matrix ? ?",0
31165,[ $ consists of sentence representations with udimensional vectors .,0
31166,We concatenate the above vectors and feed it to a highway network with two layers to generate the final sentence representation vector .,0
31167,"shows the proposed semantic retrieval framework , where the encoding - based model plays a very important role and has a great impact on the overall performance .",0
31168,"In the offline system , all questions in the FAQ set are encoded to dense realvalued vectors .",0
31169,"Then we build an ANN vector index by using an ANN tool , such as Annoy , through which we can get the most similar vectors given a vector encoded from any new question .",0
31170,IV .,0
31171,SEMANTIC RETRIEVAL FRAMEWORK,0
31172,A. Framework Overview,0
31173,"In the online system , we deploy the same module to encode the question input by the user .",0
31174,"By inputting the vector to the ANN module , we can get top similar questions with a semantic matching score .",0
31175,"Then the most similar question could be seen as synonymous to the user 's question , so they might share the same answer .",0
31176,"A more complicated rank module could be used following the ANN module to re-rank the top K candidates , such as interaction - based models , or ranking algorithms with handcrafted features .",0
31177,"However , the rank module is less important in our proposed framework than in the traditional IR - based QA frameworks .",0
31178,B. Analysis,0
31179,"As compared with the traditional IR - based QA frameworks , our framework is less dependent on the general question analysis tools like keyword extraction .",0
31180,"Besides , our framework removes the traditional recall module based on text search engines , which is replaced by the new recall module based on the sentence encoding and ANN technology .",0
31181,V .,0
31182,EXPERIMENTS,0
31183,A. Datasets,0
31184,"We conduct experiments on four sentence matching datasets , each comprising a large set of instances in the form of ? > , X , ? , where > and X are two questions , and is the label indicating whether they are paraphrases or not .",0
31185,shows a brief description of these datasets .,0
31186,The overlap rate is a ratio of the number of common words between two sentences in a sample to the average number of them .,0
31187,Quora Question,0
31188,Pair dataset is an open - domain English dataset derived from Quora.com .,0
31189,We use the same split ratio as BiMPM .,0
31190,LCQMC dataset is an open - domain Chinese dataset collected from Baidu Knows ( a popular Chinese community question answering website ) .,0
31191,Bank Question ( BQ ) dataset is a specific - domain Chinese dataset for sentence semantic equivalence identification ( SSEI ) .,0
31192,"Telephone customer service ( TCS ) dataset is a specificdomain Chinese dataset from a real - world telephone customer service scenario , where voice speeches are converted into text using the automatic speech recognition technology .",0
31193,"The evaluation metric is accuracy for the Quora dataset , and F1 for other datasets .",0
31194,B. Settings of Experiments,0
31195,"For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .",1
31196,The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .,1
31197,We set = 0.8 in the multi - task loss function .,1
31198,"For the sentence intent classification task , we only keep the sentence clusters with question number greater than 3 , and the remaining sentence clusters with question numberless than or equal to 3 are regarded as a special "" other "" cluster .",0
31199,"Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .",1
31200,An Adam optimizer is used to optimize all the trainable weights .,1
31201,The learning rate is set to 4e - 4 and the batch size is set to 200 .,1
31202,"When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .",1
31203,C. Comparing with other methods,0
31204,We compared our model with the following models :,0
31205,ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .,1
31206,It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .,0
31207,ESIM has shown excellent performance on the SNLI dataset .,0
31208,BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .,1
31209,"The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .",0
31210,"SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .",1
31211,"SSE has been proved to be effective in improving the performance of sentence encoder , recording state - of - the - art performance of the sentence - encoding models on Quora dataset .",0
31212,DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .,1
31213,It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .,0
31214,It achieves state - of - the - art performance on SNLI dataset and Quora dataset .,0
31215,a.,0
31216,The first five rows are copied from and the next two rows are copied from . a.,0
31217,The first four rows are copied from and the next two rows are reproduced using the SMP_toolkit .,0
31218,D. Results of Experiments,0
31219,The results of experiments on four sentence matching datasets are summarized as follows :,0
31220,"Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .",1
31221,LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .,1
31222,The experimental results show that our model outperforms state - of the - art models .,1
31223,BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .,1
31224,"As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .",1
31225,TCS dataset :,1
31226,As shown in show that our MSEM model achieves the best performance .,1
31227,This indicates that our model is also very effective in the spoken question - answering scenario .,0
31228,"To sum up , experimental results show that our proposed model without multi-task learning outperforms SSE , the stateof - the - art encoding - based models , across all four datasets .",0
31229,And the model with multi-task learning further improved performance ranging from 0.4 % to 1 % .,0
31230,"Compared with existing models , our model shows great advantages on datasets with low average overlap rate , which is known to be very common in realworld question answering scenarios .",0
31231,E. Ablation Study,0
31232,The above experiments show the effectiveness of our proposed multi-task training strategy .,0
31233,"In this section , we present the results of an ablation study on Quora dataset for evaluating the contribution of each component of the encoder , as shown in .",0
31234,Note that we do the significant test for each ablation experiment using the t-test ( p < 0.05 ) .,0
31235,We first study the contribution of the ARU component .,1
31236,"The accuracy decreases , the accuracy will drop to 88.25 % .",1
31237,Next we compare the effect of attentive pooling vs max pooling .,0
31238,It turns out that the attentive pooling is better than max pooling .,1
31239,"Then if we remove the highway network , the accuracy will drop to 88.36 % .",1
31240,"Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .",1
31241,A possible reason might be that the character - level embedding can better handle the out - of - vocab ( OOV ) words .,0
31242,F. Online System Evaluation,0
31243,We perform an online evaluation with a telephone customer service system .,0
31244,"We randomly select 1138 questions from the system log and send them to a baseline system and the new system , respectively .",0
31245,"The baseline system is similar to what shown in , where the retrieval module is built on Elasticsearch and 30 candidate questions will be recalled and then ranked by the MSEM model .",0
31246,"The new system is similar to what shown in , where an ANN module based on Annoy and a sentence - encoding module based on MSEM are adopted .",0
31247,We manually evaluate the returned results and measure the performance with the F1 score .,0
31248,"As shown in , the F 1 score of the new system is 14 . 26 % higher than the baseline system .",0
31249,"Obviously , the semantic competence derived from the MSEM module plays a key role in the new system .",0
31250,G. Case Studies,0
31251,We perform some case studies using the Quora test set to analyze the effectiveness of the multi -task strategy .,0
31252,We randomly select 200 sentences with a predicted intent of nonother and manually annotate the correctness of the predicted intent .,0
31253,"We find that the accuracy can reach 96.5 % , indicating that our model can address the intent classification task pretty well .",0
31254,"shows some examples where the MSEM model works , while the MSEM without multi - task fails .",0
31255,"In the first example , although the text similarity between S1 and S2 is low , our model can correctly identify that they have the same intent .",0
31256,"In the second example , S1 and S2 have high text overlap , but the model can correctly identify that they have different intents , which helps our model can better distinguish their semantics .",0
31257,"In the third example , the model classifies S1 as "" other "" and S2 as "" non - other "" , which can also help the model distinguish their semantics .",0
31258,VI .,0
31259,CONCLUSION,0
31260,"In this paper , we first propose a Multi - task Sentence Encoding Model ( MSEM ) which addresses both the paraphrase identification task and the sentence intent classification task simultaneously , and then further propose a general semantic retrieval framework that combines the sentence encoding model and approximate nearest neighbor search technology , which can find the most similar question very quickly from all available questions in a massive QA knowledge base .",0
31261,We evaluated our model on several benchmark datasets .,0
31262,Experimental results show that our proposed method is superior to many recent sentence - matching models .,0
31263,title,0
31264,Reading Wikipedia to Answer Open-Domain Questions,1
31265,abstract,0
31266,Wikipedia as the unique knowledge source : the answer to any factoid question is a text span in a Wikipedia article .,0
31267,This task of machine reading at scale combines the challenges of document retrieval ( finding the relevant articles ) with that of machine comprehension of text ( identifying the answer spans from those articles ) .,0
31268,Our approach combines a search component based on bigram hashing and TF - IDF matching with a multi - layer recurrent neural network model trained to detect answers in Wikipedia paragraphs .,0
31269,Our experiments on multiple existing QA datasets indicate that ( 1 ) both modules are highly competitive with respect to existing counterparts and ( 2 ) multitask learning using distant supervision on their combination is an effective complete system on this challenging task .,0
31270,Introduction,0
31271,"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia .",1
31272,Wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines - if they are able to leverage its power .,0
31273,"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in .",1
31274,"It is designed , however , for humans - not machines - to read .",0
31275,Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,0
31276,"In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .",1
31277,"We term this setting , machine reading at scale ( MRS ) .",1
31278,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,1
31279,"As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .",1
31280,"Large - scale QA systems like IBM 's Deep QA rely on multiple sources to answer : besides Wikipedia , it is also paired with KBs , dictionaries , and even news articles , books , etc .",0
31281,"As a result , such systems heavily rely on information redundancy among the sources to answer correctly .",0
31282,Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once .,1
31283,"This challenge thus encourages research in the ability of a machine to read , a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD , and CBT .",0
31284,"However , those machine comprehension resources typically assume that a short piece of relevant text is already identified and given to the model , which is not realistic for building an opendomain QA system .",0
31285,"In sharp contrast , methods that use KBs or information retrieval over documents have to employ search as an integral part of the solution .",0
31286,"Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension , which requires the deep understanding of text , while keeping the realistic constraint of searching over a large open resource .",0
31287,"In this paper , we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open - domain system to perform well on all of them at once .",0
31288,"We develop DrQA , a strong system for question answering from Wikipedia composed of : ( 1 ) Document Retriever , a module using bigram hashing and TF - IDF matching designed to , given a question , efficiently return a subset of relevant articles and ( 2 ) Document Reader , a multi - layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents .",0
31289,gives an illustration of DrQA .,0
31290,Our experiments show that Document Retriever outperforms the built - in Wikipedia search engine and that Document Reader reaches state - of - theart results on the very competitive SQuAD benchmark .,0
31291,"Finally , our full system is evaluated using multiple benchmarks .",0
31292,"In particular , we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to single task training .",0
31293,Related Work,0
31294,"Open-domain QA was originally defined as finding answers in collections of unstructured documents , following the setting of the annual TREC competitions",0
31295,1 .,0
31296,"With the development of KBs , many recent innovations have occurred in the context of QA from KBs with the creation of resources like WebQuestions and SimpleQuestions based on the Freebase KB , or on automatically extracted KBs , e.g. , OpenIE triples and NELL .",0
31297,"However , KBs have inherent limitations ( incompleteness , fixed schemas ) that motivated researchers to return to the original setting of answering from raw text .",0
31298,"A second motivation to cast afresh look at this problem is that of machine comprehension of text , i.e. , answering questions after reading a short text or story .",0
31299,"That subfield has made considerable progress recently thanks to new deep learning architectures like attention - based and memory - 1 http://trec.nist.gov/data/qamain.html augmented neural networks and release of new training and evaluation datasets like QuizBowl , CNN / Daily Mail based on news articles , CBT based on children books , or SQuAD and WikiReading , both based on Wikipedia .",0
31300,An objective of this paper is to test how such new methods can perform in an open - domain QA framework .,0
31301,QA using Wikipedia as a resource has been explored previously .,0
31302,perform opendomain QA using a Wikipedia - based knowledge model .,0
31303,"They combine article content with multiple other answer matching modules based on different types of semi-structured knowledge such as infoboxes , article structure , category structure , and definitions .",0
31304,"Similarly , also combine Wikipedia as a text resource with other resources , in this case with information retrieval over other documents .",0
31305,also mine knowledge from Wikipedia for QA .,0
31306,"Instead of using it as a resource for seeking answers to questions , they focus on validating answers returned by their QA system , and use Wikipedia categories for determining a set of patterns that should fit with the expected answer .",0
31307,"In our work , we consider the comprehension of text only , and use Wikipedia text documents as the sole resource in order to emphasize the task of machine reading at scale , as described in the introduction .",0
31308,"There area number of highly developed full pipeline QA approaches using either the Web , as does QuASE , or Wikipedia as a resource , as do Microsoft 's AskMSR , IBM 's DeepQA and YodaQA - the latter of which is open source and hence reproducible for comparison purposes .",0
31309,Ask,0
31310,"MSR is a search - engine based QA system that relies on "" data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers "" , i.e. , it does not focus on machine comprehension , as we do .",0
31311,Deep,0
31312,"QA is a very sophisticated system that relies on both unstructured information including text documents as well as structured data such as KBs , databases and ontologies to generate candidate answers or vote over evidence .",0
31313,"YodaQA is an open source system modeled after DeepQA , similarly combining websites , information extraction , databases and Wikipedia in particular .",0
31314,Our comprehension task is made more challenging by only using a single resource .,0
31315,"Comparing against these methods provides a useful datapoint for an "" upper bound "" benchmark on performance .",0
31316,"Multitask learning and task transfer have a rich history in machine learning ( e.g. , using ImageNet in the computer vision community ) , as well as in NLP in particular .",0
31317,Several works have attempted to combine multiple QA training datasets via multitask learning to ( i ) achieve improvement across the datasets via task transfer ; and ( ii ) provide a single general system capable of asking different kinds of questions due to the inevitably different data distributions across the source datasets .,0
31318,"used We -bQuestions , TREC and WikiAnswers with four KBs as knowledge sources and reported improvement on the latter two datasets through multitask learning .",0
31319,"combined We-bQuestions and SimpleQuestions using distant supervision with Freebase as the KB to give slight improvements on both datasets , although poor performance was reported when training on only one dataset and testing on the other , showing that task transfer is indeed a challenging subject ; see also fora similar conclusion .",0
31320,"Our work follows similar themes , but in the setting of having to retrieve and then read text documents , rather than using a KB , with positive results .",0
31321,Our System : DrQA,0
31322,"In the following we describe our system DrQA for MRS which consists of two components : ( 1 ) the Document Retriever module for finding relevant articles and ( 2 ) a machine comprehension model , Document Reader , for extracting answers from a single document or a small collection of documents .",0
31323,Document Retriever,0
31324,"Following classical QA systems , we use an efficient ( non - machine learning ) document retrieval system to first narrow our search space and focus on reading only articles that are likely to be relevant .",0
31325,"A simple inverted index lookup followed by term vector model scoring performs quite well on this task for many question types , compared to the built - in ElasticSearch based Wikipedia Search API .",0
31326,Articles and questions are compared as TF - IDF weighted bag - ofword vectors .,0
31327,We further improve our system by taking local word order into account with n-gram features .,0
31328,Our best performing system uses bigram counts while preserving speed and memory efficiency by using the hashing of to map the bigrams to 2 24 bins with an unsigned murmur3 hash .,0
31329,"We use Document Retriever as the first part of our full model , by setting it to return 5 Wikipedia articles given any question .",0
31330,Those articles are then processed by Document Reader .,0
31331,Document Reader,0
31332,"Our Document Reader model is inspired by the recent success of neural network models on machine comprehension tasks , in a similar spirit to the At -tentiveReader described in .",0
31333,"Given a question q consisting of l tokens {q 1 , . . . , q l } and a document or a small set of documents of n paragraphs where a single paragraph p consists of m tokens {p 1 , . . . , pm } , we develop an RNN model that we apply to each paragraph in turn and then finally aggregate the predicted answers .",0
31334,Our method works as follows :,0
31335,Paragraph encoding,0
31336,We first represent all tokens pi in a paragraph p as a sequence of feature vectorsp i ?,0
31337,Rd and pass them as the input to a recurrent neural network and thus obtain :,0
31338,where pi is expected to encode useful context information around token pi .,0
31339,"Specifically , we choose to use a multi - layer bidirectional long short - term memory network ( LSTM ) , and take pi as the concatenation of each layer 's hidden units in the end .",0
31340,The feature vectorp i is comprised of the following parts :,0
31341,Word embeddings : f emb ( p i ) = E (p i ) .,0
31342,We use the 300 - dimensional Glove word embeddings trained from 840B Web crawl data .,0
31343,"We keep most of the pre-trained word embeddings fixed and only fine - tune the 1000 most frequent question words because the representations of some key words such as what , how , which , many could be crucial for QA systems .",0
31344,Exact match : f exact match ( p i ) = I ( p i ? q ) .,0
31345,"We use three simple binary features , indicating whether pi can be exactly matched to one question word in q , either in its original , lowercase or lemma form .",0
31346,"These simple features turnout to be extremely helpful , as we will show in Section 5 .",0
31347,Token features :,0
31348,"We also add a few manual features which reflect some properties of token pi in its context , which include its part - of - speech ( POS ) and named entity recognition ( NER ) tags and its ( normalized ) term frequency ( TF ) .",0
31349,Aligned question embedding :,0
31350,"Following and other recent works , the last part we incorporate is an aligned question embedding f align ( p i ) = j a i , j E ( q j ) , where the attention score a i , j captures the similarity between pi and each question words q j .",0
31351,"Specifically , a i , j is computed by the dot products between nonlinear mappings of word embeddings :",0
31352,", and ? ( ) is a single dense layer with ReLU nonlinearity .",0
31353,"Compared to the exact match features , these features add soft alignments between similar but non-identical words ( e.g. , car and vehicle ) .",0
31354,Question encoding,0
31355,"The question encoding is simpler , as we only apply another recurrent neural network on top of the word embeddings of q i and combine the resulting hidden units into one single vector : {q 1 , . . . , q l } ? q.",0
31356,We compute q = j b j q j where b j encodes the importance of each question word :,0
31357,", and w is a weight vector to learn .",0
31358,Prediction,0
31359,"At the paragraph level , the goal is to predict the span of tokens that is most likely the correct answer .",0
31360,"We take the the paragraph vectors {p 1 , . . . , pm } and the question vector q as input , and simply train two classifiers independently for predicting the two ends of the span .",0
31361,"Concretely , we use a bilinear term to capture the similarity between pi and q and compute the probabilities of each token being start and end as :",0
31362,"During prediction , we choose the best span from token i to token i such that i ?",0
31363,i ? i + 15 and P start ( i ) P end ( i ) is maximized .,0
31364,"To make scores compatible across paragraphs in one or several retrieved documents , we use the unnormalized exponential and take argmax overall considered paragraph spans for our final prediction .",0
31365,Data,0
31366,Our work relies on three types of data :,0
31367,"( 1 ) Wikipedia that serves as our knowledge source for finding answers , ( 2 ) the SQuAD dataset which is our main resource to train Document Reader and ( 3 ) three more QA datasets ( Curated TREC , We-bQuestions and WikiMovies ) that in addition to SQuAD , are used to test the open - domain QA abilities of our full system , and to evaluate the ability of our model to learn from multitask learning and distant supervision .",0
31368,Statistics of the datasets are given in .,0
31369,Wikipedia ( Knowledge Source ),0
31370,We use the 2016 - 12 - 21 dump 2 of English Wikipedia for all of our full - scale experiments as the knowledge source used to answer questions .,0
31371,"For each page , only the plain text is extracted and all structured data sections such as lists and figures are stripped .",0
31372,3,0
31373,"After discarding internal disambiguation , list , index , and outline pages , we retain 5,075,182 articles consisting of 9,008,962 unique uncased token types .",0
31374,SQuAD,0
31375,The Stanford Question Answering Dataset ( SQuAD ) ) is a dataset for machine comprehension based on Wikipedia .,0
31376,"The dataset contains 87 k examples for training and 10 k for development , with a large hidden test set which can only be accessed by the SQuAD creators .",0
31377,Each example is composed of a paragraph extracted from a Wikipedia article and an associated human - generated question .,0
31378,The answer is always a span from this paragraph and a model is given credit if its predicted answer matches it .,0
31379,"Two evaluation metrics are used : exact string match ( EM ) and F 1 score , which measures the weighted average of precision and recall at the token level .",0
31380,"In the following , we use SQuAD for training and evaluating our Document Reader for the standard machine comprehension task given the rel - 2 https://dumps.wikimedia.org/enwiki/",0
31381,latest,0
31382,3,0
31383,We use the WikiExtractor script : https://github.,0
31384,com/attardi/wikiextractor.,0
31385,evant paragraph as defined in .,0
31386,"For the task of evaluating open - domain question answering over Wikipedia , we use the SQuAD development set QA pairs only , and we ask systems to uncover the correct answer spans without having access to the associated paragraphs .",0
31387,"That is , a model is required to answer a question given the whole of Wikipedia as a resource ; it is not given the relevant paragraph as in the standard SQuAD setting .",0
31388,Open -domain QA Evaluation Resources,0
31389,SQuAD is one of the largest general purpose QA datasets currently available .,0
31390,SQuAD questions have been collected via a process involving showing a paragraph to each human annotator and asking them to write a question .,0
31391,"As a result , their distribution is quite specific .",0
31392,We hence propose to train and evaluate our system on other datasets developed for open - domain QA that have been constructed in different ways ( not necessarily in the context of answering from Wikipedia ) .,0
31393,CuratedTREC,0
31394,This dataset is based on the benchmarks from the TREC QA tasks that have been curated by .,0
31395,"We use the large version , which contains a total of 2,180 questions extracted from the datasets from WebQuestions Introduced in , this dataset is built to answer questions from the Freebase KB .",0
31396,"It was created by crawling questions through the Google Suggest API , and then obtaining answers using Amazon Mechanical Turk .",0
31397,We convert each answer to text by using entity names so that the dataset does not reference Freebase IDs and is purely made of plain text question - answer pairs .,0
31398,WikiMovies,0
31399,"This dataset , introduced in , contains 96 k question - answer pairs in the domain of movies .",0
31400,"Originally created from the OMDb and MovieLens databases , the examples are built such that they can also be answered by using a subset of Wikipedia as the knowledge source ( the title and the first section of articles from the movie domain ) .",0
31401,Dataset,0
31402,Example,0
31403,Article / Paragraph SQuAD,0
31404,Q :,0
31405,How many provinces did the Ottoman empire contain in the 17th century ?,0
31406,A : 32,0
31407,Article : Ottoman Empire Paragraph :,0
31408,...,0
31409,At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states .,0
31410,"Some of these were later absorbed into the Ottoman Empire , while others were granted various types of autonomy during the course of centuries .",0
31411,Distantly Supervised Data,0
31412,"All the QA datasets presented above contain training portions , but Curated TREC , WebQuestions and WikiMovies only contain question - answer pairs , and not an associated document or paragraph as in SQuAD , and hence can not be used for training Document Reader directly .",0
31413,"Following previous work on distant supervision ( DS ) for relation extraction ) , we use a procedure to automatically associate paragraphs to such training examples , and then add these examples to our training set .",0
31414,We use the following process for each questionanswer pair to build our training set .,0
31415,"First , we : Document retrieval results .",0
31416,% of questions for which the answer segment appears in one of the top 5 pages returned by the method .,0
31417,run Document Retriever on the question to retrieve the top 5 Wikipedia articles .,0
31418,All paragraphs from those articles without an exact match of the known answer are directly discarded .,0
31419,All paragraphs shorter than 25 or longer than 1500 characters are also filtered out .,0
31420,"If any named entities are detected in the question , we remove any paragraph that does not contain them at all .",0
31421,"For every remaining paragraph in each retrieved page , we score all positions that match an answer using unigram and bigram overlap between the question and a 20 token window , keeping up to the top 5 paragraphs with the highest overlaps .",0
31422,"If there is no paragraph with non-zero overlap , the example is discarded ; otherwise we add each found pair to our DS training dataset .",0
31423,Some examples are shown in and data statistics are given in .,0
31424,"Note that we can also generate additional DS data for SQuAD by trying to find mentions of the answers not just in the paragraph provided , but also from other pages or the same page that the given paragraph was in .",0
31425,We observe that around half of the DS examples come from pages outside of the articles used in SQuAD .,0
31426,Experiments,0
31427,"This section first presents evaluations of our Document Retriever and Document Reader modules separately , and then describes tests of their combination , DrQA , for open - domain QA on the full Wikipedia .",0
31428,Finding Relevant Articles,0
31429,We first examine the performance of our Document Retriever module on all the QA datasets .,0
31430,Table 3 compares the performance of the two approaches described in Section 3.1 with that of the Wikipedia Search Engine 5 for the task of finding articles that contain the answer given a question .,0
31431,"Specifically , we compute the ratio of questions for which the text span of any of their associated answers appear in at least one the top 5 relevant pages returned by each system .",0
31432,"Results on all datasets indicate that our simple approach outperforms Wikipedia Search , especially with bigram hashing .",0
31433,"We also compare doing retrieval with Okapi BM25 or by using cosine distance in the word embeddings space ( by encoding questions and articles as bag - of - embeddings ) , both of which we find performed worse .",0
31434,Reader Evaluation on SQuAD,0
31435,Implementation details,0
31436,We use 3 - layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding .,1
31437,"We apply the Stanford CoreNLP toolkit for tokenization and also generating lemma , partof - speech , and named entity tags .",1
31438,"Lastly , all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each .",1
31439,We use Adamax for optimization as described in .,1
31440,Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs .,1
31441,presents our evaluation results on both development and test sets .,0
31442,SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best - performing systems in the table .,0
31443,Result and analysis,0
31444,"Our system ( single model ) can achieve 70.0 % exact match and 79.0 % F 1 scores on the test set , which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing .",1
31445,"Additionally , we think that our model is conceptually simpler than most of the existing systems .",0
31446,We conducted an ablation analysis on the feature vector of paragraph tokens .,0
31447,As shown in all the features contribute to the performance of our final system .,0
31448,"Without the aligned question embedding feature ( only word embedding and a few manual features ) , our system is still able to achieve F1 over 77 % .",1
31449,"More interestingly , if we remove both f aligned and f exact match , the performance drops dramatically , so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer .",1
31450,Full Wikipedia Question Answering,0
31451,"Finally , we assess the performance of our full system DrQA for answering open - domain questions using the four datasets introduced in Section 4 .",0
31452,We compare three versions of DrQA which evaluate the impact of using distant supervision and multitask learning across the training sources provided to Document Reader ( Document Retriever remains the same for each case ) :,0
31453,SQuAD : A single Document Reader model is trained on the SQuAD training set only and used on all evaluation sets .,0
31454,Fine-tune ( DS ) : A Document Reader model is pre-trained on SQuAD and then fine - tuned for each dataset independently using its distant supervision ( DS ) training set .,0
31455,Multitask ( DS ) :,0
31456,A single Document Reader model is jointly trained on the SQuAD training set and all the DS sources .,0
31457,For the full Wikipedia setting we use a streamlined model that does not use the CoreNLP parsed f token features or lemmas for f exact match .,0
31458,We,0
31459,Method Dev,0
31460,"Test EM F1 EM F1 Dynamic Coattention Networks find that while these help for more exact paragraph reading in SQuAD , they do n't improve results in the full setting .",0
31461,"Additionally , WebQuestions and WikiMovies provide a list of candidate answers ( e.g. , 1.6 million Freebase entity strings for We-bQuestions ) and we restrict the answer span must be in this list during prediction .",0
31462,Results presents the results .,0
31463,"Despite the difficulty of the task compared to machine comprehension ( where you are given the right paragraph ) and unconstrained QA ( using redundant resources ) , Dr QA still provides reasonable performance across all four datasets .",0
31464,"We are interested in a single , full system that can answer any question using Wikipedia .",0
31465,The single model trained only on SQuAD is outperformed on all four of the datasets by the multitask model that uses distant supervision .,0
31466,"However performance when training on SQuAD alone is not far behind , indicating that task transfer is occurring .",0
31467,"The majority of the improvement from SQuAD to Multitask ( DS ) however is likely not from task transfer as fine - tuning on each dataset alone using DS also gives improvements , showing that is is the introduction of extra data in the same domain that helps .",0
31468,"Nevertheless , the best single model that we can find is our overall goal , and that is the Multitask ( DS ) system .",0
31469,"We compare to an unconstrained QA system using redundant resources ( not just Wikipedia ) , Yo - daQA , giving results which were previously reported on Curated TREC and We-bQuestions .",0
31470,"Despite the increased difficulty of our task , it is reassuring that our performance is not too far behind on CuratedTREC ( 31.3 vs. 25.4 ) .",0
31471,"The gap is slightly bigger on WebQuestions , likely because this dataset was created from the specific structure of Freebase which Yod aQA uses directly .",0
31472,"DrQA 's performance on SQuAD compared to its Document Reader component on machine comprehension in shows a large drop ( from 69.5 to 27.1 ) as we now are given Wikipedia to read , not a single paragraph .",0
31473,"Given the correct document ( but not the paragraph ) we can achieve 49.4 , indicating many false positives come from highly topical sentences .",0
31474,"This is despite the fact that the Document Retriever works relatively well ( 77.8 % of the time retrieving the answer , see ) .",0
31475,It is worth noting that a large part of the drop comes from the nature of the SQuAD questions .,0
31476,"They were written with a specific paragraph in mind , thus their language can be ambiguous when the context is removed .",0
31477,"Additional resources other than SQuAD , specifically designed for MRS , might be needed to go further .",0
31478,Conclusion,0
31479,"We studied the task of machine reading at scale , by using Wikipedia as the unique knowledge source for open - domain QA .",0
31480,Our results indicate that MRS is a key challenging task for researchers to focus on .,0
31481,Machine comprehension systems alone can not solve the overall task .,0
31482,"Our method integrates search , distant supervision , and multitask learning to provide an effective complete system .",0
31483,Evaluating the individual components as well as the full system across multiple benchmarks showed the efficacy of our approach .,0
31484,YodaQA,0
31485,DrQA SQuAD + Fine-tune ( DS ) + Multitask ( DS ) SQu AD n / a 27 . Future work should aim to improve over our DrQA system .,0
31486,"Two obvious angles of attack are : ( i ) incorporate the fact that Document Reader aggregates over multiple paragraphs and documents directly in the training , as it currently trains on paragraphs independently ; and ( ii ) perform endto - end training across the Document Retriever and Document Reader pipeline , rather than independent systems .",0
31487,title,0
31488,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,1
31489,abstract,0
31490,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,1
31491,"In this paper , we approach the problems by closely modelling questions in a neural network framework .",0
31492,We first introduce syntactic information to help encode questions .,0
31493,We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them .,0
31494,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results over a competitive baseline .",0
31495,Introduction,0
31496,"Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest , including but not limited to the efforts as in .",0
31497,Many specific problems such as machine comprehension and question answering often involve modeling such question - document pairs .,0
31498,"The recent availability of relatively large training datasets ( see Section 2 for more details ) has made it more feasible to train and estimate rather complex models in an end - to - end fashion for these problems , in which a whole model is fit directly with given question - answer tuples and the resulting model has shown to be rather effective .",0
31499,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .",1
31500,We first introduced syntactic information to help encode questions .,1
31501,We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,1
31502,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results on our competitive baselines .",0
31503,"named entities , common nouns , verbs , and prepositions to test reading comprehension .",0
31504,"The Stanford Question Answering Dataset ( SQuAD ) is more recently released dataset , which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics .",0
31505,The question - answer pairs are annotated through crowdsourcing .,0
31506,Answers are spans of text marked in the original documents .,0
31507,"In this paper , we use SQuAD to evaluate our models .",0
31508,Many neural network models have been studied on the SQuAD task .,0
31509,proposed match LSTM to associate documents and questions and adapted the so - called pointer Network to determine the positions of the answer text spans .,0
31510,proposed a dynamic chunk reader to extract and rank a set of answer candidates .,0
31511,focused on word representation and presented a fine - grained gating mechanism to dynamically combine word - level and character - level representations based on the properties of words .,0
31512,"proposed a multi-perspective context matching ( MPCM ) model , which matched an encoded document and question from multiple perspectives .",0
31513,proposed a dynamic decoder and so - called highway maxout network to improve the effectiveness of the decoder .,0
31514,The bi-directional attention flow ( BIDAF ) used the bi-directional attention to obtain a question - aware context representation .,0
31515,"In this paper , we introduce syntactic information to encode questions with a specific form of recursive neural networks .",0
31516,"More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .",0
31517,Different types of questions are often used to seek for different types of information .,0
31518,"For example , a "" what "" question could have very different property from that of a "" why "" question , while they may share information and need to be trained together instead of separately .",0
31519,"We view this as a "" adaptation "" problem to let different types of questions share a basic model but still discriminate them when needed .",0
31520,"Specifically , we are motivated by the ideas "" i-vector "" in speech recognition , where neural network based adaptation is performed among different ( groups ) of speakers and we focused instead on different types of questions here .",0
31521,Word embedding,0
31522,We concatenate embedding at two levels to represent a word : the character composition and word - level embedding .,0
31523,The character composition feeds all characters of a word into a convolutional neural network ( CNN ) to obtain a representation for the word .,0
31524,And we use the pre-trained 300 - D Glo Ve vectors ( see the experiment section for details ) to initialize our word - level embedding .,0
31525,Each word is therefore represented as the concatenation of the character - composition vector and word - level embedding .,0
31526,"This is performed on both questions and documents , resulting in two matrices : the Q e ?",0
31527,RN dw fora question and the D e ?,0
31528,"R M dw fora document , where N is the question length ( number of word tokens ) , M is the document length , and d w is the embedding dimensionality .",0
31529,Input encoding,0
31530,"The above word representation focuses on representing individual words , and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context .",0
31531,We use bi-directional GRU ( BiGRU ) for both documents and questions .,0
31532,"A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end , respectively .",0
31533,"By concatenating the hidden states of these two GRUs for each word , we obtain the a representation fora question or document : Q c ?",0
31534,RN dc fora question and D c ?,0
31535,R M dc fora document .,0
31536,Alignment Questions and documents interact closely .,0
31537,"As inmost previous work , our framework use both soft attention over questions and that over documents to capture the interaction between them .",0
31538,"More specifically , in this soft - alignment layer , we first feed the contextual representation matrix Q c and D c to obtain alignment matrix U ?",0
31539,RN M :,0
31540,Each,0
31541,U ij represents the similarity between a question word Q c i and a document word D c j .,0
31542,"Word - level Q-code Similar as in , we obtain a word - level Q-code .",0
31543,"Specifically , for each document word w j , we find which words in the question are relevant to it .",0
31544,"To this end , a j ?",0
31545,RN is computed with the following equation and used as a soft attention weight :,0
31546,"With the attention weights computed , we obtain the encoding of the question for each document word w j as follows , which we call word - level Q- code in this paper :",0
31547,Question - based filtering,0
31548,"To better explore question understanding , we design this question - based filtering layer .",0
31549,"As detailed later , different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question .",0
31550,This layer is expandable with more complicated question modeling .,0
31551,"In the basic form of question - based filtering , for each question word w i , we find which words in the document are associated .",0
31552,"Similar to a j discussed above , we can obtain the attention weights on document words for each question word w i :",0
31553,By pooling b ?,0
31554,"RN M , we can obtain a question - based filtering weight bf :",0
31555,where the specific pooling function we used include max - pooling and mean - pooling .,0
31556,Then the document softly filtered based on the corresponding question D f can be calculated by :,0
31557,"Through concatenating the document representation D c , word - level Q - code Q wand question - filtered document D f , we can finally obtain the alignment layer representation :",0
31558,"where "" "" stands for element - wise multiplication and "" ? "" is simply the vector subtraction .",0
31559,Aggregation,0
31560,"After acquiring the local alignment representation , key information in document and question has been collected , and the aggregation layer is then performed to find answers .",0
31561,We use three BiGRU layers to model the process that aggregates local information to make the global decision to find the answer spans .,0
31562,We found a residual architecture as described in is very effective in this aggregation process :,0
31563,Prediction,0
31564,The SQuAD QA task requires a span of text to answer a question .,0
31565,We use a pointer network to predict the starting and end position of answers as in .,0
31566,"Different from their methods , we use a two - directional prediction to obtain the positions .",0
31567,"For one direction , we first predict the starting position of the answer span followed by predicting the end position , which is implemented with the following equations :",0
31568,"where I 3 is inference layer output , h s+ is the hidden state of the first step , and all Ware trainable matrices .",0
31569,We also perform this by predicting the end position first and then the starting position :,0
31570,We finally identify the span of an answer with the following equation :,0
31571,"P ( e ) = pooling ( [ P ( e+ ) , P ( e? ) ] ) ( 21 ) We use the mean - pooling here as it is more effective on the development set than the alternatives such as the max-pooling .",0
31572,Question Understanding and Adaptation,0
31573,Introducing syntactic information for neural question encoding,0
31574,The interplay of syntax and semantics of natural language questions is of interest for question representation .,0
31575,We attempt to incorporate syntactic information in questions representation with TreeLSTM .,0
31576,In general a TreeLSTM could perform semantic composition over given syntactic structures .,0
31577,"Unlike the chain - structured LSTM , the TreeLSTM captures long - distance interaction on a tree .",0
31578,"The update of a TreeLSTM node is described at a high level with Equation , and the detailed computation is described in ( 23 - 29 ) .",0
31579,"Specifically , the input of a TreeLSTM node is used to configure four gates : the input gate it , output gate o t , and the two forget gates f",0
31580,Lt for the left child input and f R t for the right .,0
31581,"The memory cell ct considers each child 's cell vector , c L t?1 and c R t?1 , which are gated by the left forget gate f Lt and right forget gate f R t , respectively .",0
31582,where ?,0
31583,"is the sigmoid function , is the element - wise multiplication of two vectors , and all W , U are trainable matrices .",0
31584,"To obtain the parse tree information , we use Stanford CoreNLP ( PCFG Parser ) ] to produce a binarized constituency parse for each question and build the TreeLSTM based on the parse tree .",0
31585,The root node of TreeLSTM is used as the representation for the whole question .,0
31586,"More specifically , we use it as TreeLSTM Q-code Q T L ?",0
31587,"R dc , by not only simply concatenating it to the alignment layer output but also using it as a question filter , just as we discussed in the question - based filtering section :",0
31588,"where I new is the new output of alignment layer , and function repmat copies Q T L for M times to fit with I.",0
31589,Question Adaptation,0
31590,Questions by nature are often composed to fulfill different types of information needs .,0
31591,"For example , a "" when "" question seeks for different types of information ( i.e. , temporal information ) than those fora "" why "" question .",0
31592,Different types of questions and the corresponding answers could potentially have different distributional regularity .,0
31593,Explicit question - type embedding,0
31594,"The previous models are often trained for all questions without explicitly discriminating different question types ; however , fora target question , both the common features shared by all questions and the specific features fora specific type of question are further considered in this paper , as they could potentially obey different distributions .",0
31595,In this paper we further explicitly model different types of questions in the end - to - end training .,0
31596,"We start from a simple way to first analyze the word frequency of all questions , and obtain top - 10 most frequent question types : what , how , who , when , which , where , why , be , whose , and whom , in which be stands for the questions beginning with different forms of the word be such as is , am , and are .",0
31597,"We explicitly encode question - type information to bean 11 - dimensional one - hot vector ( the top - 10 question types and "" other "" question type ) .",0
31598,Each question type is with a trainable embedding vector .,0
31599,"We call this explicit question type code , ET ?",0
31600,Rd ET .,0
31601,"Then the vector for each question type is tuned during training , and is added to the system with the following equation :",0
31602,Question adaptation,0
31603,"As discussed , different types of questions and their answers may share common regularity and have separate property at the same time .",0
31604,We also view this as an adaptation problem in order to let different types of questions share a basic model but still discriminate them when needed .,0
31605,"Specifically , we borrow ideas from speaker adaptation in speech recognition , where neural - network - based adaptation is performed among different groups of speakers .",0
31606,Conceptually we regard a type of questions as a group of acoustically similar speakers .,0
31607,Specifically we propose a question discriminative block or simply called a discriminative block ) below to perform question adaptation .,0
31608,The main idea is described below :,0
31609,"For each input question x , we can decompose it to two parts : the cluster it belong ( i.e. , question type ) and the diverse in the cluster .",0
31610,The information of the cluster is encoded in a vectorx c .,0
31611,"In order to keep calculation differentiable , we compute the weight of all the clusters based on the distances of x and each cluster center vector , instead of just choosing the closest cluster .",0
31612,Then the discriminative vector ?,0
31613,x with regard to these most relevant clusters are computed .,0
31614,All this information is combined to obtain the discriminative information .,0
31615,"In order to keep the full information of input , we also copy the input question x , together with the acquired discriminative information , to a feed - forward layer to obtain anew representation x for the question .",0
31616,"More specifically , the adaptation algorithm contains two steps : adapting and updating , which is detailed as follows :",0
31617,Adapting,0
31618,"In the adapting step , we first compute the similarity score between an input question vector x ?",0
31619,Rh and each centroid vector of K clustersx ?,0
31620,R Kh .,0
31621,Each cluster here models a question type .,0
31622,"Unlike the explicit question type modeling discussed above , here we do not specify what question types we are modeling but let the system to learn .",0
31623,"Specifically , we only need to pre-specific how many clusters , K , we are modeling .",0
31624,"The similarity between an input question and cluster centroid can be used to compute similarity weight w a : w a k = sof tmax ( cos_sim ( x , x k ) , ? ) , ?k ?",0
31625,"[ 1 , . . . , K ]",0
31626,We set ?,0
31627,equals 50 to make sure only closest class will have a high weight while maintain differentiable .,0
31628,Then we acquire a soft class - center vectorx c :,0
31629,We then compute a discriminative vector ?,0
31630,x between the input question with regard to the soft class - center vector :,0
31631,Note thatx c here models the cluster information and ?,0
31632,x represents the discriminative information in the cluster .,0
31633,"By feeding x , x c and ?",0
31634,"x into feedforward layer with Relu , we obtain x ?",0
31635,R K :,0
31636,"With x ready , we can apply Discriminative Block to any question code and obtain its adaptation Q-code .",0
31637,"In this paper , we use TreeLSTM Q-code as the input vector x , and obtain TreeLSTM adaptation Q- code Q T La ?",0
31638,R dc .,0
31639,Similar to TreeLSTM Q-code Q T,0
31640,"L , we concatenate Q T La to alignment output I and also use it as a question filter :",0
31641,The updating stage attempts to modify the center vectors of the K clusters in order to fit each cluster to model different types of questions .,0
31642,The updating is performed according to the following formula :,0
31643,"In the equation , ?",0
31644,"is an updating rate used to control the amount of each updating , and we set it to 0.01 .",0
31645,"When x is faraway from K - th cluster centerx k , w a k is close to be value 0 and the k - th cluster centerx k tends not to be updated .",0
31646,"If x is instead close to the j - th cluster centerx j , w a k is close to the value 1 and the centroid of the j - th clusterx j will be updated more aggressively using x .",0
31647,Experiment Results,0
31648,Set - Up,0
31649,We test our models on Stanford Question Answering Dataset ( SQuAD ) .,1
31650,"The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles , and the answer to each question is a span of text in the Wikipedia articles .",1
31651,"Training data includes 87,599 instances and validation set has 10,570 instances .",0
31652,The test data is hidden and kept by the organizer .,0
31653,The evaluation of SQuAD is Exact Match ( EM ) and F1 score .,0
31654,We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,1
31655,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,1
31656,"CharCNN filter length is 1 , 3 , 5 , each is 50 dimensions .",1
31657,All vectors including word embedding are updated during training .,0
31658,The cluster number K in discriminative block is 100 .,1
31659,The Adam method is used for optimization .,1
31660,And the first momentum is set to be 0.9 and the second 0.999 .,1
31661,The initial learning rate is 0.0004 and the batch size is 32 .,1
31662,"We will half learning rate when meet a bad iteration , and the patience is 7 .",0
31663,Our early stop evaluation is the EM and F 1 score of validation set .,0
31664,"All hidden states of GRUs , and TreeLSTMs are 500 dimensions , while word - level embedding d w is 300 dimensions .",1
31665,"We set max length of document to 500 , and drop the question - document pairs beyond this on training set .",1
31666,Explicit question - type dimension d ET is 50 .,1
31667,We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5 .,1
31668,Model,0
31669,EM F1,0
31670,Logistic Regression Baseline 40.4 51.0,0
31671,Match- LSTM with Ans - Ptr ( Sentence ) 54.505 67.748,0
31672,Match- LSTM with Ans - Ptr ( Boundary ) 60.474 70.695,0
31673,Dynamic Chunk Reader 62.499 70.956,0
31674,Fine - Grained Gating 62.446 73.327 Match- LSTM with Bi - Ans - Ptr ( Boundary ) 64.744 73.743,0
31675,Multi- Perspective Matching 65.551 75.118 Dynamic Coattention Networks 66,0
31676,Results,0
31677,Overall results shows the official leaderboard on SQuAD test set when we submitted our system .,0
31678,"Our model achieves a 68.73 % EM score and 77.39 % F1 score , which is ranked among the state of the art single models ( without model ensembling shows the ablation performances of various Q- code on the development set .",1
31679,"Note that since the testset is hidden from us , we can only perform such an analysis on the development set .",0
31680,"Our baseline model using no Q- code achieved a 68.00 % and 77.36 % EM and F 1 scores , respectively .",1
31681,"When we added the explicit question type T - code into the baseline model , the performance was improved slightly to 68.16 % ( EM ) and 77.58 % ( F1 ) .",1
31682,"We then used TreeLSTM introduce syntactic parses for question representation and understanding ( replacing simple question type as question understanding Q-code ) , which consistently shows further improvement .",1
31683,We further incorporated the soft adaptation .,0
31684,"When letting the number of hidden question types ( K ) to be 20 , the performance improves to 68.73%/77.74 % on EM and F1 , respectively , which corresponds to the results of our model reported in .",0
31685,"Furthermore , after submitted our result , we have experimented with a large value of K and found that when K = 100 , we can achieve a better performance of 69.10%/78.38 % on the development set .",0
31686,shows the EM / F1 scores of different question types while is the question type amount distribution on the development set .,0
31687,"In we can see that the average EM / F1 of the "" when "" question is highest and those of the "" why "" question is the lowest .",0
31688,"From we can seethe "" what "" question is the major class .",0
31689,shows the composition of F1 score .,0
31690,"Take our best model as an example , we observed a 78.38 % F1 score on the whole development set , which can be separated into two parts : one is where F1 score equals to 100 % , which means an exact match .",1
31691,This part accounts for 69 . 10 % of the entire development set .,0
31692,"And the other part accounts for 30.90 % , of which the average F1 score is 30.03 % .",0
31693,"For the latter , we can further divide it into two sub-parts : one is where the F 1 score equals to 0 % , which means that predict answer is totally wrong .",0
31694,This part occupies 14 . 89 % of the total development set .,0
31695,"The other part accounts for 16.01 % of the development set , of which average F1 score is 57.96 % .",0
31696,From this analysis we can see that reducing the zero F1 score ( 14.89 % ) is potentially an important direction to further improve the system . :,0
31697,F1 Score Analysis .,0
31698,Conclusions,0
31699,Closely modelling questions could be of importance for question answering and machine reading .,0
31700,"In this paper , we introduce syntactic information to help encode questions in neural networks .",0
31701,We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them .,0
31702,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results over a competitive baseline .",0
31703,title,0
31704,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,1
31705,abstract,0
31706,Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text .,1
31707,"Existing approaches made a significant progress comparable to human - level performance , but they are still limited in understanding , up to a few paragraphs , failing to properly comprehend lengthy document .",0
31708,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .",1
31709,"In detail , our method has two novel aspects : ( 1 ) an advanced memory - augmented architecture and ( 2 ) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory .",0
31710,Our proposed architecture is widely applicable to other models .,0
31711,"We have performed extensive experiments with well - known benchmark datasets such as TriviaQA , QUASAR - T , and SQ u AD .",0
31712,"The experimental results demonstrate that the proposed method outperforms existing methods , especially for lengthy documents .",0
31713,Introduction,0
31714,Most of the human knowledge has been stored in the form of text .,0
31715,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,1
31716,"Many neural networkbased methods have been proposed , pushing performance close to a human level .",0
31717,"Nonetheless , there still exists room to improve the performance especially in comprehending lengthy documents that involve complicated reasoning processes .",0
31718,We identify the main bottleneck as the lack of the long - term memory and its improper controlling mechanism .,0
31719,"Previously , several memory - augmenting methods have been proposed to solve the long - term de - *",0
31720,To whom correspondence should be addressed .,0
31721,pendency problem .,0
31722,"For example , in relatively simple tasks such as bAbI tasks , ; proposed methods that handle the external memory to address long - term dependency .",0
31723,"Inspired by these approaches , we develop a customized memory controller along with an external memory augmentation for complicated RC tasks .",0
31724,"However , we found that the memory controller is susceptible to information distortion as neural networks become deeper , this distortion can hinder the performance .",0
31725,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .",1
31726,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,1
31727,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,1
31728,"We conducted extensive experiments through several benchmark datasets such as TriviaQA , QUASAR - T , and SQ u AD .",0
31729,The results show that the proposed model outperforms all the published results .,0
31730,We also integrated the proposed memory controller and the expanded GRU cell block with other existing methods to ensure that our proposed components are widely applicable .,0
31731,The results show that our components consistently bring performance improvement across various state - of - the - art architectures .,0
31732,The main contributions of this work include the following :,0
31733,( 1 ) We propose an extended memory controller module for RC tasks .,0
31734,"( 2 ) We propose a densely connected encoder block with self attention to provide rich representation of given data , reducing information loss due to deep layers of the network .",0
31735,( 3 ) We present the state - of - the - art results in lengthy - document RC tasks such as TriviaQA and QUASAR - T as well as relatively short document RC tasks such as SQuAD .,0
31736,Proposed Method,0
31737,"This section presents two of our proposed components in detail , as depicted in .",0
31738,Memory Controller,0
31739,Our first proposed component is an advanced external memory controller module for solving RC tasks .,0
31740,We modified the recently proposed memory controller by using our new encoder block and layer - wise residual connections .,0
31741,"These modifications enable the memory controller to reason over a lengthy document , leading to the overall performance improvement .",0
31742,"This layer takes input as a sequence of vector representations corresponding to individual tokens , d t ?",0
31743,"R l , where l is the given vector dimension .",0
31744,"For example , such input can be the output of the co-attention layer in Section 3 .",0
31745,The operation of this layer is defined as,0
31746,"That is , at time step t , the controller generates an interface vector it for read and write operations and an output vector o t based on the input vector d t and the external memory content from the previous time step , M t?1 ?",0
31747,"R pq , where p is the memory size and q is the vector dimension of each memory .",0
31748,"Through this controller , we encode an input D = {d t } n t =1 to {x t } n t = 1 by using the encoder block , i.e. ,",0
31749,where k is the output dimension of the encoder block .,0
31750,"In general , this block is implemented as a recurrent unit , e.g. , GRU .",0
31751,"In our model , we replace it with our dense encoder block with self attention ( DEBS ) , as will be discussed in Section 2.2 .",0
31752,"To generate a memory - augmented vector z t , we concatenate x t with the vectors read from the previous time step memory , M t?1 , i.e. ,",0
31753,where s represents the number of read heads in the memory interface .,0
31754,We then feed the vector z t to the bi-directional GRU ( BiGRU ) layer and obtain the output vector h mt as,0
31755,"Afterwards , we generate output vector v t as the weighted sum of the BiGRU output and read vectors from the memory in the current step , i.e. ,",0
31756,"Finally , we add a residual connection between the input d t and the output v t to mitigate any possible information distortion that can occur while accessing the memory , resulting in a the output vector that can handle long - term dependency , i.e. ,",0
31757,"For further details on how the interface vector works , we refer the readers to as well as our supplemental material .",0
31758,Dense Encoder Block with Self Attention,0
31759,"The second novel component we propose is a dense encoder block with self attention ( DEBS ) , which further improves a GRU cell .",0
31760,"Recently , proposed that adding a connection between each layer to the other layers in convolution networks can help to properly convey the information across multiple layers .",0
31761,"Inspired by this , we add such dense connections that concatenate the input to a particular layer to its output .",0
31762,"We also add a self - attention module to this block , to properly address long - term dependency in a length document .",0
31763,"In this manner , our encoder block maintains the necessary information not only along the vertical direction ( across layers ) through dense connections but also along the horizontal direction ( across time steps ) through self attention .",0
31764,DEBS takes the input vector sequence with its length as n and transforms each vector to an ldimensional vector pt through the fully connected layer with ReLU as a nonlinear unit and generates a contextually encoded vector rt as,0
31765,Then we concatenate each output vector rt to the projected input pt to obtain gt = [ r t ; pt ] ?,0
31766,R 3 l and pass it to the self - attention layer .,0
31767,The selfattention layer then calculates the similarity map S g ?,0
31768,"R nn using the tri-linear function as where i , j = 1 , . . . , n.",0
31769,"Finally , the self - attended representation Q = {q t } n t=1 is obtained by performing column - wise softmax on S g to get the attention matrix Ag , which is further multiplied with G = {g t } n t=1 , i.e. ,",0
31770,"The final output is obtained as the concatenation of outputs from the recurrent layer ( BiGRU ) and the self - attention layer , i.e. , [r t ; qt ] ?",0
31771,R 5 l .,0
31772,Reading Comprehension Model with Proposed Components,0
31773,We apply the proposed components to our model for RC tasks .,0
31774,"As depicted in , the model consists of three major layers : the co-attention layer , the memory controller , and the prediction layer .",0
31775,"Given the embeddings of a question and a document , the co-attention layer generates queryaware contextual representations .",0
31776,The memory controller further refines these contextual representations using an external memory .,0
31777,"Based on such representations , the prediction layer determines the start and the end token indices that form the answer span .",0
31778,"In addition , we replace all the encoder block with DEBS in the three major layers .",0
31779,Embedding .,0
31780,We incorporate both word - and character - level embedding methods to obtain the vector representation of each word in the input data .,0
31781,"For word - level embedding e w , we utilize pre-trained , 300 - dimensional embedding vectors from GloVe 6B .",0
31782,The character - level word embedding e c is obtained as a 100 - dimensional vector by first applying a convolution layer with 100 filters to a sequence of 20 dimensional character embeddings learned during training and by further applying global maxpooling over the entire character - level sequence .,0
31783,"Then we obtain the embedding vector of a given word token , e , by concatenating these word - and character - level embeddings , i.e. , e = [ e w ; e c ] ? R 400 .",0
31784,"Finally , we obtain the two sets of embedding vectors of question and document token sequences",0
31785,", where m and n represent the sequence length of a question and a document , respectively .",0
31786,Co-attention layer .,0
31787,"Given E q and Ed , we feed each of them into the encoder block and obtain their contextual representations as",0
31788,These representations are used to calculate the pairwise similarity matrix S ?,0
31789,"R mn between tokens in the question and those in the document by a tri-linear function , i.e. ,",0
31790,"where i = 1 , . . . , m , j = ( 1 , . . . , n ) , and represents the element - wise multiplication and w q , w d , and w care trainable vectors .",0
31791,"We apply column - wise softmax to S to obtain the documentto - question attention matrix A. Afterwards , a question - attended document representation",0
31792,C q is calculate as,0
31793,"In addition to this , we obtain vector ?",0
31794,"Rn , corresponding to the attention of a question to document tokens , by applying softmax to the columnwise max values of S.",0
31795,Then document - attended question vector is obtained b ?,0
31796,"The final co-attended representations {d t } n t=1 is obtained by fully connected layer with ReLU as a nonlinear unit , ? , as",0
31797,Memory controller .,0
31798,This layer takes the output of the co-attention layer {d t } n t=1 as input and refine their representations using our proposed memory controller ( Section 2.1 ) .,0
31799,"Afterwards , the resulting output vector {o t } n t =1 are given as input to the prediction layer .",0
31800,Prediction layer .,0
31801,We feed the output of the memory controller {o t } n t=1 to the prediction layer to predict the start and the end token indices of the answer span .,0
31802,"First , it goes through the encoder block followed by the fully connected layer with softmax over the entire sequence to compute the probability distribution of a start index .",0
31803,The probability distribution of the end index is calculated by concatenating the output of the encoder block for the start index with the output of the memory controller and then by feeding them as input to another encoder block .,0
31804,These probability distributions are used as part of the negative log -likelihood objective function .,0
31805,Experimental Setup,0
31806,Datasets and preprocessing .,0
31807,"We perform extensive experiments with well - known benchmarks such as TriviaQA , QUASAR - T , and SQuAD , as summarized in .",0
31808,"In most of these datasets , a question q and a document dare represented as a sequence of words , and the answer span has to be selected from the document words based on the question .",0
31809,SQuAD consists of crowd - sourced questions and paragraphs from Wikipedia articles containing the answer to these questions .,0
31810,"QUASAR - T is mostly based on factoid questions with their corresponding , large - sized corpus .",0
31811,Trivia,0
31812,"QA is composed of question - answer pairs obtained from 14 trivia and quiz - league websites , along with the documents collected later that are likely to contain the answer from either web search or Wikipedia .",0
31813,In Trivia,0
31814,"QA dataset , we truncate each document to 1,200 words .",0
31815,"Even with such truncation , the average word count per document ( AWC ) of TriviaQA is approximately four times larger than that of SQ uAD .",0
31816,"In terms of the AWC , documents in Triv - iaQA , QUASAR - T , and SQuAD can be viewed as large - , medium - , and small - length documents , respectively .",0
31817,In Trivia,0
31818,"QA dataset , because a document is collected separately for an already collected questionanswer pair , the document does not sometimes have the information to properly infer the answer to the question .",0
31819,"In response , attempted to solve this problem by exposing both relevant and irrelevant paragraphs together separated based on TF - IDF scores .",0
31820,We follow this approach in Trivia QA .,0
31821,"In QUASAR - T , we follow the same preprocessing steps done by .",0
31822,Implementation details . to build the model and Sonnet 2 to implement the memory interface .,1
31823,NLTK is used for tokenizing words .,1
31824,"In the memory controller , we use four read heads and one write head , and the memory size is set to 100 36 , with all initialized as 0 .",1
31825,The hidden vector dimension l is set to 200 .,1
31826,"We use AdaDelta ( Zeiler , 2012 ) as an optimizer with a learning rate of 0.5 .",1
31827,The batch size is set to 20 for TriviaQA and 30 for SQuAD and QUASAR - T .,1
31828,We use an exponential moving average of weights with a decaying factor of 0.001 .,1
31829,"Our model does require more memory than existing methods , but a single GPU ( e.g. , M40 with 12 GB memory ) was enough to train model within a reasonable amount of time .",1
31830,Quantitative Results,0
31831,"For our quantitative comparisons , we use BiDAF with self attention ) as a baseline , which maintains the best results published on both TriviaQA and SQuAD datasets .",0
31832,"In TriviaQA and QUASAR - T dataset , we compare our model with BiDAF as well as its variant called ' BiDAF + DNC , ' which is augmented with an existing external memory architecture just before the answer prediction layer in the BiDAF .",0
31833,"Overall , in lengthy - document cases such as Trivi aQA and QUASAR - T , our model outperforms all the published results , as seen in Tables 2 and 3 , while in the short - document case such as SQuAD , we mostly achieve the best results , as seen in .",1
31834,"In the following , we present detailed analyses on each dataset .",0
31835,TriviaQA .,0
31836,"As shown in , our model , even without DEBS , outperforms the existing state - of - the - art method such as ' BiDAF + SA + SN ' by a large margin in all the cases .",0
31837,"Our model with DEBS , which replaces BiGRU encoder blocks , performs even better than that without it in all the cases except for the combination of the ' full ' and ' Wikipedia ' case , which involves documents containing no relevant information for the answer .",0
31838,"Among those methods shown in , Reading Twice for NLU uses background knowledge from Concept - Net while both M- Reader and MEMEN 37.00 42.50 39.50 44.50 formation as additional features .",0
31839,"We note that our method achieves these outstanding results without any additional features. , our simple baseline ' BiDAF + DNC , ' which involves an existing memory architecture , improves performance over BiDAF , indicating the efficacy of incorporating an external memory .",0
31840,"Moreover , our model with the proposed memory controller achieves significantly better results compared to other models .",0
31841,"Furthermore , another proposed component , DEBS , gives an additional performance boost to our model. , most of the models , if not all , use additional features such as ELMo , and the self - attention mechanism to further improve the performance .",0
31842,We also adopt these mechanisms one by one to show that our model can also benefit from these .,0
31843,"First , we adopt ELMo to our model ( without DEBS ) , which uses word embedding as the weighted sum of the hidden layers of a language model with regularization as an additional feature to our word embeddings .",0
31844,"This improves the F 1 score of our model up to 85.13 and EM to 77. 44 , showing the highest performances among all the methods without using self attention .",0
31845,"Due to the relatively short document length in SQuAD compared to TriviaQA and QUASAR - T , our model without DEBS performs worse than the baseline ' BiDAF + Self Attention + ELMo. '",0
31846,"However , after applying DEBS , our model outperforms",0
31847,"the baseline , achieving 86.73 F1 and 79.69 EM .",0
31848,QUASAR - T .,0
31849,As shown in,0
31850,SQuAD .,0
31851,As shown in,0
31852,Minimum anchor distance .,0
31853,"proposed the difficulty measure called syntactic divergence , which is computed as the edit distance between syntactic parse trees of the question and the sentence containing the answer .",0
31854,"However , this measure has limitations that the syntactic parser does notwork properly on incomplete sentences , which are common in web text .",0
31855,It also becomes difficult to compute this measure if the 77.58 84.16 QANet 76.24 84.60 SAN 76.83 84.40 Fusion Net 75.97 83.90 RaSoR + TR 75.79 83.26 Conducter - net 74.41 82.74 Reinforced Mnemonic Reader 73.20 81.80 BiDAF + Self Attention 72.14 81.05 MEMEN 70.98 80.36 Our model ( without DEBS ) 70.99 79.94 r-net 71.30 79.70 Document Reader 70.73 79.35 FastQAExt 70.85 78.86 Human Performance 82.30 91.22 answer requires multi-sentence inference .,0
31856,"Instead , we develop our own metric called a minimum anchor distance , which is simple and robust to noisy text .",0
31857,"To compute this metric , we first identify for all the co-occurring words ( anchor words ) between a document and a question while ignoring stop words .",0
31858,"Then , we compute the number of words found between the answer and all the possible anchor words and select the minimum number from these .",0
31859,"In , we show F 1 scores of our model with DEBS and the baseline with respect to the minimum anchor distance .",0
31860,The scores are obtained from the development set of Trivi - aQA ( Web ) and SQ uAD .,0
31861,The heat map at the bottom of the figure indicates the number of samples in each interval of the minimum anchor distance .,0
31862,One can see that our model performs increasingly better than the baseline as the minimum anchor distance gets larger .,0
31863,The examples shown in Table 5 indicate that documents with long dependencies tend to have a large minimum anchor distance .,0
31864,These examples show that our model predicts the remotely placed answer from the anchor word relatively well when anaphora resolution and negation are involved .,0
31865,Ablation study with an encoder block .,0
31866,We assume that the concatenation of the layer outputs in DEBS helps the memory controller store contextual representations clearly .,1
31867,"To see how DEBS affects the memory controller depending on different positions in the entire network , we conducted an ablation study by replacing the encoder block with DEBS on SQuAD .",0
31868,"As can be seen in , using DEBS in all the places improves the performance most , and furthermore , the memory controller with DEBS gives the largest performance margin .",1
31869,"This implies that DEBS can generally work as a better alternative to a BiGRU module , and DEBS is critical in maintaining the high performance of our proposed memory controller .",0
31870,Adding our proposed modules to other models .,0
31871,"To show the wide effectiveness of our proposed approaches , we choose two well - known baseline models in SQuAD : R- net and ' BiDAF + Self Attention ' .",0
31872,These models have similar architectures where the model first pairs a given question and document pair using an attention and afterward applies a self - attention mechanism .,0
31873,"We use the publicly available implementation of these models 3 , 4 .",0
31874,"In , replacing all the recurrent units with DEBS and adding our memory controller between the question - document pairing",0
31875,Dataset,0
31876,Example Question :,0
31877,What claimed the life of singer Kathleen Ferrier ?,0
31878,"Context : ( omit ) Kathleen Ferrier ( 22.III.1912 Higher Walton , Lancashire - 8. X. Trivia QA 1953 London , England ) was an English contralto singer * who achieved an international ( Web ) reputation with a repertoire extending from folksong and popular ballads to the classical works .",0
31879,"Her death from cancer , at the height of her fame , was a shock to the musical world and particularly to the general public , which was kept in ignorance of ( omit ) Question :",0
31880,What did Mote think the Yuan class system really represented ?,0
31881,"Context : The historian Frederick W. Mote wrote that the usage of the term "" social SQuAD classes "" for this system was misleading and that the position of people within the four - class system * was not an indication of their actual social power and wealth , but just entailed "" degrees of privilege "" to which they were entitled institutionally and legally , so a person 's standing within the classes was not a guarantee of their standing , ( omit ) * A word with an asterisk indicates an anchor word closest to the ground truth answer . :",0
31882,"Ablation study of replacing an encoder block with DEBS in the co-attention layer ( C ) , the memory controller ( M ) , and the prediction layer ( P ) in SQuAD . means that DEBS is used .",0
31883,"Otherwise , BiGRU is used .",0
31884,layer and the self - attention layer increases the F 1 score by around 0.5 compared to the baseline .,0
31885,Related Work,0
31886,"Numerous neural network - based methods have been proposed , pushing the performance nearly up to a human level .",0
31887,"Although slight differences exist , mostly leverage the questiondocument co-attention based on their pairwise similarity of word - level vector representations .",0
31888,These models currently work as the backbone architecture for many other models .,0
31889,"Furthermore , suggest utilizing a self attention mechanism between tokens within a document to refine contextual representations .",0
31890,Enriching the input representation from pretrained external models has been shown to be useful in improving RC task performances .,0
31891,have also improved the performance by leveraging self attention for context encoding based on convolutional neural networks .,0
31892,"refine the contextual representation with multiple hops , and use the en-coded query for refining the answer prediction as a memory , which are different from our work in terms of handling long - range dependency .",0
31893,Conclusion,0
31894,"This paper proposed two novel , crucial components for deep neural network - based RC tasks , ( 1 ) an advanced memory controller architecture and ( 2 ) a densely connected encoder block with self attention .",0
31895,"We showed the effectiveness of these approaches in handling long - range dependencies using three benchmark RC datasets such as Triv - iaQA , QUASAR - T , and SQuAD .",0
31896,Our proposed modules are widely applicable to other models to improve their performance .,0
31897,Future work includes developing a scalable read / write accessing mechanism to handle a large - scale external memory to reason over multiple documents .,0
31898,title,0
31899,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,1
31900,abstract,0
31901,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,1
31902,Most of these models suffer from reasoning overlong documents and do not trivially generalize to cases where the answer is not present as a span in a given document .,0
31903,We present a novel neural - based architecture that is capable of extracting relevant regions based on a given question - document pair and generating a well - formed answer .,0
31904,"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' .",1
31905,"The proposed architecture outperforms state - of - the - art results ( Tay et al. , 2018 ) by 12.62 % ( ROUGE - L ) relative improvement .",0
31906,Introduction,0
31907,Building Artificial Intelligence ( AI ) algorithms to teach machines to read and to comprehend text is a long - standing challenge in Natural Language Processing ( NLP ) .,0
31908,A common strategy for assessing these AI algorithms is by treating them as RC tasks .,0
31909,This can be formulated as finding an answer to a question given the document ( s ) as evidence .,0
31910,"Recently , many deep - learning based models have been proposed to solve RC tasks based on the SQuAD and Trivi - aQA datasets , reaching human level performance .",0
31911,A common approach in these models is to score and / or extract candidate spans conditioned on a given question - document pair .,0
31912,Most of these models have limited applicability to real problems for the following reasons .,0
31913,"They do not generalize well to scenarios where the answer is not present as a span , or where several discontinuous parts of the document are required to * To whom correspondence should be addressed .",0
31914,form the answer .,0
31915,"In addition , unlike humans , they can not easily skip through irrelevant parts to comprehend long documents .",0
31916,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .",1
31917,The ConZNet architecture consists of two phases .,1
31918,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,1
31919,"These relevant regions are not only useful to generate the answer , but can also be presented to the user as supporting information along with the answer .",0
31920,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .",1
31921,It has the ability to generate better well - formed answers not verbatim present in the document than span prediction models .,0
31922,"Recently , there have been several attempts to adopt condensing documents in RC tasks .",0
31923,retrieve a relevant paragraph based on the question and predict the answer span . select sentence ( s ) to make a summary of the entire document with a feed - forward network and generate an answer based on the summary .,0
31924,"Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .",1
31925,"Moreover , our decoder combines span prediction and sequence generation .",1
31926,This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,1
31927,"We evaluate our model using one of the challenging RC datasets , called ' NarrativeQA ' , which was released recently by .",0
31928,Experimental results show the usefulness of our framework for RC tasks and we outperform stateof - the - art results on this dataset .,0
31929,Proposed Architecture,0
31930,"An overview of our architecture is shown in , which consists of two phases .",0
31931,"First , the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2 .",0
31932,"Second , the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3 .",0
31933,Co-attention layer,0
31934,"The words in the document , question and answer are represented using pre-trained word embeddings .",0
31935,These wordbased embeddings are concatenated with their corresponding char embeddings .,0
31936,The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network ( CNN ) .,0
31937,We further encode the document and question embeddings using a shared bi-directional GRU to get context - aware representations .,0
31938,We compute the co-attention between document and question to get question - aware representations for the document by using tri-linear attention as proposed by .,0
31939,"Let d i be the vector representation for the document word i , q j be the vector for the question word j , and l d and l q be the lengths of the document and question respectively .",0
31940,The tri-linear attention is calculated as,0
31941,"where w d , w q , and w dq are learnable parameters and denotes the element - wise multiplication .",0
31942,We compute the attended document wordd i by first computing ?,0
31943,i = sof tmax ( a i : ) and followed byd i = lq j=1 ?,0
31944,ij q j .,0
31945,"Similarly , we compute a question to document attention vectorq by first computing b = sof tmax ( max ( a i : )) and followed byq",0
31946,to yield a query - aware contextual representation for each word in the document .,0
31947,Context Zoom Layer,0
31948,This layer finds relevant regions of text .,0
31949,"We use reinforcement learning to do that , with the goal of improving answer generation accuracy - see Section 2.4 .",0
31950,The Split Context operation splits the attended document vectors into sentences or fixed size chunks ( useful when sentence tokenization is not available fora particular language ) .,0
31951,"This results inn text regions with each having length l k , where l d = n k =1 l k .",0
31952,"We then get the representations , denoted as z k , for each text region by running a BiGRU and concatenating the last states of the forward and backward GRUs .",0
31953,"The text region representations , z k , encode how well they are related to the question , and their surrounding context .",0
31954,"Generating an answer may depend on multiple regions , and it is important for each text region to collect cues from other regions which are outside of their surroundings .",0
31955,We can compute this by using a Self - Attention layer .,0
31956,"It is a special case of co-attention where both operands ( d i and q j ) are the text fragment itself , computed by setting a ij = ??",0
31957,when i = j in Eq.,0
31958,1 .,0
31959,"These further self - attended text region representations , z k , are passed through a linear layer with tanh activation and softmax layer as follows :",0
31960,where ?,0
31961,"is the probability distribution of text regions , which is the evidence used to generate the answer .",0
31962,"The policy of the reinforcement learner is defined as ? ( r| u ; ? z ) = ? r , where ?",0
31963,"r is the probability of a text region r ( agent 's action ) being selected , u is the environment state as defined in Eq. 2 , and ?",0
31964,z are the learnable parameters .,0
31965,"During the training time we sample text regions using ? , in inference time we follow greedy evaluation by selecting most probable region ( s ) .",0
31966,Answer Generation,0
31967,This component is implemented based on the encoder - decoder architecture of .,0
31968,"The selected text regions from the Context Zoom layer are given as input to the encoder , where it s output is given to the decoder in order to generate the answer .",0
31969,The encoder block uses residual connected selfattention layer followed by a BiGRU .,0
31970,"The selected relevant text regions (? ? r ) are first passed through a separate BiGRU , then we apply a selfattention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activations .",0
31971,"The encoder 's output consists of representations of the relevant text regions , denoted bye i .",0
31972,The decoder block is based on an attention mechanism and a copy mechanism by using a pointer network similar to .,0
31973,This allows the decoder to predict words from the relevant regions as well as from the fixed vocabulary .,0
31974,"At time step t , the decoder predicts the next word in the answer using the attention distribution , context vector and current word embedding .",0
31975,"The attention distribution and context vector are obtained as follows : where ht is hidden state of the decoder , v , W e , W h , b o are learnable parameters .",0
31976,The ?,0
31977,t represents a probability distribution over words of relevant regions e i .,0
31978,The context vector is given by ct = i ?,0
31979,ti e i .,0
31980,The probability distribution to predict word wt from the fixed vocabulary ( P f v ) is computed by passing state ht and context vector ct to a linear layer followed by a softmax function denoted as,0
31981,"To allow decoder to copy words from the encoder sequence , we compute a soft gate ( P copy ) , which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (? r ) .",0
31982,The soft gate is calculated as,0
31983,"where x t is current word embedding , ht is hidden state of the decoder , ct is the context vector , and w p , v h , w x , and b care learnable parameters .",0
31984,We maintain a list of out - of - vocabulary ( OOV ) words for each document .,0
31985,The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document .,0
31986,The final probability distribution ( unnormalized ) over this extended vocabulary ( P ev ) is given by,0
31987,Training,0
31988,"We jointly estimate the parameters of our model coming from the Co-attention , Context Zoom , and Answer Generation layers , which are denoted as ? a , ? z , and ?",0
31989,g respectively .,0
31990,Estimating ?,0
31991,a and ?,0
31992,"g is straight - forward by using the cross - entropy objective J 1 ( {? a , ? g }) and the backpropagation algorithm .",0
31993,"However , selecting text regions in the Context Zoom layer makes it difficult to estimate ?",0
31994,z given their discrete nature .,0
31995,We therefore formulate the estimation of ?,0
31996,z as a reinforcement learning problem via a policy gradient method .,0
31997,"Specifically , we design a reward function over ? z .",0
31998,"We use mean F-score of ROUGE - 1 , ROUGE - 2 , and ROUGE - L as our reward function R .",0
31999,"The objective function to maximize is the expected reward under the probability distribution of current text regions ? r , i.e. , J 2 ( ? z ) = E p ( r|?z ) .",0
32000,We approximate the gradient ? ?z J 2 ( ? z ) by following the REINFORCE algorithm .,0
32001,To reduce the high variance in estimating ? ?z J 2 ( ? z ) one widely used mechanism is to subtract a baseline value from the reward .,0
32002,"It is shown that any number will reduce the variance , here we used the mean of the mini-batch reward b as our baseline .",0
32003,The final objective is to minimize the following equation :,0
32004,"where , B is the size of mini-batch , and R i is the reward of example i ?",0
32005,B. J (? ) is now fully differentiable and we use backpropagation to estimate ?.,0
32006,Experimental Results,0
32007,Dataset,0
32008,The Narrative,0
32009,"QA dataset consists of fictional stories gathered from books and movie scripts , where corresponding summaries and question - answer pairs are generated with the help of human experts and Wikipedia articles .",0
32010,The summaries in Narrative QA are 4 - 5 times longer than documents in the SQuAD dataset .,0
32011,"Moreover , answers are well - formed by human experts and are not verbatim in the story , thus making this dataset ideal for testing our model .",0
32012,The statistics of Narrative QA are available in 1 .,0
32013,Baselines,0
32014,"We compare our model against reported models in ( Seq2Seq , ASR , BiDAF ) and the Multi-range Reasoning Unit ( MRU ) in .",1
32015,"We implemented two baseline models ( Baseline 1 , Baseline 2 ) with Context Zoom layer similar to .",1
32016,In both baselines we replace the span prediction layer with an answer generation layer .,0
32017,In Baseline 1 we use an 1 please refer for more details attention based seq2seq layer without using copy mechanism in the answer generation unit similar to .,0
32018,In Baseline 2 the answer generation unit is similar to our ConZNet architecture .,0
32019,Implementation Details,0
32020,We split each document into sentences using the sentence tokenizer of the NLTK toolkit .,0
32021,"Similarly , we further tokenize each sentence , corresponding question and answer using the word tokenizer of NLTK .",0
32022,The model is implemented using Python and Tensorflow .,1
32023,All the weights of the model are initialized by Glorot Initialization and biases are initialized with zeros .,1
32024,"We use a 300 dimensional word vectors from GloVe ( with 840 billion pre-trained vectors ) to initialize the word embeddings , which we kept constant during training .",1
32025,All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between .,1
32026,We apply dropout between the layers with keep probability of 0.8 ( i.e dropout = 0.2 ) .,1
32027,The number of hidden units are set to 100 .,1
32028,"We trained our model with the AdaDelta ( Zeiler , 2012 ) optimizer for 50 epochs , an initial learning rate of 0.1 , and a minibatch size of 32 .",1
32029,The hyperparameter ' sample size ' ( number of relevant sentences ) is chosen based on the model performance on the devset .,0
32030,shows the performance of various models on Narrative QA .,0
32031,It can be noted that our model with sample size 5 ( choosing 5 relevant sentences ) outperforms the best ROUGE - L score available so far by 12.62 % compared to .,0
32032,"The low performance of Baseline 1 shows that the hybrid approach ( ConZNet ) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models ( Seq2Seq , ASR , BiDAF , MRU ) .",0
32033,Results,0
32034,"To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation , we experimented with sample sizes beyond 5 .",0
32035,The performance of our model gradually dropped from sample size 7 onwards .,1
32036,This result shows evidence that only a few relevant sentences are sufficient to answer a question .,1
32037,We also experimented with various sample sizes to seethe effect of intra sentence relations for an - swer generation .,0
32038,The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1 .,1
32039,These results show that the importance of selecting multiple relevant sentences for generating an answer .,0
32040,"In addition , the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough , they should also be related to each other .",0
32041,This result points out that the self - attention mechanism in the Context zoom layer is an important component to identify related relevant sentences .,1
32042,Conclusion,0
32043,We have proposed anew neural - based architecture which condenses an original document to facilitate fast comprehension in order to generate better well - formed answers than span based prediction models .,0
32044,Our model achieved the best performance on the challenging Narrative QA dataset .,0
32045,"Future work can focus for example on designing an inexpensive preprocess layer , and other strategies for improved performance on answer generation .",0
32046,title,0
32047,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,1
32048,abstract,0
32049,"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",1
32050,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .",1
32051,DSA attends to informative words with a dynamic weight vector .,0
32052,"We achieve new state - of - the - art results among sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while showing comparative results in Stanford Sentiment Treebank ( SST ) dataset .",0
32053,Introduction,0
32054,"In Natural Language Process ( NLP ) , most neural network - based models contain a sentence encoder to map a sequence of words into a vector .",0
32055,"The vector is then used for various downstream tasks , e.g. , sentiment analysis , natural language inference , etc .",0
32056,The key part of a sentence encoder is a computation across a variable - length input sequence fora fixed size vector .,0
32057,One of the common approaches is the max - pooling in CNN or RNN .,0
32058,Self - attention is another approach fora fixed size vector .,0
32059,"Self - attention derived from the attention mechanism , originally designed for neural machine translation , is utilized in various tasks .",0
32060,Self - attention computes attention weights by the inner product between words and the learnable weight vector .,0
32061,"The weight vector is important in that it detects informative words , yet it is static during inference .",0
32062,The significance of the role of the weight vector casts doubt on whether its being static is an optimal status .,0
32063,* Equal Contribution .,0
32064,"In parallel , recently proposed capsule network for image classification .",0
32065,"In capsule network , dynamic routing iteratively computes weights over inputs by the inner product between inputs and a weighted sum of inputs .",0
32066,"Varying with the inputs , the weighted sum detects informative inputs ; therefore it can be interpreted as a dynamic weight vector from the perspective of self - attention .",0
32067,We expect the dynamic weight vector to give rise to flexibility in self - attention since it can adapt to given sentences even after training .,0
32068,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .",1
32069,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",1
32070,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .",1
32071,It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,1
32072,Our technical contributions are as follows :,0
32073,"We design and implement Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",0
32074,We devise the dynamic weight vector with which DSA computes attention weights .,0
32075,"We achieve new state - of - the - art results in SNLI dataset , while showing comparative results in SST dataset .",0
32076,Preliminary,0
32077,"In self - attention , attention weights are computed as follows :",0
32078,where X ?,0
32079,"R dwn is an input sequence , W ?",0
32080,R dvdw is a projection matrix and v ?,0
32081,R dv is the learnable weight vector of self - attention .,0
32082,"The weight vector v plays an important role , since attention weights are computed by the inner product between v and the projection of the input sequence X .",0
32083,The weight vector v is static with respect to the input sequence X during inference .,0
32084,Replacing the weight vector v with a weight matrix enables multiple attentions .,0
32085,Our Approach,0
32086,"Our architecture , shown in , is built on CNN with Dense Connection .",0
32087,"Dynamic Self - Attention ( DSA ) , which is stacked on CNN with Dense Connection , computes attention weights over words .",0
32088,CNN with Dense Connection,0
32089,The goal of this module is to encode each word into a meaningful representation space while capturing local information .,0
32090,"We do not add any positional encoding , as suggested by ; deep convolution layers capture relative position information .",0
32091,We also enforce every output of layers to have the same number of columns by using appropriate zero padding .,0
32092,We denote a sequence of word embeddings as,0
32093,"is a composite function of the l th layer , composed of 1 D Convolution , dropout , and leaky rectified linear unit ( Leaky ReLU ) .",0
32094,We feed a sequence of word embeddings into h 1 ( ) with kernel size 1 :,0
32095,where X 1 ? Rd 1 n .,0
32096,We add Dense Connection in every layer h l ( ) with the same kernel size :,0
32097,where X l ?,0
32098,"Rd l n , and l ?",0
32099,"[ 2 , L ] .",0
32100,"We concatenate outputs of all h l ( ) , and denote it as a single function :",0
32101,"where kernel sizes of all h l ( ) in ? k ( ) are the same number k , except for h 1 ( ) .",0
32102,"We then feed outputs of two different functions ? k 1 ( ) , ? k 2 ( ) , and a sequence of word embeddings X 0 into a compression layer :",0
32103,where h c ( ) is the composite function with kernel size,0
32104,1 .,0
32105,"It compresses the first dimension of input ( i.e. , 2 L l=1 d l + d 0 ) into dc to represent a word compactly .",0
32106,"Finally , L 2 norm of every column vector x c i in the X c is normalized , which is found to help our model to converge fast and stably .",0
32107,Dynamic Self - Attention ( DSA ),0
32108,"Dynamic Self - Attention ( DSA ) iteratively computes attention weights over words with the dynamic weight vector , which varies with inputs .",0
32109,"DSA enables multiple attentions in parallel by multiplying different projection matrices to X c , the output from CNN with Dense Connection .",0
32110,"For the j th attention , DSA projects the compact representation of every word x c i with Leaky ReLU activation : x",0
32111,where W j ?,0
32112,"R dodc is a projection matrix , b j ?",0
32113,R do is a bias term for the j th attention .,0
32114,"Given the number of attentions m , i.e. , j ? [ 1 , m ] , attention weights of words are computed by following Algorithm 1 :",0
32115,"Algorithm 1 Dynamic Self - Attention 1 : procedure ATTENTION ( x j|i , r) 2 :",0
32116,"for all i th word , j th attention :",0
32117,q ij = 0 3:,0
32118,"for r iterations do for all i , j : q ij = q ij +x",0
32119,T j|i z j 8:,0
32120,"return all z jr is the number of iterations , and a ij is the attention weight for the i th word in the j th attention .",0
32121,"z j is the output for the j th attention of DSA at the r th iteration , and also the dynamic weight vector for the j th attention of DSA before r th iteration .",0
32122,"The final sentence embedding z is the concatenation of z 1 , ... , z m :",0
32123,where z ?,0
32124,R mdo is used for downstream tasks .,0
32125,We modify dynamic routing ) to make it function as self - attention with the dynamic weight vector .,0
32126,"We remove capsulization layer in capsule network which transforms scalar neurons to capsules , multi-dimensional neurons .",0
32127,"A single word is then not decomposed into multiple capsules , but represented as a single vector x c i in Eq .",0
32128,6 . Squashing function is a nonlinear function for capsules .,0
32129,We replace it with Tanh nonlinear function for scalar neurons in Line 6 of Algorithm,0
32130,"1 . We also force all the words in the j th attention to share a projection matrix W j in Eq. 6 , as an input is a variable - length sequence .",0
32131,"By contrast , each capsule in capsule network has its own projection matrix W ij .",0
32132,Dynamic Weight Vectors,0
32133,The weight vector v of self - attention in Eq. 1 is static during inference .,0
32134,"In DSA , however , the dynamic weight vector z j in Line 6 of Algorithm 1 , varies with an input sequencex j|1 , ... , x j|n , even after training .",0
32135,"In order to show how the dynamic weight vectors vary , we perform dimensionality reduction on them , z 1 at the ( r ?",0
32136,"1 ) th iteration of Algorithm 1 , by Principal Component Analysis ( PCA ) .",0
32137,"We randomly select 1,000 sentences from Stanford Natural Language Inference ( SNLI ) shows that dynamic weight vectors are scattered in all directions .",0
32138,"Thus , DSA adapts the dynamic weight vector with respect to each sentence .",0
32139,Experiments,0
32140,We evaluate our sentence embedding method with two different tasks : natural language inference and sentiment analysis .,0
32141,"We implement single DSA , multiple DSA and self - attention in Eq. 1 as a baseline .",0
32142,Both DSA and self - attention are stacked on CNN with Dense Connection for fair comparison .,0
32143,"For our implementations , we initialize word embeddings by 300D Glo Ve 840B pretrained vectors , and fix them during training .",0
32144,We use cross-entropy loss as an objective function for both tasks .,0
32145,"We set do = 600 , m = 1 for single DSA and do = 300 , m = 8 for multiple DSA .",0
32146,"In Appendix , we provide details for training our implementations , hyperparameter settings , and visualization of attention maps of DSA .",0
32147,Natural Language Inference Results,1
32148,"Natural language inference is a task of classifying the semantic relationship between two sentences , i.e. , a premise and a hypothesis .",0
32149,"We conduct experiments on Stanford Natural Language Inference ( SNLI ) dataset , consisting of human- written 570 k pairs of English sentences labeled with one of three classes :",0
32150,"Entailment , Contradiction and Neutral .",1
32151,"As the task considers the semantic relationship , SNLI is used as a benchmark for evaluating the performance of a sentence encoder .",1
32152,"We follow a conventional approach , called heuristic matching , to classify the relationship of two sentences .",0
32153,The sentences are encoded by our proposed model .,0
32154,"Given encoded sentences sh , s p for hypothesis and premise respectively , an input of the classifier Model Train ( % ) Test ( % ) Parameters ( m ) T ( s ) / epoch 600D BiLSTM with self - attention 84.5 84.2 2.8 - 300D",0
32155,Directional self - attention network 91.1 85.6 2.4 587 600D,0
32156,Gumbel TreeLSTM 93.1 86.0 10.0 - 600D,0
32157,Residual stacked encoders 91.0 86.0 29.0 - 300D,0
32158,Reinforced self - attention network 92.6 86.3 3.1 622 1200D,0
32159,Distance - based self - attention network 89 Model SST - 2 SST - 5 BiLSTM 87.5 49.5 CNN - non- static 87 is significantly faster than recent models because of its simple structure and highly parallelized computations .,0
32160,"With tradeoffs in terms of parameters and learning time per epoch , multiple DSA outperforms other models by a large margin ( + 1.1 % ) .",1
32161,"In comparison to the baseline , single DSA shows better performance than self - attention ( + 2.2 % ) .",1
32162,This confirms that the dynamic weight vector is more effective for sentence embedding .,0
32163,"Note that our implementation of the baseline , selfattention stacked on CNN with Dense Connection , shows better performance ( + 0.4 % ) than the one stacked on BiLSTM .",1
32164,Sentiment Analysis Results,1
32165,Sentiment analysis is a task of classifying sentiment in sentences .,0
32166,"We use Stanford Sentiment Treebank ( SST ) dataset , consisting of 10 k English sentences , to evaluate our model in singlesentence classification .",0
32167,"We experiment SST - 2 and SST - 5 dataset labeled with binary sentiment labels and five fine - grained labels , respectively .",0
32168,The SST results are summarized in .,0
32169,We compare single DSA with four baseline models :,0
32170,1 https://nlp.stanford.edu/projects/snli/,0
32171,"BiLSTM , CNN and self - attention with BiLSTM or CNN with dense connection .",0
32172,"Single DSA outperforms all the baseline models in SST - 2 dataset , and achieves comparative results in SST - 5 , which again verifies the effectiveness of the dynamic weight vector .",1
32173,"In contrast to the distinguished results in SNLI dataset ( + 2.2 % ) , in SST dataset , only marginal differences in the performance between DSA and the previous self - attentive models are found .",1
32174,We conclude that DSA exhibits a more significant improvement for large and complex datasets .,0
32175,Related Works,0
32176,Our work differs from early self - attention for sentence embedding in that the dynamic weight vector is not static .,0
32177,"Independently , there have recently been an approach to capsule network - based NLP .",0
32178,applied whole capsule network to text classification task .,0
32179,"However , we only utilize an algorithm , dynamic routing from capsule network , and modify it into self - attention with the dynamic weight vector , without unnecessary concepts , e.g. , capsule .",0
32180,Conclusion,0
32181,"In this paper , we have proposed Dynamic Self - Attention ( DSA ) , which computes attention weights over words with the dynamic weight vector .",0
32182,"With the dynamic weight vector , the self attention mechanism can be furnished with flexibility .",0
32183,"Our experiments show that DSA achieves new state - of - the - art results in SNLI dataset , while showing comparative results in SST dataset .",0
32184,title,0
32185,A Hierarchical Model for Data - to - Text Generation,1
32186,abstract,0
32187,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",1
32188,"These structures generally regroup multiple elements , as well as their attributes .",0
32189,Most attempts rely on translation encoder - decoder methods which linearize elements into a sequence .,0
32190,This however loses most of the structure contained in the data .,0
32191,"In this work , we propose to overpass this limitation with a hierarchical model that encodes the data - structure at the element - level and the structure level .",0
32192,Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics .,0
32193,Introduction,0
32194,"Knowledge and / or data is often modeled in a structure , such as indexes , tables , key - value pairs , or triplets .",0
32195,"These data , by their nature ( e.g. , raw data or long time - series data ) , are not easily usable by humans ; outlining their crucial need to be synthesized .",0
32196,"Recently , numerous works have focused on leveraging structured data in various applications , such as question answering or table retrieval .",0
32197,One emerging research field consists in transcribing data - structures into natural language in order to ease their understandablity and their usablity .,0
32198,"This field is referred to as "" data - to - text "" and has its place in several application domains ( such as journalism or medical diagnosis ) or wide - audience applications ( such as financial and weather reports , or sport broadcasting ) .",0
32199,"As an example , shows a data - structure containing statistics on NBA basketball games , paired with its corresponding journalistic description .",0
32200,Designing data - to - text models gives rise to two main challenges :,0
32201,1 ) understanding structured data and 2 ) generating associated descriptions .,0
32202,Recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .,0
32203,"Then , a decoder generates words conditioned on this representation .",0
32204,"With the introduction of the attention mechanism on one hand , which computes a context focused on important elements from the input at each decoding step and , on the other hand , the copy mechanism to deal with unknown or rare words , these systems produce fluent and domain comprehensive texts .",0
32205,"For instance , Roberti et al. train a characterwise encoder - decoder to generate descriptions of restaurants based on their attributes , while Puduppully et al .",0
32206,"design a more complex two - step decoder : they first generate a plan of elements to be mentioned , and then condition text generation on this plan .",0
32207,"Although previous work yield over all good results , we identify two important caveats , that hinder precision ( i.e. factual mentions ) in the descriptions :",0
32208,1 .,0
32209,Linearization of the data - structure .,0
32210,"In practice , most works focus on introducing innovating decoding modules , and still represent data as a unique sequence of elements to be encoded .",0
32211,"For example , the table from would be linearized to [ ( Hawks , H/ V , H ) , ... ,",0
32212,"( Magic , H/V , V ) , ... ] , effectively leading to losing distinction between rows , and therefore entities .",0
32213,"To the best of our knowledge , only Liu et al.",0
32214,propose encoders constrained by the structure but these approaches are designed for single - entity structures .,0
32215,Arbitrary ordering of unordered collections in recurrent networks ( RNN ) .,0
32216,"Most data - to - text systems use RNNs as encoders ( such as GRUs or LSTMs ) , these architectures have however some limitations .",0
32217,"Indeed , they require in practice their input to be fed sequentially .",0
32218,"This way of encoding unordered sequences ( i.e. collections of entities ) implicitly assumes an arbitrary order within the collection which , as demonstrated by Vinyals et al. , significantly impacts the learning performance .",0
32219,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .",1
32220,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .",1
32221,Our contribution is threefold :,0
32222,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .",1
32223,We report experiments on the RotoWire benchmark which contains around 5 K statistical tables of NBA basketball games paired with humanwritten descriptions .,0
32224,Our model is compared to several state - of - the - art models .,0
32225,Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics .,0
32226,"In the following , we first present a state - of - the art of data - to - text literature ( Section 2 ) , and then describe our proposed hierarchical data encoder ( Section 3 ) .",0
32227,"The evaluation protocol is presented in Section 4 , followed by the results ( Section 5 ) .",0
32228,Section 6 concludes the paper and presents perspectives .,0
32229,Related Work,0
32230,"Until recently , efforts to bring out semantics from structured - data relied heavily on expert knowledge .",0
32231,"For example , in order to better transcribe numerical time series of weather data to a textual forecast , Reiter et al .",0
32232,devise complex template schemes in collaboration with weather experts to build a consistent set of data - to - word rules .,0
32233,"Modern approaches to the wide range of tasks based on structured - data ( e.g. table retrieval , table classification , question answering ) now propose to leverage progress in deep learning to represent these data into a semantic vector space ( also called embedding space ) .",0
32234,"In parallel , an emerging task , called "" data - to - text "" , aims at describing structured data into a natural language description .",0
32235,"This task stems from the neural machine translation ( NMT ) domain , and early work represent the data records as a single sequence of facts to be entirely translated into natural language .",0
32236,Wiseman et al .,0
32237,"show the limits of traditional NMT systems on larger structured - data , where NMT systems fail to accurately extract salient elements .",0
32238,"To improve these models , a number of work proposed innovating decoding modules based on planning and templates , to ensure factual and coherent mentions of records in generated descriptions .",0
32239,"For example , Puduppully et al. propose a two - step decoder which first targets specific records and then use them as a plan for the actual text generation .",0
32240,"Similarly ,",0
32241,Li et al .,0
32242,proposed a delayed copy mechanism where their decoder also acts in two steps :,0
32243,1 ) using a classical LSTM decoder to generate delexicalized text and 2 ) using a pointer network to replace placeholders by records from the input data .,0
32244,"Closer to our work , very recent work have proposed to take into account the data structure .",0
32245,"More particularly ,",0
32246,Puduppully et al. follow entity - centric theories and propose a model based on dynamic entity representation at decoding time .,0
32247,It consists in conditioning the decoder on entity representations thatare updated during inference at each decoding step .,0
32248,"On the other hand , Liu et al.",0
32249,rather focus on introducing structure into the encoder .,0
32250,"For instance , they propose a dual encoder which encodes separately the sequence of element names and the sequence of element values .",0
32251,These approaches are however designed for single - entity data structures and do not account for delimitation between entities .,0
32252,Our contribution differs from previous work in several aspects .,0
32253,"First , instead of flatly concatenating elements from the data- structure and encoding them as a sequence , we constrain the encoding to the underlying structure of the input data , so that the delimitation between entities remains clear throughout the process .",0
32254,"Second , unlike all works in the domain , we exploit the Transformer architecture and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering .",0
32255,"Finally , in contrast to that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities , we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention .",0
32256,Hierarchical Encoder Model for Data - to - Text,0
32257,In this section we introduce our proposed hierarchical model taking into account the data structure .,0
32258,We outline that the decoding component aiming to generate descriptions is considered as a black - box module so that our contribution is focused on the encoding module .,0
32259,"We first describe the model overview , before detailing the hierarchical encoder and the associated hierarchical attention .",0
32260,Notation and General Overview,0
32261,Let 's consider the following notations :,0
32262,"An entity e i is a set of J i unordered records {r i , 1 , ... , r i , j , ... , r i , Ji } ; where record r i , j is defined as a pair of key k i , j and value v i , j .",0
32263,We outline that J i might differ between entities .,0
32264,A data - structure sis an unordered set of I entities e i .,0
32265,"We thus denote s := {e 1 , ... , e i , ... , e I }.",0
32266,"For each data - structure , a textual description y is associated .",0
32267,We refer to the first t words of a description y as y 1:t .,0
32268,"Thus , the full sequence of words can be noted as y = y 1:T .",0
32269,"The dataset Dis a collection of N aligned ( data- structure , description ) pairs ( s , y ) .",0
32270,"For instance , illustrates a data - structure associated with a description .",0
32271,"The data - structure includes a set of entities ( Hawks , Magic , Al Horford , Jeff Teague , ... ) .",0
32272,"The entity Jeff Teague is modeled as a set of records { ( PTS , 17 ) , ( REB , 0 ) , ( AST , 7 ) ...} in which , e.g. , the record ( PTS , 17 ) is characterized by a key ( PTS ) and a value .",0
32273,"For each data - structure sin D , the objective function aims to generate a description ?",0
32274,as close as possible to the ground truth y .,0
32275,This objective function optimizes the following log - likelihood over the whole dataset D:,0
32276,where ?,0
32277,stands for the model parameters and P (? = y | s ; ? ) the probability of the model to generate the adequate description y for table s.,0
32278,"During inference , we generate the sequence ?",0
32279,* with the maximum a posteriori probability conditioned on table s .,0
32280,"Using the chain rule , we get :",0
32281,"This equation is intractable in practice , we approximate a solution using beam search , as in .",0
32282,Our model follows the encoder - decoder architecture .,0
32283,"Because our contribution focuses on the encoding process , we chose the decoding module used in : a two - layers LSTM network with a copy mechanism .",0
32284,"In order to supervise this mechanism , we assume that each record value that also appears in the target is copied from the data- structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input .",0
32285,We now describe the hierarchical encoder and the hierarchical attention .,0
32286,Hierarchical Encoding Model,0
32287,"As outlined in Section 2 , most previous work ] make use of flat encoders that do not exploit the data structure .",0
32288,"To keep the semantics of each element from the data - structure , we propose a hierarchical encoder which relies on two modules .",0
32289,"The first one ( module A in is called low - level encoder and encodes entities on the basis of their records ; the second one ( module B ) , called high - level encoder , encodes the data - structure on the basis of its underlying entities .",0
32290,"In the low - level encoder , the traditional embedding layer is replaced by a record embedding layer as in .",0
32291,We present in what follows the record embedding layer and introduce our two hierarchical modules .,0
32292,Record Embedding Layer .,0
32293,The first layer of the network consists in learning two embedding matrices to embed the record keys and values .,0
32294,"Keys k i , j are embedded to k i , j ?",0
32295,"Rd and values v i , j to v i , j ?",0
32296,"Rd , with d the size of the embedding .",0
32297,"As in previous work , each record embedding r i , j is computed by a linear projection on the concatenation [k i , j ; v i , j ] followed by anon linearity :",0
32298,where W r ?,0
32299,R 2dd and b r ?,0
32300,Rd are learnt parameters .,0
32301,The low - level encoder aims at encoding a collection of records belonging to the same entity while the high - level encoder encodes the whole set of entities .,0
32302,Both the low - level and high - level encoders consider their input elements as unordered .,0
32303,We use the Transformer architecture from .,0
32304,"For each encoder , we have the following peculiarities :",0
32305,"the Low - level encoder encodes each entity e ion the basis of its record embeddings r i , j .",0
32306,"Each record embedding r i , j is compared to other record embeddings to learn its final hidden representation h i , j .",0
32307,"Furthermore , we add a special record [ ENT ] for each entity , illustrated in as the last record .",0
32308,"Since entities might have a variable number of records , this token allows to aggregate final hidden record representations {h i , j } Ji j= 1 in a fixedsized representation vector hi .",0
32309,the High - level encoder encodes the data - structure on the basis of its entity representation hi .,0
32310,"Similarly to the Low - level encoder , the final hidden state e i of an entity is computed by comparing entity representation hi with each others .",0
32311,"The data - structure representation z is computed as the mean of these entity representations , and is used for the decoder initialization .",0
32312,Hierarchical attention,0
32313,"To fully leverage the hierarchical structure of our encoder , we propose two variants of hierarchical attention mechanism to compute the context fed to the decoder module .",0
32314,Traditional Hierarchical Attention .,0
32315,"As in , we hypothesize that a dynamic context should be computed in two steps : first attending to entities , then to records corresponding to these entities .",0
32316,"To implement this hierarchical attention , at each decoding step t , the model learns a first set of attention scores ?",0
32317,"i , t over entities e i and a second set of attention scores ?",0
32318,"i , j,t over records r i , j belonging to entity e i .",0
32319,The ?,0
32320,"i , t scores are normalized to form a distribution over all entities e i , and ?",0
32321,"i , j, t scores are normalized to form a distribution over records r i , j of entity e i .",0
32322,"Each entity is then represented as a weighted sum of its record embeddings , and the entire data structure is represented as a weighted sum of the entity representations .",0
32323,The dynamic context is computed as :,0
32324,"where d t is the decoder hidden state at time step t , W ? ?",0
32325,R dd and W ? ?,0
32326,"R dd are learnt parameters , i ? i , t = 1 , and for all i ?",0
32327,"{ 1 , ... , I } j ? i , j, t = 1 .",0
32328,Key - guided Hierarchical Attention .,0
32329,"This variant follows the intuition that once an entity is chosen for mention ( thanks to ? i , t ) , only the type of records is important to determine the content of the description .",0
32330,"For example , when deciding to mention a player , all experts automatically report his score without consideration of its specific value .",0
32331,"To test this intuition , we model the attention scores by computing the ?",0
32332,"i , j,t scores from equation solely on the embedding of the key rather than on the full record representation h i , j :",0
32333,Please note that the different embeddings and the model parameters presented in the model components are learnt using Equation 1 .,0
32334,Experimental setup,0
32335,The Rotowire dataset,0
32336,"To evaluate the effectiveness of our model , and demonstrate its flexibility at handling heavy data - structure made of several types of entities , we used the Ro -toWire dataset .",0
32337,"It includes basketball games statistical tables paired with journalistic descriptions of the games , as can be seen in the example of .",0
32338,The descriptions are professionally written and average 337 words with a vocabulary size of 11.3K .,0
32339,"There are 39 different record keys , and the average number of records ( resp. entities ) in a single data - structure is 628 ( resp. 28 ) .",0
32340,"Entities are of two types , either team or player , and player descriptions depend on their involvement in the game .",0
32341,"We followed the data partitions introduced with the dataset and used a train / validation / test sets of respectively 3 , 398/727/728 ( data- structure , description ) pairs .",0
32342,Evaluation metrics,0
32343,We evaluate our model through two types of metrics .,0
32344,The BLEU score aims at measuring to what extent the generated descriptions are literally closed to the ground truth .,0
32345,The second category designed by is more qualitative .,0
32346,BLEU Score .,0
32347,The BLEU score is commonly used as an evaluation metric in text generation tasks .,0
32348,"It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams ( n ? 1 , 2 , 3 , 4 ) between the generated candidate and the ground truth .",0
32349,We use the implementation code released by .,0
32350,Information extraction - oriented metrics .,0
32351,These metrics estimate the ability of our model to integrate elements from the table in its descriptions .,0
32352,"Particularly , they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ .",0
32353,"To do so , we follow the protocol presented in .",0
32354,"First , we apply an information extraction ( IE ) system trained on labeled relations from the gold descriptions of the RotoWire train dataset .",0
32355,Entity - value pairs are extracted from the descriptions .,0
32356,"For example , in the sentence Isaiah Thomas led the team in scoring , totaling 23 points [... ]. , an IE tool will extract the pair ( Isaiah Thomas , 23 , PTS ) .",0
32357,"Second , we compute three metrics on the extracted information :",0
32358,"Relation Generation ( RG ) estimates how well the system is able to generate text containing factual ( i.e. , correct ) records .",0
32359,We measure the precision and absolute number ( denoted respectively RG - P % and RG - # ) of unique relations r extracted from ?,0
32360,1:T that also appear in s.,0
32361,Content Selection ( CS ) measures how well the generated document matches the gold document in terms of mentioned records .,0
32362,We measure the precision and recall ( denoted respectively CS - P % and CS - R% ) of unique relations r extracted from ?,0
32363,1:T thatare also extracted from y 1:T .,0
32364,Content Ordering ( CO ) analyzes how well the system orders the records discussed in the description .,0
32365,We measure the normalized Damerau - Levenshtein distance between the sequences of records extracted from ?,0
32366,1:T thatare also extracted from y 1:T .,0
32367,"CS primarily targets the "" what to say "" aspect of evaluation , CO targets the "" how to say it "" aspect , and RG targets both .",0
32368,"Note that for CS , CO , RG - % and BLEU metrics , higher is better ; which is not true for RG -# .",0
32369,The IE system used in the experiments is able to extract an average of 17 factual records from gold descriptions .,0
32370,"In order to mimic a human expert , a generative system should approach this number and not overload generation with brute facts .",0
32371,Baselines,1
32372,We compare our hierarchical model against three systems .,0
32373,"For each of them , we report the results of the best performing models presented in each paper .",0
32374,Wiseman is a standard encoder - decoder system with copy mechanism .,1
32375,"Li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .",1
32376,"Puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .",1
32377,Puduppully - updt .,1
32378,"It consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .",1
32379,"At each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .",1
32380,Model scenarios,0
32381,We test the importance of the input structure by training different variants of the proposed architecture :,0
32382,"Flat , where we feed the input sequentially to the encoder , losing all notion of hierarchy .",0
32383,"As a consequence , the model uses standard attention .",0
32384,"is closest to Wiseman , with the exception that we use a Transformer to encode the input sequence instead of an RNN .",0
32385,"Hierarchical - kv is our full hierarchical model , with traditional hierarchical attention , i.e. where attention over records is computed on the full record encoding , as in equation .",0
32386,"Hierarchical -k is our full hierarchical model , with key - guided hierarchical attention , i.e. where attention over records is computed only on the record key representations , as in equation .",0
32387,Implementation details,0
32388,The decoder is the one used in with the same hyper - parameters .,0
32389,"For the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .",0
32390,"To fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .",0
32391,The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300 .,1
32392,We use dropout at rate 0.5 .,1
32393,The models are trained with a batch size of 64 .,1
32394,"We follow the training procedure in and train the model for a fixed number of 25 K updates , and average the weights of the last 5 checkpoints ( at every 1 K updates ) to ensure more stability across runs .",1
32395,"All models were trained with the Adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 K steps .",1
32396,We used beam search with beam size of 5 during inference .,1
32397,All the models are implemented in Open NMT - py .,1
32398,All code is available at https://github.com/KaijuML/data-to-text-hierarchical,1
32399,Results,0
32400,Our results on the RotoWire testset are summarized in .,0
32401,"For each proposed variant of our architecture , we report the mean score over ten runs , as well as the standard deviation in subscript .",0
32402,Results are compared to baselines and variants of our models .,0
32403,We also report the result of the oracle ( metrics on the gold descriptions ) .,0
32404,"Please note that gold descriptions trivially obtain 100 % on all metrics expect RG , as they are all based on comparison with themselves .",0
32405,"RG scores are different , as the IE system is imperfect and fails to extract accurate entities 4 % of the time .",0
32406,RG -# is an absolute count .,0
32407,Ablation studies,0
32408,"To evaluate the impact of our model components , we first compare scenarios Flat , Hierarchical - k , and Hierarchical - kv .",0
32409,"As shown in , we can see the lower results obtained by the Flat scenario compared to the other scenarios ( e.g. BLEU 16.7 vs. 17.5 for resp .",1
32410,"Flat and Hierarchical -k ) , suggesting the effectiveness of encoding the data - structure using a hierarchy .",0
32411,"This is expected , as losing explicit delimitation between entities makes it harder a ) for the encoder to encode semantics of the objects contained in the table and b ) for the attention mechanism to extract salient entities / records .",0
32412,"Second , the comparison between scenario Hierarchical - kv and Hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted CS - R% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .",1
32413,"To illustrate this intuition , we depict in attention scores ( recall ?",0
32414,"i , t and ?",0
32415,"i , j, t from equations and ) for both variants Hierarchical - kv and Hierarchical -k .",0
32416,We particularly focus on the timestamp where the models should mention the number of points scored during the first quarter of the game .,0
32417,"Scores of Hierarchical -k are sharp , with all of the weight on the correct record ( PTS QTR1 , 26 ) whereas scores of Hierarchical - kv are more distributed over all PTS QTR records , ultimately failing to retrieve the correct one .",0
32418,over all models ; our best model Hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .,0
32419,"This means that our models learns to generate fluent sequences of words , close to the gold descriptions , adequately picking upon domain lingo .",0
32420,Qualitative metrics are either better or on par with baselines .,0
32421,"We show in a text generated by our best model , which can be directly compared to the gold description in .",0
32422,Generation is fluent and contains domain - specific expressions .,0
32423,"As reflected in , the number of correct mentions ( in green ) outweights the number of incorrect mentions ( in red ) .",0
32424,"Please note that , as in previous work , generated texts still contain a number of incorrect facts , as well hallucinations ( in blue ) : sentences that have no basis in the input data ( e.g. "" [... ] he 's now averaging 22 points [... ]. "" ) .",0
32425,"While not the direct focus of our work , this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts .",0
32426,"Specifically , regarding all baselines , we can outline the following statements .",0
32427,"Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman , reinforcing the crucial role of structure in data semantics and saliency .",1
32428,The analysis of RG metrics shows that Wiseman seems to be the more naturalistic in terms of number of factual mentions ( RG# ) since it is the closest scenario to the gold value ( 16.83 vs. 17.31 for resp .,0
32429,Wiseman and Hierarchical -k ) .,0
32430,"However , Wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model Hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above Wiseman .",0
32431,The comparison between the Flat scenario and Wiseman is particularly interesting .,0
32432,"Indeed , these two models share the same intuition to flatten the data -structure .",0
32433,"The only difference stands on the encoder mechanism : bi - LSTM vs. Transformer , for Wiseman and Flat respectively .",0
32434,Results shows that our Flat scenario obtains a significant higher BLEU score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( RG - P % ) thatare also included in the gold descriptions ( CS - R% ) .,1
32435,This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure .,0
32436,"Our hierarchical models outperform the two - step decoders of Li and Puduppully - plan on both BLEU and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .",1
32437,"While our models sensibly outperform in precision at factual mentions , the baseline Puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .",0
32438,The comparison with Puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering ( CO ) and RG - P% .,0
32439,"However , this does not help with Content Selection ( CS ) since our best model Hierarchical -k obtains slightly better scores .",0
32440,"Indeed , Puduppullyupdt updates representations after each mention allowing to keep track of the mention history .",0
32441,"This guides the ordering of mentions ( CO metric ) , each step limiting more the number of candidate mentions ( increasing RG - P% ) .",0
32442,"In contrast , our model encodes saliency among records / entities more effectively ( CS metric ) .",0
32443,"We note that while our model encodes the data - structure once and for all , Puduppully - updt recomputes , via the updates , the encoding at each step and therefore significantly increases computation complexity .",0
32444,"Combined with their RG -# score of 30.11 , we argue that our model is simpler , and obtains fluent description with accurate mentions in a more human - like fashion .",0
32445,We would also like to draw attention to the number of parameters used by those architectures .,0
32446,We note that our scenarios relies on a lower number of parameters ( 14 millions ) compared to all baselines ( ranging from 23 to 45 millions ) .,0
32447,"This outlines the effectiveness in the design of our model relying on a structure encoding , in contrast to other approach that try to learn the structure of data / descriptions from a linearized encoding .",0
32448,Conclusion and future work,0
32449,"In this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .",0
32450,This results in an effective and more light - weight model .,0
32451,Experimental evaluation on the RotoWire benchmark shows that our model outperforms competitive baselines in terms of BLEU score and is generally better on qualitative metrics .,0
32452,"This way of representing structured data bases may lead to automatic inference and enrichment , e.g. , by comparing entities .",0
32453,This direction could be driven by very recent operation - guided networks .,0
32454,"In addition , we note that our approach can still lead to erroneous facts or even hallucinations .",0
32455,An interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions .,0
32456,Acknowledgements,0
32457,We would like to thank the H2020 project AI4EU ( 825619 ) which partially supports Laure Soulier and Patrick Gallinari .,0
32458,title,0
32459,Deep Graph Convolutional Encoders for Structured Data to Text Generation,1
32460,abstract,0
32461,Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods .,1
32462,These approaches linearise the input graph to be fed to a recurrent neural network .,0
32463,"In this paper , we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure .",0
32464,We report results on two graphto - sequence datasets that empirically show the benefits of explicitly encoding the input graph structure .,0
32465,1,0
32466,Introduction,0
32467,Data - to - text generators produce a target natural language text from a source data representation .,0
32468,Recent neural generation approaches build on encoder - decoder architectures proposed for machine translation .,0
32469,"The source data , differently from the machine translation task , is a structured representation of the content to be conveyed .",0
32470,"Generally , it describes attributes and events about entities and relations among them .",0
32471,In this work we focus on two generation scenarios where the source data is graph structured .,0
32472,"One is the generation of multi-sentence descriptions of Knowledge Base ( KB ) entities from RDF graphs ) , namely the WebNLG task .",0
32473,The number of KB relations modelled in this scenario is potentially large and generation involves solving various subtasks ( e.g. lexicalis ation Code and data available at github.com/diegma/graph-2-text .,0
32474,2 Resource Description Framework https://www.w3.org/RDF / and aggregation ) .,0
32475,Figure ( 1 a ) shows and example of source RDF graph and target natural language description .,0
32476,"The other is the linguistic realis ation of the meaning expressed by a source dependency graph , namely the SR11 Deep generation task .",0
32477,"In this task , the semantic relations are linguistically motivated and their number is smaller .",0
32478,illustrates a source dependency graph and the corresponding target text .,0
32479,Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .,1
32480,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,0
32481,Models based on these sequential encoders have shown good results although they do not directly exploit the input structure but rather rely on a separate linearis ation step .,0
32482,"In this work , we compare with a model that explicitly encodes structure and is trained end - to - end .",0
32483,"Concretely , we use a Graph Convolutional Network ( GCN ; ) as our encoder .",0
32484,GCNs area flexible architecture that allows explicit encoding of graph data into neural networks .,0
32485,Given their simplicity and expressiveness they have been used to encode dependency syntax and predicate - argument structures in neural machine translation .,0
32486,"In contrast to previous work , we do not exploit the sequential information of the input ( i.e. , with an LSTM ) , but we solely rely on a GCN for encoding the source graph structure .",0
32487,The main contribution of this work is show - :,0
32488,Source RDF graph - target description ( a ) .,0
32489,Source dependency graph - target sentence ( b ) .,0
32490,ing that explicitly encoding structured data with GCNs is more effective than encoding a linearized version of the structure with LSTMs .,0
32491,"We evaluate the GCN - based generator on two graph - tosequence tasks , with different level of source content specification .",0
32492,"In both cases , the results we obtain show that GCNs encoders outperforms standard LSTM encoders .",0
32493,Graph Convolutional - based Generator,0
32494,"Formally , we address the task of text generation from graph - structured data considering as input a directed labeled graph X = ( V , E ) where V is a set of nodes and E is a set of edges between nodes in V .",0
32495,The specific semantics of X depends on the task at hand .,0
32496,The output Y is a natural language text verbalising the content expressed by X .,0
32497,"Our generation model follows the standard attention - based encoder - decoder architecture and predicts Y conditioned on X as P ( Y | X ) = | Y | t=1 P ( y t |y 1:t?1 , X ) .",0
32498,Graph Convolutional Encoder,0
32499,In order to explicitly encode structural information we adopt graph convolutional networks ( GCNs ) .,0
32500,GCNs area variant of graph neural networks ) that has been recently proposed by .,0
32501,The goal of GCNs is to calculate the representation of each node in a graph considering the graph structure .,0
32502,In this paper we adopt the parametrization proposed by where edge labels and directions are explicitly modeled .,0
32503,"Formally , given a directed graph X = ( V , E ) , where V is a set of nodes , and E is a set of edges .",0
32504,We represent each node v ?,0
32505,V with a feature vector xv ?,0
32506,Rd .,0
32507,The GCN calculates the representation of each node h ?,0
32508,v in a graph using the following up - date rule :,0
32509,"is an embedding of the label of the edge ( u , v) . ? is a non-linearity ( ReLU ) .",0
32510,"g u , v are learned scalar gates which weight the importance of each edge .",0
32511,"Although the main aim of gates is to down weight erroneous edges in predicted graphs , they also add flexibility when several GCN layers are stacked .",0
32512,"As with standard convolutional neural networks ) , GCN layers can be stacked to consider non-immediate neighbours .",0
32513,4,0
32514,Skip Connections,0
32515,Between GCN layers we add skip connections .,0
32516,Skip connections let the gradient flows more efficiently through stacked hidden layers thus making possible the creation of deeper GCN encoders .,0
32517,"We use two kinds of skip connections : residual connections ( He et al. , 2016 ) and dense connections .",0
32518,Residual connections consist in summing input and output representations of a GCN layer hr v = h ? v + h v .,0
32519,"Whilst , dense connections consist in the concatenation of the input and output representations",0
32520,"In this way , each GCN layer is directly fed with the output of every layer before itself .",0
32521,Decoder,0
32522,The decoder uses an LSTM and a soft attention mechanism over the representation induced by the GCN encoder to generate one wordy at the time .,0
32523,"The prediction of wordy t + 1 is conditioned on the previously predicted words y 1:t encoded in the vector wt and a context vector ct dynamically created attending to the graph representation induced by the GCN encoder as P ( y t+1 |y 1:t , X ) = sof tmax ( g ( w t , ct ) ) , where g ( ) is a neural network with one hidden layer .",0
32524,"The model is trained to optimize negative log likelihood : L N LL = ? | Y | t=1 log P ( y t |y 1:t?1 , X )",0
32525,Generation Tasks,0
32526,"In this section , we describe the instantiation of the input graph X for the generation tasks we address .",0
32527,WebNLG,0
32528,Task,0
32529,The WebNLG task aims at the generation of entity descriptions from a set of RDF triples related to an entity of a given category .,0
32530,"RDF triples are of the form ( subject relation object ) , e.g. , ( Aenir preceded By Castle ) , and form a graph in which edges are labelled with relations and vertices with subject and object entities .",0
32531,"For instance , Figure ( 1 a ) shows a set of RDF triples related to the book Above the Veil and its verbalis ation .",0
32532,"The generation task involves several micro-planning decisions such as lexicalis ation ( followed By is verbalised as sequel to ) , aggregation ( sequel to Aenir and Castle ) , referring expressions ( subject of the second sentence verbalised as pronoun ) and segmentation ( content organised in two sentences ) .",0
32533,Reification,0
32534,"We formulate this task as the generation of a target description Y from a source graph X = ( V , E ) where X is build from a set of RDF triples as follows .",0
32535,We reify the relations from the RDF set of triples .,0
32536,"That is , we see the relation as a concept in the KB and introduce a new relation node for each relation of each RDF triple .",0
32537,The new relation node is connected to the subject and object entities by two new binary relations A0 and A1 respectively .,0
32538,"For instance , ( pre-ceded By A0 Aenir ) and ( precede d By A1 Castle ) .",0
32539,"Thus , E is the set of entities including reified relations and Va set of labelled edges with labels { A0 , A1 } .",0
32540,The reification of relations is useful in two ways .,0
32541,The encoder is able to produce a hidden state for each relation in the input ; and it permits to model an arbitrary number of KB relations efficiently .,0
32542,SR11 Deep,0
32543,Task,0
32544,"The surface realis ation shared task proposed two generation tasks , namely shallow and deep realis ation .",0
32545,"Here we focus on the deep task where the input is a semantic dependency graph that represents a target sentence using predicate - argument structures ( NomBank ; , PropBank ; ) .",0
32546,"This task covers a more complex semantic representation of language meaning ; on the other hand , the representation is closer to surface form .",0
32547,Nodes in the graph are lemmas of the target sentence .,0
32548,"Only complementizers that , commas , and to infinitive nodes are removed .",0
32549,Edges are labelled with NomBank and PropBank labels .,0
32550,5 Each node is also associated with morphological ( e.g. num=sg ) and punctuation features ( e.g. bracket = r ) .,0
32551,"The source graph X = ( V , E ) is a semantic dependency graph .",0
32552,"We extend this representation to model morphological information , i.e. each node in V is of the form ( lemma , features ) .",0
32553,"For this task we modify the encoder , Section 2 , to represent each input node ash v = [ h l ; hf ] , where each input node is the concatenation of the lemma and the sum of feature vectors .",0
32554,Experiments,0
32555,We tested our models on the WebNLG and SR11 Deep datasets .,0
32556,The WebNLG dataset contains 18102 training and 871 development datatext pairs .,0
32557,"The test dataset is split in two sets , test Seen ( 971 pairs ) and a test set with new unseen categories for KB entities .",0
32558,As here we are interested only in the modelling aspects of the structured input data we focus on our evaluation only on the test partition with seen categories .,0
32559,Sequential Encoders For both WebNLG and SR11 Deep tasks we used a standard sequence - to - sequence model with an LSTM encoder as baseline .,0
32560,Both take as input a linearised version of the source graph .,0
32561,"For the WebNLG baseline , we use the linearis ation scripts provided by .",0
32562,For the SR11 Deep baseline we follow a similar linearis ation procedure as proposed for AMR graphs .,0
32563,We built a linearis ation based on a depth first traversal of the input graph .,0
32564,Siblings are traversed in random order ( they are anyway shuffled in the given dataset ) .,0
32565,We repeat a child node when anode is revisited by a cycle or has more than one parent .,0
32566,The baseline model for the WebNLG task uses one layer bidirectional LSTM encoder and one layer LSTM decoder with embeddings and hidden units set to 256 dimensions .,0
32567,For the SR11 Deep task we used the same architecture with 500 - dimensional hidden states and embeddings .,0
32568,All hyperparameters tuned on the development set .,0
32569,GCN Encoders,0
32570,The GCN models consist of a GCN encoder and LSTM decoder .,0
32571,"For the WebNLG task , all encoder and decoder embeddings and hidden units use 256 dimensions .",0
32572,We obtained the best results with an encoder with four GCN layers with residual connections .,0
32573,"For the SR11 Deep task , we set the encoder and decoder to use 500 - dimensional embeddings and hidden units of size 500 .",0
32574,"In this task , we obtained the best development performance by stacking seven GCN layers with dense connections .",0
32575,We use delexicalis ation for the WebNLG dataset and apply the procedure provided for the baseline in .,0
32576,"For the SR11 Deep dataset , we performed entity anonymis ation .",0
32577,"First , we compacted nodes in the tree corresponding to a single named entity ( see for details ) .",0
32578,"Next , we used a name entity recogniser ) to tag entities in the input with type information ( e.g. person , location , date ) .",0
32579,"Two entities of the same type in a given input will be given a numerical suffix , e.g. PER 0 and PER 1 .",0
32580,"A GCN - based Generator For the WebNLG task , we extended the GCN - based model to use pre-trained word Embeddings ( GloVe ) and Copy mechanism , we name this variant GCN EC .",0
32581,"To this end , we did not use delexicalis ation but rather represent multi-word subject ( object ) entities with each word as a separate node connected with special Named Entity ( NE ) labelled edges .",0
32582,"For instance , the book entity Into Battle is represented as ( Into NE Battle ) .",0
32583,Encoder ( decoder ) embeddings and hidden dimensions were set to 300 .,0
32584,The model stacks six GCN layers and uses a single layer LSTM decoder .,0
32585,Evaluation metrics,0
32586,"As previous works in these tasks , we evaluated our models using BLEU , ME - TEOR ( Denkowski and Lavie , 2014 ) and TER automatic metrics .",0
32587,During preliminary experiments we noticed considerable variance from different model initialis ations ; we thus run 3 experiments for each model and report average and standard deviation for each metric .,0
32588,Results,1
32589,WebNLG task,1
32590,In we report results on the WebNLG test data .,0
32591,"In this setting , the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder , with .009 BLEU points .",1
32592,The GCN model is also more stable than the baseline with a standard deviation of .004 vs . 010 .,1
32593,We also compared the GCN EC model with the neural models submitted to the WebNLG shared task .,0
32594,The GCN EC model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points ; and MELBOURNE by .014 BLEU points .,1
32595,GCN EC is behind ADAPT which relies on sub-word encoding .,0
32596,SR11 Deep task,1
32597,"In this more challenging task , the GCN encoder is able to better capture the William Anders was a crew member of OPERATOR ' s Apollo 8 alongside backup pilot Buzz Aldrin and backup pilot Buzz Aldrin .",0
32598,"GCNEC william anders , who retired on the 1st of september 1969 , was a crew member on apollo 8 along with commander frank borman and backup pilot buzz aldrin . SR11 Deep ( SROOT SROOT will ) ( will P . ) ( will SBJ temperature ) ( temperature A1 economy ) ( economy AINV the ) ( economy SUFFIX 's ) ( will VC be ) ( be VC take ) ( take A1 temperature ) ( take A2 from ) ( from A1 point ) ( point A1 vantage ) ( point AINV several ) ( take AM - ADV with ) ( with A1 reading ) ( reading A1 on ) ( on A1 trade ) ( trade COORD output ) ( output COORD housing ) ( housing COORD and ) ( and CONJ inflation ) ( take AM - MOD will ) ( take AM - TMP week ) ( week AINV this )",0
32599,Gold,0
32600,"The economy 's temperature will betaken from several vantage points this week , with readings on trade , output , housing and inflation .",0
32601,"Baseline the economy 's accords will betaken from several phases this week , housing and inflation readings on trade , housing and inflation .",0
32602,"GCN the economy 's temperatures will betaken from several vantage points this week , with reading on trades output , housing and inflation .: GCN ablation study ( layers ( L ) and skipconnections : none , residual ( res ) and dense ( den ) ) .",0
32603,Average and standard deviation of BLEU scores over three runs on the WebNLG dev. set .,0
32604,Number of parameters ( millions ) including embeddings .,0
32605,"structure of the input graph than the LSTM encoder , resulting in . 647 BLEU for the GCN vs. . 377 BLEU of the LSTM encoder as reported in .",0
32606,When we add linguistic features to the GCN encoding we get . 666 BLEU points .,0
32607,"We also compare the neural models with upper bound results on the same dataset by the pipeline model of The STUMBA - D and TBDIL model obtains respectively .794 and . 805 BLUE , outperforming the GCN - based model .",1
32608,"It is worth noting that these models rely on separate modules for syntax prediction , tree linearis ation and morphology generation .",0
32609,"In a multi-lingual setting , our model will not need to re-train some modules for different languages , but rather it can exploit them for multi -task training .",0
32610,"Moreover , our model could also exploit other supervision signals at training time , such as gold POS tags and gold syntactic trees as used in .",0
32611,Qualitative Analysis of Generated Text,0
32612,We manually inspected the outputs of the LSTM and GCN models .,0
32613,shows examples of source graphs and generated texts ( we in - cluded more examples in Section A ) .,0
32614,Both models suffer from repeated and missing source content ( i.e. source units are not verbalised in the output text ( under-generation ) ) .,0
32615,"However , these phenomena are less evident with GCNbased models .",0
32616,We also observed that the LSTM output sometimes presents hallucination ( overgeneration ) cases .,0
32617,Our intuition is that the strong relational inductive bias of helps the GCN encoder to produce a more informative representation of the input ; while the LSTM - based encoder has to learn to produce useful representations by going through multiple different sequences over the source data .,0
32618,Ablation Study,0
32619,In ( BLEU ) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset .,0
32620,The first thing we notice is the importance of skip connections between GCN layers .,1
32621,Residual and dense connections lead to similar results .,1
32622,"Dense connections ( Table 4 ( SIZE ) ) produce models bigger , but slightly less accurate , than residual connections .",1
32623,The best GCN model has slightly more parameters than the baseline model ( 4.9 M vs. 4.3M ) .,0
32624,Conclusion,0
32625,We compared LSTM sequential encoders with a structured data encoder based on GCNs on the task of structured data to text generation .,0
32626,"On two different tasks , WebNLG and SR11 Deep , we show that explicitly encoding structural information with GCNs is beneficial with respect to sequential encoding .",0
32627,"In future work , we plan to apply the approach to other input graph representations like Abstract Meaning Representations ( AMR ; ) and scoped semantic representations .",0
32628,We implemented all our models using OpenNMTpy .,0
32629,For all experiments we used a batch size of 64 and Adam as the optimizer with an initial learning rate of 0.001 .,0
32630,"For GCN models and baselines we used a one - layer LSTM decoder , we used dropout in both encoder and decoder with a rate of 0.3 .",0
32631,We adopt early stopping on the development set using BLEU scores and we trained for a maximum of 30 epochs .,0
32632,", New York ) ( Adirondack Regional Airport cityServed Lake Placid , New York ) ( Adirondack Regional Airport cityServed Saranac Lake , New York ) ( Saranac Lake , New York country United States ) LSTM Adirondack Regional Airport serves the cities of Lake Placid and Saranac Lake ( Harrietstown ) in the United States .",0
32633,"GCN Adirondack Regional Airport serves the city of Saranac Lake , which is part of Harrietstown , Essex County , New York , United States .",0
32634,"GCNEC adirondack regional airport serves the cities of lake placid and saranac lake , essex county , new york , united states .",0
32635,"adirondack regional airport serves the city of saranac lake , essex county , new york , united states .",0
32636,"Construction of Adisham Hall , Sri Lanka began in 1927 and was completed in 1931 .",0
32637,"GCNEC adisham hall , sri lanka , constructed in 1931 , is located in sri lanka .",0
32638,the hall has the architectural style ' tudor revival ' . SR11 Deep ( SROOT SROOT say ) ( say A0 economist ) ( say A1 be ) ( be SBJ export ) ( be VC think ) ( think A1 export ) ( think C - A1 have ) ( have VC rise ) ( rise A1 export ) ( rise A2 strongly ) ( strongly COORD but ) ( but CONJ not ) ( not AINV enough ) ( not AINV offset ) ( offset A1 jump ) ( jump A1 in ) ( in A1 import ) ( jump AINV the ) ( offset A2 export ) ( not AINV probably ) ( strongly TMP in ) ( in A1 august ) ( say P . ),0
32639,Gold,0
32640,A.2 More example outputs,0
32641,"Exports are thought to have risen strongly in August , but probably not enough to offset the jump in imports , economists said .",0
32642,"LSTM exports said exports are thought to have rising strongly , but not enough to offset exports in the imports in august .",0
32643,"GCN exports was thought to have risen strongly in august but not probably to offset the jump in imports , economists said .",0
32644,"SR11 Deep ( SROOT SROOT be ) ( be P ? ) ( be SBJ we ) ( be TMP be ) ( be SBJ project ) ( project A1 research ) ( be VC curtail ) ( curtail A1 project ) ( curtail AM - CAU to ) ( to A1 cut ) ( cut A0 government ) ( cut A1 funding ) ( funding A0 government ) ( to DEP due ) ( to R - AM - TMP when ) ( be VC catch ) ( catch A1 we ) ( catch A2 with ) ( with SUB down ) ( down SBJ grant ) ( grant AINV our ) ( catch P "" ) ( catch P "" )",0
32645,Gold,0
32646,"When research projects are curtailed due to government funding cuts , are we "" caught with our grants down "" ?",0
32647,"LSTM is when research projects is supposed to cut "" due "" projects is caught with the grant down .",0
32648,"GCN when research projects are curtailed to government funding cuts due to government funding cuts , were we caught "" caught "" with our grant down ?",0
32649,title,0
32650,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,1
32651,*,0
32652,abstract,0
32653,"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) .",1
32654,Modern neural generation systems conflate these two steps into a single end - to - end differentiable system .,0
32655,"We propose to split the generation process into a symbolic text - planning stage that is faithful to the input , followed by a neural generation stage that focuses only on realization .",0
32656,"For training a plan - to - text generator , we present a method for matching reference texts to their corresponding text plans .",0
32657,"For inference time , we describe a method for selecting high - quality text plans for new inputs .",0
32658,We implement and evaluate our approach on the WebNLG benchmark .,0
32659,Our results demonstrate that decoupling text planning from neural realization indeed improves the system 's reliability and adequacy while maintaining fluent output .,0
32660,We observe improvements both in BLEU scores and in manual evaluations .,0
32661,"Another benefit of our approach is the ability to output diverse realizations of the same input , paving the way to explicit control over the generated text structure .",0
32662,"John , birth Place , London John , employer , IBM",0
32663,With a possible output : *,0
32664,Introduction,0
32665,"Consider the task of data to text generation , as exemplified in the WebNLG corpus .",0
32666,The system is given a set of RDF triplets describing facts ( entities and relations between them ) and has to produce a fluent text that is faithful to the facts .,0
32667,An example of such triplets is :,0
32668,1 .,0
32669,"John , who was born in London , works for IBM .",0
32670,Other outputs are also possible :,0
32671,"2 . John , who works for IBM , was born in London .",0
32672,3 .,0
32673,"London is the birthplace of John , who works for IBM .",0
32674,IBM employs,0
32675,"John , who was born in London .",0
32676,"These variations result from different ways of structuring the information : choosing which fact to mention first , and in which direction to express each fact .",0
32677,"Another choice is to split the text into two different sentences , e.g. ,",0
32678,John works for IBM .,0
32679,John was born in London .,0
32680,"Overall , the choice of fact ordering , entity ordering , and sentence splits for these facts give rise to 12 different structures , each of them putting the focus on somewhat different aspect of the information .",0
32681,"Realistic inputs include more than two facts , greatly increasing the number of possibilities .",0
32682,Another axis of variation is in how to verbalize the information for a given structure .,0
32683,"For example , ( 2 ) can also be verbalized as 2 a .",0
32684,John works for IBM and was born in London . and ( 5 ) as : 5 a .,0
32685,John is employed by IBM .,0
32686,He was born in London .,0
32687,We refer to the first set of choices ( how to structure the information ) as text planning and to the second ( how to verbalize a plan ) as plan realization .,0
32688,The distinction between planning and realization is at the core of classic natural language generation ( NLG ) works .,0
32689,"However , a recent wave of neural NLG systems ignores this distinction and treat the problem as a single end - to - end task of learning to map facts from the input to the output text .",0
32690,"These neural systems encode the input facts into an intermediary vector - based representation , which is then decoded into text .",0
32691,"While not stated in these terms , the neural system designers hope for the network to take care of both the planning and realization aspect of text generation .",0
32692,"A notable exception is the work of , who introduce a neural content - planning module in the end - to - end architecture .",0
32693,"While the neural methods achieve impressive levels of output fluency , they also struggle to maintain coherency on longer texts , struggle to produce a coherent order of facts , and are often not faithful to the input facts , either omitting , repeating , hallucinating or changing facts ( the NLG community refers to such errors as errors inadequacy or correctness of the generated text ) .",0
32694,"When compared to templatebased methods , the neural systems win in fluency but fall short regarding content selection and faithfulness to the input .",0
32695,"Also , they do not allow control over the output 's structure .",0
32696,"We speculate that this is due to demanding too much of the network : while the neural system excels at capturing the language details required for fluent realization , they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner .",0
32697,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .",1
32698,The text planner determines the information structure and expresses it unambiguously - in our case as a sequence of ordered trees .,1
32699,This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts .,0
32700,"Once the plan is determined , 2 a neural generation system is used to transform it into fluent , natural language text .",1
32701,"By being able to follow the plan structure closely , the network is alleviated from the need to determine higher - level structural decisions and can track what was already covered more easily .",0
32702,"This allows the network to perform the task it excels in , producing fluent , natural language outputs .",0
32703,"We demonstrate our approach on the WebNLG corpus and show it results in outputs which are as fluent as neural systems , but more faithful to the input facts .",0
32704,The method also allows explicit control of the output structure and the generation of diverse outputs ( some diversity examples are available in the Appendix ) .,0
32705,We release our code and the corpus extended with matching plans in https://github.com/AmitMY/ chimera .,1
32706,Overview of the Approach,0
32707,Task Description,0
32708,Our method is concerned with the task of generating texts from inputs in the form of RDF sets .,0
32709,"Each input can be considered as a graph , where the entities are nodes , and the RDF relations are directed labeled edges .",0
32710,Each input is paired with one or more reference texts describing these triplets .,0
32711,The reference can be either a single sentence or a sequence of sentences .,0
32712,"Formally , each input G consists of a set of triplets of the form ( s i , r i , oi ) , where s i , oi ?",0
32713,"V ( "" subject "" and "" object "" ) correspond to entities from DBPedia , and r i ?",0
32714,"R is a labeled DBPedia relation ( V and R are the sets of entities and relations , respectively ) .",0
32715,"For example , shows a triplet set G and shows a reference text .",0
32716,"We consider the data set as a set of input-output pairs , where the same G may appear in several pairs , each time with a different reference .",0
32717,Method Overview,0
32718,We split the generation process into two parts : text planning and sentence realization .,0
32719,"Given an input G , we first generate a text plan plan ( G ) specifying the division of facts to sentences , the order in which the facts are expressed in each sentence , and the ordering of the sentences .",0
32720,This data - to - plan step is non-neural ( Section 3 ) .,0
32721,"Then , we generate each sentence according to the plan .",0
32722,This plan - to - sentence step is achieved through an NMT system ( Section 4 ) .,0
32723,demonstrates the entire process .,0
32724,"To facilitate our plan - based architecture , we devise a method to annotate ( G , ref ) pairs with the corresponding plans ( Section 3.1 ) , and use it to construct a dataset which is used to train the planto - text translation .",0
32725,The same dataset is also used to devise a plan selection method ( Section 3.2 ) .,0
32726,General Applicability,0
32727,It is worth considering the dataset - specific vs. general applicability aspects of our method .,0
32728,"On the low - level details , this work is very much dataset dependent .",0
32729,"We show how to represent plans for specific datasets , and , importantly for this work , how to automatically construct plans for this dataset given inputs and expected natural language outputs .",0
32730,"The method of plan construction will likely not generalize "" as is "" to other datasets , and the plan structure itself may also be found to be lacking for more demanding generation tasks .",0
32731,"However , on a higher level , our proposal is very general : intermediary plan structures can be helpful , and one should consider ways of obtaining them , and of using them .",0
32732,"In the short term , this will likely take the form of ad - hoc explorations of plan structures for specific tasks , as we do here , to establish their utility .",0
32733,"In the longer term , research may evolve to looking into how general - purpose plan are structured .",0
32734,"Our main message is that the separation of planning from realization , even in the context of neural generation , is a useful one to be considered .",0
32735,Text Planning,0
32736,Plan structure,0
32737,Our text plans capture the division of facts to sentences and the ordering of the sentences .,0
32738,"Additionally , for each sentence , the plan captures ( 1 ) the ordering of facts within the sentence ; ( 2 ) The ordering of entities within a fact , which we call the direction of the relation .",0
32739,"For example , the { A , location , B } relation can be expressed as either A is located in B or B is the location of A ; ( 3 ) the structure between facts that share an entity , namely chains and sibling struc - tures as described below .",0
32740,John,0
32741,"London England A text plan is modeled as a sequence of sentence plans , to be realized in order .",0
32742,"Each sentence plan is modeled as an ordered tree , specifying the structure in which the information should be realized .",0
32743,Structuring each sentence as a tree enables a clear succession between different facts through shared entities .,0
32744,"Our text - plan design assumes that each entity is mentioned only once in a sentence , which holds in the WebNLG corpus .",0
32745,The ordering of the entities and relations within a sentence is determined by a pre-order traversal of the tree .,0
32746,shows an example of a text plan .,0
32747,"Formally , given the input G , a text plan T is a sequences of sentence plans T = s 1 , ... , s NT .",0
32748,"A sen - tence plan sis a labeled , ordered tree , with arcs of the form ( h , , m ) , where h , m ?",0
32749,"V are head and modifier nodes , each corresponding to an input entity , and = ( r , d) is the relation between nodes , where r ?",0
32750,"R is the RDF relation , and d ? {? , ?}",0
32751,denotes the direction in which the relation is ex-,0
32752,"Chains ( h , 1 , m ) , ( m , 2 , x ) represent a succession of facts that share a middle entity ( ) , while siblings - nodes with the same parent - ( h , 1 , m 1 ) , ( h , 2 , m 2 ) represents a succession of facts about the same entity ( ) .",0
32753,Sibling and chain structures can be combined ( ) .,0
32754,"An example of an input we addressed in the WebNLG corpus , and matching text plan is given in .",0
32755,Exhaustive generation,0
32756,"For small - ish input graphs G- such as those in the WebNLG task we consider here - it is trivial to generate all possible plans by first considering all the ways of grouping the input into sets , then from each set generating all possible trees by arranging it as an undirected graph and performing several DFS traversals starting from each node , where each DFS traversal follows a different order of children .",0
32757,Adding Plans to Training Data,0
32758,"While the input RDFs and references are present in the training dataset , the plans are not .",0
32759,"We devise a method to recover the latent plans for most of the input -reference pairs in the training set , constructing a new dataset of ( G , ref , T ) triplets of inputs , reference texts , and corresponding plans .",0
32760,"We define the reference ref , and the text - plan T to be consistent with each other iff ( a ) they exhibit the same splitting into sentences - the facts in every sentence in ref are grouped as a sentence plan in T , and ( b ) for each corresponding sentence and sentence - plan , the order of the entities is identical .",0
32761,"The matching of plans to references is based on the observations that ( a ) it is relatively easy to identify entities in the reference texts , and a pair of entities in an input is unique to a fact ; ( b ) it is relatively easy to identify sentence splits ; ( c ) a reference text and it s matching plan must share the same entities in the same order , and with the same sentence splits .",0
32762,Sentence split consistency,0
32763,"We define a set of triplets to be potentially consistent with a sentence iff each triplet contains at least one entity from the sentence ( either its subject or object appear in the sentence ) , and each entity in the sentence is covered by at least one triplet .",0
32764,"Given a reference text , we split it into sentences using NLTK , and look for divisions of G into disjoint sets such that each set is consistent with a corresponding sentence .",0
32765,"For each such division , we consider the exhaustive set of all induced plans .",0
32766,"Facts order consistency A natural criterion would be to consider a reference sentence and a sentence - plan originating from the corresponding RDF as matching iff the sets of entities in the sentence and the plan are identical , and all entities appear in the same order .",0
32767,"Based on this , we could represent each sentence and each plan as a sequence of entities , and verify the sequences match .",0
32768,"However , using this criterion is complicated by the fact that it is not trivial to map between the entities in the plan ( that originate from the RDF triplets ) and the entities in the text .",0
32769,"In particular , due to language variability , the same plan entity may appear in several forms in the textual sentences .",0
32770,"Some of these variations ( i.e. "" A.F.C Fylde "" vs. "" AFC Fylde "" ) can be recognized heuristically , while others require external knowledge ( "" UK conservative party "" vs. "" the Tories "" ) , and some are ambiguous and require full - fledged co-reference resolution ( "" them "" , "" he "" , "" the former "" ) .",0
32771,"Hence , we relax our matching criterion to allow for possible unrecognized entities in the text .",0
32772,"Concretely , we represent each sentence plan as a sequence of its entities ( pe 1 , ... , pe k ) , and each sentence as the sequence of its entities which we managed to recognize and to match with an input entity ( se 1 , ... , se m ) , m ? k.",0
32773,tions hold :,0
32774,"( 1 ) The sentence entities ( se 1 , ... , se m ) area proper sub- sequence of the plan entities ( pe 1 , ... , pe k ) ; and ( 2 ) each of the remaining entities in the plan already appeared previously in the plan .",0
32775,"The second condition accounts for the fact that most un - identified entities are due to pronouns and similar non-lexicalized referring expressions , and that these only appear after a previous occurrence of the same entity in the text .",0
32776,Test - time Plan Selection,0
32777,"To select the plan to be realized , we propose a mechanism for ranking the possible plans .",0
32778,"Our plan scoring method is a product - of - experts model , where each expert is a conditional probability estimate for some property of the plan .",0
32779,The conditional probabilities are MLE estimates based on the plans in the training set constructed in section 3.1 .,0
32780,Estimates involving relation names are smoothed using Lidstone smoothing to account for unseen relations .,0
32781,We use the following experts : Relation direction,0
32782,For every relation r ?,0
32783,"R , we compute its probability to be expressed in the plan in its original order ( d =? ) or in the reverse order ( d =? ) : p dir ( d =? | R ) .",0
32784,This captures the tendency of certain relations to be realized in the reversed order to how they are defined in the knowledge base .,0
32785,"For example , in the WebNLG corpus the relation "" manager "" is expressed as a variation of "" is managed by "" instead of one of "" is the manager of "" in 68 % of its occurrences ( p dir ( d =? | manager ) = 0.68 ) .",0
32786,Global direction,0
32787,"We find that while the probability of each relation to be realized in a reversed order is usually below 0.5 , still in most plans of longer texts there are one or two relations that appear in the reversed order .",0
32788,We capture this tendency using an expert that considers the conditional probability p gd ( nr = n| | G | ) of observing n reversed edges in an input with | G | triplets .,0
32789,Splitting tendencies,0
32790,"For each input size , we keep track of the possible ways in which the set of facts can be split to subsets of particular sizes .",0
32791,"That is , we keep track of probabilities such asp s ( s = [ 3 , 2 , 2 ] | 7 ) of realizing an input of 7 RDF triplets as three sentences , each realizing the corresponding number of facts .",0
32792,Relation transitions,0
32793,"We consider each sentence plan as a sequence of the relation types expressed in it r 1 , . . . , r k followed by an EOS symbol , and compute the markov transition probabilities over this sequence : p trans ( r 1 , r 2 , . . . , r k , EOS ) = ?",0
32794,"i = 1 , k pt ( r i +1 |r i ) .",0
32795,The expert is the product of the transition probabilities of the individual sentence plans in the text plan .,0
32796,"This captures the tendencies of relations to follow each other and in particular , the tendencies of related relations such as birth - place and birth - date to group , allowing their aggregation in the generated text ( John was born in London on Dec 12th , 1980 ) .",0
32797,Each of the possible plans are then scored based on the product of the above quantities .,0
32798,"The scores work well for separating good from lousy text plans , and we observe a threshold above which most generated plans result in adequate texts .",0
32799,We demonstrate in Section 6 that realizing highly - ranked plans manages to obtain good automatic realization scores .,0
32800,We note that the plan in is the one our ranking algorithm ranked first for the input in .,0
32801,Possible Alternatives,0
32802,"In addition to the single plan selection , the explicit planning stage opens up additional possibilities .",0
32803,"Instead of choosing and realizing a single plan , we can realize a diverse set of high - scoring plans , or realizing a random high - scoring plan , resulting in a diverse and less templatic set of texts across runs .",0
32804,"This relies on the combination of two factors : the ability of the scoring component to select plans that correspond to plausible human - authored texts , and the ability of the neural realizer to faithfully realize the plan into fluent text .",0
32805,"While it is challenging to directly evaluate the plans adequacy , we later show an evaluation of the plan realization component .",0
32806,shows three random plans for the same graph and their realizations .,0
32807,Further examples of the diversity of generation are given in the appendix .,0
32808,"The explicit and symbolic planning stage also allows for user control over the generated text , either by supplying constraints on the possible plans ( e.g. , number of sentences , entities to focus on , the order of entities / relations , or others ) or by supplying complete plans .",0
32809,We leave these options for future work .,0
32810,"We note that for an input of n triplets , there are O ( 2 2 n + n * n! ) possible plans , making this method prohibitive for even moderately sized input graphs .",0
32811,"However , it is sufficient for the WebNLG dataset in which n ?",0
32812,"7 . For larger graphs , better plan scoring and more efficient search algorithms should be devised .",0
32813,We leave this for future work .,0
32814,( a ),0
32815,The Dessert Bionico requires Granola as one of its ingredients and originates from the Jalisco region of Mexico .,0
32816,Bionico is a food found in the Mexico region Jalisco .,0
32817,The Dessert Bionico requires Granola as an ingredient .,0
32818,( c ),0
32819,Bionico contains Granola and is served as a Dessert .,0
32820,"Bionico is a food found in the region of Jalisco , Mexico",0
32821,Plan Realization,0
32822,"For plan realization , we use an off - the - shelf vanilla neural machine translation ( NMT ) system to translate plans to texts .",0
32823,"The explicit division to sentences in the text plan allows us to realize each sentence plan individually which allows the realizer to follow the plan structure within each ( rather short ) sentence , reducing the amount of information that the model needs to remember .",0
32824,"As a result , we expect a significant reduction in over - and under- generation of facts , which are common when generating longer texts .",0
32825,"Currently , this comes at the expense of not modeling discourse structure ( i.e. , referring expressions ) .",0
32826,"This deficiency maybe handled by integrating the discourse into the text plan , or as a post-processing step .",0
32827,8 . We leave this for future work .,0
32828,"To use text plans as inputs to the NMT , we linearize each sentence plan by performing a preorder traversal of the tree , while indicating the tree structure with brackets ( ) .",0
32829,"The directed relations ( r , d ) are expressed as a sequence of two or more tokens , the first indicating the direction and the rest expressing the relation .",0
32830,"Entities thatare identified in the reference text are replaced with single , entity - unique tokens .",0
32831,This allows the NMT system to copy such entities from the input rather than generating them .,0
32832,is an example of possible text resulting from such linearization .,0
32833,Training details,0
32834,We use a standard NMT setup with a copy- attention mechanism 10 and the pre-trained Glo Ve.,0
32835,"6B word em - 8 Minimally , each entity occurrence can keep track of the number of times it was already mentioned in the plan .",0
32836,"Other alternatives include using a full - fledged referring expression generation system such as NeuralREG ( Ferreira et al. , 2018 )",0
32837,We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase .,0
32838,"Concretely , we use the Open NMT toolkit with the copy attn flag .",0
32839,Exact parameter values are beddings 11 .,0
32840,"The pretrained embeddings are used to initialize the relation tokens in the plans , as well as the tokens in the reference texts .",0
32841,Generation details,0
32842,We translate each sentence plan individually .,0
32843,"Once the text is generated , we replace the entity tokens with the full entity string as it appears in the input graph , and lexicalize all dates as Month DAY + ordinal , YEAR ( i.e. , July 4th , 1776 ) and for numbers with units ( i.e. , "" 5 "" ( minutes ) ) we remove the parenthesis and quotation marks ( 5 minutes ) .",0
32844,Experimental Setup,0
32845,"The WebNLG challenge consists of mapping sets of RDF triplets to text including referring expression generation , aggregation , lexicalization , surface realization , and sentence segmentation .",0
32846,It contains sets with up to 7 triplets each along with one or more reference texts for each set .,0
32847,"The test set is split into two parts : seen , containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data , and unseen , containing inputs extracted for entities and relations belonging to 5 unseen categories .",0
32848,"While the unseen category is conceptually appealing , we view the seen category as the more relevant setup : generating fluent , adequate and diverse text for a mix of known relation types is enough of a challenge also without requiring the system to invent verbalizations for unknown relation types .",0
32849,Any realistic generation system could afford to provide at least a few verbalizations for each relation of interest .,0
32850,We thus focus our attention mostly on the seen case ( though our system does also perform well on the unseen case ) .,0
32851,"Following Section 3.1 , we manage to match a detailed in the appendix .",0
32852,11 nlp.stanford.edu/data/glove.6B.zip consistent plan for 76 % of the reference texts and use these plan - text pairs to train the plan realization NMT component .,0
32853,"Overall , the WebNLG training set contains 18 , 102 RDF - text pairs while our plan - enhanced corpus contains 13 , 828 plantext pairs .",0
32854,12,0
32855,Compared Systems,0
32856,"We compare to the best submissions in the WebNLG challenge : Melbourne , an end - to - end system that scored best on all categories in the automatic evaluation , and UPF - FORGe , a classic grammar - based NLG system that scored best in the human evaluation .",1
32857,"Additionally , we developed an end - to - end neural baseline which outperforms the WebNLG neural systems .",1
32858,"It uses a set encoder , an LSTM decoder with attention , a copy - attention mechanism and a neural checklist model , as well as applying entity dropout .",1
32859,The entity - dropout and checklist component are the key differentiators from previous systems .,0
32860,We refer to this system as StrongNeural .,0
32861,6 Experiments and Results,1
32862,Automatic Metrics,0
32863,"We begin by comparing our plan - based system ( BestPlan ) to the state - of - the - art using the common automatic metrics : BLEU , Meteor , ROUGE Land CIDEr , using the nlg- eval 13 tool on the entire test set and on each part separately ( seen and unseen ) .",0
32864,"In the original challenge , the best performing system in automatic metric was based on end - toend NMT ( Melbourne ) .",0
32865,Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics,1
32866,Manual Evaluation,0
32867,"Next , we turn to manually evaluate our system 's performance regarding faithfulness to the input on the one hand and fluency on the other .",0
32868,"We describe here the main points of the manual evaluation setup , with finer details in the appendix .",0
32869,Faithfulness,0
32870,"As explained in Section 3 , the first benefit we expect of our plan - based architecture is to make the neural systems task simpler , helping it to remain faithful to the semantics expressed in the plan which in turn is guaranteed to be faithful to the original RDF input ( by faithfulness , we mean expressing all facts in the graph and only facts from the graph : not dropping , repeating or hallucinating facts ) .",0
32871,We conduct a manual evaluation over the seen portion of the WebNLG human evaluated test set ( 139 input sets ) .,0
32872,We compare Best - Plan and StrongNeural .,0
32873,"15 For each output text , we manually mark which relations are expressed in it , which are omitted , and which relations exist with the wrong lexicalization .",0
32874,"We also count the number of relations the system over generated , either repeating facts or inventing new facts .",0
32875,16 shows the results .,0
32876,"BestPlan reduces all error types compared to StrongNeural , by 85 % , 56 % and 90 % respectively .",0
32877,"While on - par regarding automatic metrics , BestPlan substantially outperforms the new state - of - the - art end - to - end neural system in semantic faithfulness .",0
32878,"For example ,",0
32879,StrongNeural ( 4 b ) and,0
32880,BestPlan ( 4 c ) on the last input in the seen test set ( 4 b ) .,0
32881,"While both systems chose three sentences split and aggregated details about birth in one sentence and details about the occupation in another , StrongNeural also expressed the information in chronological order .",0
32882,"However , StrongNeural failed to generate facts 3 and 5 .",0
32883,"BestPlan made a lexicalization mistake in the third sentence by expressing "" October "" before the actual date , which is probably caused by faulty entity matching for one of the references , and ( by design ) did not generate any referring expression , which we leave for future work .",0
32884,"39 Over- generation 3 29 Fluency Next , we assess whether our systems succeed at maintaining the high - quality fluency of the neural systems .",0
32885,"We perform pairwise evaluation via Amazon Mechanical Turk wherein each task the worker is presented with an RDF set ( both in a graph form , and textually ) , and two texts in random order , one from BestPlan , the other from a competing system .",0
32886,"We compare Best - Plan against a strong end - to - end neural system ( StrongNeural ) , a grammar - based system which StrongNeural Reference UPF - FORGe BestPlan - 0.6 % - 5.4 % + 5.1 % : MTurk average worker score for BestPlan compared to each system .",0
32887,"It is a worse than the reference texts , on - par with the neural end - to - end system , and a better than the previous state - of - the - art .",0
32888,BestPlan StrongNeural,0
32889,"is the state - of - the - art in human evaluation ( UPF - FORGe ) , and the human - supplied WebNLG references ( Reference ) .",0
32890,"The workers were presented with three possible answers : BestPlan text is better ( scored as 1 ) , the other text is better ( scored as - 1 ) , and both texts are equally fluent ( scored as 0 ) .",0
32891,shows the average worker score given to each pair divided by the number of texts compared .,0
32892,"BestPlan performed on - par with StrongNeural , and surpassed the previous state - of - the - art UPF - FORGe .",0
32893,"It , however , scored worse than the reference texts , which is expected given that it does not produce referring expressions .",0
32894,"Our approach manages to keep the same fluency level typical to end - to - end neural systems , thanks to the NMT realization component .",0
32895,Plan Realization Consistency,0
32896,We test the extent to which the realizer generates texts thatare consistent with the plans .,0
32897,"For several subsets of ranked plans ( best plan , top 1 % , and top 10 % ) for the seen and unseen test sets separately , we realize up to 100 randomly selected text - plans per input .",0
32898,We realize each sentence plan and evaluate using two criteria :,0
32899,( 1 ) Do all entities from the plan appear in the realization ;,0
32900,( 2 ) Like the consis - : Surface realizer performance .,0
32901,Entities : Percent of sentence plans that were realized with all the requested entities .,0
32902,"Order : of the sentences that were realized with all requested entities , percentage of realizations that followed the requested entity order .",0
32903,"tency we defined above , do all entities appear in the same order in the plan and the realization .",0
32904,indicates that for decreasingly probable plans our realizer does worse in the first criterion .,0
32905,"However , for both parts of the test set , if the realizer managed to express all of the entities , it expressed them in the requested order , meaning the outputs are consistent with plans .",0
32906,"This opens up a potential for user control and diverse outputs , by choosing different plans for realization .",0
32907,"Finally , we verify that the realization of potentially diverse plans is not only consistent with each given plan but also preserves output quality .",0
32908,"For each input , we realize a random plan from the top 10 % .",0
32909,"We repeat this process three times with different random seeds to generate different outputs , and mark these systems as Random Plan - 1/ 2 / 3 .",0
32910,"shows that these random plans maintain decent quality on the automatic metrics , with a limited performance drop , and the automatic score is stable across random seeds .",0
32911,Related Work,0
32912,Text planning is a major component in classic NLG .,0
32913,"For example , shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan , manually tagging them , and learning to rank them using the RankBoost algorithm ( Schapire , 1999 ) .",0
32914,"Our planning approach is similar , but we only have a set of "" good "" reference plans without internal ranks .",0
32915,"While the sentence planning decides on the aggregation , one crucial decision left is sentence order .",0
32916,"We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence , not on the content .",0
32917,devised a probabilistic model for sentence ordering which correlated well with human ordering .,0
32918,"Our plan selection procedure is admittedly simple , and can be improved by integrating insights from previous text planning works .",0
32919,"Many generation systems are based on a black - box NMT component , with various pre-processing transformation of the inputs ( such as delexicalization ) and outputs to aid the generation process .",0
32920,Generation from structured data often requires referring to a knowledge base .,0
32921,This led to input-coverage tracking neural components such as the checklist model and copy-mechanism .,0
32922,Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach .,0
32923,"While our explicit planning stage reduces the amount of over-generation , our realizer maybe further improved by using a checklist model .",0
32924,"More complex tasks , like RotoWire require modeling also document - level planning .",0
32925,explored a method to explicitly model document planning using the attention mechanism .,0
32926,"The neural text generation community has also recently been interested in "" controllable "" text generation , where various aspects of the text ( often sentiment ) are manipulated or transferred .",0
32927,"In contrast , like in , here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation .",0
32928,Conclusion,0
32929,"We proposed adding an explicit symbolic planning component to a neural data - to - text NLG system , which eases the burden on the neural component concerning text structuring and fact tracking .",0
32930,"Consequently , while the plan - based system performs on par with a strong end - to - end neural system regarding automatic evaluation metrics and human fluency evaluation , it substantially outperforms the end - to - end system regarding faithfulness to the input .",0
32931,"Additionally , the planning stage allows explicit user- control and generating diverse sentences , to be pursued in future work .",0
32932,Appendices A Diverse Outputs,0
32933,"We demonstrate the ability of the model to produce diverse outputs by showing examples of generation from graphs with 4 , 5 or 6 edges .",0
32934,"For each graph , we show every kth plan , where k is chosen so that our 25 examples cover the top 10 % of the plans , and order them by the scores assigned to them by the scoring model ( the score is shown to the right of each plan , as well as the rank in the list ) .",0
32935,"Higher scoring plans correspond to more natural plans , according to our model , but all of them are viable options .",0
32936,"Then , for each plan we show the corresponding text generated by the NMT model .",0
32937,This provides a glimpse of : ( 1 ) the quality of the scoring model ; ( 2 ) the diversity of the plans ; ( 3 ) the naturalness of the generation .,0
32938,"For the plans , color boxes indicate entities , and gray boxes around them indicate bracketing .",0
32939,Vertical bars indicate sentence splits .,0
32940,"For the generated text , each entity is underlines with the color corresponding to its box .",0
32941,A.1 Example : Graph with 4 Edges shows a random 4 - edge graph from the seen part of the test set .,0
32942,shows the plans and the corresponding texts .,0
32943,shows a random 5 - edge graph from the seen part of the test set .,0
32944,shows the plans and the corresponding texts .,0
32945,shows a random 6 - edge graph from the seen part of the test set .,0
32946,shows the plans and the corresponding texts . :,0
32947,Example of a graph with 6 edges B Manual Evaluation Setup,0
32948,"When performing pairwise system comparisons , we show the user , for each set of RDFs , the two texts produced by the compared systems in random order , along with the RDF triplets in textual and image forms as a reference .",0
32949,"For consistency , both texts are normalized by lower - casing and splitting tokens on punctuation .",0
32950,The same interface is used for turkers ( for the fluency task ) and local annotators ( for the faithfulness task ) .,0
32951,B.1 Fluency Evaluation by Crowd,0
32952,We evaluate on the RDF sets in the original WebNLG manual evaluation setup .,0
32953,The task is performed by mechanical - turk workers .,0
32954,The workers are presented with the question :,0
32955,""" Which text reads more fluently ? "" which can be answered by either Text 1 , Text 2 or Both are equally good or bad .",0
32956,"We paid 0.08 $ per hit , employing three workers on each .",0
32957,"For qualification , workers were required to have over 98 % hit approval rate , and over 1000 approved hits .",0
32958,B.2 Faithfulness Evaluation by Expert,0
32959,"To obtain reliable fine - grained evaluation of semantic faithfulness , the first author annotated the system outputs of StrongNeural and BestPlan .",0
32960,"For each text , we present all the RDF input triplets , and ask the annotator to choose for each triplet one of three options :",0
32961,( 1 ) This triplet is expressed in the text ; ( 2 ) This triplet is not expressed in the text ( ommitted ) ; ( 3 ) The text expresses a relation between the two entities that is different than the one specified for them in the RDF triplet ( wrong lexicalization ) .,0
32962,"Also , for each text , we ask the annotator to count the number of facts that were wrongly over generated , counting both repeated facts and hallucinated ones .",0
32963,C Training Parameters,0
32964,"For the realization model we use the Open - NMT toolkit with pretrained Glo Ve. 6B word embeddings , downloaded from http://nlp. stanford.edu/data/glove.6B.zip.",0
32965,We used the default parameters ( except for the - copy attn flag ) .,0
32966,This corresponds to the following values :,0
32967,train steps = 40000,0
32968,title,0
32969,Pragmatically Informative Text Generation,1
32970,abstract,0
32971,We improve the informativeness of models for conditional text generation using techniques from computational pragmatics .,0
32972,"These techniques formulate language production as a game between speakers and listeners , in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes .",0
32973,"While such approaches are widely used in cognitive science and grounded language learning , they have received less attention for more standard language generation tasks .",0
32974,"We consider two pragmatic modeling methods for text generation : one where pragmatics is imposed by information preservation , and another where pragmatics is imposed by explicit modeling of distractors .",0
32975,We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations .,0
32976,Introduction,0
32977,Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures .,1
32978,"While such approaches are capable of modeling a variety of pragmatic phenomena , their main application in natural language processing has been to improve the informativeness of generated text in grounded language learning problems .",0
32979,"In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .",0
32980,"Our work builds on a line of learned Rational Speech Acts ( RSA ) models , in which generated strings are selected to optimize the behav - Human - written A cheap coffee shop in riverside with a 5 out of 5 customer rating is Fitzbillies .",1
32981,Fitzbillies is family friendly and serves English food .,0
32982,Base sequence - to - sequence model ( S0 ) Fitzbillies is a family friendly coffee shop located near the river .,0
32983,Distractor - based pragmatic system ( S D 1 ),0
32984,Fitzbillies is a family friendly coffee shop that serves English food .,0
32985,It is located in riverside area .,0
32986,It has a customer rating of 5 out of 5 and is cheap .,0
32987,Reconstructor - based pragmatic system ( S R 1 ),0
32988,Fitzbillies is a family friendly coffee shop that serves cheap English food in the riverside area .,0
32989,It has a customer rating of 5 out of 5 . :,0
32990,Example outputs of our systems on the E2E generation task .,0
32991,"While a base sequence - to - sequence model ( S 0 , Sec. 2 ) fails to describe all attributes in the input meaning representation , both of our pragmatic systems ( S R 1 , Sec. 3.1 and SD 1 , Sec. 3.2 ) and the human - written reference do .",0
32992,ior of an embedded listener model .,0
32993,"The canonical presentation of the RSA framework ( Frank and Goodman , 2012 ) is grounded in reference resolution : models of speakers attempt to describe referents in the presence of distractors , and models of listeners attempt to resolve descriptors to referents .",1
32994,"Recent work has extended these models to more complex groundings , including images and trajectories .",0
32995,"The techniques used in these settings are similar , and the primary intuition of the RSA framework is preserved : from the speaker 's perspective , a good description is one that picks out , as discriminatively as possible , the content the speaker intends for the listener to identify .",0
32996,"Outside of grounding , cognitive modeling , and targeted analysis of linguistic phenomena , rational speech acts models have seen limited application in the natural language processing literature .",0
32997,"In this work we show that they can be extended to a distinct class of language generation problems that use as referents structured descriptions of lingustic content , or other natural language texts .",0
32998,"In accordance with the maxim of quantity or the Q-principle , pragmatic approaches naturally correct underinformativeness problems observed in state - of - theart language generation systems ( S 0 in ) .",0
32999,We present experiments on two language generation tasks : generation from meaning representations and summarization .,0
33000,"For each task , we evaluate two models of pragmatics : the reconstructor - based model of and the distractor - based model of .",0
33001,"Both models improve performance on both tasks , increasing ROUGE scores by 0.2-0.5 points on the CNN / Daily Mail abstractive summarization dataset and BLEU scores by 2 points on the End - to - End ( E2E ) generation dataset , obtaining new state - of - the - art results .",0
33002,Tasks,0
33003,"We formulate a conditional generation task as taking an input i from a space of possible inputs I ( e.g. , input sentences for abstractive summarization ; meaning representations for structured generation ) and producing an output o as a sequence of tokens ( o 1 , . . . , o T ) .",0
33004,"We build our pragmatic approaches on top of learned base speaker models S 0 , which produce a probability distribution S 0 ( o | i ) over output text for a given input .",0
33005,"We focus on two conditional generation tasks where the information in the input context should largely be preserved in the output text , and apply the pragmatic procedures outlined in Sec. 3 to each task .",0
33006,"For these S 0 models we use systems from past work thatare strong , but may still be underinformative relative to human reference outputs ( e.g. , ) .",0
33007,Meaning Representations,0
33008,We evaluate on the E2E task of generation from meaning representations containing restaurant attributes ) .,0
33009,"We report the task 's five automatic metrics : BLEU , NIST , METEOR , ROUGE - L and CIDEr .",0
33010,compares the performance of our base S 0 and pragmatic models to the baseline T - Gen system and the best previous result from the 20 primary systems evaluated in the E2E challenge .,0
33011,"The systems obtaining these results encompass a range of approaches : a template system ) , a neural model , models trained with reinforcement learning , and systems using ensembling and reranking .",0
33012,"To ensure that the benefit of the reconstructor - based pragmatic approach , which uses two models , is not due solely to a model combination effect , we also compare to an ensemble of two base models ( S 0 2 ) .",0
33013,"This ensemble uses a weighted combination of scores of two independently - trained S 0 models , following Eq. 1 ( with weights tuned on the development data ) .",0
33014,"Both of our pragmatic systems improve over the strong baseline S 0 system on all five metrics , with the largest improvements ( 2.1 BLEU , 0.2 NIST , 0.8 METEOR , 1.5 ROUGE - L , and 0.1 CIDEr ) from the S R 1 model .",0
33015,"This S R 1 model outperforms the previous best results obtained by any system in the E2E challenge on BLEU , NIST , and CIDEr , with comparable performance on METEOR and ROUGE - L. :",0
33016,Test results for the non-anonymized CNN / Daily Mail summarization task .,0
33017,"We compare to extractive baselines , and the best previous abstractive results of , and .",0
33018,"We bold our highest performing model on each metric , as well as previous work if it outperforms all of our models .",0
33019,Pragmatic Models,0
33020,"To produce informative outputs , we consider pragmatic methods that extend the base speaker models , S 0 , using listener models , L , which produce a distribution L( i | o ) over possible inputs given an output .",0
33021,"Listener models are used to derive pragmatic speakers , S 1 ( o | i ) , which produce output that has a high probability of making a listener model L identify the correct input .",0
33022,There area large space of possible choices for designing Land deriving S 1 ; we follow two lines of past work which we categorize as reconstructor - based and distractor - based .,0
33023,We tailor each of these pragmatic methods to both our two tasks by developing reconstructor models and methods of choosing distractors .,0
33024,Reconstructor - Based Pragmatics,0
33025,Pragmatic approaches in this category rely on a reconstructor listener model L R defined independently of the speaker .,0
33026,This listener model produces a distribution L R ( i | o ) over all possible input contexts i ?,0
33027,"I , given an output description o.",0
33028,"We use sequence - to - sequence or structured classification models for L R ( described below ) , and train these models on the same data used to supervise the S 0 models .",0
33029,"The listener model and the base speaker model together define a pragmatic speaker , with output score given by :",0
33030,where ?,0
33031,is a rationality parameter that controls how much the model optimizes for discriminative outputs ( see and for a discussion ) .,0
33032,We select an output text sequence o for a given input i by choosing the highest scoring output under Eq. 1 from a set of candidates obtained by beam search in S 0 ( | i ) .,0
33033,Distractor - Based Pragmatics,0
33034,Pragmatic approaches in this category derive pragmatic behavior by producing outputs that distinguish the input i from an alternate distractor input ( or inputs ) .,0
33035,We construct a distractor ?,0
33036,for a given input i in a task - dependent way .,0
33037,"We follow the approach of Cohn - Gordon et al. , outlined briefly here .",0
33038,"The base speakers we build on produce outputs incrementally , where the probability of o t , the word output at time t , is conditioned on the input and the previously generated words : S 0 ( o t | i , o <t ) .",0
33039,"Since the output is generated incrementally and there is no separate listener model that needs to condition on entire output decisions , the distractor - based approach is able to make pragmatic decisions at each word rather than choosing between entire output candidates ( as in the reconstructor approaches ) .",0
33040,The listener L D and pragmatic speaker SD 1 are derived from the base speaker S 0 and a belief distribution pt ( ) maintained at each timestep t over the possible inputs ID :,0
33041,where ?,0
33042,"is again a rationality parameter , and the initial belief distribution p 0 ( ) is uniform , i.e. , p 0 ( i ) = p 0 ( ? ) = 0.5 . Eqs. 2 and 4 are normalized over the true input i and distractor ?; Eq. 3 is normalized over the output vocabulary .",0
33043,We construct an output text sequence for the pragmatic speaker SD 1 incrementally using beam search to approximately maximize Eq. 3 .,0
33044,Meaning Representations,0
33045,A distractor MR is automatically constructed for each input to be the most distinctive possible against the input .,0
33046,We construct this distractor by masking each present input attribute and replacing the value of each nonpresent attribute with the value that is most frequent for that attribute in the training data .,0
33047,"For example , for the input MR in , the distractor is NEAR [ BURGER KING ].",0
33048,Summarization,0
33049,"For each extracted input sentence i ( p ) , we use the previous extracted sentence i ( p?1 ) from the same document as the distractor input ?",0
33050,( for the first sentence we do not use a distractor ) .,0
33051,This is intended to encourage outputs o ( p ) to contain distinctive information against other summaries produced within the same document .,0
33052,Experiments,0
33053,"For each of our two conditional generation tasks we evaluate on a standard benchmark dataset , following past work by using automatic evaluation against human - produced reference text .",0
33054,"We choose hyperparameters for our models ( beam size , and parameters ? and ? ) to maximize task metrics on each dataset 's development set ; see Appendix A.2 for the settings used . :",0
33055,"Test results for the E2E generation task , in comparison to the T - Gen baseline",0
33056,Abstractive Summarization,1
33057,"We evaluate on the CNN / Daily Mail summarization dataset , using non-anonymized preprocessing .",0
33058,"As in previous work , we evaluate using ROUGE and ME - TEOR .",0
33059,"compares our pragmatic systems to the base S 0 model ( with scores taken from Chen and Bansal ( 2018 ) ; we obtained comparable performance in our reproduction 3 ) , an ensemble of two of these base models , and the best previous abstractive summarization result for each metric on this dataset .",0
33060,"We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .",0
33061,"The pragmatic methods obtain improvements of 0.2-0.5 in ROUGE scores and 0.2-1.8 METEOR over the base S 0 model , with the distractor - based approach SD 1 outperforming the reconstructorbased approach S R 1 .",1
33062,"SD 1 is strong across all metrics , obtaining results competitive to the best previous abstractive systems . ( b ) Coverage ratios by attribute type ( columns ) for the base model S0 , and for the pragmatic system SD 1 when constructing the distractor by masking the specified attribute ( rows ) .",1
33063,Cell colors are the degree the coverage ratio increases ( green ) or decreases ( red ) relative to S0 . :,0
33064,"Coverage ratios for the E2E task by attribute type , estimating how frequently the values for each attribute from the input meaning representations are mentioned in the output text .",0
33065,Analysis,0
33066,"The base speaker S 0 model is often underinformative , e.g. , for the E2E task failing to mention certain attributes of a MR , even though almost all the training examples incorporate all of them .",0
33067,"To better understand the performance improvements from the pragmatic models for E2E , we compute a coverage ratio as a proxy measure of how well content in the input is preserved in the generated outputs .",0
33068,The coverage ratio for each attribute is the fraction of times there is an exact match between the text in the generated output and the attribute 's value in the source MR ( for instances where the attribute is specified ) .,0
33069,4 ( a ) shows coverage ratio by attribute category for all models .,0
33070,"The S R 1 model increases the coverage ratio when compared to S 0 across all attributes , showing that using the reconstruction model score to select outputs does lead to an increase in mentions for each attribute .",0
33071,"Coverage ratios increase for SD 1 as well in four out of six categories , but the increase is typically less than that produced by S R 1 .",0
33072,"While SD 1 optimizes less explicitly for attribute mentions than S R 1 , it still provides a potential method to control generated outputs by choosing alternate distractors .",0
33073,shows coverage ratios for SD 1 when masking only a single attribute in the distractor .,0
33074,"The highest coverage ratio for each attribute is usually obtained when masking that attribute in the distractor MR ( entries on the main diagonal , underlined ) , in particular for FAMILYFRIENDLY ( FF ) , FOOD , PRICERANGE ( PR ) , and AREA .",0
33075,"However , masking a single attribute sometimes results in decreasing the coverage ratio , and we also observe substantial increases from masking other attributes : e.g. , masking either FAMILYFRIENDLY or CUSTOMERRAT - ING ( CR ) produces an equal increase in coverage ratio for the CUSTOMERRATING attribute .",0
33076,"This may reflect underlying correlations in the training data , as these two attributes have a small number of possible values ( 3 and 7 , respectively ) .",0
33077,Conclusion,0
33078,"Our results show that S 0 models from previous work , while strong , still imperfectly capture the behavior that people exhibit when generating text ; and an explicit pragmatic modeling procedure can improve results .",0
33079,"Both pragmatic methods evaluated in this paper encourage prediction of outputs that can be used to identify their inputs , either by reconstructing inputs in their entirety or distinguishing true inputs from distractors , so it is perhaps unsurprising that both methods produce similar improvements in performance .",0
33080,"Future work might allow finer - grained modeling of the tradeoff between underand over-informativity within the sequence generation pipeline ( e.g. , with a learned communication cost model ) or explore applications of pragmatics for content selection earlier in the generation pipeline .",0
33081,A Supplemental Material,0
33082,A.1 Reconstructor Model Details,0
33083,"For the reconstructor - based speaker in the E2E task , we first follow the same data preprocessing steps as , which includes a delexicalization module that deals with sparsely occurring MR attributes ( NAME , by mapping such values to placeholder tokens .",0
33084,"MRs have only a few possible values for most attributes : six out of eight attributes have fewer than seven unique values , and the remaining two attributes ( NAME , NEAR ) are handled by our S 0 and SD 1 using delexicalized placeholders , following .",0
33085,"In this way , the reconstructor only needs to predict the presence of these two attributes with a boolean variable , and other attributes with the corresponding categorical variable .",0
33086,We use a one layer bi-directional GRU for the shared sentence encoder .,0
33087,We concatenate the latent vectors from both directions to construct a bi-directional encoded vector hi for every single word vector d i as :,0
33088,"Since not all words contribute equally to predicting each MR attribute , we thus use an attention mechanism to determine the importance of every single word .",0
33089,The aggregated sentence vector for task k is calculated by,0
33090,"The task - specific sentence representation is then used as input to k layers with softmax outputs , returning a probability vector Y ( k ) for each of the k MR attributes .",0
33091,A.2 Hyperparameters,0
33092,"For structured generation , we use beam size 10 , ? = 0.4 , and ? = 0.2 , tuned to maximize the normalized average of all five metrics on the development set .",0
33093,"For abstractive summarization , we use beam size 20 , ? = 0.9 , and ? = 1.0 , tuned to maximize ROUGE - L on the development set .",0
33094,title,0
33095,Data - to - Text Generation with Content Selection and Planning,1
33096,abstract,0
33097,"Recent advances in data - to - text generation have led to the use of large - scale datasets and neural network models which are trained end - to - end , without explicitly modeling what to say and in what order .",0
33098,"In this work , we present a neural network architecture which incorporates content selection and planning without sacrificing end - to - end training .",0
33099,We decompose the generation task into two stages .,0
33100,"Given a corpus of data records ( paired with descriptive documents ) , we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account .",0
33101,Automatic and human - based evaluation experiments show that our model 1 outperforms strong baselines improving the state - of - the - art on the recently released ROTOWIRE dataset .,0
33102,Introduction,0
33103,Data - to - text generation broadly refers to the task of automatically producing text from non-linguistic input .,0
33104,"The input maybe in various forms including data bases of records , spreadsheets , expert system knowledge bases , simulations of physical systems , and soon .",0
33105,"shows an example in the form of a data base containing statistics on NBA basketball games , and a corresponding game summary .",0
33106,"Traditional methods for data - to - text generation ) implement a pipeline of modules including content planning ( selecting specific content from some input and determining the structure of the output text ) , sentence planning ( determining the structure and lexical content of each sentence ) and surface realization ( converting the sentence plan to a surface string ) .",0
33107,"Recent neural generation systems ) do not explicitly model any of these stages , rather they are trained in an end - to - end fashion using the very successful encoder - decoder architecture as their backbone .",0
33108,"Despite producing over all fluent text , neural systems have difficulty capturing long - term structure and generating documents more than a few sentences long .",0
33109,"show that neural text generation techniques perform poorly at content selection , they struggle to maintain inter-sentential coherence , and more generally a reasonable ordering of the selected facts in the output text .",0
33110,Additional challenges include avoiding redundancy and being faithful to the input .,0
33111,"Interestingly , comparisons against templatebased methods show that neural techniques do not farewell on metrics of content selection recall and factual output generation ( i.e. , they often hallucinate statements which are not supported by the facts in the data base ) .",0
33112,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .",0
33113,Our model learns a content plan from the input and conditions on the content plan in order to generate the output document ( see for an illustration ) .,1
33114,"An explicit content planning mechanism has at least three advantages for multi-sentence document generation : it represents a high - level organization of the document structure allowing the decoder to concentrate on the easier tasks of sentence planning and surface realization ; it makes the process of data - to - document generation more interpretable by generating an intermediate representation ; and reduces redundancy in the output , since it is less likely for the content plan to contain the same information in multiple places .",0
33115,"We train our model end - to - end using neural networks and evaluate its performance on ROTOWIRE , a recently released dataset which contains statistics of NBA basketball games paired with human - written summaries ( see ) .",1
33116,Automatic and human evaluation shows that modeling content selection and planning improves generation considerably over competitive baselines .,0
33117,The Boston Celtics defeated the host Indiana Pacers 105 - 99 at Bankers Life Fieldhouse on Saturday .,0
33118,"In a battle between two injury - riddled teams , the Celtics were able to prevail with a much needed road victory .",0
33119,"The key was shooting and defense , as the Celtics outshot the Pacers from the field , from three - point range and from the free - throw line .",0
33120,Boston also held Indiana to 42 percent from the field and 22 percent from long distance .,0
33121,"The Celtics also won the rebounding and assisting differentials , while tying the Pacers in turnovers .",0
33122,"There were 10 ties and 10 lead changes , as this game went down to the final seconds .",0
33123,"Boston ( 5 - 4 ) has had to deal with a gluttony of injuries , but they had the fortunate task of playing a team just as injured here .",0
33124,"Isaiah Thomas led the team in scoring , totaling 23 points and five assists on 4 - of - 13 shooting .",0
33125,He got most of those points by going 14 - of - 15 from the free - throw line .,0
33126,Kelly,0
33127,"Olynyk got a rare start and finished second on the team with his 16 points , six rebounds and four assists .",0
33128,Related Work,0
33129,The generation literature provides multiple examples of content selection components developed for various domains which are either hand - built or learned from data .,0
33130,"Likewise , creating summaries of sports games has been a topic of interest since the early beginnings of generation systems .",0
33131,"Earlier work on content planning has relied on generic planners , based on Rhetorical Structure Theory .",0
33132,They defined content planners by analysing the target texts and devising hand - crafted rules .,0
33133,studied ordering constraints for content plans and learn a content planner from an aligned corpus of inputs and human outputs .,0
33134,A few researchers ) rank content plans according to a ranking function .,0
33135,More recent work focuses on end - to - end systems instead of individual components .,0
33136,"However , most models make simplistic assumptions such as generation without any content selection or planning or content selection without planning .",0
33137,An exception are who incorporate content plans represented as grammar rules operating on the document level .,0
33138,"Their approach works reasonably well with weather forecasts , but does not scale easily to larger data bases , with richer vocabularies , and longer text descriptions .",0
33139,The model relies on the EM algorithm to learn the weights of the grammar rules which can be very many even when tokens are aligned to data base records as a preprocessing step .,0
33140,Our work is closest to recent neural network models which learn generators from data and accompanying text resources .,0
33141,Most previous approaches generate from Wikipedia infoboxes focusing either on single sentences or short texts ( Perez-Beltrachini and Lapata 2018 ) .,0
33142,"use a neural encoder - decoder model to generate weather forecasts and soccer commentaries , while generate NBA game summaries ( see ) .",0
33143,"They introduce a new dataset for data - to - document generation which is sufficiently large for neural network training and adequately challenging for testing the capabilities of document - scale text generation ( e.g. , the average summary length is 330 words and the average number of input records is 628 ) .",0
33144,"Moreover , they propose various automatic evaluation measures for assessing the quality of system output .",0
33145,Our model follows on from addressing the challenges for data - to - text generation identified in their work .,0
33146,We are not aware of any previous neural network - based approaches which incorporate content selection and planning mechanisms and generate multi-sentence documents .,0
33147,"Perez- Beltrachini and Lapata ( 2018 ) introduce a content selection component ( based on multi-instance learning ) without content planning , while propose a sentence planning mechanism which orders the contents of a Wikipedia infobox in order to generate a single sentence .",0
33148,Problem Formulation,0
33149,The input to our model is a table of records ( see left hand - side ) .,0
33150,"Each record r j has four features including its type ( r j , 1 ; e.g. , LOSS , CITY ) , entity ( r j , 2 ; e.g. , Pacers , Miles Turner ) , value ( r j , 3 ; e.g. , 11 , Indiana ) , and whether a player is on the home - or away - team ( r j , 4 ; see column H/V in ) , represented as {r j , k } 4 k=1 .",0
33151,The output y is a document containing words y = y 1 y | y | where | y | is the document length .,0
33152,"shows the over all architecture of our model which consists of two stages : ( a ) content selection and planning operates on the input records of a data base and produces a content plan specifying which records are to be verbalized in the document and in which order ( see ) and ( b ) text generation produces the output text given the content plan as input ; at each decoding step , the generation model attends over vector representations of the records in the content plan .",0
33153,Let r = {r j } | r| j=1 denote a table of input records and y the output text .,0
33154,"We model p ( y|r ) as the joint probability of text y and content plan z , given input r.",0
33155,"We further decompose p ( y , z |r ) into p ( z | r ) , a content selection and planning In the following we explain how the components p ( z |r ) and p ( y|r , z ) are estimated .",0
33156,Record Encoder,0
33157,"The input to our model is a table of unordered records , each represented as features {r j , k } 4 k=1 .",0
33158,"Following previous work , we embed features into vectors , and then use a multilayer perceptron to obtain a vector representation r j for each record : r j = ReLU(W r [r j , 1 ; r j , 2 ; r j ,3 ; r j , 4 ] + b r ) where [ ; ] indicates vector concatenation , W r ?",0
33159,"R n4n , b r ?",0
33160,"Rn are parameters , and ReLU is the rectifier activation function .",0
33161,Content Selection,0
33162,Gate,0
33163,The context of a record can be useful in determining its importance vis - a-vis other records in the table .,0
33164,"For example , if a player scores many points , it is likely that other meaningfully related records such as field goals , three - pointers , or rebounds will be mentioned in the output summary .",0
33165,"To better capture such dependencies among records , we make use of the content selection gate mechanism as shown in .",0
33166,We first compute the attention scores ?,0
33167,"j , k over the input table and use them to obtain an attentional vector r att j for each record r j :",0
33168,where W a ?,0
33169,"R nn , W g ?",0
33170,"R n 2n are parameter matrices , and k =j ? j , k = 1 . We next apply the content selection gating mechanism tor j , and obtain the new record representation r cs j via : g j = sigmoid r att jr cs j = g jr j where denotes element - wise multiplication , and gate g j ?",0
33171,"[ 0 , 1 ] n controls the amount of information flowing from r j .",0
33172,"In other words , each element in r j is weighed by the corresponding element of the content selection gate g j .",0
33173,Content Planning,0
33174,"In our generation task , the output text is long but follows a canonical structure .",0
33175,"Game summaries typically begin by discussing which team won / lost , following with various statistics involving individual players and their teams ( e.g. , who performed exceptionally well or under-performed ) , and finishing with any upcoming games .",0
33176,We hypothesize that generation would benefit from an explicit plan specifying both what to say and in which order .,0
33177,Our model learns such content plans from training data .,0
33178,"However , notice that RO - TOWIRE ( see ) and most similar data - to - text datasets do not naturally contain content plans .",0
33179,"Fortunately , we can obtain these relatively straightforwardly following an information extraction approach ( which we explain in Section 4 ) .",0
33180,"Suffice it to say that plans are extracted by mapping the text in the summaries onto entities in the input table , their values , and types ( i.e. , relations ) .",0
33181,A plan is a sequence of pointers with each entry pointing to an input record {r j } | r| j=1 .,0
33182,An excerpt of a plan is shown in .,0
33183,The order in the plan corresponds to the sequence in which entities appear in the game summary .,0
33184,Let z = z 1 . . . z | z | denote the content planning sequence .,0
33185,"Each z k points to an input record , i.e. , z k ? {r j } | r| j=1 .",0
33186,"Given the input records , the probability p ( z | r ) is decomposed as :",0
33187,"Since the output tokens of the content planning stage correspond to positions in the input sequence , we make use of Pointer Networks .",0
33188,The latter use attention to point to the tokens of the input sequence rather than creating a weighted representation of source encodings .,0
33189,"As shown in , given {r j } | r| j= 1 , we use an LSTM decoder to generate tokens corresponding to positions in the Celtics input .",0
33190,"The first hidden state of the decoder is initialized by avg ( {r cs j } | r| j=1 ) , i.e. , the average of record vectors .",0
33191,"At decoding step k , let h k be the hidden state of the LSTM .",0
33192,"We model p ( z k = r j | z <k , r ) as the attention over input records :",0
33193,"where the probability is normalized to 1 , and W care parameters .",0
33194,"Once z k points to record r j , we use the corresponding vector r cs j as the input of the next LSTM unit in the decoder .",0
33195,Text Generation,0
33196,The probability of output text y conditioned on content plan z and input table r is modeled as :,0
33197,where y <t = y 1 . . . y t?1 .,0
33198,"We use the encoder - decoder architecture with an attention mechanism to compute p ( y|r , z ) .",0
33199,We first encode the content plan z into {e k } | z | k= 1 using a bidirectional LSTM .,0
33200,"Because the content plan is a sequence of input records , we directly feed the corresponding record vectors {r cs j } | r| j= 1 as input to the LSTM units , which share the record encoder with the first stage .",0
33201,The text decoder is also based on a recurrent neural network with LSTM units .,0
33202,The decoder is initialized with the hidden states of the final step in the encoder .,0
33203,"At decoding step t , the input of the LSTM unit is the embedding of the previously predicted wordy t?1 .",0
33204,Let d t be the hidden state of the t - th LSTM unit .,0
33205,The probability of predicting y t from the output vocabulary is computed via :,0
33206,( 1 ),0
33207,"are parameters , and | V y | is the output vocabulary size .",0
33208,"We further augment the decoder with a copy mechanism , i.e. , the ability to copy words directly from the value portions of records in the content plan ( i.e. , {z k } | z | k=1 ) .",0
33209,We experimented with joint ) and conditional copy methods ) .,0
33210,"Specifically , we introduce a variable u t ? { 0 , 1 } for each time step to indicate whether the predicted token y t is copied ( u t = 1 ) or not ( u t = 0 ) .",0
33211,The probability of generating y t is computed by :,0
33212,where u t is marginalized out .,0
33213,Joint Copy,0
33214,The probability of copying from record values and generating from the vocabulary is globally normalized :,0
33215,where y t ?,0
33216,"z k indicates that y t can be copied from z k , W b is shared as in Equation , and W y , by are shared as in Equation .",0
33217,Conditional Copy,0
33218,"The variable u t is first computed as a switch gate , and then is used to obtain the output probability :",0
33219,where ?,0
33220,"t , k and p gen ( y t |y <t , z , r ) are computed as in Equations ( 1 ) - ( 2 ) , and w u ?",0
33221,"Rn , bu ?",0
33222,R are parameters .,0
33223,"Following and , if y t appears in the content plan during training , we assume that y t is copied ( i.e. , u t = 1 ) .",0
33224,2,0
33225,Training and Inference,0
33226,"Our model is trained to maximize the log - likelihood of the gold 3 content plan given table records rand the gold output text given the content plan and where z and y represent content plan and output text candidates , respectively .",0
33227,"For each stage , we utilize beam search to approximately obtain the best results .",0
33228,Experimental Setup,0
33229,Data,0
33230,"We trained and evaluated our model on , a dataset of basketball game summaries , paired with corresponding box - and line - score tables .",0
33231,"The summaries are professionally written , relatively well structured and long ( 337 words on average ) .",0
33232,"The number of record types is 39 , the average number of records is 628 , the vocabulary size is 11.3 K words and token count is 1.6 M .",0
33233,The dataset is ideally suited for document - scale generation .,0
33234,"We followed the data partitions introduced in Wiseman et al. : we trained on 3,398 summaries , tested on 728 , and used 727 for validation .",0
33235,Content Plan Extraction,0
33236,We extracted content plans from the ROTOWIRE game summaries following an information extraction ( IE ) approach .,0
33237,"Specifically , we used the IE system introduced in Wiseman et al . which identifies candidate entity ( i.e. , player , team , and city ) and value ( i.e. , number or string ) pairs that appear in the text , and then predicts the type ( aka relation ) of each candidate pair .",0
33238,"For instance , in the document in Table 1 , the IE system might identify the pair "" Jeff Teague , 20 "" and then predict that that their relation is "" PTS "" , extracting the record ( Jeff Teague , 20 , PTS ) .",0
33239,"train an IE system on RO - TOWIRE by determining word spans which could represent entities ( i.e. , by matching them against players , teams or cities in the data base ) and numbers .",0
33240,"They then consider each entity - number pair in the same sentence , and if there is a record in the data base with matching entities and values , the pair is assigned the corresponding record type or otherwise given the label "" none "" to indicate unrelated pairs .",0
33241,We adopted their IE system architecture which predicts relations by ensembling 3 convolutional models and 3 bidirectional LSTM models .,0
33242,"We trained this system on the train - 3 Strictly speaking , the content plan is not gold since it was not created by an expert but is the output of a fairly accurate IE system .",0
33243,ing portion of the ROTOWIRE corpus .,0
33244,4,0
33245,"On held - out data it achieved 94 % accuracy , and recalled approximately 80 % of the relations licensed by the records .",0
33246,"Given the output of the IE system , a content plan simply consists of ( entity , value , record type , h/ v ) tuples in their order of appearance in a game summary ( the content plan for the summary in is shown in ) .",0
33247,Player names are pre-processed to indicate the individual 's first name and surname ( see Isaiah and Thomas in ; team records are also pre-processed to indicate the name of team 's city and the team itself ( see Boston and Celtics in ) .,0
33248,Training Configuration,0
33249,We validated model hyperparameters on the development set .,0
33250,We did not tune the dimensions of word embeddings and LSTM hidden layers ; we used the same value of 600 reported in .,0
33251,"We used one - layer pointer networks during content planning , and two - layer LSTMs during text generation .",1
33252,Input feeding was employed for the text decoder .,1
33253,We applied dropout ) at a rate of 0.3 .,1
33254,"Models were trained for 25 epochs with the Adagrad optimizer ; the initial learning rate was 0.15 , learning rate decay was selected from { 0.5 , 0.97 } , and batch size was 5 .",1
33255,"For text decoding , we made use of BPTT ) and set the truncation size to 100 .",1
33256,We set the beam size to 5 during inference .,1
33257,All models are implemented in Open NMT - py .,1
33258,Results,1
33259,Automatic Evaluation,0
33260,We evaluated model output using the metrics defined in .,0
33261,The idea is to employ a fairly accurate IE system ( see the description in Section 4 ) on the gold and automatic summaries and compare whether the identified relations align or diverge .,0
33262,Let ?,0
33263,"be the gold output , and y the system output .",0
33264,Content selection ( CS ) measures how well ( in terms of precision and recall ) the records extracted from y match those found in ?.,0
33265,Relation generation ( RG ) measures the factuality of the generation system as the proportion of records extracted from y which are also found in r ( in terms of precision and number of unique relations ) .,0
33266,Content ordering ( CO ) measures how well the system orders the records it has chosen and is computed as the normalized Damerau - Levenshtein Distance between the sequence of records extracted from y and ?.,0
33267,"In addition to these metrics , we report BLEU , with human - written game summaries as reference .",0
33268,Our results on the development set are summarized in .,0
33269,"We compare our Neural Content Planning model ( NCP for short ) against the two encoder - decoder ( ED ) models presented in with joint copy ( JC ) and conditional copy ( CC ) , respectively .",0
33270,"In addition to our own re-implementation of these models , we include the best scores reported in which were obtained with an encoder - decoder model enhanced with con - also shows results when NCP uses oracle content plans ( OR ) as input .",0
33271,"In addition , we report the performance of a template - based generator which creates a document consisting of eight template sentences : an introductory sentence ( who won / lost ) , six player - specific sentences ( based on the six highest - scoring players in the game ) , and a conclusion sentence .",0
33272,"As can be seen , NCP improves upon vanilla encoderdecoder models ( ED + JC , ED + CC ) , irrespective of the copy mechanism being employed .",1
33273,"In fact , NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements .",1
33274,"Overall , NCP + CC achieves best content selection and content ordering scores in terms of BLEU .",1
33275,"Compared to the best reported system in Wiseman et al. , we achieve an absolute improvement of approximately 12 % in terms of relation generation ; content selection precision also improves by 5 % and recall by 15 % , content ordering increases by 3 % , and BLEU by 1.5 points .",1
33276,The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .,0
33277,"As far as the template - based system is concerned , we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics .",1
33278,"This is not surprising as the template system is provided with domain knowledge which our model does not have , and thus represents an upper-bound on content selection and relation generation .",0
33279,We also measured the degree to which the game summaries generated by our model contain redundant information as the proportion of non-duplicate records extracted from the summary by the IE system .,0
33280,84.5 % of the records in NCP + CC are non-duplicates compared to who obtain 72.9 % showing that our model is less repetitive .,0
33281,We further conducted an ablation study with the conditional copy variant of our model ( NCP + CC ) to establish whether improvements are due to better content selection ( CS ) and / or content planning ( CP ) .,0
33282,"We see in that content selection and planning individually contribute to performance improvements over the baseline ( ED + CC ) , and accuracy further increases when both components are taken into account .",0
33283,In addition we evaluated these components on their own ( independently of text generation ) by comparing the output of the planner ( see p ( z | r ) block in against gold content plans obtained using the IE system ( see row NCP in .,0
33284,"Compared to the full system ( NCP + CC ) , content selection precision and recall are higher ( by 4.5 % and 2 % , respectively ) as well as content ordering ( by 1.8 % ) .",0
33285,"In another study , we used the CS and CO metrics to measure how well the generated text follows the content plan produced by the planner ( instead of arbitrarily adding or removing information ) .",0
33286,We found out that NCP + CC generates game summaries which follow the content plan closely :,0
33287,"CS precision is higher than 85 % , CS recall is higher than 93 % , and CO higher than 84 % .",0
33288,This reinforces our claim that higher accuracies in the content selection and planning phases will result in further improvements in text generation .,0
33289,The test set results in follow a pattern similar to the development set .,0
33290,"NCP achieves higher accuracy in all metrics including relation generation , content selection , content ordering , and BLEU compared to .",0
33291,We provide examples of system output in and the supplementary material .,0
33292,Human - Based Evaluation,0
33293,We conducted two human evaluation experiments using the Amazon Mechanical Turk ( AMT ) crowdsourcing platform .,0
33294,The first study assessed relation generation by examining whether improvements in relation generation at tested by automatic evaluation metrics are indeed corroborated by human judgments .,0
33295,"We compared our best performing model ( NCP + CC ) , with gold reference summaries , a template system and the best model of Wiseman et al ..",0
33296,AMT workers were presented with a spe - The Golden State Warriors defeated the Boston Celtics 104 - 88 at TD Garden on Friday .,0
33297,"The Warriors ( 10 - 2 ) came into this game winners of five of their last six games , but the Warriors ( 6 - 6 ) were able to pull away in the second half .",0
33298,Klay,0
33299,"Thompson led the way for the Warriors with 28 points on 12 - of - 21 shooting , while Kevin Durant added 23 points , 10 rebounds , seven assists and two steals .",0
33300,"Stephen Curry added 16 points and eight assists , while Draymond Green rounded out the box score with 11 points , eight rebounds and eight assists .",0
33301,"For the Celtics , it was Isaiah Thomas who shot just 4 - of - 12 from the field and finished with 18 points .",0
33302,"Avery Bradley added 17 points and 10 rebounds , while the rest of the Celtics combined to score just seven points .",0
33303,Boston will look to get back on track as they play host to the 76ers on Friday ..,0
33304,"Text that accurately reflects a record in the associated box or line score is in blue , erroneous text is in red .",0
33305,"cific NBA game 's box score and line score , and four ( randomly selected ) sentences from the summary .",0
33306,They were asked to identify supporting and contradicting facts mentioned in each sentence .,0
33307,We randomly selected 30 games from the test set .,0
33308,Each sentence was rated by three workers .,0
33309,"The left two columns in contain the average number of supporting and contradicting facts per sentence as determined by the crowdworkers , for each model .",0
33310,"The template - based system has the highest number of supporting facts , even compared to the human gold standard .",0
33311,"TEMPL does not perform any content selection , it includes a large number of facts from the data base and since it does not perform any generation either , it exhibits a few contradictions .",0
33312,"Compared to WS - 2017 and the Gold summaries , NCP + CC displays a larger number of supporting facts .",0
33313,All models are significantly 5 different in the number of supporting facts ( # Supp ) from TEMPL ( using a one - way ANOVA with posthoc Tukey HSD tests ) .,0
33314,NCP+CC is significantly different from WS - 2017 and Gold .,0
33315,"With respect to contradicting facts ( # Cont ) , Gold and TEMPL are not significantly different from each other but are significantly different from the neural systems .",0
33316,"In the second experiment , we assessed the generation quality of our model .",0
33317,We elicited judgments for the same 30 games used in the first study .,0
33318,"For each game , participants were asked to compare a human - written summary , NCP with conditional copy ( NCP + CC ) , best model , and the template system .",0
33319,"Our study used Best - Worst Scaling ( BWS ; Louviere , Flynn , and Marley 2015 ) , a technique shown to be less labor - intensive and providing more reliable results as compared to rating scales .",0
33320,We arranged every 4 - tuple of com -5 All significance differences reported throughout this paper are with a level less than 0.05 . :,0
33321,"Average number of supporting ( # Support ) and contradicting ( # Contra ) facts in game summaries and bestworst scaling evaluation ( higher is better ) for grammaticality ( Gram ) , Coherence ( Cohere ) , and Conciseness ( Concise ) .",0
33322,peting summaries into 6 pairs .,0
33323,"Every pair was shown to three crowdworkers , who were asked to choose which summary was best and which was worst according to three criteria : Grammaticality ( is the summary fluent and grammatical ? ) ,",0
33324,Coherence ( is the summary easy to read ?,0
33325,"does it follow a natural ordering of facts ? ) , and Conciseness ( does the summary avoid redundant information and repetitions ? ) .",0
33326,The score of a system for each criterion is computed as the difference between the percentage of times the system was selected as the best and the percentage of times it was selected as the worst ) .,0
33327,The scores range from ? 100 ( absolutely worst ) to + 100 ( absolutely best ) .,0
33328,The results of the second study are summarized in .,0
33329,Gold summaries were perceived as significantly better compared to the automatic systems across all criteria ( again using a one - way ANOVA with post - hoc Tukey HSD tests ) .,0
33330,NCP+ CC was perceived as significantly more grammatical than WS - 2017 but not compared to TEMPL which does not suffer from fluency errors since it does not perform any generation .,0
33331,NCP+ CC was perceived as significantly more coherent than TEMPL and WS - 2017 .,0
33332,"The template fairs poorly on coherence , its output is stilted and exhibits no variability ( see top block in ) .",0
33333,"With regard to conciseness , the neural systems are significantly worse than TEMPL , while NCP + CC is significantly better than WS - 2017 .",0
33334,By design the template can not repeat information since there is no redundancy in the sentences chosen to verbalize the summary .,0
33335,"Taken together , our results show that content planning improves data - to - text generation across metrics and systems .",0
33336,"We find that NCP + CC over all performs best , however there is a significant gap between automatically generated summaries and human - authored ones .",0
33337,Conclusions,0
33338,We presented a data - to - text generation model which is enhanced with content selection and planning modules .,0
33339,"Experimental results ( based on automatic metrics and judgment elicitation studies ) demonstrate that generation quality improves both in terms of the number of relevant facts contained in the output text , and the order according to which these are presented .",0
33340,"Positive side - effects of content planning are additional improvements in the grammaticality , and conciseness of the generated text .",0
33341,"In the future , we would like to learn more detail - oriented plans involving inference over multiple facts and entities .",0
33342,We would also like to verify our approach across domains and languages .,0
33343,Comparison with the Results in Webpage,0
33344,There was a bug in the dataset creation of which they identified and corrected .,0
33345,They also posted updated scores on their webpage .,0
33346,We have used this corrected dataset in our experiments .,0
33347,We then discovered a bug in their code which computes the automatic metrics .,0
33348,The scores reported in this paper are using the corrected automatic metrics .,0
33349,"To make the scores on our paper comparable to the numbers published on the webpage of , we recompute here our scores with older IE metrics ( without the bug fix ) in and ( test set ) .",0
33350,"shows two sample documents generated using the template system , and our neural content planning model with conditional copy ( NCP + CC ) .",0
33351,The text is highlighted in blue if it agrees with respective box / line scores and red if the text contradicts box / line scores .,0
33352,We also use the orange color to highlight repetitions .,0
33353,Qualitative Examples,0
33354,The template documents are gold standard in relation generation accuracy and they appear all in blue .,0
33355,The documents of show instances of contradictions and tend to be verbose containing duplicate text too .,0
33356,"In contrast , our neural content planning model generates more factual text with fewer contradictions to box / line scores and less duplicate information .",0
33357,"3 PT , 6 - 6 FT ) to go with 3 rebounds .",0
33358,"Nikola Jokic scored 17 points ( 6 - 10 FG , 0 - 0 3 PT , 5 - 7 FT ) to go with 11 rebounds .",0
33359,"Markieff Morris scored 15 points ( 5 - 12 FG , 0 - 0 3 PT , 5 - 5 FT ) to go with 3 rebounds .",0
33360,John,0
33361,"Wall scored 15 points ( 5 - 14 FG , 0 - 4 3 PT , 5 - 6 FT ) to go with 7 rebounds .",0
33362,"Danilo Gallinari scored 14 points ( 3 - 11 FG , 1 - 8 3 PT , 7 - 9 FT ) to go with 4 rebounds .",0
33363,"Jusuf Nurkic scored 13 points ( 6 - 6 FG , 0 - 0 3 PT , 1 - 2 FT ) to go with 7 rebounds .",0
33364,"The Washington Wizards ' next game will beat home against the Dallas Mavericks , while the Denver Nuggets will travel to play the Bulls .",0
33365,WS - 2017,0
33366,"The Golden State Warriors defeated the Boston Celtics , 104 - 88 , at TD Garden on Wednesday .",0
33367,"The Warriors ( 10 - 2 ) checked in to Saturday 's contest with only two road wins in their last 11 games , but they were able to come away with a win against the Celtics ( 6 - 6 ) on Friday .",0
33368,"The Warriors ( 10 - 2 ) were able to pull away in the second half , outscoring the Celtics ( 6 - 6 ) by a 31 - 9 margin over the final 12 minutes .",0
33369,"However , Golden State was able to pull away in the second half , outscoring the Celtics by a 31 - 9 margin over the final 12 minutes .",0
33370,"The Warriors were led by Kevin Durant 's 23 points , which he supplemented with seven rebounds , seven assists , two steals and a block .",0
33371,"Stephen Curry was next with 16 points , eight assists , three rebounds and four steals .",0
33372,"Klay Thompson was next with a 28 - point , 10 - rebound double - double that also included three assists , two steals and a block .",0
33373,"Draymond Green was next with 11 points , eight rebounds , eight assists and two blocks .",0
33374,"Draymond Green was next with 11 points , eight rebounds , eight assists and two blocks .",0
33375,"Draymond Green supplied 11 points , eight rebounds , eight assists , two blocks and a steal .",0
33376,David,0
33377,"West paced the reserves with 4 points , two rebounds , a block and a block .",0
33378,"The Celtics were paced by Thomas ' 18 points , which he supplemented with four assists , two rebounds and four steals .",0
33379,"Avery Bradley posted a 17 - point , 10 - rebound double - double that also included two assists , two steals and a block .",0
33380,"Avery Bradley posted a 17 - point , 10 - rebound double - double that also included two assists , two steals and a block .",0
33381,Kelly,0
33382,"Olynyk led the second unit with 11 points , three rebounds , two assists and a pair of steals .",0
33383,"The Warriors head back home to face off with the Detroit Pistons on Friday night , while the Celtics remain home and await the Toronto Raptors for a Wednesday night showdown .",0
33384,NCP+CC,0
33385,The Golden State Warriors defeated the Boston Celtics 104 - 88 at TD Garden on Friday .,0
33386,"The Warriors ( 10 - 2 ) came into this game winners of five of their last six games , but the Warriors ( 6 - 6 ) were able to pull away in the second half .",0
33387,Klay,0
33388,"Thompson led the way for the Warriors with 28 points on 12 - of - 21 shooting , while Kevin Durant added 23 points , 10 rebounds , seven assists and two steals .",0
33389,"Stephen Curry added 16 points and eight assists , while Draymond Green rounded out the box score with 11 points , eight rebounds and eight assists .",0
33390,"For the Celtics , it was Isaiah Thomas who shot just 4 - of - 12 from the field and finished with 18 points .",0
33391,"Avery Bradley added 17 points and 10 rebounds , while the rest of the Celtics combined to score just seven points .",0
33392,Boston will look to get back on track as they play host to the 76ers on Friday . :,0
33393,"Example documents from the template - based system , WS - 2017 , the best system of Wiseman et al. , and our Neural Content Planning model with conditional copy ( NCP + CC ) .",0
33394,"Text that accurately reflects a record in the associated box or line score is recorded in blue , erroneous text is marked in red , duplicate text is marked in orange .",0
33395,title,0
33396,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,1
33397,abstract,0
33398,Natural language generation lies at the core of generative dialogue systems and conversational agents .,1
33399,"We describe an ensemble neural language generator , and present several novel methods for data representation and augmentation that yield improved results in our model .",0
33400,"We test the model on three datasets in the restaurant , TV and laptop domains , and report both objective and subjective evaluations of our best model .",0
33401,"Using a range of automatic metrics , as well as human evaluators , we show that our approach achieves better results than state - of - the - art models on the same datasets .",0
33402,Introduction,0
33403,"There has recently been a substantial amount of research in natural language processing ( NLP ) in the context of personal assistants , such as Cortana or Alexa .",0
33404,"The capabilities of these conversational agents are still fairly limited and lacking in various aspects , one of the most challenging of which is the ability to produce utterances with humanlike coherence and naturalness for many different kinds of content .",0
33405,This is the responsibility of the natural language generation ( NLG ) component .,0
33406,Our work focuses on language generators whose inputs are structured meaning representations ( MRs ) .,0
33407,An MR describes a single dialogue act with a list of key concepts which need to be conveyed to the human user during the dialogue .,0
33408,"Each piece of information is represented by a slotvalue pair , where the slot identifies the type of information and the value is the corresponding content .",0
33409,"Dialogue act ( DA ) types vary depending on the dialogue manager , ranging from simple ones , such as a goodbye DA with no slots at all , to complex ones , such as an inform DA containing multiple slots with various types of values ( see example in Utt .",0
33410,Located near,0
33411,"The Bakers , kid - friendly restaurant , The Golden Curry , offers Japanese cuisine with a moderate price range .",0
33412,A natural language generator must produce a syntactically and semantically correct utterance from a given MR .,0
33413,"The utterance should express all the information contained in the MR , in a natural and conversational way .",0
33414,"In traditional language generator architectures , the assembling of an utterance from an MR is performed in two stages : sentence planning , which enforces semantic correctness and determines the structure of the utterance , and surface realization , which enforces syntactic correctness and produces the final utterance form .",0
33415,Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method .,0
33416,"The handcrafted aspects , however , lead to decreased portability and potentially limit the variability of the outputs .",0
33417,New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs .,0
33418,"The alignment provides valuable information during training , but the semantic annotation is costly .",0
33419,"The most recent methods do not require aligned data and use an end - to - end approach to training , performing sentence planning and surface realization simultaneously .",0
33420,"The most successful systems trained on unaligned data use recurrent neural networks ( RNNs ) paired with an encoder - decoder system design , but also other concepts , such as imitation learning .",0
33421,"These NLG models , however , typically require greater amount of data for training due to the lack of semantic alignment , and they still have problems producing syntactically and semantically correct output , as well as being limited in naturalness .",0
33422,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .",1
33423,"We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .",1
33424,We use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .,0
33425,"On the largest of the datasets , the E2E dataset with nearly 50 K samples , we also demonstrate that our model significantly outperforms the baseline E2E NLG Challenge 1 system in human evaluation .",0
33426,"Finally , after augmenting our model with stylistic data selection , subjective evaluations reveal that it can still produce over all better results despite a significantly reduced training set .",0
33427,Related Work,0
33428,NLG is closely related to machine translation and has similarly benefited from recent rapid development of deep learning methods .,0
33429,State - of - the - art NLG systems build thus on deep neural sequenceto - sequence models with an encoder - decoder architecture equipped with an attention mechanism .,0
33430,"They typically also rely on slot delexicalization , which allows the model to better generalize to unseen inputs , as exemplified by TGen .",0
33431,"However , point out that there are frequent scenarios where delexicalization behaves inadequately ( see Section 5.1 for more details ) , and show that a character - level approach to NLG may avoid the need for delexicalization , at the potential cost of making more semantic omission errors .",0
33432,The end - to - end approach to NLG typically requires a mechanism for aligning slots on the output utterances : this allows the model to generate Our work builds upon the successful attentional encoder - decoder framework for sequenceto - sequence learning and expands it through ensembling .,0
33433,"We explore the feasibility of a domainindependent slot aligner that could be applied to any dataset , regardless of its size , and beyond the reranking task .",0
33434,"We also tackle some challenges caused by delexicalization in order to improve the quality of surface realizations , while retaining the ability of the neural model to generalize .",0
33435,Datasets,0
33436,We evaluated the models on three datasets from different domains .,0
33437,The primary one is the recently released E2E restaurant dataset with 48 K samples .,0
33438,"For benchmarking we use the TV dataset and the Laptop dataset with 7 K and 13K samples , respectively .",0
33439,"summarizes the proportions of the training , validation , and test sets for each dataset .",0
33440,E2E,0
33441,Dataset,0
33442,The E2E dataset is by far the largest one available for task - oriented language generation in the restaurant domain .,0
33443,The human references were Note that the number of MRs in the E2E dataset was cutoff at 10 K for the sake of visibility of the small differences between other column pairs .,0
33444,"collected using pictures as the source of information , which was shown to inspire more informative and natural utterances .",0
33445,"With nearly 50 K samples , it offers almost 10 times more data than the San Francisco restaurant dataset introduced in Wen et al. ( 2015 b ) , which has frequently been used for benchmarks .",0
33446,"The reference utterances in the E2E dataset exhibit superior lexical richness and syntactic variation , including more complex discourse phenomena .",0
33447,It aims to provide higher - quality training data for end - to - end NLG systems to learn to produce more naturally sounding utterances .,0
33448,The dataset was released as apart of the E2E NLG Challenge .,0
33449,"Although the E2E dataset contains a large number of samples , each MR is associated on average with 8.65 different reference utterances , effectively offering less than 5 K unique MRs in the training set ( .",0
33450,"Explicitly providing the model with multiple ground truths , it offers multiple alternative utterance structures the model can learn to apply for the same type of MR .",0
33451,"The delexicalization , as detailed later in Section 5.1 , improves the ability of the model to share the concepts across different MRs .",0
33452,"The dataset contains only 8 different slot types , which are fairly equally distributed .",0
33453,"The number of slots in each MR ranges between 3 and 8 , but the majority of MRs consist of 5 or 6 slots .",0
33454,"Even though most of the MRs contain many slots , the majority of the corresponding human utterances , however , consist of one or two sentences only ( Table 3 ) , suggesting a reasonably high level of sentence complexity in the references .",0
33455,TV and Laptop Datasets,0
33456,The reference utterances in the TV and the Laptop datasets were collected using Amazon Mechani - :,0
33457,"Average number of sentences in the reference utterance for a given number of slots in the corresponding MR , along with the proportion of MRs with specific slot counts . cal Turk ( AMT ) , one utterance per MR .",0
33458,"These two datasets are similar in structure , both using the same 14 DA types .",0
33459,"The Laptop dataset , however , is almost twice as large and contains 25 % more slot types .",0
33460,"Although both of these datasets contain more than a dozen different DA types , the vast majority ( 68 % and 80 % respectively ) of the MRs describe a DA of either type inform or recommend , which in most cases have very similarly structured realizations , comparable to those in the E2E dataset .",0
33461,"DA s such as suggest , ? request , or goodbye are represented by less than a dozen samples , but are significantly easier to learn to generate an utterance from because the corresponding MRs contain three slots at the most .",0
33462,"input sequence to pay attention to , given the output generated so far .",0
33463,"In this attentional encoderdecoder architecture , the probability of the output at each time step t of the decoder depends on a distinct context vector qt in the following way :",0
33464,"wherein the place of function g we use the softmax function over the size of the vocabulary , and st is a hidden state of the decoder RNN at time step t , calculated as :",0
33465,"The context vector qt is obtained as a weighted sum of all the hidden states h 1 , . . . , h L of the encoder :",0
33466,where ?,0
33467,"t , i corresponds to the attention score the t- th word in the target sentence assigns to the i - th item in the input MR .",0
33468,We compute the attention score ?,0
33469,"t , i using a multi - layer perceptron ( MLP ) jointly trained with the entire system .",0
33470,"The encoder 's and decoder 's hidden states at time i and t , respectively , are concatenated and used as the input to the MLP , namely :",0
33471,"where W and ware the weight matrix and the vector of the first and the second layer of the MLP , respectively .",0
33472,The learned weights indicate the level of influence of the individual words in the input sequence on the prediction of the word at time step t of the decoder .,0
33473,The model thus learns a soft alignment between the source and the target sequence .,0
33474,Ensembling,0
33475,"In order to enhance the quality of the predicted utterances , we create three neural models with different encoders .",0
33476,"Two of the models use a bidirectional LSTM encoder , whereas the third model has a CNN ( Le - Cun et al. , 1998 ) encoder .",0
33477,We train these models individually for a different number of epochs and then combine their predictions .,0
33478,"Initially , we attempted to combine the predictions of the models by averaging the logprobability at each time step and then selecting the word with the maximum log-probability .",0
33479,"We noticed that the quality , as well as the BLEU score of our utterances , decreased significantly .",0
33480,"We believe that this is due to the fact that different models learn different sentence structures and , hence , combining predictions at the probability level results in incoherent utterances .",0
33481,"Therefore , instead of combining the models at the log - probability level , we accumulate the top 10 predicted utterances from each model type using beam search and allow the reranker ( see Section 4.4 ) to rank all candidate utterances taking the proportion of slots they successfully realized into consideration .",0
33482,"Finally , our system predicts the utterance that received the highest score .",0
33483,Slot Alignment,0
33484,"Our training data is inherently unaligned , meaning our model is not certain which sentence in a multisentence utterance contains a given slot , which limits the model 's robustness .",0
33485,"To accommodate this , we create a heuristic - based slot aligner which automatically preprocesses the data .",0
33486,It s primary goal is to align chunks of text from the reference utterances with an expected value from the MR .,0
33487,Applications of our slot aligner are described in subsequent sections and in .,0
33488,"In our task , we have a finite set of slot mentions which must be detected in the corresponding utterance .",0
33489,"Moreover , from our training data we can see that most slots are realized by inserting a specific set of phrases into an utterance .",0
33490,"Using this insight , we construct a gazetteer , which primarily searches for overlapping content between the MR and each sentence in an utterance , by associating all possible slot realizations with their appropriate slot type .",0
33491,"We additionally augment the gazetteer using a small set of handcrafted rules which capture cases not easily encapsulated by the above process , for example , associating the priceRange slot with a chunk of text using currency symbols or relevant lexemes , such as "" cheap "" or "" highend "" .",0
33492,"While handcrafted , these rules are transferable across domains , as they target the slots , not the domains , and mostly serve to counteract the noise in the E2E dataset .",0
33493,"Finally , we use Word - Net to further augment the size of our gazetteer by accounting for synonyms and other semantic relationships , such as associating "" pasta "" with the food [ Italian ] slot .",0
33494,Reranker,0
33495,"As discussed in Section 4.2 , our model uses beam search to produce a pool of the most likely utterances for a given MR .",0
33496,"While these results have a probability score provided by the model , we found that relying entirely on this score often results in the system picking a candidate which is objectively worse than a lower scoring utterance ( i.e. one missing more slots and / or realizing slots incorrectly ) .",0
33497,We therefore augment that score by multiplying it by the following score which takes the slot alignment into consideration :,0
33498,"where N is the number of all slots in the given MR , and N u and No represent the number of unaligned slots ( those not observed by our slot aligner ) and over- generated slots ( those which have been realized but were not present in the original MR ) , respectively .",0
33499,Data Preprocessing,0
33500,Delexicalization,0
33501,We enhance the ability of our model to generalize the learned concepts to unseen MRs by delexicalizing the training data .,0
33502,"Moreover , it reduces the amount of data required to train the model .",0
33503,"We identify the categorical slots whose values always propagate verbatim to the utterance , and replace the corresponding values in the utterance with placeholder tokens .",0
33504,The placeholders are eventually replaced in the output utterance in postprocessing by copying the values from the input MR .,0
33505,"Examples of such slots would be name or near in the E2E dataset , and screensize or processor in the TV and the Laptop dataset .",0
33506,Previous work identifies categorical slots as good delexicalization candidates that improve the performance of the model .,0
33507,"However , we chose not to delexicalize those categorical slots whose values can be expressed in alternative ways , such as "" less than $ 20 "" and "" cheap "" , or "" on the riverside "" and "" by the river "" .",0
33508,"Excluding these from delexicalization may lead to an increased number of incorrect realizations , but it encourages diversity of the model 's outputs by giving it a freedom to choose among alternative ways of expressing a slot - value in different contexts .",0
33509,"This , however , assumes that the training set contains a sufficient number of samples displaying this type of alternation so that the model can learn that certain phrases are synonymous .",0
33510,"With its multiple human references for each MR , the E2E dataset has this property .",0
33511,"As point out , delexicalization affects the sentence planning and the lexical choice around the delexicalized slot value .",0
33512,"For example , the realization of the slot food [ Italian ] in the phrase "" serves Italian food "" is valid , while the realization of food [ fast food ] in "" serves fast food food "" is clearly undesired .",0
33513,"Similarly , a naive delexicalization can result in "" a Italian restaurant "" , whereas the article should be "" an "" .",0
33514,Another problem with articles is singular versus plural nouns in the slot value .,0
33515,"For example , the slot accessories in the TV dataset , can take on values such as "" remote control "" , as well as "" 3D glasses "" , where only the former requires an article before the value .",0
33516,We tackle this issue by defining different placeholder tokens for values requiring different treatment in the realization .,0
33517,"For instance , the value "" Italian "" of the food slot is replaced by slot vow cuisine food , indicating that the value starts with a vowel and represents a cuisine , while "" fast food "" is replaced by slot con food , indicating that the value starts with a consonant and can not be used as a term for cuisine .",0
33518,"The model thus learns to generate "" a "" before slot con food and "" an "" before slot vow cuisine food when appropriate , as well as to avoid generating the word "" food "" after food - slot placeholders that do not contain the word "" cuisine "" .",0
33519,All these rules are general and can automatically be applied across different slots and domains .,0
33520,Data Expansion,0
33521,Slot Permutation,0
33522,"In our initial experiments , we tried expanding the training set by permuting the slot ordering in the MRs as suggested in .",0
33523,"From different slot orderings of every MR we sampled five random permutations ( in addition to the original MR ) , and created new pseudo - samples with the same reference utterance .",0
33524,The training set thus increased six times in size .,0
33525,"Using such an augmented training set might add to the model 's robustness , nevertheless it did not prove to be helpful with the E2E dataset .",0
33526,"In this dataset , we observed the slot order to be fixed across all the MRs , both in the training and the test set .",0
33527,"As a result , for the majority of the time , the model was training on MRs with slot orders it would never encounter in the test set , which ultimately led to a decreased performance in prediction on the test set .",0
33528,Utterance / MR Splitting,0
33529,"Taking a more utterance - oriented approach , we augment the training set with single - sentence utterances paired with their corresponding MRs .",0
33530,These new pseudo - samples are generated by splitting the existing reference utterances into single sentences and using the slot aligner introduced in Section 4.3 to identify the slots that correspond to each sentence .,0
33531,"The MRs of the new samples are created as the corresponding subsets of slots and , whenever the sentence contains the name ( of the restaurant / TV / etc. ) or a pronoun referring to it ( such as "" it "" or "" its "" ) , the name slot is included too .",0
33532,"Finally , a new position slot is appended to every new MR , indicating whether it represents the first sentence or a subsequent sentence in the original utterance .",0
33533,An example of this splitting technique can be seen in .,0
33534,The training set almost doubled in size through this process .,0
33535,"Since the slot aligner works heuristically , not all utterances are successfully aligned with the MR .",0
33536,"The vast majority of such cases , however , is caused by reference utterances in the datasets having incorrect or entirely missing slot mentions .",0
33537,"There is a noticeable proportion of those , so we leave them in the training set with the unaligned slots removed from the MR so as to avoid confusing the model when learning from such samples .",0
33538,Sentence Planning via,0
33539,Data Selection,0
33540,The quality of the training data inherently imposes an upper bound on the quality of the predictions of our model .,0
33541,"Therefore , in order to bring our model to produce more sophisticated utterances , we experimented with filtering the training data to contain only the most natural sounding and structurally complex utterances for each MR .",0
33542,"For instance , we prefer having an elegant , singlesentence utterance with an apposition as the reference for an MR , rather than an utterance composed of three simple sentences , two of which begin with "" it "" ( see the examples in ) .",0
33543,"We assess the complexity and naturalness of each utterance by the use of discourse phenomena , such as contrastive cues , subordinate clauses , or aggregation .",0
33544,We identify these in the utterance 's parse - tree produced by the Stanford CoreNLP toolkit by defining a set of rules for extracting the discourse phenomena .,0
33545,"Furthermore , we consider the number of sentences used to convey all the information in the corresponding MR , as longer sentences tend to exhibit more advanced discourse phenomena .",0
33546,"Penalizing utterances for too many sentences contributes to reducing the proportion of generic reference utter - ances , such as the "" simple "" example in the above table , in the filtered training set .",0
33547,Evaluation,0
33548,Researchers in NLG have generally used both automatic and human evaluation .,0
33549,"Our results report the standard automatic evaluation metrics : BLEU , NIST , METEOR , and ROUGE - L .",0
33550,"For the E2E dataset experiments , we additionally report the results of the human evaluation carried out on the CrowdFlower platform as apart of the E2E NLG Challenge .",0
33551,Experimental Setup,0
33552,We built our ensemble model using the seq2seq framework for TensorFlow .,1
33553,"Our individual LSTM models use a bidirectional LSTM encoder with 512 cells per layer , and the CNN models use a pooling encoder as in .",1
33554,The decoder in all models was a 4 - layer RNN decoder with 512 LSTM cells per layer and with attention .,1
33555,The hyperparameters were determined empirically .,0
33556,"After experimenting with different beam search parameters , we settled on the beam width of 10 .",0
33557,"Moreover , we employed the length normalization of the beams as defined in , in order to encourage the decoder to favor longer sequences .",0
33558,"The length penalty providing the best results on the E2E dataset was 0.6 , whereas for the TV and Laptop datasets it was 0.9 and 1.0 , respectively .",0
33559,Experiments on the E2E Dataset,1
33560,We start by evaluating our system on the E2E dataset .,0
33561,"Since the reference utterances in the test set were kept secret for the E2E NLG Challenge , we carried out the metric evaluation using the validation set .",0
33562,This was necessary to narrow down the models that perform well compared to the baseline .,0
33563,The final model selection was done based on a human evaluation of the models ' outputs on the test set .,0
33564,Automatic Metric Evaluation,1
33565,"In the first experiment , we assess what effect the augmenting of the training set via utterance splitting has on the performance of different models .",0
33566,The results in show that both the LSTM and the CNN models clearly benefit from additional pseudo - samples in the training set .,1
33567,This can likely be attributed to the model having access to more granular information about which parts of the utterance correspond to which slots in the MR .,0
33568,"This may assist the model in sentence planning and building a stronger association between parts of the utterance and certain slots , such as that "" it "" is a substitute for the name .",0
33569,Testing our ensembling approach reveals that reranking predictions pooled from different models produces an ensemble model that is over all more robust than the individual submodels .,0
33570,"The submodels fail to perform well in all four metrics at once , whereas the ensembling creates a new model that is more consistent across the different metric types .",0
33571,3,0
33572,"While the ensemble model decreases the proportion of incorrectly realized slots compared to its individual submodels on the validation set , on the test set it only outperforms two of the submodels in this aspect ( Table 8 ) .",0
33573,"Analyzing the outputs , we also observed that the CNN model surpassed the two LSTM models in the ability to realize the "" fast food "" and "" pub "" values reliably , both of which were hardly present in the validation set but very frequent in the test set .",0
33574,"On the official E2E test set , our ensemble model performs comparably to the baseline model , TGen , in terms of automatic metrics ) .",1
33575,Human Evaluation,0
33576,It is known that automatic metrics function only as a general and vague indication of the quality of an utterance in a dialogue .,0
33577,Systems which score similarly according to these metrics could produce utterances thatare significantly different because automatic metrics fail to capture many of the characteristics of natural sounding utterances .,0
33578,"Therefore , to better assess the structural complexity of the predictions of our model , we present the results of a human evaluation of the models ' outputs in terms of both naturalness and quality , carried out by the E2E NLG Challenge organizers .",0
33579,"Quality examines the grammatical correctness and adequacy of an utterance given an MR , whereas naturalness assesses whether a predicted utterance could have been produced by a native speaker , irrespective of the MR .",0
33580,"To obtain these scores , crowd workers ranked the outputs of 5 randomly selected systems from worst to best .",0
33581,The final scores were produced using the TrueSkill algorithm through pairwise comparisons of the human evaluation scores among the 20 competing systems .,0
33582,"Our system , trained on the E2E dataset without stylistic selection ( Section 5.3 ) , achieved the highest quality score in the E2E NLG Challenge , and was ranked second in naturalness .",0
33583,"The system 's performance in quality ( the primary metric ) was significantly better than the competition according to the TrueSkill evaluation , which used bootstrap resampling with a p-level of p ? 0.05 .",0
33584,Comparing these results with the scores achieved by the baseline model in quality and naturalness ( 5th and 6th,0
33585,Ex. # 1,0
33586,"The Cricketers is a cheap Chinese restaurant near All Bar One in the riverside area , but it has an average customer rating and is not family friendly .",0
33587,Ex. # 2,0
33588,"If you are looking for a coffee shop near The Rice Boat , try Giraffe .",0
33589,"place , respectively ) reinforces our belief that models that perform similarly on the automatic metrics ) can exhibit vast differences in the structural complexity of their generated utterances .",0
33590,Experiments with Data Selection,0
33591,"After filtering the E2E training set as described in Section 5.3 , the new training set consisted of approximately 20 K pairs of MRs and utterances .",0
33592,"Interestingly , despite this drastic reduction in training samples , the model was able to learn more complex utterances that contained the natural variations of the human language .",0
33593,"The generated utterances exhibited discourse phenomena such as contrastive cues ( see Example # 1 in , as well as a more conversational style .",0
33594,"Nevertheless , the model also failed to realize slots more frequently .",0
33595,"In order to observe the effect of stylistic data selection , we conducted a human evaluation where we assessed the utterances based on error rate and naturalness .",0
33596,The error rate is calculated as the percentage of slots the model failed to realize divided by the total number of slots present among all samples .,0
33597,"The annotators ranked samples of utterance triples - corresponding to three different ensemble models - by naturalness from 1 to 3 ( 3 being the most natural , with possible ties ) .",0
33598,"The conservative model combines three submodels all trained on the full training set , the progressive one combines submodels solely trained on the filtered dataset , and finally , the hybrid is an ensemble of three models only one of which is trained on the full training set , so as to serve as a fallback .",0
33599,"The impact of the reduction of the number of training samples becomes evident by looking at the score of the progressive model , where this model trained solely on the reduced dataset had the highest error rate .",0
33600,"We observe , however , that a hybrid ensemble model manages to perform the best in terms of the error rate , as well as the naturalness .",0
33601,These results suggest that filtering the dataset through careful data selection can help to achieve better and more natural sounding utterances .,0
33602,"It significantly improves the model 's ability to produce more elegant utterances beyond the "" [ name ] is ...",0
33603,"It is / has ... "" format , which is only too common in neural language generators in this domain .",0
33604,Experiments on TV and Laptop Datasets,1
33605,"In order to provide a better frame of reference for the performance of our proposed model , we utilize the RNNLG benchmark toolkit 5 to evaluate our system on two additional , widely used datasets in NLG , and compare our results with those of a state - of - the - art model , SCLSTM .",0
33606,"As shows , our ensemble model performs competitively with the baseline on the TV dataset , and it outperforms it on the Laptop dataset by a wide margin .",1
33607,We believe the higher error rate of our model can be explained by the significantly less aggressive slot delexicalization than the one used in SCLSTM .,0
33608,"That , however , gives our model a greater lexical freedom and , with it , the ability to produce more natural utterances .",0
33609,The model trained on the Laptop dataset is also a prime example of how an ensemble model is capable of extracting the best learned concepts from each individual submodel .,0
33610,"By combining their knowledge and compensating thus for each other 's weaknesses , the ensemble model can achieve a lower error rate , as well as a better over all quality , than any of the submodels individually .",0
33611,Conclusion and Future Work,0
33612,In this paper we presented our ensemble attentional encoder - decoder model for generating natural utterances from MRs .,0
33613,"Moreover , we presented novel methods of representing the MRs to improve performance .",0
33614,Our results indicate that the proposed utterance splitting applied to the training set greatly improves the neural model 's accuracy and ability to generalize .,0
33615,"The ensembling method paired with the reranking based on slot alignment also contributed to the increase in quality of the generated utterances , while minimizing the number of slots thatare not realized during the generation .",0
33616,"This also enables the use of a less aggressive delexicalization , which in turn stimulates diversity in the produced utterances .",0
33617,"We showed that automatic slot alignment can be utilized for expanding the training data , as well as for utterance reranking .",0
33618,"Our alignment currently relies in part on empirically observed heuristics , and a more robust aligner would allow for more flexible expansion into new domains .",0
33619,"Since the stylistic data selection noticeably improved the diversity of our system 's outputs , we believe this is a method with future potential , which we intend to further explore .",0
33620,"Finally , it is clear that current automatic evaluation metrics in NLG are only sufficient for providing a vague idea as to the system 's performance ; we postulate that leveraging the reference data to train a classifier will result in a more conclusive automatic evaluation metric .",0
33621,title,0
33622,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,1
33623,abstract,0
33624,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",1
33625,"The most widely used sequence - to - sequence neural methods are word - based : as such , they need a pre-processing step called delexicalization ( conversely , relexicalization ) to deal with uncommon or unknown words .",0
33626,"These forms of processing , however , give rise to models that depend on the vocabulary used and are not completely neural .",0
33627,"In this work , we present an end - to - end sequence - to - sequence model with attention mechanism which reads and generates at a character level , no longer requiring delexicalization , tokenization , nor even lowercasing .",0
33628,"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training .",1
33629,"These skills are obtained thanks to two major features : ( i ) the possibility to alternate between the standard generation mechanism and a copy one , which allows to directly copy input facts to produce outputs , and ( ii ) the use of an original training pipeline that further improves the quality of the generated texts .",0
33630,"We also introduce a new dataset called E2E + , designed to highlight the copying capabilities of character - based models , that is a modified version of the well - known E2E dataset used in the E2E Challenge .",0
33631,"We tested our model according to five broadly accepted metrics ( including the widely used bleu ) , showing that it yields competitive performance with respect to both character - based and word - based approaches .",0
33632,Introduction,0
33633,The ability of recurrent neural networks ( RNNs ) to model sequential data stimulated interest towards deep learning models which face data - to - text generation .,0
33634,An interesting application is the generation of descriptions for factual tables that consist of a set of field - value pairs ; an example is shown in .,0
33635,We present in this paper an effective end - to - end approach to this task .,0
33636,"Sequence - to - sequence frameworks have proved to be very effective in natural language generation ( NLG ) tasks , as well as in machine translation and in language modeling .",0
33637,"Usually , data are represented word - by - word both in input and output sequences ; anyways , such schemes ca n't be effective without a special , non-neural delexicalization phase that handles unknown words , such as proper names or foreign words ( see ) .",0
33638,"The delexicalization step has the benefit of reducing the dictionary size and , consequently , the data sparsity , but it is affected by various shortcomings .",0
33639,"In particular , according to - it needs some reliable mechanism for entity identification , i.e. the recognition of named entities inside text ; - it requires a subsequent "" re-lexicalization "" phase , where the original named entities take back placeholders ' place ; - it can not account for lexical or morphological variations due to the specific entity , such as gender and number agreements , that ca n't be achieved without a clear context awareness .",0
33640,"Recently , some strategies have been proposed to solve these issues : and face this problem using a special neural copying mechanism that is quite effective in alleviating the out - of - vocabulary words problem , while tries to extend neural networks with a post-processing phase that copies words as indicated by the model 's output sequence .",0
33641,"Some character - level aspects appear as a solution of the issue as well , either as a fallback for rare words , or as subword units .",0
33642,"A significantly different approach consists in employing characters instead of words , for input slot - value pairs tokenization as well as for the generation of the final utterances , as done for instance in .",0
33643,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .",1
33644,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .",1
33645,"As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .",1
33646,"Because of this , it opens up the possibility , not viable when using words , to adapt already trained networks to deal with different datasets .",0
33647,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .",1
33648,"In section 2 , after resuming the main ideas on encoder - decoder methods with attention , we detail our model : section 2.2 is devoted to explaining the copy mechanism while in section 2.3 our peculiar training procedure is presented .",0
33649,"Section 3 includes the datasets descriptions , some implementation specifications , the experimental framework and the analysis and evaluation of the achieved results .",0
33650,"Finally , in section 4 some conclusions are drawn , outlining future work .",0
33651,Model Description,0
33652,Summary on Encoder - decoder,0
33653,Architectures with Attention,0
33654,"The sequence - to - sequence encoder - decoder architecture with attention is represented in figure 1 : on the left , the encoder , a bi-directional RNN , outputs one annotation h j for each input token x j .",0
33655,Each vector h j corresponds to the concatenation of the hidden states produced by the backward and forward RNNs .,0
33656,"On the right side of the figure , we find the decoder , which produces one state s i for each time step ; on the center of the figure the attention mechanism is shown .",0
33657,The main components of the attention mechanism are : ( i ) the alignment model e ij,0
33658,"which is parameterized as a feedforward neural network and scores how well input in position j- th and output observed in the i - th time instant match ; T x and Ty are the length of the input and output sequences , respectively .",0
33659,( ii ) the attention probability distribution ?,0
33660,ij,0
33661,( e i is the vector whose j- th element is e ij ) ( iii ) the context vector Ci,0
33662,weighted sum of the encoder annotations h j .,0
33663,"According to , the context vector Ci is the key element for evaluating the conditional probability P ( y i |y 1 , . . . , y i?1 , x ) to output a target token y i , given the previously outputted tokens y 1 , . . . , y i?1 and the input x .",0
33664,They in fact express this probability as :,0
33665,"where g is a non-linear , potentially multi -layered , function .",0
33666,"So doing , the explicit information about y 1 , . . . , y i ?1 and x is replaced with the knowledge of the context Ci and the decoder state s i .",0
33667,"The model we present in this paper incorporates two additional mechanisms , detailed in the next sections : a character - wise copy mechanism and a peculiar training procedure based on GRUs switch .",0
33668,Learning to Copy,0
33669,"On top of the just recalled model , we build a character - based copy mechanism inspired by the Pointer - Generator Network , a word - based model that hybridizes the Bahdanau traditional model and a Pointer Network .",0
33670,"Basing on these ideas , in our model we identify two probability distributions that , differently from what done by and , act now on characters rather than on words : the alphabet distribution P alph and the attention distribution P att .",0
33671,"The former is the network 's generative probability of sampling a given character at time i , recalled in eq. ( 4 ) :",0
33672,where V and bare trainable parameters .,0
33673,"The latter is the distribution reminded in eq. ( 2 ) , created by the attention mechanism over the input tokens , i.e. in our case , over input characters :",0
33674,"In our method this distribution is used for directly copying characters from the input to the output , pointing their input positions , while in P att is used only internally to weigh the input annotations and create the context vector Ci .",0
33675,"The final probability of outputting a specific character c is obtained combining P alph and P att through the quantity p gen , defined later , which acts as a soft switch between generating cor copying it :",0
33676,where P i alph [ c ] is the component of P i alph corresponding to that character c .,0
33677,"The backpropagation training algorithm , therefore , brings p gen close to 1 when it is necessary to generate the output as in a standard encoder - decoder with attention ( P i ( c ) P i alph [ c ] ) ; conversely , p gen will be close to 0 ( i.e. P i ( c ) j| xi =c P j att ( c ) ) when a copying step is needed .",0
33678,"The model we propose therefore learns when to sample from P alph for selecting the character to be generated , and when to sample from P att for selecting the character that has to be copied directly from the input .",0
33679,"This copy mechanism is fundamental to output all the unknown words present in the input , i.e. words which never occur in the training set .",0
33680,"In fact , generating characters in the right order to reproduce unknown words is a sub -task not "" solvable "" by a naive sequence - to - sequence model , which learns to output only known words .",0
33681,The generation probability p gen ?,0
33682,"[ 0 , 1 ] is computed as follows :",0
33683,where ?,0
33684,"is the sigmoid function , ?",0
33685,"i? 1 is the last output character 's embedding , s i is the current decoder 's cell state and Ci is the current context vector .",0
33686,"W y , W s , W c and W pare the parameters whose training allows p gen to have the convenient value .",0
33687,"We highlight that in our formulation p i ?1 gen , i.e. the value of p gen at time i ?",0
33688,"1 , contributes to the determination of pi gen .",0
33689,"In fact , in a character - based model it is desirable that this probability remains unchanged for a fair number of time steps , and knowing its last value helps this behavior .",0
33690,"This never happens in word - based models ( such as ) , in which copying for a single time step is usually enough .",0
33691,Switching GRUs,0
33692,"Aiming at improving performance , we enrich our model ' training pipeline with an additional phase , which forces an appropriate language representation inside the recurrent components of the model .",0
33693,"In order to achieve this goal , the encoder and the decoder do not own a fixed GRU , differently from what happens in classical end - to - end approaches .",0
33694,"The recurrent module is passed each time as a parameter , depending on which one of the two training phases is actually performed .",0
33695,"In the first phase , similar to the usual one , the GRU assigned to the encoder deals with a tabular representation x as input , the GRU assigned to the decoder has to cope with natural language , and the model generates an output utteranc ? y = F ( x ) .",0
33696,"Conversely , in the second phase GRUs are switched and we use as input the just obtained natural language utterance ?",0
33697,to generate a new tabl ?,0
33698,.,0
33699,"Therefore , the same model can build both F and G , thanks to the switch of GRUs .",0
33700,"In other words , the learning iteration is performed as follows .",0
33701,"- A dataset example ( x , y ) is given .",0
33702,x is a tabular meaning representation and y is the corresponding reference sentence .,0
33703,"The higher training time , direct consequence of the just described technique , is a convenient investment , as it brings an appreciable improvement of the model 's performance ( see section 3.3 ) .",0
33704,Experiments,0
33705,Datasets,0
33706,"We tested our model on four datasets , whose main descriptive statistics are given in table 1 : among them , the most known and frequently used in literature is the E2E dataset , used as benchmark for the E2E Challenge organized by the Heriot - Watt University in 2017 .",0
33707,"It is a crowdsourced collection of roughly 50,000 instances , in which every input is a list of slot - value pairs and every expected output is the corresponding natural language sentence .",0
33708,"The dataset has been partitioned by the challenge organizers in predefined training , validation and test sets , conceived for training data - driven , end - to - end Natural Language Generation models in the restaurant domain .",0
33709,"However , during our experiments , we noticed that the values contained in the E2E dataset area little naive in terms of variability .",0
33710,"In other words , a slot like name , that could virtually contain a very broad range of different values , is filled alternating between 19 fixed possibilities .",0
33711,"Moreover , values are partitioned among training , validation and test set , in such away that test set always contains values thatare also present in the training set .",0
33712,"Consequently , we created a modified version of the E2E dataset , called E2E + , as follows : we selected the slots that represent more copy - susceptible attributes , i.e. name , near and food , and conveniently replaced their values , in both meaning representations and reference sentences .",0
33713,"New values for food are picked from Wikipedia 's list of adjectival forms of countries and nations 1 , while both name and near are filled with New York restaurants ' names contained in the Entree dataset presented in .",0
33714,"It is worth noting that none of the values of name are found in near ; likewise , values that belong to the training set are not found in the validation set nor in the test one , and vice versa .",0
33715,"This value partitioning shall ensure the absence of generation bias in the copy mechanism , stimulating the models to copy attribute values , regardless of their presence in the training set .",0
33716,The MR and 1st reference fields in table 4 are instances of this new dataset .,0
33717,"Finally , we decided to test our model also on two datasets , Hotel and Restaurant , frequently used in literature ( for instance in and ) .",0
33718,"They are built on a 12 attributes ontology : some attributes are common to both domains , while others are domain specific .",0
33719,"Every MR is a list of key - value pairs enclosed in a dialogue act type , such as inform , used to present information about restaurants , confirm , to check that a slot value has been recognized correctly , and reject , to advise that the user 's constraints can not be met .",0
33720,"For the sake of compatibility , we filtered out from Hotel and Restaurant all inputs whose dialogue act type was not inform , and removed the dialogue act type .",0
33721,"Besides , we changed the format of the key - value pairs to E2E - like ones .",0
33722,"Tables are encoded simply converting all characters to ASCII and feeding every corresponding index to the encoder , sequentially .",0
33723,"The resulting model 's vocabulary is independent of the input , allowing the application of the transfer learning procedure .",0
33724,Implementation Details,0
33725,"We developed our system using the PyTorch framework 2 , release 0.4.1 3 .",1
33726,"The training has been carried out as described in subsection 2.3 : this training procedure needs the two GRUs to have the same dimensions , in terms of input size , hidden size , number of layers and presence of a bias term .",0
33727,"Moreover , they both have to be bidirectional , even if the decoder ignores the backward part of its current GRU .",0
33728,"We minimize the negative log - likelihood loss using teacher forcing and Adam , the latter being an optimizer that computes individual adaptive learning rates .",1
33729,"As a consequence of the length of the input sequences , a characterbased model is often subject to the exploding gradient problem , that we solved via the well - known technique of gradient norm clipping .",0
33730,We also propose a new formulation of P ( c ) that helps the model to learn when it is necessary to start a copying phase :,0
33731,"Sometimes , our model has difficulty in focusing on the first letter it has to copy .",0
33732,"This maybe caused by the variety of characters it could be attending on ; instead , it seems easier to learn to focus on the most largely seen characters , as for instance ' ' and ' [ ' .",0
33733,"As these special characters are very often the prefix of the words we need to copy , when this focus is achieved , we would like the attention distribution to be translated one step to the right , over the first letter that must be copied .",0
33734,"Therefore , the final probability of outputting a specific character c , introduced in eq. , is modified to P i , j?1 att , i.e. the attention distribution shifted one step to the right and normalized .",0
33735,"Notice that P i,j?1 att is the only shifted probability , while P i alph remains unchanged .",0
33736,"Therefore , if the network is generating the next token ( i.e. pi gen 1 ) , the shift trick does not involve P i ( c ) and the network samples the next character from P i alph , as usual .",0
33737,"This means that the shift operation is not degrading the generation ability of the model , whilst improving the copying one .",0
33738,Results and Discussion,1
33739,"In order to show that our model represents an effective and relevant improvement , we carry out two different experimentations : an ablation study and a comparison with two well - known models .",0
33740,"The first model is the encoder - decoder architecture with attention mechanism by ( hereafter "" EDA "" ) , used character - by - character .",0
33741,"The second one is TGen , a word - based model , still derived from , but integrating a beam search mechanism and a reranker over the top k outputs , in order to dis advantage utterances that do not verbalize all the information contained in the MR .",0
33742,We chose it because it has been adopted as baseline in the E2E NLG Challenge 4 .,0
33743,"We used the official code provided in the E2E NLG Challenge website for TGen , and we developed our models and EDA in PyTorch , training them on NVIDIA GPUs .",0
33744,"Hyperparameter tuning is done through 10 - fold cross - validation , using the bleu metric for evaluating each model .",0
33745,The training stopping criterion was based on the absence of models ' performance improvements ( see ) .,0
33746,We evaluated the models ' performance on test sets ' output utterances using the Evaluation metrics script 5 provided by the E2E NLG Challenge organizers .,0
33747,"It rates quality according to five different metrics : bleu , nist , meteor , rouge_l and cider .",0
33748,"Our first experimentation , the ablation study , refers to the E2E dataset because of its wide diffusion , and is shown in table 2 ; "" EDA _CS "" identifies our model , and ' C ' and ' S ' stand for "" Copy "" and "" Switch "" , the two major improvements presented in this work .",0
33749,It is evident that the partially - improved networks are able to provide independent benefits to the performance .,0
33750,"Those components cooperate positively , as EDA_CS further enhances those results .",0
33751,"Furthermore , the obtained bleu metric value on the E2E test set would allow our model to be ranked fourth in the E2E NLG Challenge , while its baseline TGen was ranked tenth .",0
33752,"Our second experimentation , the comparison study , is shown in table",0
33753,3 .,0
33754,"The character - based design of EDA_CS led us to explore in this context also a possible behavior as a transfer learning capable model : in order to test this hypothesis , we used the weights learned during training on the E2E + dataset as the starting point for a fine - tuning phase on all the other datasets .",0
33755,"We chose E2E + because it reduces the generation bias , as discussed in subsection 3.1 .",0
33756,We named this approach EDA_CS TL .,0
33757,"A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets , and three out of five higher metrics values on the E2E dataset .",1
33758,"However , in the case of E2E + , TGen achieves three out of five higher metrics values .",1
33759,"These results suggest that EDA_CS and TGen are comparable , at least from the point of view of automatic metrics ' evaluation .",0
33760,"A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets ( for the majority of metrics ) ; on E2E , EDA_CS TL outperforms EDA_CS only in one case ( i.e. meteor metric ) .",1
33761,"Moreover , EDA_CS TL shows a bleu increment of at least 14 % with respect to TGen 's score when compared to both Hotel and Restaurant datasets .",1
33762,"Finally , the baseline model , EDA , is largely outperformed by all other examined methods .",1
33763,"Therefore , we can claim that our model exploits its transfer learning capabilities effectively , showing very good performances in a context like data - to - text generation in which the portability of features learned from different datasets , in the extent of our knowledge , has not yet been explored .",0
33764,"We highlight that EDA_CS 's model 's good results are achieved even if it consists in a fully end - to - end model which does not benefit from the delexicalizationrelexicalization procedure , differently from TGen .",0
33765,"Most importantly , the latter represents a word - based system : as such , it is bound to a specific , limited vocabulary , in contrast to the general - purpose character one used in our work .",0
33766,"reports the output of the analyzed models for a couple of MR , taken from the E2E + test set .",0
33767,"The EDA 's inability to copy is clear , as it tends , in its output , to substitute those values of name , food and near that do not appear in the training set with known ones , guided by the first few characters of the input slot 's content .",0
33768,"Besides , it shows serious coverage issues , frequently ' forgetting ' to report information , and / or repeating more times the same ones .",0
33769,"These troubles are not present in EDA_CS output utterances : the model nearly always renders all of the input slots , still without duplicating any of them .",0
33770,"This goal is achieved even in absence of explicit coverage techniques thanks to our peculiar training procedure , detailed in section 2.3 , that for each input sample minimizes also the loss on the reconstructed tabular input .",0
33771,"It is worth noting that the performance of TGen and EDA _ CS are over all comparable , especially when they deal with names or other expressions not present in training .",0
33772,The joint analysis of the matrix of the attention distribution P ij att and the vector p gen allows a deeper understanding of how our model works .,0
33773,"In figure 2 every row shows the attention probability distribution "" seen "" when an output character is produced at the i - th time instant ( i.e. the vector P ij att , 1 ? j ? T x ) , while every column shows values of the attention distribution corresponding to a specific input position j ( i.e. the vector P ij att , 1 ? i ?",0
33774,Ty ) .,0
33775,"We can therefore follow the white spots , corresponding to higher values of attention , to understand the flow of the model 's attention during the generation of the output utterance .",0
33776,"Moreover , p gen values , which lie in the numeric interval [ 0 , 1 ] , help us in the interpretation of the attention : they are represented as a grayscale vector from zero ( black ) to one ( white ) under the matrices .",0
33777,Values close to 0 mean copying and those near 1 mean generating .,0
33778,We can note that our model 's behavior varies significantly depending on the dataset it has been trained on ..,0
33779,"Copying common words leads the model to "" uncertain "" values of pgen dataset : as observed before , attribute values in this dataset have a very low variability ( and are already present in the training set ) , so that they can be individually represented and easily generated by the decoder .",0
33780,"In this case , a typical pattern is the copy of only the first , discriminating character , clearly noticeable in the graphical representation of the p gen vector , and the subsequent generation of the others .",0
33781,"Notice that the attention tends to remain improperly focused on the same character for more than one output time step , as in the first letter of "" high "" .",0
33782,"On the other hand , the copy mechanism shows it s full potential when the system must learn to copy attribute values , as in the E2E + dataset .",0
33783,"In figure 2 b the diagonal attention pattern is pervasive : ( i ) it occurs when the model actually copies , as in "" Harley Davidson "" and "" Coco Pazzo "" , and ( ii ) as a soft track for the generation , as in "" customer rating "" , where the copy - first - generate - rest behavior emerges again .",0
33784,"A surprising effect is shown in figure 3 , when the model is expected to copy words that , instead , are usually generated : an initial difficulty in copying the word "" The "" , that is usually a substring of a slot value , is ingeniously overcome as follows .",0
33785,"The first character is purely generated , as shown by the white color in the underlying vector , and the sequence of the following characters , "" he_ "" , is half - generated and half - copied .",0
33786,"Then , the value of p gen gets suddenly but correctly close to 0 ( black ) until the closing square bracket is met .",0
33787,The network 's output is not affected negatively by this confusion and the attention matrix remains quite well - formed .,0
33788,"As a final remark , the metrics used , while being useful , well - known and broadly accepted , do not reflect the ability to directly copy input facts to produce outputs , so settling the rare word problem .",0
33789,Conclusion,0
33790,We showed in this paper an effective character - based end - to - end model that faces data - to - text generation tasks .,0
33791,"It takes advantage of a copy mechanism , that deals successfully with the rare word problem , and of a specific training procedure , characterized by the switching GRUs mechanism .",0
33792,These innovative contributions to state - of - art further improve the quality of the generated texts .,0
33793,"We highlight that our formulation of the copy mechanism is an original character - based adaptation of , because of the use of p i ?1 gen to determine the value of pi gen , at the following time step .",0
33794,This helps the model in choosing whether to maintain the same value for a fair number of time steps or not .,0
33795,"Besides , the use of characters allows the creation of more general models , which do not depend on a specific vocabulary ; it also enables a very effective straightforward transfer learning procedure , which in addition eases training on small datasets .",0
33796,"Moreover , outputs are obtained in a completely end - to - end fashion , in contrast to what happens for the chosen baseline word - based model , whose performances are comparable or even worse .",0
33797,"One future improvement of our model could be the "" reinforcement "" of the learning iteration described in section 2.3 : for each dataset example ( x , y ) , we could consider , as an ulterior example , the reverse instance ( y , x ) .",0
33798,"The network obtained this way should be completely reversible , and the interchangeability of input and output languages could open up new opportunities in neural machine translation , such as two - way neural translators .",0
33799,"New metrics that give greater importance to rare words might be needed in the future , with the purpose of better assess performances of able - to - copy NLG models on datasets such as the E2E + one .",0
